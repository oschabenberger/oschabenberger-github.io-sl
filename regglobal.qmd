::: content-hidden
$$
{{< include latexmacros.tex >}}
$$
:::

# The Classical Linear Model {#sec-reg-linear}

The classical (standard) linear model is $$
\bY = \bX\bbeta + \bepsilon, \quad \quad \bepsilon \sim (\bzero,\sigma^2\bI)
$$ $\bY$ is an $(n \times 1)$ vector of target values, $\bX$ is an $(n \times (p+1))$ 
matrix of $p$ input variables plus an intercept, and $\bepsilon$ is an $(n \times 1)$ 
random vector whose elements have mean zero and the same variance $\sigma^2$. 
If $\bX$ is of full rank, the OLS estimator 
$$
\widehat{\bbeta} = \left(\bXpX^{-1}\right) \bX^\prime\bY
$$ 
is the unbiased estimator with smallest variance. Predicted values and fitted 
residuals are given by

\begin{align*}
    \widehat{\by} &= \bX\widehat{\bbeta} = \bH\by \\
    \widehat{\bepsilon} = \by - \widehat{\by} &= (\bI - \bH)\by
\end{align*}

where $\bH$ is the "Hat" matrix $\bH = \bX(\bXpX)^{-1}\bX^\prime$.

The OLS estimator has variance 
$$
\Var[\widehat{\bbeta}] = \sigma^2 (\bXpX)^{-1}
$$ 
We saw in @sec-idempotent that $\bH$ is an orthogonal **projection** matrix--it 
is symmetric and idempotent. Because $\bH$ is a projection matrix, $\bI - \bH$ 
is also a projection matrix. To emphasize this fact we can write $\by$ as the 
sum of two components 
$$
\by = \bH\by + (\bI - \bH)\by
$$ 
This identity simply states that the target variable is projected onto two spaces: 
the space generated by the columns of $\bX$, represented by the projection matrix 
$\bH$, and its complement (residual) space. Furthermore, these projections are
orthogonal: 
$$
\bH (\bI - \bH) = \bH - \bH\bH = \bH - \bH = \bzero
$$

Projection properties are useful in establishing properties of fitted values and 
residuals.

\begin{align*}
    \bX^\prime \widehat{\bepsilon} &= \bzero \\
    \Var[\widehat{\by}] &= \Var[\bH\by] = \sigma^2\bH \\
    \Var[\widehat{y}_i] &= \sigma^2 h_{ii} \\
    \Var[\widehat{\bepsilon}] &= \Var[(\bI-\bH)\by] = \sigma^2(\bI - \bH) \\
    \Var[\widehat{\epsilon}_i] &= \sigma^2(1-h_{ii})
\end{align*}

Several interesting facts can be gleaned from these expressions.

1.  The first result, that the fitted residuals are orthogonal to the inputs, 
follows from the orthogonality of least squares, but can be shown easily using 
projection properties 
$$
\bX^\prime\widehat{\bepsilon} = \bX^\prime(\bI-\bH)\by = (\bX^\prime - \bX^\prime\bH)\bY = (\bX^\prime-\bX^\prime)\by = \bzero
$$
The orthogonality of inputs and residuals implies that for any column in $\bX$ 
$$
\sum_{i=1}^n \, x_{ij}\widehat{\epsilon}_i = 0, \quad j=1,\cdots, p
$$
This is also true for the intercept column, so that $\sum_{i=1}^n \widehat{\epsilon}_i = 0$; 
the fitted residuals sum to zero.

2.  The variance of the $i$^th^ fitted value depends on the irreducible variability 
$\sigma^2$ and on the $i$^th^ diagonal element of the Hat matrix, $h_{ii}$. The 
Hat matrix in turn depends only on the input variables. In other words, the 
variability of the fitted values does not depend on any of the target values in 
the data set.

3.  The variance of a fitted residual is $\sigma^2(1-h_{ii})$. Since $\sigma^2$ 
is the variance of the model errors (the irreducible variance), the variability 
of the residuals is smaller if $h_{ii} > 0$. The leverages $h_{ii}$ cannot be 
larger than 1, otherwise $\sigma^2(1-h_{ii})$ is not a valid variance. In fact, 
the leverages are bounded $1/n \le h_{ii} \le 1$.

## Simple and Multiple Linear Regression

The SLR and MLR models are examples of the classical linear model. In the SLR 
case there is a single input variable 
$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$ 
in the MLR case there are multiple input variables. These can be distinct variables, 
or transformations and/or combinations of variables. For example, 
$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \epsilon_i
$$ 
has three inputs formed from the main effects of $x_1$ and $x_2$ and their 
interaction $x_1x_2$. More on the use and interpretation of interactions below.

### Computing Estimates

::: example
::: example-header
Example: Auto Data from ISLR
:::

::: example-container
The `Auto` data is a data set used in @James2013_ISLR2 (ISLR2). It comprises 
information on 392 automobiles, such as mileage (mpg), horsepower, number of 
cylinders, engine displacement (cu. inches), weight (lbs), etc. The original
data set has 397 observations of which five have a missing value for the 
`horsepower` variable. The data set that comes with the ISLR2 library does
not contain these five observations.

Suppose we want to develop a model that predicts `mpg` from input variables
`cylinders`, `displacement`, `weight` and `horsepower`.

:::{.panel-tabset group="language"}

## R

Most `R` functions for statistical modeling support a formula expression to specify 
models directly based on information in data frames. You do not have to set up 
separate objects for $\bX$ and $\by$. Also, `R` provides standard errors, 
$t$-statistics, $p$-values, and other estimates by default, and has default 
methods for handling missing values.

```{r}
library(ISLR2)
data(Auto)
head(Auto)

linreg <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)
summary(linreg)

```

The `Estimate` column of the `lm` summary reports the $\widehat{\beta}_k$ estimates, 
the `Std. Error` column reports their standard errors. 

The residual standard error of `{r} summary(linreg)$sigma` is $\widehat{\sigma}$, 
the square root of the estimator $\widehat{\sigma}^2$ derived above.


## Python

The following statements load the data set from DuckDB into a Pandas dataframe, 
removes the observations with missing values and display the first observations.

```{python}
import pandas as pd
import duckdb 

con = duckdb.connect(database="ads.ddb", read_only=True)
Auto_data = con.sql("SELECT * FROM auto").df().dropna()
con.close()

Auto_data.head()
```

The following statements fit the multiple linear regression model using the 
`statsmodels` library.

Suppose we want to develop a model that predicts `mpg` from other variables. A multiple linear regression model with inputs `cylinders`, `displacement`, `weight` and `horsepower` is fit in Python with `scikit-learn` (`sklearn`) as follows.

```{python}
import statsmodels.api as sm
import numpy as np

X = Auto_data[['cylinders', 'displacement', 'weight', 'horsepower']]
X = sm.add_constant(X)
y = Auto_data['mpg']

linmod = sm.OLS(y, X).fit()

print(linmod.summary())
```

An intercept is not automatically added to the model in the fitting routine,
so it must be added to the `X` matrix prior to calling `sm.OLS`. 

The coefficient estimates, their standard errors, $t$-values anr $p$-values are
shown in the body of the `OLS Regression Results`.
:::

The regression model trained by ordinary least squares is 
$$
\text{mpg}_i = \beta_0 + \beta_1\text{cylinders}_i+\beta_2\text{displacement}_i+\beta_3\text{weight}_i+\beta_4\text{horsepower}_i + \epsilon_{i}
$$
and it is assumed that $\epsilon_i \sim iid (0,\sigma^2)$. The OLS estimates are

-   $\widehat{\beta}_0 = 45.7568$
-   $\widehat{\beta}_1 = -0.3933$
-   $\widehat{\beta}_2 = 0.0001$
-   $\widehat{\beta}_3 = -0.0053$
-   $\widehat{\beta}_4 = -0.0428$

The predicted miles per gallon of an automobile for which the data frame is representative, is 

\begin{align*}
\widehat{\text{mpg}} = 45.7568 &-0.3933\times\text{cylinders} + 0.0001\times\text{displacement} \\
 &-0.0053\times \text{weight} -0.0428\times\text{horsepower}
\end{align*}

The $p$-values in the regression summary associated with individual input variables
test the hypotheses that the coefficient for the input is zero, given all other
input variables in the model. These are known as **partial** tests because the
other variables remain in the model. For example, the $p$-value for the test
of the hypothesis that `cylinders` makes a significant contribution given that
`displacement`, `weight`, and `horsepower` are in the model is 0.338. 

The $F$-statistic of 234.2 and its very small associated $p$-value tests that
the coefficients of the four variables are simultaneously zero, $H: \beta_1 = 
\beta_2 = \beta_3 = \beta_4 = 0$. This hypothesis is soundly rejected. 

The four input variables combined explain 70.8\% of the variability of mileage 
(`mpg`) through a multiple linear regression. 

:::
:::



### Coefficient Interpretation

How do we interpret the regression coefficients of the fitted model

\begin{align*}
\widehat{\text{mpg}} = 45.7587 &-0.3933\times\text{cylinders} + 0.0001\times\text{displacement} \\
 &-0.0053\times \text{weight} -0.0428\times\text{horsepower}
\end{align*}

Since the model is linear, it is tempting to state that, for example, a change 
in 1 unit of `displacement` causes a change of 0.0001388 in miles per gallon. 
This interpretation is not correct, because

1.  We cannot conclude causality between inputs and the target variable. The 
data are purely observational so we can at best state that changes in the input
variables are **associated** with different predicted values for miles per gallon.

2.  We cannot interpret one input variable in the absence of the others. The signs 
of the regression coefficients are somewhat counter-intuitive. Why would mileage 
go down for cars with more cylinders but go up with greater displacement. Does 
adding cylinders not imply a larger engine displacement? The point is that the 
inputs are related to each other, they do not vary freely from each other. When
we interpret the magnitude of a regression coefficient in terms of the change 
in the target variable that corresponds to a unit change in the input variable, 
we are implicitly holding all other predictors fixed (ceteris paribus).

The correct interpretation of the displacement coefficient is thus 

>when displacement increases by one cubic inch and all other attributes remain 
constant, the expected mileage increases by 0.0001388 miles per gallon.

::: callout-note
The *all-other-variables-held-fixed* interpretation is also important when 
interpreting hypothesis test results. The $p$-values of variables `cylinders` 
and `displacement` are 0.33 and 0.98, respectively, suggesting that these variables 
do not make significant contributions toward explaining miles per gallon. These 
$p$-values are interpreted in the presence of the other variables in the model.

The correct interpretation is that the number of cylinders is not a significant 
predictor of miles per gallon in a model that accounts for engine displacement, 
vehicle weight, and horsepower.
:::

### $R^2$, the Coefficient of Determination

The variability in the target $\by$, not accounting for any information provided 
by the input variables can be estimated as 
$$
s^2 = \frac{1}{n-1}\sum_{i=1}^n (y_i - \overline{y})^2
$$ 
If the $y_i$ had the same mean, this would be an unbiased estimator of $\Var[Y]$. 
However, the regression model states very clearly that the mean of $Y$ is a 
function of the $x$-inputs. This estimator is then a biased estimator of 
$\Var[Y] = \sigma^2$. The numerator of $s^2$ is called the **total sum of squares** 
(SST). If SST captures variability of $Y$ about a constant mean, how much of 
this is attributable to the input variables? To answer this we can look at the
variability **not** attributable to the $x$s, the error sum of squares 
$$
\text{SSE} = \sum_{i=1}^n \widehat{\epsilon}_i = \sum_{i=1}^n (y_i - \widehat{y}_i)^2
$$ 
The ratio 
$$
R^2 = \frac{\text{SST}-\text{SSE}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}
$$ 
is known as the **coefficient of determination** or **R-squared**. The name 
R-squared comes from a simple relationship between $R^2$ and the Pearson correlation 
coefficient in the SLR case. If $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, then 
$R^2$ is the square of the correlation coefficient between $X$ and $Y$:

::: example
::: example-header
Example: Auto Data (Cont'd)
:::

::: example-container

:::{.panel-tabset group="language"}

## R

```{r}
slr <- lm(mpg ~ horsepower, data=Auto)
summary(slr)$r.squared

cor(Auto$mpg, Auto$horsepower)^2
```

The squared correlation coefficient between `mpg` and `horsepower` is the same as 
the $R^2$ statistic shown in the `lm` summary.

You can compute the correlation coefficient between the two variables from simple 
linear regression output, but you need to take into account the sign of the 
regression coefficient. The correlation coefficient has the same sign as $\beta_1$.

```{r}
cor(Auto$mpg, Auto$horsepower)

as.numeric(sign(slr$coefficients[2])) * sqrt(summary(slr)$r.squared)
```

## Python

```{python}
import numpy as np

X = Auto_data[['horsepower']]
X = sm.add_constant(X)
y = Auto_data['mpg']

slr = sm.OLS(y, X).fit()

r_squared = slr.rsquared
print(round(r_squared,5))

corr_squared = np.corrcoef(Auto_data['mpg'], Auto_data['horsepower'])[0, 1]**2
print(round(corr_squared,5))
```

The squared correlation coefficient between `mpg` and `horsepower` is the same as 
the $R^2$ statistic shown in the `fit.OLS` summary.

```{python, message=FALSE, warning=FALSE}
corr = np.corrcoef(Auto_data['mpg'], Auto_data['horsepower'])[0, 1]
print(round(corr,5))

sign_coef = np.sign(slr.params[1]) * np.sqrt(slr.rsquared)
print(round(sign_coef,5))
```


:::
:::
:::

$R^2$ ranges between 0 and 1; it achieves the lower bound $R^2=0$ if SSE = SST, 
the input variables do not explain any variability in $Y$. $R^2 = 1$ results when 
SSE = 0, the model fits the data "perfectly", it interpolates the $y_i$.

The straightforward interpretation of $R^2$ as the proportion of variability 
explained by the input variables unfortunately can lead one to chase models that 
have a high $R^2$. This is a terrible practice for a number of reasons

-   The value of $R^2$ does not tell us whether the model assumptions are met. 
You can explain a substantial amount of variability in the data with a seriously 
deficient model. For example, the four regressor model fit to the Auto data earlier 
has $R^2 = 0.708$, explaining 70.8% of the variability in miles per gallon. A 
look at the residuals from that model shows that there is substantial trends 
in the residuals (@fig-auto-residuals). Larger fitted values have larger 
variability and there is a definite trend in the residuals; the model is not (yet) correct.

```{r, echo=FALSE, out.width="75%", fig.align='center'}
#| label: fig-auto-residuals
#| lightbox:
 
yseq <- seq(5,35,0.5)
df <- data.frame(r=rstudent(linreg),x=linreg$fitted.values)
lo <- loess(r ~ x, data=df)
plot(linreg$fitted.values,
     rstudent(linreg),
     las=1,
     bty="l",
     xlab="Fitted Values",
     ylab="RStudent Residuals")
lines(x=yseq, y=predict(lo,newdata=yseq),col="red")
```

Here is an example with simulated data where the mean function is not a straight 
line and the error variance depends on $x$. A simple linear regression model is 
clearly not appropriate, but it explains more than 75\% of the variability in $y$.

:::{.panel-tabset group="language"}

## R

```{r, fig.align="center", out.width="75%"}
x <- seq(0.15, 1, l = 100)
set.seed(123456)
eps <- rnorm(n = 100, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25 * sin(4 * pi * x)) + eps
slr <- lm(y ~ x)

plot(x,y,type="p",col="red",las=1,bty="l")
abline(slr$coefficients)

summary(slr)$r.squared
```

## Python

```{python}
import matplotlib.pyplot as plt
from scipy import stats

x = np.linspace(0.15, 1, 100)
np.random.seed(123456)
eps = np.random.normal(scale=0.25 * x**2, size=100)

y = 1 - 2 * x * (1 + 0.25 * np.sin(4 * np.pi * x)) + eps

X = sm.add_constant(x)
slr = sm.OLS(y, X).fit()

plt.figure(figsize=(10, 6))
plt.scatter(x, y, color='red')

x_range = np.array([min(x), max(x)])
X_range = sm.add_constant(x_range)
y_pred = slr.predict(X_range)
plt.plot(x_range, y_pred, color='blue')

plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, alpha=0.3)
plt.show()

print(round(slr.rsquared,5))
```

:::

-  $R^2$ is a function of SSE, the prediction error on the training data set. 
This can be made arbitrarily small by adding input variables. If $R^2_{\text{cur}}$ 
is the coefficient of determination in a linear regression model, and you add a 
new predictor $x_{p+1}$, then 
$$
R^2_{\text{new}} \ge R^2_{\text{cur}}
$$ 
Predictors that make no relevant contribution will increase $R^2$ because their 
addition still reduces SSE.
    
- A model that interpolates the data (fits "perfectly") is not a good model. It 
is certainly not perfect if the goal is to build a model that generalizes well 
to unseen observations. Different metrics are needed to develop models that 
generalize and do not overfit the data.

Chasing $R^2$ values invariably leads to **overfitting** and models that memorize 
too much of the training observations to perform well on new data.

### Measuring Prediction Error

Since we can make SSE on the training data arbitrarily small by adding more 
input variables, the MSPE on the training data is not a good metric if we want 
a model that performs well in predicting new observations. Instead of MSE~Tr~ 
the test error MSE~Te~ should be used. As discussed in @sec-train-test-validate, 
the test error can be estimated by holding out some observations from training 
in a test data set or by cross-validation (@sec-cross-validation). 

In the classical linear model leave-one-out cross-validation is particularly 
appealing. In addition to not depending on any random selection of data points as 
test data sets or $k$-fold cross-validation do, LOOCV error can be calculated in 
the linear model without re-fitting the model $n$ times. All the necessary pieces 
to compute the LOOCV error can be assembled on the same pass through the data 
that calculates the OLS estimates. The key is the Sherman, Morrison, Woodbury 
formula.

#### Sherman, Morrison, Woodbury formula

This remarkable formula is at the heart of many regression-type diagnostics and 
cross-validation techniques. A version of this formula was first given by Gauss in 1821. 
Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.

Suppose we are in a full-rank linear modeling context with design matrix 
$\bX_{(n \times p + 1)}$, so that the inverse $\left( \bXpX \right)^{-1}$ exists. 
In diagnosing the quality of a model, we are interested in measuring the prediction 
error for the $i$^th^ observation as if the data point had not contributed to the 
analysis. This is an example of a leave-one-out estimate: remove an observation 
from the data, redo the analysis, and measure how well the quantity of interest 
can be computed for the withheld observation.

If you do this in turn for all $n$ observations, you must fit the model $n + 1$ 
times, an overall fit to the training data with $n$ observations, and $n$ additional 
fits with training data sets of size $n - 1$, leaving out each observation in turn. 
The computationally expensive part of fitting the linear model is building the 
cross-product matrix $\bXpX$ and computing its inverse $\left( \bXpX \right)^{-1}$.

The Sherman-Morrison-Woodbury formula allows us to compute the inverse of the 
cross-product matrix based on $\left( \bXpX \right)^{- 1}$ as if the $i$^th^ 
observation had been removed.

Denote as $\bX_{-i}$ the design matrix with the $i$^th^ observation removed. Then

$$
\left( \bX_{-i}^\prime\bX_{-i} \right)^{- 1} = \left( \bXpX - \bx_{i}\bx_{i}^{\prime} \right)^{-1}\  = \left( \bXpX \right)^{- 1} + \frac{\left( \bXpX \right)^{-1}{\bx_{i}\bx_{i}^{\prime}\left( \bXpX \right)}^{- 1}}{1 - \bx_{i}^{\prime}\left( \bXpX \right)^{-1}\bx_{i}}
$$

The quantities on the right hand side are available in the standard regression 
calculations based on $n$ data points. Because of this remarkable result, 
leave-one-out statistics can be calculated easily---without retraining any 
models---based on the fit to the full training data alone. 

::: callout-note
Note that the quantity in the denominator of the right-hand side is the diagonal 
value of $\bI - \bH$, where $\bH$ is the Hat matrix. If $h_{ii}$ denotes the 
diagonal values of $\bH$, we can write the update formula as

$$
\left( \bXpX \right)^{- 1} + \frac{\left( \bXpX \right)^{- 1}{\bx_{i}\bx_{i}^{\prime}\left( \bXpX \right)}^{- 1}}{1 - h_{ii}}
$$

The **leverage** values $h_{ii}$ play an important role in the computation of 
residual, influence, and case-deletion diagnostics in linear models.
:::


#### PRESS Statistic

If we denote the predicted value of $y_i$, obtained in a regression without the 
$i$^th^ observation, as $\widehat{y}_{-i}$, then the leave-one-out residual
$$
y_i - \widehat{y}_{-i}
$$
is the test error for the $i$^th^ observation. Using the Sherman-Morrison-Woodbury 
result, it is a neat exercise to show that this is simply
$$
y_i - \widehat{y}_{-i} = \frac{y_i - \widehat{y}_i}{1-h_{ii}}
$$
The leave-one-out error for the $i$^th^ observation is obtained by dividing the
$i$^th^ residual by one minus the leverage value. When these deviations are 
squared and summed across the entire data set the PRESS statistic results
$$
\text{PRESS} = \sum_{i=1}^n (y_i - \widehat{y}_{-i})^2 = \sum_{i=1}^n \left(\frac{y_i - \widehat{y}_i}{1-h_{ii}}\right)^2
$$
The name is derived from **prediction sum of squares**. The average PRESS value 
estimates the mean square test error
$$
\text{MSE}_{Te} = \frac{1}{n}\text{PRESS}
$$

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
For the four regressor model the PRESS statistic and the MSE~Te~ can be calculated by extracting the leverage
values

:::{.panel-tabset group="language"}

## R

```{r}

mlr <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)

leverage <- hatvalues(mlr)
PRESS_res <- mlr$residuals / (1-leverage)
PRESS <- sum(PRESS_res^2)
MSE_Te  <- PRESS/length(leverage)
cat("PRESS statistic: ", PRESS, "\n")
cat("MSE Test based on LOOCV: ", MSE_Te,"\n")
```

You can validate this result the hard way by fitting $n$ separate regression models, 
leaving one observation out each time and predicting that observation to obtain 
the PRESS residual.

```{r}
PRESS <- 0
for (i in 1:nrow(Auto)) {
    m <- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto[-i,])
    yhat_minus_i <- predict(m,newdata=Auto[i,])
    PRESS <- PRESS + (Auto[i,"mpg"] - yhat_minus_i)^2
}
cat("MSE Test based on LOOCV: ", PRESS/nrow(Auto))
```

## Python

```{python}
import pandas as pd

X = Auto_data[['cylinders', 'displacement', 'weight', 'horsepower']]
X = sm.add_constant(X)
y = Auto_data['mpg']
mlr = sm.OLS(y, X).fit()

leverage = mlr.get_influence().hat_matrix_diag

# Calculate PRESS residuals and PRESS statistic
PRESS_res = mlr.resid / (1 - leverage)
PRESS = np.sum(PRESS_res**2)

# Calculate MSE Test based on LOOCV
MSE_Te = PRESS / len(leverage)

print(f"PRESS statistic: {PRESS:.4f}")
print(f"MSE Test based on LOOCV: {MSE_Te:.4f}")
```

You can validate this result the hard way by fitting $n$ separate regression models, 
leaving one observation out each time and predicting that observation to obtain 
the PRESS residual.

```{python}
# Alternative calculation of PRESS using explicit leave-one-out
PRESS = 0
for i in range(len(Auto_data)):
    # Create training data without the i-th observation
    X_train = Auto_data.drop(Auto_data.index[i])[['cylinders', 'displacement', 'weight', 'horsepower']]
    X_train = sm.add_constant(X_train)
    y_train = Auto_data.drop(Auto_data.index[i])['mpg']
    
    # Fit the model
    mlr = sm.OLS(y_train, X_train).fit()
    
    # Predict for the held-out observation
    X_test = Auto_data.iloc[i][['cylinders', 'displacement', 'weight', 'horsepower']].to_frame().T
    X_test.insert(0,'intercept',1)
    yhat_minus_i = float(mlr.predict(X_test).iloc[0])
    
    # Add squared error to PRESS
    PRESS += (Auto_data.iloc[i]['mpg'] - yhat_minus_i)**2

print(f"MSE Test based on LOOCV: {PRESS/len(Auto_data):.4f}")
```

:::
::::
:::

### Interactions

What is the difference between the following models

\begin{align*}
Y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\\
Y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon
\end{align*}

Both models depend on $x_1$ and $x_2$. In the first case the variables enter the 
model as **main effects**, that is, by themselves. Each input variable is allowed 
to make its contribution on the outcome given the presence of the other. The 
second model contains the additional **interaction** term $x_1 x_2$. To be more 
precise, this is a **two-way** interaction term because it involves two input 
variables. A three-way interaction term would be $x_1 x_2 x_3$.

Suppose that $\beta_3$ is not zero, how should we interpret the presence of an 
interaction term in the model? We can no longer state that $\beta_1$ measures 
the effect on the target variable when $x_1$ is changed by one unit. The effect 
of changing $x_1$ by one unit in the second model is now a function of $x_2$. To 
see this, consider the mean of $Y$ at two points, $x_1$ and $x_1+1$.

\begin{align*}
    \Exp[Y | x_1] &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 \\
    \Exp[Y | x_1 + 1] &= \beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \beta_3 (x_1+1)x_2
\end{align*}

The difference between the two is 
$$
\Exp[Y | x_1 + 1] - \Exp[Y | x_1] = \beta_1 + \beta_3 x_2
$$
The effect of $x_1$ is now a function of $x_2$. That is the very meaning of an 
interaction. The effect of one variable (or factor) depends on another variable 
(or factor) and vice versa. In the example the effect of $x_1$ is a linear 
regression in $x_2$.

::: callout-tip
When building models with interactions, it is customary to include lower-order 
effects in the model if higher-order effects are significant. For example, if 
$x_1 x_2$ is in the model one includes the main effects $x_1$ and $x_2$ regardless 
of their significance. Similarly, if a three-way interaction is significant one 
includes the two-way interactions and the main effects in the model. The argument 
for doing so is that in order for two things to interact they must be 
present--otherwise, what interacts?
:::

The presence/absence of an interaction between a categorical input variable and 
a numeric input variable can be seen by comparing the trends as in  @fig-interactions. 
With a two-level categorical factor, representing for example, a treatment and 
a placebo, the absence of interactions manifests itself in parallel lines. The 
effect of the treatment is the distance between the two lines and is the same 
for all values of $x_1$. Similarly, the effect of $x_1$, the slope of the line, 
is the same for both groups. In the presence of an interaction the slopes are 
not the same and the distance between the lines (the effect of $x_2$) depends 
on the value for $x_1$.


![Models with a categorical and a continuous input with and without interactions.](images/Interactions.png){#fig-interactions .lightbox fig-align="center" width="85%"}

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
We are now considering a series of model for the `Auto` data, based on the same 
four input variables used earlier.

:::{.panel-tabset group="language"}

## R

The first four models add inputs and also two-way interactions of all inputs in 
the model. The formula expression `y ~ (x1 + x2 + x3)^2` is a shorthand for 
including the main effects and two-way interactions of the three inputs. `R` 
displays the interaction terms as `x1:x2`, `x1:x3`, and `x2:x3` in the output.

Models 5 and 6 then add up to three-way and four-way interactions, respectively. 
For each model we calculate the number of non-zero coefficients (the rank of $\bX$), 
SSE, $R^2$ and the PRESS statistic.

```{r}
calcPress <- function(linModel) {
    leverage <- hatvalues(linModel)
    r <- linModel$residuals
    Press_res <- r/(1-leverage)
    Press <- sum(Press_res^2)
    return(list("ncoef"=linModel$rank,
                "R2"   =summary(linModel)$r.squared, 
                "SSE"  =sum(r^2),
                "Press"=Press))
}

l1 <- lm(mpg ~ cylinders, data=Auto)
l2 <- lm(mpg ~ (cylinders + displacement)^2, data=Auto )
l3 <- lm(mpg ~ (cylinders + displacement + horsepower)^2, data=Auto )
l4 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^2, data=Auto )
l5 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^3, data=Auto )
l6 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^4, data=Auto )

df <- rbind(as.data.frame(calcPress(l1)), 
            as.data.frame(calcPress(l2)),
            as.data.frame(calcPress(l3)),
            as.data.frame(calcPress(l4)),
            as.data.frame(calcPress(l5)),
            as.data.frame(calcPress(l6))
      )
knitr::kable(df,format="simple")

```

The model complexity increases from the first to the sixth model; the models 
have more parameters and more intricate interaction terms. The SSE  values decrease 
as terms are added to the model, and $R^2$ increases accordingly. The PRESS 
statistic is always larger than the SSE, which makes sense because it is based 
on squaring the Press residuals $(y_i - \widehat{y}_i)/(1-h_{ii})$ which are larger 
than the ordinary residuals $y_i - \widehat{y}_i$. 

From the fifth to the sixth model only one additional parameter is added to the 
model, the four-way interaction of all inputs. $R^2$ barely increases but the 
PRESS statistic increases compared to the model with only three-way interaction. 
Interestingly, none of the effects in the four-way model are significant given
the presence of other terms in the model.

```{r}
summary(l6)
```

## Python

The first four models add inputs and also two-way interactions of all inputs in 
the model. The code uses `patsy`'s `dmatrices` function to build model 
expressions similar to the  formula expression in `R`.

Models 5 and 6 then add up to three-way and four-way interactions, respectively. 
For each model we calculate the number of non-zero coefficients (the rank of $\bX$), 
SSE, $R^2$ and the PRESS statistic.

```{python}
import statsmodels.api as sm
import pandas as pd
import numpy as np
from patsy import dmatrices
from tabulate import tabulate

# Define function to calculate PRESS and other statistics
def calc_press(model):
    leverage = model.get_influence().hat_matrix_diag
    r = model.resid
    press_res = r / (1 - leverage)
    press = np.sum(press_res**2)
    
    return {
        "ncoef": model.df_model + 1,  # +1 for intercept
        "R2": model.rsquared,
        "SSE": np.sum(r**2),
        "Press": press
    }

formula1 = "mpg ~ cylinders"
y, X = dmatrices(formula1, data=Auto_data, return_type='dataframe')
l1 = sm.OLS(y, X).fit()

formula2 = "mpg ~ (cylinders + displacement)**2"
y, X = dmatrices(formula2, data=Auto_data, return_type='dataframe')
l2 = sm.OLS(y, X).fit()

formula3 = "mpg ~ (cylinders + displacement + horsepower)**2"
y, X = dmatrices(formula3, data=Auto_data, return_type='dataframe')
l3 = sm.OLS(y, X).fit()

formula4 = "mpg ~ (cylinders + displacement + horsepower + weight)**2"
y, X = dmatrices(formula4, data=Auto_data, return_type='dataframe')
l4 = sm.OLS(y, X).fit()

formula5 = "mpg ~ (cylinders + displacement + horsepower + weight)**3"
y, X = dmatrices(formula5, data=Auto_data, return_type='dataframe')
l5 = sm.OLS(y, X).fit()

formula6 = "mpg ~ (cylinders + displacement + horsepower + weight)**4"
y, X = dmatrices(formula6, data=Auto_data, return_type='dataframe')
l6 = sm.OLS(y, X).fit()

df = pd.DataFrame([
    calc_press(l1),
    calc_press(l2),
    calc_press(l3),
    calc_press(l4),
    calc_press(l5),
    calc_press(l6)
])

print(tabulate(df, headers='keys', tablefmt='simple'))
```

```{python}
print(l6.summary())
```

:::
::::
:::

## Hypothesis Testing {#sec-ss-reduction-test}

When testing hypothesis in statistical models it is useful to think of the hypothesis 
as imposing a **constraint** on the model. The test then boils down to comparing a 
constrained and an unconstrained model in such a way that we can make probability 
statements about the validity of the constraint. If it is highly unlikely that 
the constraint holds, we reject the hypothesis.

This principle applies to hypothesis testing in many model families, what differs 
is how the impact of the constraint on the model is measured. In least-squares 
estimation we look at how a sum of squares changes as the constraint is imposed. 
In models fit by maximum likelihood we measure how much the log likelihood changes 
when the constraint is imposed.

Suppose we have a model with four predictors, 
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
$$
and want to test the hypothesis that the absence of $x_3$ and $x_4$ does not 
make the model worse. The constraint we impose on the model is
$$
H: \beta_3 = \beta_4 = 0
$$
This is a hypothesis with two degrees of freedom, since two parameters of the 
model are constrained simultaneously.

::: callout-tip
You can usually figure out the degrees of freedom in a hypothesis by counting 
equal signs.
:::

The unconstrained and constrained models are also called the **full** and the 
**reduced** models, respectively. In this case the full model is
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
$$
and the reduced model is
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \epsilon
$$

:::{.definition}
::::{.definition-header}
Definition: Nested Models
::::
::::{.definition-container}
Two models are said to be **nested** when one model can be derived from the other 
by imposing constraints on the model parameters. The full and reduced models in 
the hypothesis testing context are nested models. On the contrary, if two models 
are not nested, the hypothesis testing framework described here does not apply.
::::
:::

If based on the data the hypothesis cannot be rejected, we conclude that the full 
model is not significantly improved over the reduced model. The strength of evidence 
in favor of the hypothesis depends on how much variability is accounted for in 
the model when the constraint is relaxed, relative to the overall variability in 
the system. To measure that we need to introduce the idea of partitioning sums 
of squares.

### Partitioning Variability through Sums of Squares

The difference between the total sum of squares, 
$\text{SST} = \sum_{i=1}^n(y_i - \overline{y})^2$, which does not depend on the 
inputs, and the error sum of squares $\text{SSE} = \sum_{i=1}^n (y_i - \widehat{y}_i)^2$, 
is the **model** sum of squares
$$
\text{SSM} = \sum_{i=1}^n\left(\widehat{y}_i-\overline{y}\right)^2 = \widehat{\bbeta}\bX^\prime\by - n\overline{y}^2
$$

The total and model sums of squares are also called the **corrected** total and 
model sums of squares because they adjust for an overall estimate of the mean of 
$Y$ if there are no inputs. In other words, they account for the intercept $\beta_0$ 
in the model; $\overline{y}$ is the estimate of $\beta_0$ in an intercept-only model.

Another way of looking at SSM is as a measure of the combined contribution of 
$\beta_1, \ldots, \beta_p$ beyond the intercept. The notation 
$$
\text{SSM} = SS(\beta_1, \cdots,\beta_p | \beta_0)
$$
makes this explicit. We can now think of other sum of squares, for example, 
$SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2)$ is the sum of squares contribution 
when $\beta_3$ and $\beta_4$ are added to a model that contains $\beta_0, \beta_1$, 
and $\beta_2$. Algebraically,
$$
SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) = SS(\beta_1, \beta_2, \beta_3, \beta_4 | \beta_0) - SS(\beta_1, \beta_2 | \beta_0)
$$
This will be one part of measuring the strength of hypothesis $H: \beta_3 = \beta_4 = 0$, 
the change in the model sum of squares between the full model with four inputs and 
the reduced model with two inputs However, there has to be more to it. If the data 
are very noisy, this change will have to be large to convince us of evidence against 
the hypothesis. If the data have small error variability, a smaller change in model 
sums of squares will suffice. This leads to considering the following test statistic:
$$
F_{obs} = \frac{SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) / 2}{\widehat{\sigma}^2}
$$
Notice that the sum of squares in the numerator is divided by the degrees of freedom 
of the hypothesis. To find an estimator for the variance in the denominator we 
rely on SSE in the larger of the two models, the unconstrained model, because it 
is more likely to be an unbiased estimator of $\sigma^2$.


::: {.definition} 
:::: {.definition-header}
Definition: Sum of Squares Reduction Test
::::
:::: {.definition-container}
Suppose that $\text{SSE}_f$ and $\text{SSE}_r$ are the error sum of squares in a 
full and reduced model where the reduced model is defined by a constraint $H$ with 
$q$ degrees of freedom imposed on the full model. The statistic
$$
F_{obs} = \frac{(\text{SSE}_r - \text{SSE}_f)/q}{\text{SSE}_f/\text{dfE}_f}
$$
follows an F distribution with $q$ numerator and $\text{dfE}_f$ denominator 
degrees of freedom if the model errors follow a Gaussian distribution, 
$\bepsilon \sim G(\bzero,\sigma^2\bI)$.
$\text{dfE}_f$ are the degrees of freedom associated with SSE in the full model, 
$n-r(\bX)_f$.
::::
:::

The sum of squares reduction test is very general and a very powerful tool to answer 
questions about the parameters in a linear model. However, in order to use any 
hypothesis testing framework that works with probability statements, a distributional 
assumption is required. We can always calculate $F_{obs}$ between two models. 
Computing $p$-values, that is, the probability $\Pr(F_{q,\text{dfE}_f} > F_{obs})$, 
is only valid if the data are normally distributed.

::: {.callout-caution}
If you reject the hypothesis $H$ based on a small $p$-value, you do not conclude 
that the full model is the *correct* model. You can say that there is significant 
evidence that the constraint can be relaxed. You might have compared a really 
bad model and a bad model. 
:::

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
To test the hypothesis that the coefficient for `weight` and `horsepower` are 
simultaneously zero in a model that accounts for `cylinders` and `displacement`, we can use the sum of squares reduction test.

:::{.panel-tabset group="language"}

## R

```{r}
lm_full <- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)
lm_red  <- lm(mpg ~ cylinders + displacement                      , data=Auto)

SSE_full <- sum(lm_full$residuals^2)
SSE_red  <- sum(lm_red$residuals^2)
q <- lm_full$rank - lm_red$rank
sigma2_hat <- SSE_full / (lm_full$df.residual)

Fobs <- ((SSE_red-SSE_full)/q)/sigma2_hat
pvalue <- 1-pf(Fobs,q,lm_full$df.residual)

cat("SSE_r: ", SSE_red, "SSE_f: ", SSE_full, "\n")
cat("sigma2_hat: ", sigma2_hat, "\n")
cat("Fobs: ", Fobs, "Pr(F > Fobs): ", pvalue)
```

Removing `weight` and `horsepower` from the four-predictor model increases the 
error sum of squares from `{r} round(SSE_full,4)` to `{r} round(SSE_red,4)`. The 
F statistic for this reduction test is $F_{obs} =$ `{r} round(Fobs,3)` and the 
$p$-value is very small (`{r} pvalue`). We reject the hypothesis, the two-predictor
model is not a sufficient model to explain mileage compared to the four-predictor 
model.

You can get to this result more quickly by using the `anova` function in `R`:

```{r}
anova(lm_red, lm_full)
```

## Python


```{python}
import scipy.stats as stats

X_full = Auto_data[['cylinders', 'displacement', 'weight', 'horsepower']]
X_full = sm.add_constant(X_full)
y = Auto_data['mpg']
lm_full = sm.OLS(y, X_full).fit()

# Fit the reduced model
X_red = Auto_data[['cylinders', 'displacement']]
X_red = sm.add_constant(X_red)
lm_red = sm.OLS(y, X_red).fit()

SSE_full = sum(lm_full.resid**2)
SSE_red = sum(lm_red.resid**2)

q = lm_full.df_model - lm_red.df_model
df_residual = lm_full.df_resid

sigma2_hat = SSE_full / df_residual

Fobs = ((SSE_red - SSE_full) / q) / sigma2_hat
pvalue = 1 - stats.f.cdf(Fobs, q, df_residual)

# Print results
print(f"SSE_r: {SSE_red:.4f} SSE_f: {SSE_full:.4f}")
print(f"sigma2_hat: {sigma2_hat:.4f}")
print(f"Fobs: {Fobs:.4f} Pr(F > Fobs): {pvalue:.6f}")
```

Removing `weight` and `horsepower` from the four-predictor model increases the 
error sum of squares from 8342.566 to 6963.433. The 
F statistic for this reduction test is $F_{obs} =$ 38.323 and the 
$p$-value is very small. We reject the hypothesis, the two-predictor
model is not a sufficient model to explain mileage compared to the four-predictor 
model.

You can get to this result more quickly by using the `anova_lm` function from 
`statsmodels`:

```{python}
from statsmodels.stats.anova import anova_lm
print("\nANOVA Table:")
print(anova_lm(lm_red, lm_full))
```


:::
::::
:::

### Sequential and Partial Sums of Squares

The sum of squares reduction test is helpful to understand the difference between 
two important special types of sum of squares, **sequential** and **partial** ones. 
How do we measure the contribution an individual predictor makes to the model
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
$$
Sequential sums of squares measures the contribution of an input relative to the 
inputs that precede it in the model. Partial sums of squares measure the contribution 
relative to all other inputs in the model. For example, if the input of interest
is $x_3$ in a four-regressor model, 
$$
SS(\beta_3 | \beta_0, \beta_1, \beta_2)
$$
is the sequential sum of squares for $x_3$ and 
$$
SS(\beta_3 | \beta_0, \beta_1, \beta_2, \beta_4)
$$
is the partial sum of squares. $\beta_4$ does not appear in the sequential sum of 
squares for $x_3$ because it appears after $\beta_3$ in the model formula. In other
words, the sequential sum of squares for $x_3$ does not adjust for $x_4$ at all, 
while the partial sum of squares does. Clearly, the two types of sum of squares 
are not identical, unless the inputs are **orthogonal** (independent).

Sequential sum of squares have an appealing additive property, the overall model 
sum of squares can be accumulated from a series of sequential terms,

\begin{align*}
    \text{SSM} &= SS(\beta_1 | \beta_0)\\
    &+ SS(\beta_2 | \beta_0, \beta_1) \\
    &+ SS(\beta_3 | \beta_0, \beta_1, \beta_2) \\
    &+ \cdots \\
    &+ SS(\beta_p | \beta_0, \beta_1, \beta_2, \ldots, \beta_{p-1})
\end{align*}

Partial sum of squares do not add up to anything meaningful, unless the inputs 
are orthogonal. 

This raises an interesting question. If we divide a coefficient estimate by its 
standard error and compute a $p$-value from a $t$-distribution, what kind of 
hypothesis is being tested?

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
Here is the summary of the four-predictor model for `mpg`.

:::{.panel-tabset group="language"}

## R

```{r}
summary(lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto))
```

How do we interpret the $p-$-value of 0.337513 for the `cylinders` variable or 
the $p$-value of 0.987709 for the `displacement` variable? A two-sided $t$-test 
is equivalent to an F test with 1 numerator degrees of freedom where $F_{obs}$ is 
the square of the $t_{obs}$ statistic. Let's first compute the sequential sum of 
squares tests and see if they match the $p$-values in the output.

```{r}
lm1 <- lm(mpg ~ cylinders, data=Auto)
lm2 <- lm(mpg ~ cylinders + displacement, data=Auto)
lm3 <- lm(mpg ~ cylinders + displacement + weight, data=Auto)
lm4 <- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)

anova(lm1, lm2, lm3, lm4)
```

The $p$-values are different from those shown in the `lm` output, so 
`Pr(>|t|) = 0.987709` cannot have a sequential interpretation (adding `displacement` 
to a model that contains `cylinders` only). The $p$-values in the `lm` summary 
have a **partial** interpretation as seen by performing reduction tests between 
the following reduced models and the full model with four predictors:

```{r}
lm_nocyl <- lm(mpg ~ displacement + weight + horsepower, data=Auto)
lm_nodis <- lm(mpg ~ cylinders + weight + horsepower, data=Auto)
lm_nowgt <- lm(mpg ~ cylinders + displacement + horsepower, data=Auto)
lm_nohp  <- lm(mpg ~ cylinders + displacement + weight, data=Auto)

anova(lm_nocyl,lm4)
anova(lm_nodis,lm4)
anova(lm_nowgt,lm4)
anova(lm_nohp ,lm4)

```

The $p$-values of the partial reduction F-tests are identical to those for the 
$t$-tests in the `lm` output. The $F_{obs}$ statistics are the squared values 
of the $t_{obs}$ statistics.

On the other hand, if you ask for the analysis of variance on the full model with 
the `aov` function, you get **sequential** tests of the inputs.

```{r}
summary(aov(lm4))
```

## Python

```{python}
formula1 = "mpg ~ cylinders + displacement + weight + horsepower"
y, X = dmatrices(formula1, data=Auto_data, return_type='dataframe')
mlr = sm.OLS(y, X).fit()
print(mlr.summary())
```

How do we interpret the $p-$-value of 0.338 for the `cylinders` variable or 
the $p$-value of 0.988 for the `displacement` variable? A two-sided $t$-test 
is equivalent to an F test with 1 numerator degrees of freedom where $F_{obs}$ is 
the square of the $t_{obs}$ statistic. Let's first compute the sequential sum of 
squares tests and see if they match the $p$-values in the output.

```{python}
y, X = dmatrices("mpg ~ cylinders", data=Auto_data, return_type='dataframe')
lm1 = sm.OLS(y, X).fit()

y, X = dmatrices("mpg ~ cylinders + displacement", data=Auto_data, return_type='dataframe')
lm2 = sm.OLS(y, X).fit()

y, X = dmatrices("mpg ~ cylinders + displacement + weight", data=Auto_data, return_type='dataframe')
lm3 = sm.OLS(y, X).fit()

anova_lm(lm1, lm2, lm3, mlr)
```

The $p$-values are different from those shown in the summary of the four input
model, so `Pr(>|t|) = 0.988` cannot have a sequential interpretation (adding `displacement` 
to a model that contains `cylinders` only). The $p$-values in the model summary 
have a **partial** interpretation as seen by performing reduction tests between 
the following reduced models and the full model with four predictors:

```{python}
y, X = dmatrices("mpg ~ displacement + weight + horsepower", data=Auto_data, return_type='dataframe')
lm_nocyl = sm.OLS(y, X).fit()

y, X = dmatrices("mpg ~ cylinders + weight + horsepower", data=Auto_data, return_type='dataframe')
lm_nodis = sm.OLS(y, X).fit()

y, X = dmatrices("mpg ~ cylinders + displacement + horsepower", data=Auto_data, return_type='dataframe')
lm_nowgt = sm.OLS(y, X).fit()


y, X = dmatrices("mpg ~ cylinders + displacement + weight", data=Auto_data, return_type='dataframe')
lm_nohp = sm.OLS(y, X).fit()

anova_lm(lm_nocyl,mlr)
anova_lm(lm_nodis,mlr)
anova_lm(lm_nowgt,mlr)
anova_lm(lm_nohp ,mlr)

```


:::
::::
:::

## Prediction

To obtain the fitted values in the linear model $\bY = \bX\bbeta + \bepsilon$ 
we apply the OLS estimates and get
$$
\widehat{\by} = \bX\widehat{\bbeta}
$$
To predict at a previously unobserved value of the inputs, a new observation 
$\bx_0$, we apply the regression equation and the OLS estimates to $\bx_0$:
$$
\widehat{\by}_0 = \bx_0^\prime \widehat{\bbeta}
$$

This seems simple, there must be a trick to it. The "trick" lies in the question: 
**what** is being predicted?

The target of the prediction (the **what**) can be $Y$, the target variable, or 
$\Exp[Y]$, the mean of the target variable. How are these different? $Y$ is a 
random variable and $\Exp[Y]$ is a constant; $\Var[Y] = \sigma^2$ and $\Var[\Exp[Y]] = 0$. 
The uncertainty in predicting an individual observation and predicting the mean 
function will differ. The former has to take into account the inherent variability 
$\sigma^2$ in the data. 

How are the predicted values themselves different?

For illustration, consider an SLR model $Y = \beta_0 + \beta_1x + \epsilon$. 
To predict $Y$ at $X=x_0$ we choose the obvious expression 
$$
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0 + \widehat{\epsilon}
$$
substituting estimates for all unknowns on the right hand side. Since $\epsilon$ 
is a zero-mean random variable and cannot be observed directly, the best estimate 
is $\widehat{\epsilon} = 0$ which leads to 
$$
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0
$$

That is the same expression we use to predict $\Exp[Y] = \beta_0 + \beta_1 x_0$, 
substituting the OLS estimates for unknowns on the right hand side:
$$
\widehat{\Exp}[Y_0] = \widehat{\beta}_0 + \widehat{\beta}_1x_0
$$
And therein lies the crux. The predicted values for an observation and for the
mean of an observation are the same. But their variability is not the same. We 
need to be very clear about what it is we are shooting for. More frequently one 
is interested in predicting observations, not averages of observations. In a study 
of health outcomes over time you might be more interested in predicting how a 
patient does at time $t$, rather than how the population of patients does on average
at time $t$. Yet when folks see the different levels of confidence we have in 
the two predictions, they wish they could make predictions for the average.

### Prediction Variance

The variance of $\widehat{\Exp}[Y_0] = \bx_0^\prime\widehat{\bbeta}$ is 
straightforward and depends only on the variance of $\widehat{\bbeta}$:
$$
\Var{\widehat{\Exp}[Y_0]} = \sigma^2 \, \bx_0^\prime(\bXpX)^{-1}\bx_0
$$
To account for the variability in the data, the proper variance to consider when 
predicting an individual observation is 
$$
\Var[\widehat{Y}_0 - Y_0] = \Var[\widehat{Y}_0 - \epsilon] = \Var[\bx_0^\prime\widehat{\bbeta}-\epsilon] = \sigma^2\left(1+\bx_0^\prime(\bXpX)^{-1}\bx_0\right)
$$

The additional $1+$ does not seem like a big deal but has substantial numeric 
consequences.

### Confidence and Prediction Intervals

When the errors have a Gaussian distribution, the following random variables have 
$t$ distributions with $n-r(\bX)$ degrees of freedom:

$$
\begin{align*}
    t &= \frac{\widehat{Y}_0 - \Exp[Y]_0}{\sqrt{\Var[\widehat{Y}_0]}} \\
    t &= \frac{\widehat{Y}_0 - Y_0}{\sqrt{\Var[\widehat{Y}_0]-Y_0}} 
\end{align*}
$$

The first is used to construct $(1-\alpha)$-level **confidence** intervals for 
the mean of the target
$$
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \bx_0^\prime(\bXpX)^{-1}\bx_0}
$$
The second is used to construct $(1-\alpha)$-level **prediction** intervals for 
an individual target
$$
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \left(1+ \bx_0^\prime(\bXpX)^{-1}\bx_0 \right)}
$$

:::{.example}
::::{.example-header}
Example: PISA OECD Study
::::
::::{.example-container}
PISA (Program for International Student Assessment) is an OECD study in 65 countries
to evaluate the performance of 15-year old students in math, science, and reading. 
Among the questions the study is trying to address is whether the educational 
level in a country is influenced by economic wealth, and if so, to what extent.

:::{.panel-tabset group="language"}

## R

<!---
For some reason reading the pisa table from duckdb in R
crashes during knitting, but not when running interactively.
It works when knitting the Python code. The problem persists
after updating the duckdb library. Have to go with csv file for now.
--->

```{r read_pisa, warning=FALSE, message=FALSE}

pisa <- read.csv("data/pisa.csv", stringsAsFactors =TRUE)
head(pisa)

```

The following statements compute the simple linear regression of `MathMean` on the 
log GDP and 95\% prediction and confidence intervals.

```{r}
pisa_slr <- lm(MathMean ~ logGDPp, data=pisa)

xvals <- data.frame(logGDPp=seq(from=6, to = 14, by=0.1))
p_pred <- data.frame(predict(pisa_slr, newdata=xvals, interval="prediction"))
p_conf <- data.frame(predict(pisa_slr, newdata=xvals, interval="confidence"))

head(p_pred)
head(p_conf)

```

## Python

```{python Py_read_pisa}
import pandas as pd

pisa = pd.read_csv('data/pisa.csv').dropna()
pisa.head()
```

The following statements compute the simple linear regression of `MathMean` on the 
log GDP and 95\% prediction and confidence intervals.

```{python}
import statsmodels.api as sm
import pandas as pd
import numpy as np

X = sm.add_constant(pisa['logGDPp'])
y = pisa['MathMean']
pisa_slr = sm.OLS(y, X).fit()

xvals = pd.DataFrame({'logGDPp': np.arange(6, 14.1, 0.1)})
X_pred = sm.add_constant(xvals)

intervals = pd.DataFrame(
    pisa_slr.get_prediction(X_pred).summary_frame(alpha=0.05),
    columns=['mean', 'mean_se', 'mean_ci_lower', 'mean_ci_upper', 'obs_ci_lower', 'obs_ci_upper']
)

p_pred = intervals[['mean', 'obs_ci_lower', 'obs_ci_upper']]
p_pred.columns = ['fit', 'lwr', 'upr']  

p_conf = intervals[['mean', 'mean_ci_lower', 'mean_ci_upper']]
p_conf.columns = ['fit', 'lwr', 'upr']  

print("Prediction intervals:")
print(p_pred.head())

print("\nConfidence intervals:")
print(p_conf.head())
```

:::

Both types of intervals are most narrow near the center of the $x$ data range and 
widen toward the edges (@fig-pred-conf-intervals). A prediction outside of the hull 
of the data will be much less precise than a prediction near the center of the data. 
The additional variance term that distinguishes the variance of $\widehat{y}_0$ from 
that of $\widehat{y}_0 - y_0$ has considerable effect; the prediction intervals 
are much wider than the confidence intervals.

```{r, echo=FALSE, out.width="80%", fig.align='center'}
#| label: fig-pred-conf-intervals
#| fig.cap: 95% prediction and confidence intervals.
plot(pisa$logGDPp,
     pisa$MathMean,col="red", 
     ylab="MathMean", 
     xlab="log GdP",
     xlim=c(5,16),
     ylim=c(200,700),
     las=1,
     bty="l")

#abline(pisa_slr, lwd=2)
lines(xvals$logGDPp,predict(pisa_slr,newdata=xvals),lwd=2)
lines(xvals$logGDPp,p_pred$lwr,col="blue",lwd=1.5, lty="dotted")
lines(xvals$logGDPp,p_pred$upr,col="blue",lwd=1.5, lty="dotted")

lines(xvals$logGDPp,p_conf$lwr,col="darkgreen",lwd=1.5, lty="dashed")
lines(xvals$logGDPp,p_conf$upr,col="darkgreen",lwd=1.5, lty="dashed")
legend("topleft",legend=c("95% Prediction interval","95% Confidence interval"),
       lty=c("dotted","dashed"),
       lwd=1.5,
       col=c("blue","darkgreen"), cex=0.8)

```

::::
:::

## Diagnostics

Model diagnostics are useful to examine model assumptions and to decide whether 
the model is an adequate description of the data. Questions we would like to 
answer through model diagnostics include

+ Are the assumptions of the model met? 

  + Linearity (zero mean errors, $\Exp[\bepsilon] = \bzero$)

  + Equal variance (homoscedasticity, $\Var[\bepsilon] = \sigma^2\bI$)

  + Independence of the errors $\Cov[\epsilon_i, \epsilon_j] = 0$
  
  + Can we assume that the errors are normally distributed?

+ How well does the model predict new observations? 

+ Do observations have undue influence on the results?

+ Does the relationship between the inputs negatively affect the analysis?

The three basic types of linear model diagnostics are 

1. **Residual** diagnostics to examine linearity, equal variance assumptions, 
and to detect outliers. Residual analysis relies on functions of $y_i - \widehat{y}_i$ 
to study the behavior of the unobservable $\epsilon_i$. 

2. **Case-deletion** diagnostics find data points that exert high **influence** on 
the analysis. These diagnostics ask how an aspect of the analysis (variability, 
predicted values, coefficient estimates, ...) changes if an observation is removed 
from the analysis. 

3. **Collinearity** diagnostics examine the relationships among the inputs and 
whether they impact the analysis in a negative way. The situations at the end of 
the extremes include completely orthogonal inputs ($\bXpX$ is a diagonal matrix) 
and inputs that are linear combinations of each other ($\bXpX$ is non-singular 
and a unique OLS solution does not exist). Most applications fall in-between 
unless one or more inputs are factors.

### Leverage

Linear model diagnostics depend on the leverage values $h_{ii}$, the diagonal 
values of the Hat matrix $\bH$. That is not surprising because the fitted values 
are linear combinations of the entries in the Hat matrix
$$
\widehat{\by} = \bH\by
$$
The $i$^th^ fitted value is a linear combination of the entries in the $i$^th^ 
row of $\bH$ with the elements of $\by$
$$
\widehat{y}_i = \bx_i^\prime\widehat{\bbeta} = \sum_{j=1}^n h_{ij}y_j
$$

From the last expression it is easy to establish that 
$$
\Var[\widehat{y}_i] = \sigma^2 \, \bx_i^\prime(\bXpX)^{-1}\bx_i = \sigma^2 h_{ii}
$$

We can think of the leverages $h_{ii}$ as standardized squared distance measures 
that tell us how far the $i$^th^ data point is from the center of the $x$-data. 

It is illustrative to look at the average variance of the fitted values 
across the data set, 
$$
\frac{1}{n}\sum_{i=1}^n \Var[\widehat{y}_i] = \frac{1}{n} \sum_{i=1}^n \sigma^2h_{ii} = \sigma^2\left(\frac{p+1}{n}\right)
$${#eq-average-pred-var}

The last result follows because $\bH$ is a projection matrix and thus $tr(\bH)$ 
equals its rank, $p+1$.

What does @eq-average-pred-var tell us about the precision of the estimated regression output? 
Suppose $p=4$ and $n=10$. The average variance is then $\sigma^2/2$. When $p=4$ and 
$n=1000$, the average variance is $\sigma^2/200$. As sample size increases, the 
estimates become more precise as $p$ remains fixed. As $p$ increases for a given sample 
size, the average variance of the fitted values increases. In high-dimensional 
problems---where $p$ is large---OLS estimates have high variability and are unstable. 
Regularized estimation methods such as Ridge or Lasso regression can perform better 
in those circumstances.

Since $\sum_{i=1}^n h_{ii} = p+1$, the average leverage value in the data is $(p+1)/n$, 
and a good threshold for *high leverage* points is $h_{ii} > 2(p+1)/n$. This is not 
necessarily a problematic data point, it simply states that the point is an outlying 
point in the $x$-space. High leverage points have the potential to influence aspects 
of the analysis, to be highly influential data points. More on this below.

In summary, here are some important results involving the leverage values $h_{ii}$:

+ $h_{ii} = \bx_i^\prime (\bXpX)^{-1}\bx_i$

+ $\frac{1}{n} \le h_{ii} \le 1$. This holds only for the $n$ training observations, 
the leverage $\bx_0^\prime(\bXpX)^{-1}\bx_0$ of a new data point is not bounded 
in this way.

+ $\overline{h} = (p+1)/n$; high leverage points are those for which $h_{ii} > 2(p+1)/n$.

+ $\Var[\widehat{y}_i] = \sigma^2 h_{ii}$

+ $\Var[y_i - \widehat{y}_i] = \sigma^2 (1-h_{ii})$

+ $\widehat{y}_i = \sum_{j=1}^n h_{ij}y_j$, the $i$^th^ fitted value is a linear 
combination of the target values with the values in the $i$^th^ row of $\bH$.

+ $\sum_{j=1}^n h_{ij} = 1$, the sum of the leverage values in the $i$^th^ row of 
$\bH$ is 1. Since the leverage values are bounded,  they sum to 1 within a row, 
and the fitted values are linear combinations of $\bH$, this shows how a data 
point with $h_{ii} \approx 1$ has outsize influence. The fitted value is almost 
entirely determined by the input values of that observation.

:::{.example}
::::{.example-header}
Example: Same Leverage--Different Influence
::::
::::{.example-container}
This simulation demonstrates the concept of high leverage points with and without 
high influence. Data are simulated under the model
$$ 
Y = \beta_0 + \beta_1 x + \epsilon
$$ 
with $\epsilon \sim G(0,0.25^2), \beta_0 = 1, \beta_1 = 0.5$. The design points for 
the input variable are spread evenly from 1 to 2, and a high leverage point is 
added at $x=4$. There are 22 observations, so the threshold for a high leverage 
point is $2(p+1)/n = 2*2/22 = 0.18$.

Two data sets are simulated. One in which the target value at $x=4$ concurs with 
the mean function, one in which the target value is unusually high (@fig-leverage-example).

:::{.panel-tabset group="language"}

```{r, echo=FALSE}
set.seed(187)

x <- seq(from=1, to=2, by=0.05)
xlev <- 4  # the high-leverage point
x <- c(x,xlev)

mn <- 1 + 0.5*x
y1 <- rnorm(n=length(x), mean=0, sd=0.25) + mn

y2 <- y1
y2[length(y1)] <- rnorm(1,0,0.25) + 1 + 1*xlev


dfr <- data.frame(x=x,y1=y1,y2=y2)
write.csv(dfr,file="data/sim_leverage.csv",row.names=FALSE)
```


## R

```{r}

datfr <- read.csv(file="data/sim_leverage.csv")

lm1 <- lm(y1 ~ x, data=datfr)
lm2 <- lm(y2 ~ x, data=datfr)

lm1$coefficients
lm2$coefficients

# the point at xlev has the same leverage in both regressions
round(hatvalues(lm1),4)
round(hatvalues(lm2),4)
```

## Python


```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm

datfr = pd.read_csv("data/sim_leverage.csv")

X = sm.add_constant(datfr['x'])
y1 = datfr['y1']
y2 = datfr['y2']

lm1 = sm.OLS(y1, X).fit()
lm2 = sm.OLS(y2, X).fit()

print("lm1 coefficients:")
print(lm1.params)

print("\nlm2 coefficients:")
print(lm2.params)

leverage1 = np.round(lm1.get_influence().hat_matrix_diag, 4)
leverage2 = np.round(lm2.get_influence().hat_matrix_diag, 4)

# the point at xlev has the same leverage in both regressions
print(leverage1)
print(leverage2)
```

:::

The fitted linear regressions are quite different. The parameter estimates in the 
first model, where $y|x=4$ is not unusual, are close to the true values 
$\beta_0 = 1, \beta_1 = 0.5$. In the second regression the parameter estimates 
are very different from the true values, the estimates are biased.

The leverage values in both models are identical, since they depend only on the 
$x$-data. The data point at $x=4$ has high leverage, its value of 
`{r} round(hatvalues(lm1)[length(x)],4)` exceeds the threshold considerably. 
No other data point has high leverage.

@fig-leverage-example displays the data and fitted regressions for the two data 
sets. Although $x=4$ is a high leverage point, it has no undue influence on the
estimated regression in the left panel. In the right panel, the high leverage 
point is a highly influential point due to its unusual $y$-value. The data point 
exerts its leverage by pulling the estimated regression towards it.

```{r, echo=FALSE, out.width="90%", fig.align='center'}
#| label: fig-leverage-example
#| fig.cap: A data point with high-leverage at $x=4$ has little influence in one analysis and is highly influential in another analysis depending on its $y$-value. The dashed line is the true mean function.
#| 
par(mfrow=c(1,2))
par(mar=c(4.1, 4.1, 2, 3)) # bottom, left, top, right
plot(x,y1,xlab="X",ylab="Y",col="red",pch=16,
     cex=1.5,
     ylim=c(0,5.5),
     las=1)
abline(lm1, col="red",lwd=2)
abline(a=1,b=0.5,lwd=2,col="black",lty="dashed")
text(1.5, 5, adj=0, 
     cex=0.8,
     expression(hat(y) == 0.986 + 0.502*x))

plot(x,y2,xlab="X",ylab="Y",col="blue",pch=16,
     cex=1.5, 
     ylim=c(0,5.5),
     las=1)
abline(lm2, col="blue", lwd=2)
abline(a=1,b=0.5,lwd=2,col="black",lty="dashed")
text(1.5, 5, adj=0, 
     cex=0.8,
     expression(hat(y) == 0.023 + 1.16*x))
```
::::
:::

### Residual Diagnostics

Basic questions about the correctness of the model revolve around the properties 
of $\bepsilon$. The usual assumption $\bepsilon \sim (\bzero, \sigma^2\bI)$ states 
that the errors have zero mean, are uncorrelated, and have equal variance. Although 
$\bepsilon$ is unobservable, we should be able to check those assumptions by looking 
at the OLS residuals of the fitted model, $\widehat{\bepsilon} = \by - \widehat{\by}$. 
These are also called the **raw residuals**.

Unfortunately, the properties of $\widehat{\bepsilon}$ match the properties of 
$\bepsilon$ only partially. Because $\widehat{\bepsilon}$ is the result of fitting 
a model to data, the fitted residuals obey constraints that do not affect the
model errors $\bepsilon$. Because
$$
\bX^\prime \widehat{\bepsilon} = \bX^\prime (\bI-\bH)\by = \bzero
$$
the raw residuals sum to zero across each column of the $\bX$ matrix. In other 
words, there are only $n-r(\bX)$ degrees of freedom in the raw residual vector. 
From a statistical perspective, the residuals have zero mean, 
$\Exp[\widehat{\bepsilon}] = \bzero$ and share this property with the model errors. 
The variance of $\bepsilon$ and $\widehat{\bepsilon}$ is different, however:

\begin{align*}
    \Var[\bepsilon] &= \sigma^2 \bI \\
    \Var[\widehat{\bepsilon}] &= \sigma^2(\bI - \bH) \\
    \Var[\widehat{\epsilon}_i] &= \sigma^2 (1-h_{ii})
\end{align*}

While the model errors are uncorrelated, the fitted residuals are correlated, 
$\bI - \bH$ is not a diagonal matrix. The fitted residuals also do not have the 
same variance; the variance depends on the leverage of the $i$^th^ data point. 

These properties (or lack thereof) should give pause in using the raw residuals 
to diagnose the assumptions of equal variance or uncorrelated errors. Instead, 
residual diagnostics use transformations of the raw residuals.

#### Studentized residuals

The unequal variance of the residuals can be handled by **standardizing**, dividing 
the residual by its standard deviation (the square root of its variance)
$$
\frac{\widehat{\epsilon}_i}{\sigma\sqrt{1-h_{ii}}}
$$
$\sigma$ is unknown and the obvious solution is to substitute an estimator. 
Statisticians refer to this process, using an estimate to scale a random variable, 
as **studentization**. The studentized residual is thus
$$
r_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}\sqrt{1-h_{ii}}}
$$
The usual estimator for $\sigma$ is the square root of the estimator of $\sigma^2$, 
$$
\widehat{\sigma} = \sqrt{ \frac{\text{SSE}}{n-r(\bX)}}
$$

#### R-student residuals

A further adjustment can be made to the studentized residuals. Rather than use an 
estimate of the variance that is derived from all the data, an estimator can be 
used that does not depend on the $i$^th^ observation. This technique is called 
**external studentization** in contrast to the **internal studentization** that 
gives rise to $r_i$. 

Fortunately such an external estimate of the variance $\sigma^2$ that does not 
rely on the $i$^th^ observation can be computed based on the analysis of all $n$ 
observations. Not surprisingly, as with PRESS residuals discussed earlier, the 
leverage plays a role again:
$$
\widehat{\sigma}^2_{-i} = \frac{(n-r(\bX))\widehat{\sigma}^2 - \frac{\widehat{\epsilon}^2_i}{1-h_{ii}}}{n-r(\bX)-1}
$$

The externally studentized residual is called the R-student residual,
$$
t_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}_{-i}\sqrt{1-h_{ii}}}
$$

Since $\widehat{\epsilon}_i$ and $\widehat{\sigma}_{-i}$ are independent, $t_i$ 
behaves like a $t$-distributed random variable. The R-student residuals are good 
diagnostics to detect outliers and high-influence points (**hip**s). Outliers are 
observations unusual in $y$-space. They are not necessarily hips, unless they are 
also high leverage points (see the previous example).

The $t_i$ work well for outliers and hips because an outlier has large 
$\widehat{\epsilon}_i$ and a hip has small $\sqrt{1-h_{ii}}$. Both effects increase 
the value of $t_i$. This is also true for the (internally) studentized residual. 
In addition, outliers or hips will have a large 
$$
\frac{\widehat{\epsilon}^2_i}{1-h_{ii}}
$$
the adjustment term in the computation of $\widehat{\sigma}^2_{-i}$. For those 
data points $\widehat{\sigma}^2_{-i} < \widehat{\sigma}^2$ and $t_i$ will be more 
sensitive than $r_i$.

You can obtain all three sets of residuals easily in `R`:

+ The `residual` vector returned on the `lm` return object contains the $\widehat{\epsilon}_i$.

+ The `rstandard()` function returns the studentized residuals $r_i$ (unfortunate 
function name)

+ The `rstudent()` function returns the R-student residuals $t_i$


When testing model assumptions such as linearity, equal variance (homoscedasticity), 
and checking for outliers, the R-student residuals are the preferred quantities. 
The threshold $|r_i| > 2$ is often applied to indicate outlying observations. 


::: {.example}
::::{.example-header}
Example: Boston Housing Values 
::::
::::{.example-container}
To demonstrate residual analysis in a linear model we use the Boston housing data 
that is part of the `MASS` library in `R`. The data set comprises 506 observations 
on the median value of owner-occupied houses (`medv` in \$000s) in Boston suburbs 
and 13 variables describing the towns and properties. 

The following statements fit a multiple linear regression model to predict median 
home value as a function of all but two inputs. The formula syntax `medv ~ .` 
requests to include all variables in the dataframe as inputs. `medv ~ . -indus -age` 
requests inclusion of all inputs except `indus` and `age`.

:::{.panel-tabset group="language"}

## R

``` {r model_fit, warning=FALSE, message=FALSE}
library(MASS)
fit <- lm(medv ~ . - indus - age, data=Boston)
summary(fit)
```

All 11 input variables are significant in this model; it explains 74\% of the 
variability in median home values.

A plot of the R-student residuals against the observation number helps to identify
outlying observations. Observations with $t_i$ values outside the 
[-2, +2] interval are outliers. The plot also shows whether the equal variance assumption
is reasonable; if the assumption is met the residuals show as a band of equal width.

``` {r RStudent_plot, out.width="80%", fig.align="center"}
RStudent <- rstudent(fit)

par(mar=c(5.1, 4.1, 2, 3))
plot(RStudent, xlab="Obs no.", las=1,bty="l")
abline(h= 2, lty="dashed")
abline(h=-2, lty="dashed")
```


A plot of the residuals against leverage shows observations that are unusual with respect to $y$
(large absolute value of the residual), with respect to $x$ (large leverage), or both. 
A high leverage point that is also an outlier is a highly influential data point.

``` {r RStudent_vs_leverage, out.width="80%", fig.align="center"}
leverage <- hatvalues(fit)
lev_threshold <- 2*fit$rank/length(fit$residuals)

par(mar=c(5.1, 4.1, 2, 3))
plot(leverage, RStudent, xlab="Leverage")
abline(h= 2, lty="dashed")
abline(h=-2, lty="dashed")
abline(v=lev_threshold, lty="dotted")
```

## Python

```{python}
import pandas as pd
import duckdb 
print(duckdb.__version__)

con = duckdb.connect(database="ads.ddb", read_only=True)
Boston = con.sql("SELECT * FROM Boston").df().dropna()
con.close()
```


```{python}
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

X = Boston.drop(['medv', 'indus', 'age'], axis=1)
X = sm.add_constant(X)
y = Boston['medv']

fit = sm.OLS(y, X).fit()
print(fit.summary())
```

All 11 input variables are significant in this model; it explains 74\% of the 
variability in median home values.

A plot of the R-student residuals against the observation number helps to identify
outlying observations. Observations with $t_i$ values outside the 
[-2, +2] interval are outliers. The plot also shows whether the equal variance assumption
is reasonable; if the assumption is met the residuals show as a band of equal width.

```{python, out.width="80%", fig.align="center"}
from statsmodels.graphics.regressionplots import influence_plot
from statsmodels.stats.outliers_influence import OLSInfluence

influence = OLSInfluence(fit)
rstudent = influence.resid_studentized_external

plt.figure(figsize=(10, 6))
plt.plot(rstudent, 'o')
plt.axhline(y=2, linestyle='--', color='r')
plt.axhline(y=-2, linestyle='--', color='r')
plt.xlabel('Obs no.')
plt.ylabel('Studentized Residuals')
plt.title('Studentized Residuals Plot')
plt.grid(True, alpha=0.3)
plt.show()
```

A plot of the residuals against leverage shows observations that are unusual with respect to $y$
(large absolute value of the residual), with respect to $x$ (large leverage), or both. 
A high leverage point that is also an outlier is a highly influential data point.

```{python, out.width="80%", fig.align="center"}

leverage = influence.hat_matrix_diag
lev_threshold = 2 * (fit.df_model + 1) / len(fit.resid)

# Plot leverage vs studentized residuals
plt.figure(figsize=(10, 6))
plt.scatter(leverage, rstudent)
plt.axhline(y=2, linestyle='--', color='r')
plt.axhline(y=-2, linestyle='--', color='r')
plt.axvline(x=lev_threshold, linestyle=':', color='g')
plt.xlabel('Leverage')
plt.ylabel('Studentized Residuals')
plt.title('Leverage vs Studentized Residuals')
plt.grid(True, alpha=0.3)
plt.show()
```

:::

In a simple linear regression model you can plot the residuals against the input $x$.
In a multiple linear regression model (or a SLR) you plot the residuals against the
fitted values. The residuals should display no obvious trend against $\widehat{y}$.

When plotting the residuals against the fitted value, add the predictions from
a smoothing spline or other nonparametric model to identify trends in the 
residuals. Ideally, the smoothing spline should not show any gross departures
from a flat line at zero. 

The following plot raises concerns. We might not have
the right set of inputs. Inputs might need to be transformed or 
additional/different terms are needed in the model, for example, interactions
between the inputs.

:::{.panel-tabset group="language"}

## R

``` {r Rstudent_vs_fit, out.width="80%", fig.align="center"}
yhat <- predict(fit)
plot(yhat,RStudent)
abline(h=2, lty="dashed")
abline(h=-1, lty="dashed")
loe <- loess(RStudent ~ yhat)
lines(predict(loe,newdata=data.frame(yhat=seq(-2,40,1))),
      col="red",lwd=2)
```

## Python

```{python Py_Rstudent_vs_fit, out.width="80%", fig.align="center"}
from statsmodels.nonparametric.smoothers_lowess import lowess

yhat = fit.fittedvalues

# Plot fitted values vs studentized residuals
plt.figure(figsize=(10, 6))
plt.scatter(yhat, rstudent)

# Add reference lines
plt.axhline(y=2, linestyle='--', color='r')
plt.axhline(y=-1, linestyle='--', color='r')

lowess_result = lowess(rstudent, yhat, frac=3/4, it=3)
plt.plot(lowess_result[:, 0], lowess_result[:, 1], color='red', linewidth=2)

plt.xlabel('Fitted Values')
plt.ylabel('Studentized Residuals')
plt.title('Fitted Values vs Studentized Residuals')
plt.grid(True, alpha=0.3)
plt.show()

```

:::

::::
:::

#### Partial regression plots

It is tempting to create plots comparing residuals against the values of the input 
variables. This is meaningful in a simple linear regression with one input and 
can help suggest transformations of $X$ to achieve linearity and help diagnose
heteroscedasticity.

In @fig-var-increases, the variance of $Y$ increases with the value of $X$ and 
the relationship between $Y$ and $X$ is a second-degree polynomial. 

```{r, echo=FALSE, fig.align="center", out.width="80%"}
#| fig.cap: Simulated data where the variance of the observations increases with $X$.
#| label: fig-var-increases
set.seed(543)
x <- seq(1,10,0.1) 
x <- x + rnorm(length(x),0,1)
mn <- 2 - 0.5*x + 0.3*x^2
y <- rnorm(n=length(x), mean=mn, sd=sqrt(x))
lfit <- lm(y ~ x)
ri <- rstudent(lfit)

plot(x,ri,ylab="Rstudent",las=1,bty="l")
abline(h=0,col="red",lty="dashed")

```

The plot of the Rstudent residuals shows the increasing variability in $x$ and a 
systematic quadratic trend in the residuals.

In a multiple linear regression (MLR) model it is tempting to create plots such 
as the previous one for all input variables. Unfortunately, such plots can be misleading 
because in an MLR model values of one input are changing with the values of other 
inputs. A way around this problem seems to be a plot of residuals versus the fitted 
values $\widehat{y}_i$ but that is not an optimal solution either; this plot does
not tell us anything about the $X$s that could be transformed to improve the model. 
It mushes together the contributions of all inputs.

So, how can we diagnose whether $X_j$ needs to be transformed to account for 
non-linearity and visualize the relationship in such a way that accounts for the 
other inputs in the model? The answer is the **partial regression** plot, also 
known as the **added-variable** plot.

Suppose we partition the $\bX$ matrix of the MLR model as follows
$$
\bX = [\bX_{-j}, \bx_j]
$$
so that $\bX_{-j}$ contains all inputs (including the intercept) except for the 
$j$^th^ input. Now consider two new regression models:

- Regress $\by$ on $\bX_{-j}$. Denote the residuals from this regression as $\textbf{e}_{y,-j}$

- Regress $\bx_j$ on $\bX_{-j}$. Denote the residuals from this regression as $\textbf{e}_{x,-j}$

The added variable plot for input $j$ displays $\textbf{e}_{y,-j}$ on the vertical 
axis and $\textbf{e}_{x,-j}$ on the horizontal axis.

:::{.example}
::::{.example-header}
Example: Auto (Cont'd)
::::
::::{.example-container}

:::{.panel-tabset group="language}

## R

For the four-regressor model in the Auto example, added-variable plots can be 
constructed with the `avPlots()` function in the `car` package.

```{r}
mlr <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)
car::avPlots(mlr)
```

## Python

For the four-regressor model in the Auto example, added-variable plots can be 
constructed with the `plot_ccpr` function in `statsmodels.graphics.regressionplots`.

```{python}
from statsmodels.graphics.regressionplots import plot_ccpr

X = Auto_data[['cylinders', 'displacement', 'weight', 'horsepower']]
X = sm.add_constant(X)
y = Auto_data['mpg']
mlr = sm.OLS(y, X).fit()

# Create added variable plots (equivalent to car::avPlots in R)
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

# Plot each predictor variable
predictors = ['cylinders', 'displacement', 'weight', 'horsepower']
for i, pred in enumerate(predictors):
    plot_ccpr(mlr, pred, ax=axes[i])
    axes[i].set_title(f'Added Variable Plot for {pred}')
    
plt.tight_layout()
plt.show()
```


:::

::::
:::

Take the scatter of points in an added variable plot. This is a regression through 
the origin (a no-intercept) model of the form
$$
\textbf{e}_{y,-j} = \beta_j \, \textbf{e}_{x,-j} + \bepsilon^*
$$

Using $\beta_j$ to denote the slope in this regression is no accident. The estimate 
of $\beta_j$ in this regression is the same as the estimate in the multiple linear 
regression model. 

The added-variable plot is a visual representation of how $X_j$ fares in the 
$p$-input model even if we are looking only at a two-dimensional plot. The partial 
regressions show the effect of $X_j$ on $y$ as if it was added last to the model. 
If the residual point scatter in the added-variable plot suggests nonlinearity, 
a transformation of $X_j$ is in order. 

Inputs that are highly significant in the multiple linear regression model will 
have a tight point cloud in the added-variable plot. Inputs that are correctly 
specified in the model will show non-systematic scatter of the points around the 
line. Note that we are not looking for horizontal point clouds in the added-variable 
plots, because the points are arranged around a non-zero line, its slope corresponds 
to the coefficient estimate.

### Case Deletion Diagnostics

Case deletion diagnostics express how much an aspect of the model changes when 
an observation is removed from the analysis. The RStudent residual is a leave-one-out
diagnostic in this spirit, as it uses an estimator of $\sigma$ that does not incorporate
the $i$th data point.

The two most important case deletion diagnostics are Cook's D and the DFFITS. 
The name DFFITS stands for **d**i**ff**erence in **fit**, **s**tandardized. The 
statistic measures the change in predicted value in units of standard errors when
the $i$th observation is deleted. We are concerned when a DFFIT exceeds in 
absolute value the threshold of $2\sqrt{(p+1)/n}$.

The Cook's D ("D" for distance) statistic measures the change in the parameter 
estimates $\boldsymbol{\beta}$ when the $i$th observation is removed. If the purpose 
of modeling is to build a model that predicts well, focus more on DFFITS. If the 
purpose of modeling is to test hypotheses, then focus more on Cook's D. We are 
concerned if the D statistic exceeds 1.

#### Cook's D

:::{.example}
::::{.example-header}
Example: Boston Housing Values (Cont'd)
::::
::::{.example-container}

:::{.panel-tabset group="language"}

## R

Computing Cook's D for our model is easy:

``` {r cooks_d, out.width="85%"}
D <- cooks.distance(fit)
D[which.max(D)]
D[which(D > 0.1)]
```

There are no data points with a D > 1. We conclude that there are no data points
that unduly influence the regression coefficients.

A plot of the D statistic against the observation number shows that there is a
group of data points with much higher values of the D statistic. This group also
coincides with larger residuals in the previous plots.

``` {r cooks_d_plot, out.width="85%", fig.align="center"}
plot(D, xlab="Obs no.")
```

Here are the observations with the larger D values

``` {r large_D}
D[which(D > 0.035)]
```

What do these observations have in common?

``` {r}
subset(Boston, D > 0.035)
```

## Python

Computing Cook's D for our model extracts the D statistic from the 
influence statistics in `statsmodels`:


```{python}
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import OLSInfluence

influence = OLSInfluence(fit)
D = influence.cooks_distance[0]  # cooks_distance returns a tuple, we need the first element

max_idx = np.argmax(D)
print(f"Maximum Cook's distance: {D[max_idx]:.4f} at observation {max_idx}")

# Find observations with Cook's distance > 0.1
large_D_indices = np.where(D > 0.1)[0]
if len(large_D_indices) > 0:
    print(f"Observations with Cook's distance > 0.1:")
    for idx in large_D_indices:
        print(f"Observation {idx}: {D[idx]:.4f}")
```

There are no data points with a D > 1. We conclude that there are no data points
that unduly influence the regression coefficients.

A plot of the D statistic against the observation number shows that there is a
group of data points with much higher values of the D statistic. This group also
coincides with larger residuals in the previous plots.

```{python Py_cooks_d_plot, out.width="85%", fig.align="center"}
plt.figure(figsize=(10, 6))
plt.stem(D, markerfmt='ro', basefmt='b-')
plt.xlabel('Obs no.')
plt.ylabel("Cook's distance")
plt.title("Cook's Distance Plot")
plt.grid(True, alpha=0.3)
plt.show()
```

Here are the observations with the larger D values.
What do these observations have in common?

```{python}
# Find observations with Cook's distance > 0.035
influential_indices = np.where(D > 0.035)[0]
if len(influential_indices) > 0:
    print(f"Observations with Cook's distance > 0.035:")
    influential_data = Boston.iloc[influential_indices]
    print("\nDetails of influential observations:")
    print(influential_data)
```

:::

::::
:::

#### DFFITs

:::{.example}
::::{.example-header}
Example: Boston Housing Values (Cont'd)
::::
::::{.example-container}

:::{.panel-tabset group="language"}

## R

You can calculate the DFFITs statistics with the `dffits()` function. The following
plot shows those statistics along with the threshold.

``` {r dffits}
p_plus1 <- length(fit$coefficients)
n_used <- length(fit$fitted.values)
threshold_dff <- 2 * sqrt(p_plus1/n_used) 
dff <- dffits(fit)
dff[which(abs(dff) > threshold_dff)]
```


``` {r dff_plot, out.width="85%", fig.align="center"}
plot(dff,ylab="DFFITs")
abline(h=threshold_dff)
abline(h=-threshold_dff)
```

## Python

You can extract the DFFITs statistics from the influence object returned by
`OLSInfluence` in `statsmodels`. The following
plot shows those statistics along with the threshold.


```{python out.width="85%", fig.align="center"}

influence = OLSInfluence(fit)
dff = influence.dffits[0]  # dffits returns a tuple, we need the first element

# Calculate threshold for DFFITS
# 2 * sqrt(p/n) where p is the number of parameters and n is the number of observations
p = fit.df_model + 1  # +1 for intercept
n = len(fit.resid)
threshold_dff = 2 * np.sqrt(p/n)
print(f"DFFITS threshold: {threshold_dff:.4f}")

# Find observations with DFFITS > threshold
influential_indices = np.where(np.abs(dff) > threshold_dff)[0]
if len(influential_indices) > 0:
    print("Observations with DFFITS > threshold:")
    for idx in influential_indices:
        print(f"Observation {idx}: {dff[idx]:.4f}")

# Plot DFFITS
plt.figure(figsize=(10, 6))
plt.plot(dff, 'o')
plt.axhline(y=threshold_dff, color='r', linestyle='-')
plt.axhline(y=-threshold_dff, color='r', linestyle='-')  
plt.xlabel('Obs no.')
plt.ylabel('DFFITS')
plt.title('DFFITS Plot')
plt.grid(True, alpha=0.3)
plt.show()
```

:::
::::
:::

The same group of observations with high D values has also high DFFITs. Contrary to
the D values, the observations exceed the threshold for the DFFITs. We conclude that
the data points are not influential on the regression coefficient estimates, but 
they are influential on the predicted values. If the model is used to predict 
median home values, we should consider refining the model or excluding the outlying 
observations and refitting.

### Collinearity Diagnostics {#sec-collinearity-diag}

When inputs are related to each other it has two important consequences

- we cannot interpret the regression coefficient for one input without considering
the other inputs
- with increasing dependence among the inputs, the least squares estimation
procedure becomes increasingly numerically unstable.

The condition when inputs are linearly related is called **collinearity** and it
negatively affects any calculations that involve the $(\textbf{X}^\prime\textbf{X})^{-1}$ 
matrix (which is about all the calculations.) 

The extreme case is when an input variable is linear combination of other inputs, for example
$$
Z = aX_2 + bX_3
$$
Adding $Z$ to a model that contains inputs $X_2$ and $X_3$ leads to a singular 
(rank-deficient) $\bX$ matrix. The inverse cross-product matrix $(\bXpX)^{-1}$ 
does not exist and the OLS estimator cannot be computed. Software uses instead 
a **generalized** inverse matrix $(\bXpX)^{-}$ to find a solution, which happens 
to be not unique.

::: {.example}
::::{.example-header}
Example: Boston Housing Values
::::
::::{.example-container}
The following code fits a linear model with four inputs. The variable `newvar` is 
the sum of the `zn` and `nox` variables. With `zn` and `nox` already in the model, 
`newvar` does not provide any additional information. The $\bX$ matrix is singular 
and an estimate for the coefficient of `newvar` cannot be found.

:::{.panel-tabset group="language"}

## R

```{r}
B <- Boston
B$newvar <- B$zn+B$nox
singular_model <- lm(medv ~ crim + zn + nox + newvar, data=B)
summary(singular_model)
```

`R` indicates the singularity in $\bX$ with `NA` for the coefficient associated 
with the singularity. Note that this depends on the order in which the variables 
enter the model. When `newvar` is placed before `zn` and `nox` in the model, the 
`nox` coefficient is `NA`.

## Python

```{python}
# Create a copy of Boston
B = Boston.copy()

B['newvar'] = B['zn'] + B['nox']

# Add a constant (intercept) to the model
X = sm.add_constant(B[['crim', 'zn', 'nox', 'newvar']])
singular_model = sm.OLS(B['medv'], X).fit()

print(singular_model.summary())
```

The second note at the bottom of the output alerts us to a singularity 
problem in the model. Although coefficient solutions are computed for the 
intercept and four input variables, the model contains only 3 degrees
of freedom because of the linear relationship between the variables.
:::
::::
:::

Collinearity is the condition where inputs are highly correlated. They do not follow 
exact linear dependencies but are close to linearly dependent (a so-called near-linear 
dependency). The situation is more complex than involving just two input variables.
For example, $X_1$ might not be strongly correlated with $X_2$ but can be strongly 
correlated with a linear combination of $X_2$, $X_3$, and $X_8$. This condition 
is called **multicollinearity**.

When multicollinearity is strong, the OLS estimates of the regression coefficients 
become unstable, small perturbations in the target values or inputs can lead to 
large changes in the coefficients. The $\widehat{\beta}_j$ can be of the wrong 
size and/or sign.

A nontechnical diagnostic for multicollinearity is to compute the matrix of pairwise 
correlations among the inputs. Large values of $\text{Corr}[X_j,X_k]$ is a sufficient 
condition for collinearity, but it is not a necessary condition. Even with weak 
pairwise correlations you can have strong linear dependencies among multiple inputs.

Nevertheless, the pairwise correlation plot is a good place to start.

:::{.panel-tabset group="language}

## R

``` {r corrplot, out.width="75%", fig.align="center"}
library("corrplot")
X <- model.matrix(fit)
corrplot(cor(X[,2:12]), method = "circle", diag=FALSE, tl.col="black")
```

## Python

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Extract the design matrix, similar to model.matrix() in R
X = fit.model.exog

# Convert to DataFrame for easier handling of column names
X_df = pd.DataFrame(X, columns=fit.model.exog_names)
X_without_intercept = X_df.iloc[:, 1:12]

corr_matrix = X_without_intercept.corr()

# Create correlation plot similar to corrplot with circle method
plt.figure(figsize=(10, 8))
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # To hide diagonal
sns.heatmap(corr_matrix, 
            mask=mask,
            annot=True, 
            fmt=".2f", 
            cmap="coolwarm", 
            linewidths=0.5,
            square=True,
            cbar_kws={"shrink": .5})

plt.tight_layout()
plt.show()
```

:::

We see some strong **pairwise** relationships between `tax` and `rad`, between `dis` and `nox`, 
and between `lstat` and `rm`. Do we need to worry?

Here are some other, informal, ways to diagnose a multicollinearity problem:


- The $R^2$ statistic indicates the model explains substantial variability in $Y$, 
but none or few of the inputs show statistically significant $p$-values. Because 
the $t$-tests are partial tests, the other input variables act as proxy for the
variable being tested.

- Different variable selection methods lead to very different models.

- Standard errors of coefficients and/or fitted values are unusually large.

- Slight perturbations of the data, for example, by adding some small Gaussian 
random noise, change the results dramatically.

A formal diagnosis relies on the computation of **variance inflation factors** (VIFs) 
or **condition indices*.

#### Variance inflation factors

Each predictor (input) variable in a linear model is associated with a variance 
inflation factor that quantifies the strength of linear dependencies between this 
input and all other inputs.

The $j$^th^ VIF measures how many times more variable the variance of the 
standardized coefficients are due to the involvement of $X_j$ in linear dependencies 
involving the other $X$s.

You can find the $\text{VIF}_j$ from the $R^2$ statistic of a multiple linear 
regression of $X_j$ on 
all the other input variables. 
$$ 
\text{VIF}_j = \frac{1}{1-R^2_j}
$$
For example, the VIF for `rad` can be obtained by regressing all other inputs onto `rad`

:::{.example}
::::{.example-header}
Example: Boston Housing Values (Cont'd)
::::
::::{.example-container}

:::{.panel-tabset group="language"}

## R

``` {r vif_rad}
vif_calc <- lm(rad ~ crim + zn + chas + nox + rm + dis + tax + 
                   ptratio + black + lstat, data=Boston)
R2_rad <- summary(vif_calc)$r.squared
VIF_rad <- 1 / (1-R2_rad)
VIF_rad
```

## Python

```{python}
import statsmodels.api as sm
import pandas as pd
import numpy as np
from patsy import dmatrices
from tabulate import tabulate

formula1 = "rad ~ crim + zn + chas + nox + rm + dis + tax + ptratio + black + lstat"
y, X = dmatrices(formula1, data=Boston, return_type='dataframe')
vif_calc = sm.OLS(y, X).fit()

R2_rad = vif_calc.rsquared
VIF_rad = 1 / (1-R2_rad)
print(f"Variance inflation factor for rad: {VIF_rad:.4f}")
```

:::
::::
:::

:::{.callout-caution}
When calculating variance inflation factors this way, make sure that the response
variable does not appear on the right hand side of the model formula. The 
expression `lm(rad ~ .)` in `R` would include the target variable `medv` on the 
right hand side. Variance inflation factors capture relationships among the inputs 
and are not related to the target variable.
:::

Notice that a variance inflation factor **does not** depend on $Y$. It is
solely based on relationships among the inputs (predictors). Also, you can 
see from the model equation above that the VIF discovers more than a pairwise
dependence on other variables. It models one input as a function of all other
inputs.

:::{.example}
::::{.example-header}
Example: Boston Housing Values (Cont'd)
::::
::::{.example-container}
:::{.panel-tabset group="language"}

## R

To compute variance inflation factors in `R` directly, use the `vif` function 
in the `car` package (Companion to Applied Regression).

``` {r VIF, warning=FALSE, message=FALSE}
library(car)
vif(fit)
```

The VIF for `rad` reported by `vif()` matches the previous calculation.

## Python

To compute variance inflation factors in Python directly, use the 
`variance_inflation_factor` function in `statsmodels`.

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import numpy as np

X = fit.model.exog 

vif_data = pd.DataFrame()
vif_data["Variable"] = fit.model.exog_names
vif_data["VIF"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]

print(vif_data)
```

:::
::::
:::

Another way to compute the variance inflation factors is to fit the linear 
regression with a scaled and centered $\bX^*$ matrix. The columns of $\bX^*$ are 
centered at their sample mean and are scaled by dividing by $\sqrt{n-1}$ times 
their standard error. As a result, the $\bX^{*\prime} \bX^*$ matrix is the matrix 
of the empirical pairwise correlations of the inputs. The regression coefficients 
of this model are called the **standardized** coefficients ($\beta^*_j$) and the 
variance inflation factors are 
$$
\text{VIF}_j = \widehat{\text{Var}}[\widehat{\beta}^*_j] / \widehat{\sigma}^2 
$$

:::{.example}
::::{.example-header}
Example: Variance Inflation Factors from Scaled-Centered Regression; Boston Data (Cont'd)
::::
::::{.example-container}
To compute the VIFs in the model for the Boston data, we compute first the centered-and-scaled 
$\bX$ matrix. The `scale()` function in `R` centers and scales the data by default
but uses the standard deviation as the scaling factor. We use a custom scaling so 
that the $\bX^{*\prime}\bX^*$ matrix equals the empirical correlation matrix.

We use `model.matrix()` to extract the $\bX$ matrix from the model object computed 
earlier. The intercept column is replaced with the target values so we can use 
this matrix as input to a call to `lm`.

:::{.panel-tabset group="language"}

## R

```{r}
X <- model.matrix(fit)
n_1 <- dim(X)[1] - 1
scaled_X <- scale(X,
                  center=apply(X,2,mean),
                  scale =apply(X,2,sd)*sqrt(n_1))
scaled_X[,1] <- Boston$medv
colnames(scaled_X)[1] <- "medv"
ll <- lm(medv ~ ., data=data.frame(scaled_X))
summary(ll)
```

The variance inflation factors are obtained by dividing the square values in 
the `Std. Error` column with the estimator of the residual variance

```{r}
s <- summary(ll)
vif <- s$coefficients[,2]^2 / s$sigma^2
vif
```

## Python

```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm

X = fit.model.exog 

n_1 = X.shape[0] - 1

# Scale the design matrix
scaled_X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0, ddof=1) * np.sqrt(n_1))
scaled_X[:, 0] = Boston['medv']

# Create a DataFrame with column names
scaled_X_df = pd.DataFrame(scaled_X)
scaled_X_df.columns = fit.model.exog_names
scaled_X_df.rename(columns={scaled_X_df.columns[0]: 'medv'}, inplace=True)

# Fit linear model with standardized variables
formula = 'medv ~ ' + ' + '.join(scaled_X_df.columns[1:])
ll = sm.formula.ols(formula=formula, data=scaled_X_df).fit()

print(ll.summary())
```

The variance inflation factors are obtained by dividing the square values for 
the standard errors with the estimator of the residual variance

```{python}
s_coefficients = ll.bse
s_sigma = ll.scale ** 0.5

vif = s_coefficients[1:] ** 2 / s_sigma ** 2

print(vif)
```

:::

::::
:::

The smallest possible VIF value is 1.0, it indicates that the input is not linearly
related to the other variables. The thresholds are as follows

- 1 < VIF < 10: moderate collinearity
- 10 < VIF < 30: moderate to severe collinearity
- VIF > 30: severe collinearity problem


#### Condition index and condition number

A formal diagnostic for multicollinearity, based on the eigenvalue decomposition 
of the (scaled-and-centered) $\bX^*$ matrix, examines the spread of the eigenvalues 
of \bX^{*\prime}\bX$. If $\bX^*$ is a centered and scaled version of $\bX$ such 
that $\bX^{*\prime}\bX$ is the empirical correlation matrix, its eigen decomposition 
is
$$
    \bX^{*\prime}\bX = \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^\prime
$$
where $\textbf{Q}$ is a $(p+p)$ orthogonal matrix of eigenvectors and $\boldsymbol{\Lambda}$ 
is a diagonal matrix with the eigenvalues $\lambda_j$ on its diagonal. The number 
of eigenvalues close to zero indicates the number of near linear dependencies in 
$\bX$ (or $\bX^*$). In this centered-and-scaled form the eigenvalues satisfy
$\sum_{j=1}^p \lambda_j = p$, so if some eigenvalues get small, others need to 
get bigger. 

The **condition index** associated with the $j$^th^ eigenvalue is 
$$
\phi_j = \frac{\max(\lambda_j)}{\lambda_j}
$$
and the **condition number** is $\max(\phi_j)$.

:::{.panel-tabset group="language"}

## R

```{r}
library(Matrix)

X <- model.matrix(fit)[,2:12]
X_star <- scale(X,
                center=apply(X,2,mean),
                scale=apply(X,2,sd)*sqrt((n_1)))
XpX_scaled <- t(X_star) %*% X_star
eigen_decomp <- eigen(XpX_scaled)
evals <- eigen_decomp$values

evals

cat("Sum of eigenvalues: ", sum(evals),"\n")

cond_index <- max(evals)/evals
cat("Condition indices: ", cond_index,"\n")

cond_number <- max(cond_index)
cat("Condition number: ", cond_number,"\n")
```

## Python

```{python}
import numpy as np
import pandas as pd
from scipy import linalg

X = fit.model.exog[:, 1:12] 

n_1 = X.shape[0] - 1

# Scale the design matrix
X_means = np.mean(X, axis=0)
X_sds = np.std(X, axis=0, ddof=1) * np.sqrt(n_1)
X_star = (X - X_means) / X_sds

XpX_scaled = X_star.T @ X_star

# Perform eigendecomposition
evals, evecs = linalg.eigh(XpX_scaled)

print("Eigenvalues:", evals)
print("Sum of eigenvalues: ", sum(evals))

cond_index = max(evals) / evals
print("Condition indices: ", cond_index)

cond_number = max(cond_index)
print(f"Condition number: {cond_number:.4f}")
```
:::


Condition indices larger than 900 indicate that near linear dependencies exist.

:::{.callout-caution}
In contrast to variance inflation factors, where the $j$^th^ factor is associated 
with the $j$^th^ input, the eigenvalue $\lambda_j$ is not associated with a 
particular input. It is associated with a linear combination of all the $X$s.
:::


An obvious remedy of the multicollinearity is to remove inputs that are associated 
with high variance inflation factors and to refit the model. If you cannot remove 
the variables from the model a different estimation method is called for. Regularization 
methods such as Ridge regression or Lasso regression handle high-dimensional
problems and reduce the instability of the least-squares estimates by shrinking 
their values (suppressing the high variability of the coefficients). At the cost 
of introducing some bias, these estimators drastically reduce variability for an 
overall better mean square prediction error.


