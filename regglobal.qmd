::: content-hidden
$$
{{< include latexmacros.tex >}}
$$
:::

# The Classical Linear Model {#sec-reg-linear}

The classical (standard) linear model is $$
\bY = \bX\bbeta + \bepsilon, \quad \quad \bepsilon \sim (\bzero,\sigma^2\bI)
$$ $\bY$ is an $(n \times 1)$ vector of target values, $\bX$ is an $(n \times (p+1))$ matrix of $p$ input variables plus an intercept, and $\bepsilon$ is an $(n \times 1)$ random vector whose elements have mean zero and the same variance $\sigma^2$. If $\bX$ is of full rank, the OLS estimator $$
\widehat{\bbeta} = \left(\bXpX^{-1}\right) \bX^\prime\bY
$$ is the unbiased estimator with smallest variance. Predicted values and fitted residuals are given by

\begin{align*}
    \widehat{\by} &= \bX\widehat{\bbeta} = \bH\by \\
    \widehat{\bepsilon} = \by - \widehat{\by} &= (\bI - \bH)\by
\end{align*}

where $\bH$ is the "Hat" matrix $\bH = \bX(\bXpX)^{-1}\bX^\prime$.

The OLS estimator has variance $$
\Var[\widehat{\bbeta}] = \sigma^2 (\bXpX)^{-1}
$$ We saw in @sec-idempotent that $\bH$ is an orthogonal **projection** matrix--it is symmetric and idempotent. Because $\bH$ is a projection matrix, $\bI - \bH$ is also a projection matrix. To emphasize this fact we can write $\by$ as the sum of two components $$
\by = \bH\by + (\bI - \bH)\by
$$ This identity simply states that the target variable is projected onto two spaces: the space generated by the columns of $\bX$, represented by the projection matrix $\bH$, and its complement (residual) space. Furthermore, these projections are orthogonal: $$
\bH (\bI - \bH) = \bH - \bH\bH = \bH - \bH = \bzero
$$

Projection properties are useful in establishing properties of fitted values and residuals.

\begin{align*}
    \bX^\prime \widehat{\bepsilon} &= \bzero \\
    \Var[\widehat{\by}] &= \Var[\bH\by] = \sigma^2\bH \\
    \Var[\widehat{y}_i] &= \sigma^2 h_{ii} \\
    \Var[\widehat{\bepsilon}] &= \Var[(\bI-\bH)\by] = \sigma^2(\bI - \bH) \\
    \Var[\widehat{\epsilon}_i] &= \sigma^2(1-h_{ii})
\end{align*}

Several interesting facts can be gleaned from these expressions.

1.  The first result, that the fitted residuals are orthogonal to the inputs, follows from the orthogonality of least squares, but can be shown easily using projection properties $$
    \bX^\prime\widehat{\bepsilon} = \bX^\prime(\bI-\bH)\by = (\bX^\prime - \bX^\prime\bH)\bY = (\bX^\prime-\bX^\prime)\by = \bzero
    $$ The orthogonality of inputs and residuals implies that for any column in $\bX$ $$
    \sum_{i=1}^n \, x_{ij}\widehat{\epsilon}_i = 0, \quad j=1,\cdots, p
    $$ This is also true for the intercept column, so that $\sum_{i=1}^n \widehat{\epsilon}_i = 0$; the fitted residuals sum to zero.

2.  The variance of the $i$^th^ fitted value depends on the irreducible variability $\sigma^2$ and on the $i$^th^ diagonal element of the Hat matrix, $h_{ii}$. The Hat matrix in turn depends only on the input variables. In other words, the variability of the fitted values does not depend on any of the target values in the data set.

3.  The variance of a fitted residual is $\sigma^2(1-h_{ii})$. Since $\sigma^2$ is the variance of the model errors (the irreducible variance), the variability of the residuals is smaller if $h_{ii} > 0$. The leverages $h_{ii}$ cannot be larger than 1, otherwise $\sigma^2(1-h_{ii})$ is not a valid variance. In fact, the leverages are bounded $1/n \le h_{ii} \le 1$.

## Simple and Multiple Linear Regression

The SLR and MLR models are examples of the classical linear model. In the SLR case there is a single input variable $$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$ in the MLR case there are multiple input variables. These can be distinct variables, or transformations and/or combinations of variables. For example, $$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \epsilon_i
$$ has three inputs formed from the main effects of $x_1$ and $x_2$ and their interaction $x_1x_2$. More on the use and interpretation of interactions below.

### Computing Estimates

::: example
::: example-header
Example: Auto Data from ISLR
:::

::: example-container
The `Auto` data is a data set used in @James2013_ISLR2 (ISLR2). It comprises information on 397 automobiles, such as mileage (mpg), horsepower, number of cylinders, engine displacement (cu. inches), weight (lbs), etc. The following statements load the data set from DuckDB into a Pandas dataframe, drop records with missing values, and display the first observations.

```{python}
import pandas as pd
import duckdb 

con = duckdb.connect(database="ads.ddb")
auto = con.sql("SELECT * FROM auto").df().dropna()
con.close()

auto.head()
```

Suppose we want to develop a model that predicts `mpg` from other variables. A multiple linear regression model with inputs `cylinders`, `displacement`, `weight` and `horsepower` is fit in Python with `scikit-learn` (`sklearn`) as follows.

```{python}
from sklearn import linear_model

auto_x = auto[['cylinders', 'displacement', 'weight', 'horsepower']]
auto_y = auto['mpg']

# Create linear regression object
regr = linear_model.LinearRegression()

# Fit the model, the intercept is added by default 
regr.fit(auto_x, auto_y)

y_hat = regr.predict(auto_x)
residuals = auto_y - y_hat

# The coefficients
print("Intercept : \n", regr.intercept_)
print(regr.feature_names_in_)
print("Coefficients of inputs: \n", regr.coef_)
print("Coefficient of determination: %.2f" % regr.score(auto_x,auto_y))
```

`auto_x` and `auto_y` are the $x$-matrix and the $y$ vector for the regression model. Note that an intercept is added automatically by the model training routine, so it is not provided in `auto_x`. After a linear regression object is created, the `fit` method is called to compute the OLS estimates. The `predict` method computes $\widehat{\by}$ and the result is used to compute the vector of residuals, $\widehat{\bepsilon}$.

The remaining statements print the intercept, the regression coefficients for the input variables and the coefficient of determination, also known as the $R^2$ statistic.

The regression model fit with the preceding statements is $$
\text{mpg}_i = \beta_0 + \beta_1\text{cylinders}_i+\beta_2\text{displacement}_i+\beta_3\text{weight}_i+\beta_4\text{horsepower}_i + \epsilon_{i}
$$ and it is assumed that $\epsilon_i \sim iid (0,\sigma^2)$. The OLS estimates are

-   $\widehat{\beta}_0 = 45.756770$

-   $\widehat{\beta}_1 = -0.393285$

-   $\widehat{\beta}_2 = 0.0001388$

-   $\widehat{\beta}_3 = -0.005277$

-   $\widehat{\beta}_4 = -0.0428124$

The predicted miles per gallon of an automobile for which the data frame is representative, is 

\begin{align*}
\widehat{\text{mpg}} = 45.75677 &-0.393285\times\text{cylinders} + 0.0001388\times\text{displacement} \\
 &-0.005277\times \text{weight} -0.0428124\times\text{horsepower}
\end{align*}
:::
:::

`scikit-learn` computes the OLS estimates but does not provide standard errors or an estimate of $\sigma^2$. These estimates of variability are necessary to compute $p$-values, confidence and prediction intervals, to test hypotheses, etc. The lack of uncertainty quantification is rooted in the algorithmic modeling approach that does not appeal to an underlying stochastic data-generating mechanism.

The estimate of $\sigma^2$ and the standard errors of the regression coefficients can be calculated the hard way, by performing the matrix math. First, we augment the matrix of input variables with a column of 1s to reflect the intercept. The estimate of $\sigma^2$ is based on the error (residual) sum of squares $$
\widehat{\sigma}^2 = \frac{\text{SSE}}{n-(p+1)} = \frac{1}{n-(p+1)}\sum_{i=1}^n\,\widehat{\epsilon}_i^2
$$

```{python}
import numpy as np

n = len(auto_x)
p = len(auto_x.columns)

X1 = np.empty(shape=(n, p+1), dtype=float)
X1[:, 0] = 1
X1[:, 1:(p+1)] = auto_x

# recompute OLS estimate
beta_hat = np.linalg.inv(X1.T @ X1) @ X1.T @ auto_y
print(np.round(beta_hat,6))

# Sum of squares error = residual sum of squares
SSE = residuals.T @ residuals
sigma2_hat = SSE / (n - (p+1))
print(f"Estimate of sigma^2: {sigma2_hat}")
var_beta_hat = np.linalg.inv(X1.T @ X1) * sigma2_hat
for p_ in range(p+1):
    standard_error = np.sqrt(var_beta_hat[p_, p_])
    print(f"beta_hat[{p_}]: {round(beta_hat[p_],6)}  std. error: {round(standard_error,6)}")
```

Using `R`, working with linear regression models is much simpler. Most `R` functions for statistical modeling support a formula expression to specify models directly based on information in data frames. You do not have to set up separate objects for $\bX$ and $\by$. Also, `R` provides standard errors, $t$-statistics, $p$-values, and other estimates by default, and has default methods for handling missing values.

```{r}
library(ISLR2)

head(Auto)

linreg <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)
summary(linreg)

```

The `Estimate` column of the `lm` summary reports the $\widehat{\beta}_k$ estimates, the `Std. Error` column reports their standard errors. These values match the Python computations above.

The residual standard error of `{r} summary(linreg)$sigma` is $\widehat{\sigma}$, the square root of the estimator $\widehat{\sigma}^2$ derived above.

### Coefficient Interpretation

How do we interpret the regression coefficients of the fitted model

\begin{align*}
\widehat{\text{mpg}} = 45.75677 &-0.393285\times\text{cylinders} + 0.0001388\times\text{displacement} \\
 &-0.005277\times \text{weight} -0.0428124\times\text{horsepower}
\end{align*}

Since the model is linear, it is tempting to state that, for example, a change in 1 unit of `displacement` causes a change of 0.0001388 in miles per gallon. This interpretation is not correct, because

1.  We cannot conclude causality between inputs and the target variable. The data are purely observational so we can at best state that changes in the input variables are **associated** with different predicted values for miles per gallon.

2.  We cannot interpret one input variable in the absence of the others. The signs of the regression coefficients are somewhat counter-intuitive. Why would mileage go down for cars with more cylinders but go up with greater displacement. Does adding cylinders not imply a larger engine displacement? The point is that the inputs are related to each other, they do not vary freely from each other. When we interpret the magnitude of a regression coefficient in terms of the change in the target variable that corresponds to a unit change in the input variable, we are implicitly holding all other predictors fixed (ceteris paribus).

The correct interpretation of the displacement coefficient is thus *when displacement increases by one cubic inch and all other attributes remain constant, the expected mileage increases by 0.0001388 miles per gallon*.

::: callout-note
The *all-other-variables-held-fixed* interpretation is also important when interpreting hypothesis test results. The $p$-values of variables `cylinders` and `displacement` are 0.33 and 0.98, respectively, suggesting that these variables do not make significant contributions toward explaining miles per gallon. These $p$-values are interpreted in the presence of the other variables in the model. The correct interpretation is that the number of cylinders is not a significant predictor of miles per gallon in a model that accounts for engine displacement, vehicle weight, and horsepower.
:::

### $R^2$, the Coefficient of Determination

The variability in the target $\by$, not accounting for any information provided by the input variables can be estimated as $$
    s^2 = \frac{1}{n-1}\sum_{i=1}^n (y_i - \overline{y})^2
$$ If the $y_i$ had the same mean, this would be an unbiased estimator of $\Var[Y]$. However, the regression model states very clearly that the mean of $Y$ is a function of the $x$-inputs. This estimator is then a biased estimator of $\Var[Y] = \sigma^2$. The numerator of $s^2$ is called the **total sum of squares** (SST). If SST captures variability of $Y$ about a constant mean, how much of this is attributable to the input variables? To answer this we can look at the variability **not** attributable to the $x$s, the error sum of squares $$
\text{SSE} = \sum_{i=1}^n \widehat{\epsilon}_i = \sum_{i=1}^n (y_i - \widehat{y}_i)^2
$$ The ratio $$
R^2 = \frac{\text{SST}-\text{SSE}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}
$$ is known as the **coefficient of determination** or **R-square**. The name R-square comes from a simple relationship between $R^2$ and the Pearson correlation coefficient in the SLR case. If $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, then $R^2$ is the square of the correlation coefficient between $X$ and $Y$:

::: example
::: example-header
Example: Auto Data (Cont'd)
:::

::: example-container
```{r}
slr <- lm(mpg ~ horsepower, data=Auto)
summary(slr)$r.squared

cor(Auto$mpg, Auto$horsepower)^2
```

The squared correlation coefficient between `mpg` and `horsepower` is the same as the $R^2$ statistic reported by `lm`.

You can compute the correlation coefficient between the two variables from simple linear regression output, but you need to take into account the sign of the regression coefficient. The correlation coefficient has the same sign as $\beta_1$.

```{r}
cor(Auto$mpg, Auto$horsepower)

as.numeric(sign(slr$coefficients[2])) * sqrt(summary(slr)$r.squared)
```
:::
:::

$R^2$ ranges between 0 and 1; it achieves the lower bound $R^2=0$ if SSE = SST, the input variables do not explain any variability in $Y$. $R^2 = 1$ results when SSE = 0, the model fits the data "perfectly", it interpolates the $y_i$.

The straightforward interpretation of $R^2$ as the proportion of variability explained by the input variables unfortunately can lead one to chase models that have a high $R^2$. This is a terrible practice for a number of reasons

-   The value of $R^2$ does not tell us whether the model assumptions are met. You can explain a substantial amount of variability in the data with a seriously deficient model. For example, the four regressor model fit to the Auto data earlier has $R^2 = 0.71$, explaining 71% of the variability in miles per gallon. A look at the residuals from that model shows that there is substantial trends in the residuals (@fig-auto-residuals). Larger fitted values have larger variability and there is a definite trend in the residuals; the model is not (yet) correct.

```{r, echo=FALSE, out.width="75%", fig.align='center'}
#| label: fig-auto-residuals
#| lightbox:
 
yseq <- seq(5,35,0.5)
df <- data.frame(r=rstudent(linreg),x=linreg$fitted.values)
lo <- loess(r ~ x, data=df)
plot(linreg$fitted.values,rstudent(linreg))
lines(x=yseq, y=predict(lo,newdata=yseq),col="red")
```

Here is an example with simulated data where the mean function is not a straight line and the error variance depends on $x$.
A simple linear regression model is clearly not appropriate, but it explains more than 75\% of the variability in $y$.

```{r, fig.align="center", out.width="75%"}
x <- seq(0.15, 1, l = 100)
set.seed(123456)
eps <- rnorm(n = 100, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25 * sin(4 * pi * x)) + eps
slr <- lm(y ~ x)

plot(x,y,type="p",col="red")
abline(slr$coefficients)

summary(slr)$r.squared
```

-   $R^2$ is a function of SSE, the prediction error on the training data set. This can be made arbitrarily small by adding input variables. If $R^2_{\text{cur}}$ is the coefficient of determination in a linear regression model, and you add a new predictor $x_{p+1}$, then $$
    R^2_{\text{new}} \ge R^2_{\text{cur}}
    $$ Predictors that make no relevant contribution will increase $R^2$ because their addition reduces SSE.
    
- A model that interpolates the data (fits "perfectly") is not a good model. It is certainly not perfect if the goal is to build a model that generalizes well to unseen observations. Different metrics are needed to develop models that generalize and do not overfit the data.
Chasing $R^2$ values invariably leads to **overfitting** and models that memorize too much of the training observations to perform well on new data.

### Measuring Prediction Error

Since we can make SSE on the training data arbitrarily small by adding more input variables, the MSPE on the training data is not a good metric if we want a model that performs well in predicting new observations. Instead of MSE~Tr~ the test error MSE~Te~ should be used. As discussed in @sec-train-test-validate, the test error can be estimated by holding out some observations from training in a test data set or by cross-validation (@sec-cross-validation). 

In the classical linear model leave-one-out cross-validation is particularly appealing. In addition to not depending on any random selection of data points as test data sets or $k$-fold cross-validation do, LOOCV error can be calculated in the linear model without re-fitting the model $n$ times. All the necessary pieces to compute the LOOCV error can be assembled on the same pass through the data that calculates the OLS estimates. The key is the Sherman, Morrison, Woodbury formula.

#### Sherman, Morrison, Woodbury Formula

This remarkable formula is at the heart of many regression-type diagnostics and cross-validation techniques. 
A version of this formula was first given by Gauss in 1821. 
Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.

Suppose we are in a full-rank linear modeling context with design matrix $\bX_{(n \times p + 1)}$, so that the inverse $\left( \bXpX \right)^{-1}$ exists. In diagnosing the quality of a model, we are interested in measuring the prediction error for the $i$^th^ observation as if the data point had not contributed to the analysis. This is an example of a leave-one-out estimate: remove an observation from the data, redo the analysis, and measure how well the quantity of interest can be computed for the withheld observation.

If you do this in turn for all $n$ observations, you must fit the model $n + 1$ times, an overall fit to the training data with $n$ observations, and $n$ additional fits with training data sets of size $n - 1$, leaving out each observation in turn. The computationally expensive part of fitting the linear model is building the cross-product matrix $\bXpX$ and computing its inverse $\left( \bXpX \right)^{-1}$.

The Sherman-Morrison-Woodbury formula allows us to compute the inverse of the cross-product matrix based on $\left( \bXpX \right)^{- 1}$ as if the $i$^th^ observation had been removed.

Denote as $\bX_{-i}$ the design matrix with the $i$^th^ observation removed. Then

$$\left( \bX_{-i}^\prime\bX_{-i} \right)^{- 1} = \left( \bXpX - \bx_{i}\bx_{i}^{\prime} \right)^{-1}\  = \left( \bXpX \right)^{- 1} + \frac{\left( \bXpX \right)^{-1}{\bx_{i}\bx_{i}^{\prime}\left( \bXpX \right)}^{- 1}}{1 - \bx_{i}^{\prime}\left( \bXpX \right)^{-1}\bx_{i}}$$

The quantities on the right hand side are available in the standard regression calculations based on $n$ data points.
Because of this remarkable result, leave-one-out statistics can be calculated easily---without retraining any models---based on the fit to the full training data alone. 

::: callout-note
Note that the quantity in the denominator of the right-hand side is the diagonal value of $\bI - \bH$, where $\bH$ is the hat matrix. If $h_{ii}$ denotes the diagonal values of $\bH$, we can write the update formula as

$$\left( \bXpX \right)^{- 1} + \frac{\left( \bXpX \right)^{- 1}{\bx_{i}\bx_{i}^{\prime}\left( \bXpX \right)}^{- 1}}{1 - h_{ii}}$$

The **leverage** values $h_{ii}$ play an important role in the computation of residual, influence, and case-deletion diagnostics in linear models.
:::


#### PRESS Statistic

If we denote the predicted value of $y_i$, obtained in a regression without the $i$^th^ observation, as $\widehat{y}_{-i}$, then the leave-one-out residual
$$
y_i - \widehat{y}_{-i}
$$
is the test error for the $i$^th^ observation. Using the Sherman-Morrison-Woodbury result, it is a neat exercise to show that this is simply
$$
y_i - \widehat{y}_{-i} = \frac{y_i - \widehat{y}_i}{1-h_{ii}}
$$
The leave-one-out error for the $i$^th^ observation is obtained by dividing the $i$^th^ residual by one minus the leverage value. When these deviations are squared and summed across the entire data set the PRESS statistic results
$$
\text{PRESS} = \sum_{i=1}^n (y_i - \widehat{y}_{-i})^2 = \sum_{i=1}^n \left(\frac{y_i - \widehat{y}_i}{1-h_{ii}}\right)^2
$$
The name is derived from **prediction sum of squares**. The average PRESS value estimates the mean square test error
$$
\text{MSE}_{Te} = \frac{1}{n}\text{PRESS}
$$

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
For the four regressor model the PRESS statistic and the MSE~Te~ can be calculated by extracting the leverage
values
```{r}

mlr <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)

leverage <- hatvalues(mlr)
PRESS_res <- mlr$residuals / (1-leverage)
PRESS <- sum(PRESS_res^2)
MSE_Te  <- PRESS/length(leverage)
cat("PRESS statistic: ", PRESS, "\n")
cat("MSE Test based on LOOCV: ", MSE_Te,"\n")
```

You can validate this result the hard way by fitting $n$ separate regression models, leaving one observation out each time and predicting that observation to obtain the PRESS residual.

```{r}
PRESS <- 0
for (i in 1:nrow(Auto)) {
    m <- lm(mpg ~ cylinders + displacement + weight + horsepower, 
            data=Auto[-i,])
    yhat_minus_i <- predict(m,newdata=Auto[i,])
    PRESS <- PRESS + (Auto[i,"mpg"] - yhat_minus_i)^2
}
cat("MSE Test based on LOOCV: ", PRESS/nrow(Auto))
```

::::
:::

### Interactions

What is the difference between the following models

\begin{align*}
Y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\\
Y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon
\end{align*}

Both models depend on $x_1$ and $x_2$. In the first case the variables enter the model as **main effects**, that is, by themselves. Each input variable is allowed to make its contribution on the outcome given the presence of the other. The second model contains the additional **interaction** term $x_1 x_2$. To be more precise, this is a **two-way** interaction term because it involves two input variables. A three-way interaction term would be $x_1 x_2 x_3$.

Suppose that $\beta_3$ is not zero, how should we interpret the presence of an interaction term in the model? We can no longer state that $\beta_1$ measures the effect on the target variable when $x_1$ is changed by one unit. The effect of changing $x_1$ by one unit in the second model is now a function of $x_2$. To see this, consider the mean of $Y$ at two points, $x_1$ and $x_1+1$.

\begin{align*}
    \Exp[Y | x_1] &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 \\
    \Exp[Y | x_1 + 1] &= \beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \beta_3 (x_1+1)x_2
\end{align*}

The difference between the two is 
$$
\Exp[Y | x_1 + 1] - \Exp[Y | x_1] = \beta_1 + \beta_3 x_2
$$
The effect of $x_1$ is now a function of $x_2$. That is the very meaning of an interaction. The effect of one variable (or factor) depends on another variable (or factor) and vice versa. In the example the effect of $x_1$ is a linear regression in $x_2$.

::: callout-tip
When building models with interactions, it is customary to include lower-order effects in the model if higher-order effects are significant. For example, if $x_1 x_2$ is in the model one includes the main effects $x_1$ and $x_2$ regardless of their significance. Similarly, if a three-way interaction is significant one includes the two-way interactions and the main effects in the model. The argument for doing so is that in order for two things to interact they must be present--otherwise, what interacts?
:::

The presence/absence of an interaction between a categorical input variable and a numeric input variable can be seen by comparing the trends as in  @fig-interactions. With a two-level categorical factor, representing for example, a treatment and a placebo, the absence of interactions manifests itself in parallel lines. The effect of the treatment is the distance between the two lines and is the same for all values of $x_1$. Similarly, the effect of $x_1$, the slope of the line, is the same for both groups. In the presence of an interaction the slopes are not the same and the distance between the lines (the effect of $x_2$) depends on the value for $x_1$.


![Models with a categorical and a continuous input with and without interactions.](images/Interactions.png){#fig-interactions .lightbox fig-align="center" width="75%"}

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
We are now considering a series of model for the `Auto` data, based on the same four input variables used earlier.

The first four models add inputs and also two-way interactions of all inputs in the model.
The formula expression `y ~ (x1 + x2 + x3)^2` is a shorthand for including the main effects and two-way interactions of the three inputs. `R` displays the interaction terms as `x1:x2`, `x1:x3`, and `x2:x3` in the output.

Models 5 and 6 then add up to three-way and four-way interactions, respectively. For each model we calculate the number of non-zero coefficients (the rank of $\bX$), SSE, $R^2$ and the PRESS statistic.

```{r}
calcPress <- function(linModel) {
    leverage <- hatvalues(linModel)
    r <- linModel$residuals
    Press_res <- r/(1-leverage)
    Press <- sum(Press_res^2)
    return(list("ncoef"=linModel$rank,
                "R2"   =summary(linModel)$r.squared, 
                "SSE"  =sum(r^2),
                "Press"=Press))
}

l1 <- lm(mpg ~ cylinders, data=Auto)
l2 <- lm(mpg ~ (cylinders + displacement)^2, data=Auto )
l3 <- lm(mpg ~ (cylinders + displacement + horsepower)^2, data=Auto )
l4 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^2, data=Auto )
l5 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^3, data=Auto )
l6 <- lm(mpg ~ (cylinders + displacement + horsepower + weight)^4, data=Auto )

df <- rbind(as.data.frame(calcPress(l1)), 
            as.data.frame(calcPress(l2)),
            as.data.frame(calcPress(l3)),
            as.data.frame(calcPress(l4)),
            as.data.frame(calcPress(l5)),
            as.data.frame(calcPress(l6))
      )
knitr::kable(df,format="simple")

```

The model complexity increases from the first to the sixth model; the models have more parameters and more intricate interaction terms. The SSE  values decrease as terms are added to the model, and $R^2$ increases accordingly. The PRESS statistic is always larger than the SSE, which makes sense because it is based on squaring the Press residuals $(y_i - \widehat{y}_i)/(1-h_{ii})$ which are larger than the ordinary residuals $y_i - \widehat{y}_i$. 

From the fifth to the sixth model only one additional parameter is added to the model, the four-way interaction of all inputs. $R^2$ barely increases but the PRESS statistic increases compared to the model with only three-way interaction. Interestingly, none of the effects in the four-way model are significant given the presence of other terms in the model.

```{r}
summary(l6)
```
::::
:::

## Hypothesis Testing {#sec-ss-reduction-test}

When testing hypothesis in statistical models it is useful to think of the hypothesis as imposing a **constraint** on the model. The test then boils down to comparing a constrained and an unconstrained model in such a way that we can make probability statements about the validity of the constraint. If it is highly unlikely that the constraint holds, we reject the hypothesis.

This principle applies to hypothesis testing in many model families, what differs is how the impact of the constraint on the model is measured. In least-squares estimation we look at how a sum of squares changes as the constraint is imposed. In models fit by maximum likelihood we measure how much the log likelihood changes when the constraint is imposed.

Suppose we have a model with four predictors, 
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
$$
and want to test the hypothesis that the absence of $x_3$ and $x_4$ does not make the model worse. The constraint we impose on the model is
$$
H: \beta_3 = \beta_4 = 0
$$
This is a hypothesis with two degrees of freedom, since two parameters of the model are constrained simultaneously.

::: callout-tip
You can usually figure out the degrees of freedom in a hypothesis by counting equal signs.
:::

The unconstrained and constrained models are also called the **full** and the **reduced** models, respectively. In this case the full model is
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
$$
and the reduced model is
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \epsilon
$$
:::{.definition}
::::{.definition-header}
Definition: Nested Models
::::
::::{.definition-container}
Two models are said to be **nested** when one model can be derived from the other by imposing constraints on the model parameters. The full and reduced models in the hypothesis testing context are nested models. On the contrary, if two models are not nested, the hypothesis testing framework described here does not apply.
::::
:::

If based on the data the hypothesis cannot be rejected, we conclude that the full model is not significantly improved over the reduced model. The strength of evidence in favor of the hypothesis depends on how much variability is accounted for in the model when the constraint is relaxed, relative to the overall variability in the system. To measure that we need to introduce the idea of partitioning sums of squares.

### Partitioning Variability through Sums of Squares

The difference between the total sum of squares, $\text{SST} = \sum_{i=1}^n(y_i - \overline{y})^2$, which does not depend on the inputs, and the error sum of squares $\text{SSE} = \sum_{i=1}^n (y_i - \widehat{y}_i)^2$, is the **model** sum of squares
$$
\text{SSM} = \sum_{i=1}^n\left(\widehat{y}_i-\overline{y}\right)^2 = \widehat{\bbeta}\bX^\prime\by - n\overline{y}^2
$$

The total and model sums of squares are also called the **corrected** total and model sums of squares because they adjust for an overall estimate of the mean of $Y$ if there are no inputs. In other words, they account for the intercept $\beta_0$ in the model; $\overline{y}$ is the estimate of $\beta_0$ in an intercept-only model.

Another way of looking at SSM is as a measure of the combined contribution of $\beta_1, \ldots, \beta_p$ beyond the intercept. The notation 
$$
\text{SSM} = SS(\beta_1, \cdots,\beta_p | \beta_0)
$$
makes this explicit. We can now think of other sum of squares, for example, $SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2)$ is the sum of squares contribution when $\beta_3$ and $\beta_4$ are added to a model that contains $\beta_0, \beta_1$, and $\beta_2$. Algebraically,
$$
SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) = SS(\beta_1, \beta_2, \beta_3, \beta_4 | \beta_0) - SS(\beta_1, \beta_2 | \beta_0)
$$
This will be one part of measuring the strength of hypothesis $H: \beta_3 = \beta_4 = 0$, the change in the model sum of squares between the full model with four inputs and the reduced model with two inputs However, there has to be more to it. If the data are very noisy, this change will have to be large to convince us of evidence against the hypothesis. If the data have small error variability, a smaller change in model sums of squares will suffice. This leads to considering the following test statistic:
$$
F_{obs} = \frac{SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) / 2}{\widehat{\sigma}^2}
$$
Notice that the sum of squares in the numerator is divided by the degrees of freedom of the hypothesis. To find an estimator for the variance in the denominator we rely on SSE in the larger of the two models, the unconstrained model, because it is more likely to be an unbiased estimator of $\sigma^2$.


::: {.definition} 
:::: {.definition-header}
Definition: Sum of Squares Reduction Test
::::
:::: {.definition-container}
Suppose that $\text{SSE}_f$ and $\text{SSE}_r$ are the error sum of squares in a full and reduced model where the reduced model is defined by a constraint $H$ with $q$ degrees of freedom imposed on the full model. The statistic
$$
F_{obs} = \frac{(\text{SSE}_r - \text{SSE}_f)/q}{\text{SSE}_f/\text{dfE}_f}
$$
follows an F distribution with $q$ numerator and $\text{dfE}_f$ denominator degrees of freedom if the model errors follow a Gaussian distribution, $\bepsilon \sim G(\bzero,\sigma^2\bI)$.
$\text{dfE}_f$ are the degrees of freedom associated with SSE in the full model, $n-r(\bX)_f$.
::::
:::

The sum of squares reduction test is very general and a very powerful tool to answer questions about the parameters in a linear model. However, in order to use any hypothesis testing framework that works with probability statements, a distributional assumption is required. We can always calculate $F_{obs}$ between two models. Computing $p$-values, that is, the probability $\Pr(F_{q,\text{dfE}_f} > F_{obs})$, is only valid if the data are normally distributed.

::: {.callout-caution}
If you reject the hypothesis $H$ based on a small $p$-value, you do not conclude that the full model is the *correct* model. You can say that there is significant evidence that the constraint can be relaxed. You might have compared a really bad model and a bad model. 
:::

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
To test the hypothesis that the coefficient for `weight` and `horsepower` are simultaneously zero in a model that accounts for `cylinders` and `displacement`, we can use the sum of squares reduction test.

```{r}
lm_full <- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)
lm_red  <- lm(mpg ~ cylinders + displacement                      , data=Auto)

SSE_full <- sum(lm_full$residuals^2)
SSE_red  <- sum(lm_red$residuals^2)
q <- lm_full$rank - lm_red$rank
sigma2_hat <- SSE_full / (lm_full$df.residual)

Fobs <- ((SSE_red-SSE_full)/q)/sigma2_hat
pvalue <- 1-pf(Fobs,q,lm_full$df.residual)

cat("SSE_r: ", SSE_red, "SSE_f: ", SSE_full, "\n")
cat("sigma2_hat: ", sigma2_hat, "\n")
cat("Fobs: ", Fobs, "Pr(F > Fobs): ", pvalue)
```

Removing `weight` and `horsepower` from the four-predictor model increases the error sum of squares from `{r} round(SSE_full,5)` to `{r} round(SSE_red,5)`. The F statistic for this reduction test is $F_{obs} =$ `{r} round(Fobs,4)` and the $p$-value is very small (`{r} pvalue`). We reject the hypothesis, the two-predictor model is not a sufficient model to explain mileage compared to the four-predictor model.

You can get to this result more quickly by using the `anova` function in `R`:

```{r}
anova(lm_red, lm_full)
```
::::
:::

### Sequential and Partial Sums of Squares

The sum of squares reduction test is helpful to understand the difference between two important special types of sum of squares, **sequential** and **partial** ones. How do we measure the contribution an individual predictor makes to the model
$$
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
$$
Sequential sums of squares measures the contribution of an input relative to the inputs that precede it in the model. Partial sums of squares measure the contribution relative to all other inputs in the model. For example, if the input of interest is $x_3$ in a four-regressor model, 
$$
SS(\beta_3 | \beta_0, \beta_1, \beta_2)
$$
is the sequential sum of squares for $x_3$ and 
$$
SS(\beta_3 | \beta_0, \beta_1, \beta_2, \beta_4)
$$
is the partial sum of squares. $\beta_4$ does not appear in the sequential sum of squares for $x_3$ because it appears after $\beta_3$ in the model formula. In other words, the sequential sum of squares for $x_3$ does not adjust for $x_4$ at all, while the partial sum of squares does. Clearly, the two types of sum of squares are not identical, unless the inputs are **orthogonal** (independent).

Sequential sum of squares have an appealing additive property, the overall model sum of squares can be accumulated from a series of sequential terms,

\begin{align*}
    \text{SSM} &= SS(\beta_1 | \beta_0)\\
    &+ SS(\beta_2 | \beta_0, \beta_1) \\
    &+ SS(\beta_3 | \beta_0, \beta_1, \beta_2) \\
    &+ \cdots \\
    &+ SS(\beta_p | \beta_0, \beta_1, \beta_2, \ldots, \beta_{p-1})
\end{align*}

Partial sum of squares do not add up to anything meaningful, unless the inputs are orthogonal. 

This raises an interesting question. If we divide a coefficient estimate by its standard error and compute a $p$-value from a $t$-distribution, what kind of hypothesis is being tested?

::: {.example}
:::: {.example-header}
Example: Auto Data (Cont'd)
::::
:::: {.example-container}
Here is the summary of the four-predictor model for `mpg`.

```{r}
summary(lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto))
```

How do we interpret the $p-$-value of 0.337513 for the `cylinders` variable or the $p$-value of 0.987709 for the `displacement` variable? A two-sided $t$-test is equivalent to an F test with 1 numerator degrees of freedom where $F_{obs}$ is the square of the $t_{obs}$ statistic. Let's first compute the sequential sum of squares tests and see if they match the $p$-values in the output.

```{r}
lm1 <- lm(mpg ~ cylinders, data=Auto)
lm2 <- lm(mpg ~ cylinders + displacement, data=Auto)
lm3 <- lm(mpg ~ cylinders + displacement + weight, data=Auto)
lm4 <- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)

anova(lm1, lm2, lm3, lm4)
```

The $p$-values are different from those shown in the `lm` output, so `Pr(>|t|) = 0.987709` cannot have a sequential interpretation (adding `displacement` to a model that contains `cylinders` only). The $p$-values in the `lm` summary have a **partial** interpretation as seen by performing reduction tests between the following reduced models and the full model with four predictors:

```{r}
lm_nocyl <- lm(mpg ~ displacement + weight + horsepower, data=Auto)
lm_nodis <- lm(mpg ~ cylinders + weight + horsepower, data=Auto)
lm_nowgt <- lm(mpg ~ cylinders + displacement + horsepower, data=Auto)
lm_nohp  <- lm(mpg ~ cylinders + displacement + weight, data=Auto)

anova(lm_nocyl,lm4)
anova(lm_nodis,lm4)
anova(lm_nowgt,lm4)
anova(lm_nohp ,lm4)

```
The $p$-values of the partial reduction F-tests are identical to those for the $t$-tests in the `lm` output. The $F_{obs}$ statistics are the squared values of the $t_{obs}$ statistics.

On the other hand, if you ask for the analysis of variance on the full model with the `aov` function, you get **sequential** tests of the inputs.

```{r}
summary(aov(lm4))
```

::::
:::

## Prediction

To obtain the fitted values in the linear model $\bY = \bX\bbeta + \bepsilon$ we apply the OLS estimates and get
$$
\widehat{\by} = \bX\widehat{\bbeta}
$$
To predict at a previously unobserved value of the inputs, a new observation $\bx_0$, we apply the regression equation and the OLS estimates to $\bx_0$:
$$
\widehat{\by}_0 = \bx_0^\prime \widehat{\bbeta}
$$

This seems simple, there must be a trick to it. The "trick" lies in the question: **what** is being predicted?

The target of the prediction (the **what**) can be $Y$, the target variable, or $\Exp[Y]$, the mean of the target variable. How are these different? $Y$ is a random variable and $\Exp[Y]$ is a constant; $\Var[Y] = \sigma^2$ and $\Var[\Exp[Y]] = 0$. The uncertainty in predicting an individual observation and predicting the mean fucntion will differ. The former has to take into account the inherent variability $\sigma^2$ in the data. 

How are the predicted values themselves different?

For illustration, cosnider an SLR model $Y = \beta_0 + \beta_1x + \epsilon$. To predict $Y$ at $X=x_0$ we choose the obvious expression 
$$
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0 + \widehat{\epsilon}
$$
substituting estimates for all unknowns on the right hand side. Since $\epsilon$ is a zero-mean random variable and cannot be observed directly, the best estimate is $\widehat{\epsilon} = 0$ which leads to 
$$
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0
$$

That is the same expression we use to predict $\Exp[Y] = \beta_0 + \beta_1 x_0$, substituting the OLS estimates for unknowns on the right hand side:
$$
\widehat{\Exp}[Y_0] = \widehat{\beta}_0 + \widehat{\beta}_1x_0
$$
And therein lies the crux. The predicted values for an observation and for the mean of an observation are the same. But their variability is not the same. We need to be very clear about what it is we are shooting for. More frequently one is interested in predicting observations, not averages of observations. In a study of health outcomes over time you might be more interested in predicting how a patient does at time $t$, rather than how the population of patients does on average at time $t$. Yet when folks see the different levels of confidence we have in the two predictions, they wish they could make predictions for the average.

### Prediction Variance

The variance of $\widehat{\Exp}[Y_0] = \bx_0^\prime\widehat{\bbeta}$ is straightforward and depends only on the variance of $\widehat{\bbeta}$:
$$
\Var{\widehat{\Exp}[Y_0]} = \sigma^2 \, \bx_0^\prime(\bXpX)^{-1}\bx_0
$$
To account for the variability in the data, the proper variance to consider when predicting an individual observation is 
$$
\Var[\widehat{Y}_0 - Y_0] = \Var[\widehat{Y}_0 - \epsilon] = \Var[\bx_0^\prime\widehat{\bbeta}-\epsilon] = \sigma^2\left(1+\bx_0^\prime(\bXpX)^{-1}\bx_0\right)
$$

The additional $1+$ does not seem like a big deal but has substantial numeric consequences.

### Confidence and Prediction Intervals

When the errors have a Gaussian distribution, the following random variables have $t$ distributions with $n-r(\bX)$ degrees of freedom:

\begin{align*}
    t &= \frac{\widehat{Y}_0 - \Exp[Y]_0}{\sqrt{\Var[\widehat{Y}_0]} \\
    t &= \frac{\widehat{Y}_0 - Y_0}{\sqrt{\Var[\widehat{Y}_0]-Y_0} \\
\end{align*}

The first is used to construct $(1-\alpha)$-level **confidence** intervals for the mean of the target
$$
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \bx_0^\prime(\bXpX)^{-1}\bx_0}
$$
The second is used to construct $(1-\alpha)$-level **prediction** intervals for an individual target
$$
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \left(1+ \bx_0^\prime(\bXpX)^{-1}\bx_0 \right)}
$$

:::{.example}
::::{.example-header}
Example: PISA OECD Study
::::
::::{.example-container}
PISA (Program for International Student Assessment) is an OECD study in 65 countries to evaluate the performance of 15-year old students in math, science, and reading. Among the questions the study is trying to address is whether the educational level in a country is influenced by economic wealth, and if so, to what extent.

```{r}
pisa <- read.csv(file="data/pisa.csv",stringsAsFactors=TRUE)
head(pisa)
```

The following statements compute the simple linear regression of `MathMean` on the log GDP and 95\% prediction and confidence intervals.

```{r}
pisa_slr <- lm(MathMean ~ logGDPp, data=pisa)

xvals <- data.frame(logGDPp=seq(from=6, to = 14, by=0.1))
p_pred <- data.frame(predict(pisa_slr, newdata=xvals, interval="prediction"))
p_conf <- data.frame(predict(pisa_slr, newdata=xvals, interval="confidence"))

head(p_pred)
head(p_conf)

```

Both types of intervals are most narrow near the center of the $x$ data range and widen toward the edges. A prediction outside of the hull of the data will be much less precise than a prediction near the center of the data. The additional variance term that distinguishes the variance of $\widehat{y}_0$ $\widehat{y}_0 - y_0$ has a considerable effect; the prediction intervals are much wider than the confidence intervals (@fig-pred-conf-intervals).


```{r, echo=FALSE, out.width="80%", fig.align='center'}
#| label: fig-pred-conf-intervals
#| fig.cap: 95% prediction and confidence intervals.
plot(pisa$logGDPp,
     pisa$MathMean,col="red", 
     ylab="MathMean", 
     xlab="log GdP",
     xlim=c(5,16),
     ylim=c(200,700),
     las=1,
     bty="l")

#abline(pisa_slr, lwd=2)
lines(xvals$logGDPp,predict(pisa_slr,newdata=xvals),lwd=2)
lines(xvals$logGDPp,p_pred$lwr,col="blue",lwd=1.5, lty="dotted")
lines(xvals$logGDPp,p_pred$upr,col="blue",lwd=1.5, lty="dotted")

lines(xvals$logGDPp,p_conf$lwr,col="darkgreen",lwd=1.5, lty="dashed")
lines(xvals$logGDPp,p_conf$upr,col="darkgreen",lwd=1.5, lty="dashed")
legend("topleft",legend=c("95% Prediction interval","95% Confidence interval"),
       lty=c("dotted","dashed"),
       lwd=1.5,
       col=c("blue","darkgreen"), cex=0.8)

```

::::
:::

## Diagnostics

Model diagnostics are useful to examine model assumptions and to decide whether the model
is an adequate description of the data. Questions we would like to answer through model diagnostics include

+ Are the assumptions of the model met? 

  + Linearity (zero mean errors, $\Exp[\bepsilon] = \bzero$)

  + Equal variance (homoscedasticity, $\Var[\bepsilon] = \sigma^2\bI$)

  + Independence of the errors $\Cov[\epsilon_i, \epsilon_j] = 0$
  
  + Can we assume that the errors are normally distributed?

+ How well does the model predict new observations? 

+ Do observations have undue influence on the results?

+ Does the relationship between the inputs negatively affect the analysis?

The three basic types of linear model diagnostics are 

1. **Residual** diagnostics to examine linearity, equal variance assumptions, and to detect outliers. Residual analysis relies on functions of $y_i - \widehat{y}_i$ to study the behavior of the unobservable $\epsilon_i$. 

2. **Case-deletion** diagnostics find data points that exert high **influence** on the analysis. These diagnostics ask how an aspect of the analysis (variability, predicted values, coefficient estimates, ...) changes if an observation is removed from the analysis. 

3. **Collinearity** diagnostics examine the relationships among the inputs and whether they impact the analysis in a negative way. The situations at the end of the extremes include completely orthogonal inputs ($\bXpX$ is a diagonal matrix) and inputs that are linear combinations of each other ($\bXpX$ is non-singular and a unique OLS solution does not exist). Most applications fall in-between unless one or more inputs are factors.

### Leverage

Linear model diagnostics depend on the leverage values $h_{ii}$, the diagonal values of the Hat matrix $\bH$. That is not surprising because the fitted values are linear combinations of the entries in the Hat matrix
$$
\widehat{\by} = \bH\by
$$
The $i$^th^ fitted value is a linear combination of the entries in the $i$^th^ row of $\bH$ with the elements of $\by$
$$
\widehat{y}_i = \bx_i^\prime\widehat{\bbeta} = \sum_{j=1}^n h_{ij}y_j
$$

From the last expression it is easy to establish that 
$$
\Var[\widehat{y}_i] = \sigma^2 \, \bx_i^\prime(\bXpX)^{-1}\bx_i = \sigma^2 h_{ii}
$$

We can think of the leverages $h_{ii}$ as standardized squared distance measures that tell us how far the $i$^th^ data point is from the center of the $x$-data. 

It is illustrative to look at the mean of the variances of the fitted values across the data set, 
$$
\frac{1}{n}\sum_{i=1}^n \Var[\widehat{y}_i] = \frac{1}{n} \sum_{i=1}^n \sigma^2h_{ii} = \sigma^2\left(\frac{p+1}{n}\right)
$$
The last result follows because $\bH$ is a projection matrix and thus $tr(\bH)$ equals its rank, $p+1$.

What does that result tell us about the precision of the estimated regression output? Suppose $p=4$ and $n=10$. The average variance is then $\sigma^2/2$. When $p=4$ and $n=1000$, the average variance is $\sigma^2/200$. As sample size increases, more precise estimates result as $p$ remains fixed. As $p$ increases for a given sample size, the average variance of the fitted values increases. In high-dimensional problems, where $p$ is large, OLS estimates have high variability and are unstable. Regularized estimation methods such as Ridge or Lasso regression can perform better in those circumstances.

Since $\sum_{i=1}^n h_{ii} = p+1$, the average leverage value in the data is $(p+1)/n$, and a good threshold for *high leverage* points is $h_{ii} > 2(p+1)/n$. This is not necessarily a problematic data point, it simply states that the point is an outlying point in the $x$-space. High leverage points have the potential to influence aspects of the analysis, to be highly influential data points. More on this below.

In summary, here are some important results involving the leverage values $h_{ii}$:

+ $h_{ii} = \bx_i^\prime (\bXpX)^{-1}\bx_i$

+ $\frac{1}{n} \le h_{ii} \le 1$. This holds only for the $n$ training observations, the leverage $\bx_0^\prime(\bXpX)^{-1}\bx_0$ of a new data point is not bounded in this way.

+ $\overline{h} = (p+1)/n$; high leverage points are those for which $h_{ii} > 2(p+1)/n$.

+ $\Var[\widehat{y}_i] = \sigma^2 h_{ii}$

+ $\Var[y_i - \widehat{y}_i] = \sigma^2 (1-h_{ii})$

+ $\widehat{y}_i = \sum_{j=1}^n h_{ij}y_j$, the $i$^th^ fitted value is a linear combination of the target values with the values in the $i$^th^ row of $\bH$.

+ $\sum_{j=1}^n h_{ij} = 1$, the sum of the leverage values in the $i$^th^ row of $\bH$ is 1. Since the leverage values are bounded,  they sum to 1 within a row, and the fitted values are linear combinations of $\bH$, this shows how a data point with $h_{ii} \approx 1$ has outsize influence. The fitted value is almost entirely determined by the input values of that observation.

:::{.example}
::::{.example-header}
Example: Same Leverage--Different Influence
::::
::::{.example-container}
This simulation demonstrates the concept of high leverage points with and without high influence. Data are simulated under the model
$$ 
Y = \beta_0 + \beta_1 x + \epsilon
$$ 
with $\epsilon \sim G(0,0.25^2), \beta_0 = 1, \beta_1 = 0.5$. The design points for the input variable are spread evenly from 1 to 2, and a high leverage point is added at $x=4$. There are 22 observations, so the threshold for a high leverage point is $2(p+1)/n = 2*2/22 = 0.18$.

Two data sets are simulated. One in which the target value at $x=4$ concurs with the mean function, one in which the target value is unusually high.

```{r}
set.seed(187)

x <- seq(from=1, to=2, by=0.05)
xlev <- 4  # the high-leverage point
x <- c(x,xlev)

mn <- 1 + 0.5*x
y1 <- rnorm(n=length(x), mean=0, sd=0.25) + mn

y2 <- y1
y2[length(y1)] <- rnorm(1,0,0.25) + 1 + 1*xlev

lm1 <- lm(y1 ~ x)
lm2 <- lm(y2 ~ x)

lm1$coefficients
lm2$coefficients

# the point at xlev has the same leverage in both regressions
round(hatvalues(lm1),4)
round(hatvalues(lm2),4)
```

The fitted linear regressions are quite different. The parameter estimates in the first model, where $y|x=4$ is not unusual are close to the true values $\beta_0 = 1, \beta_1 = 0.5$. In the second regression the parameter estimates are very different from the true values, the estimates are biased.

The leverage values in both models are identical, since they depend only on the $x$-data. The data point at $x=4$ has high leverage, its value of `{r} round(hatvalues(lm1)[length(x)],4)` exceeds the threshold considerably. No other data point has high leverage.

@fig-leverage-example displays the data and fitted regressions for the two data sets. Although $x=4$ is a high leverage point, it has no undue influence on the estimated regression in the left panel. In the right panel, the high leverage point is a highly influential point due to its unusual $y$-value. The data point exerts its leverage by pulling the estimated regression towards it.

```{r, echo=FALSE, out.width="90%", fig.align='center'}
#| label: fig-leverage-example
#| fig.cap: A data point with high-leverage at $x=4$ has little influence in one analysis and is highly influential in another analysis depending on its $y$-value. The dashed line is the true mean function.
#| 
par(mfrow=c(1,2))
par(mar=c(4.1, 4.1, 2, 3)) # bottom, left, top, right
plot(x,y1,xlab="X",ylab="Y",col="red",pch=16,
     cex=1.5,
     ylim=c(0,5.5),
     las=1)
abline(lm1, col="red",lwd=2)
abline(a=1,b=0.5,lwd=2,col="black",lty="dashed")
text(1.5, 5, adj=0, 
     cex=0.8,
     expression(hat(y) == 0.986 + 0.502*x))

plot(x,y2,xlab="X",ylab="Y",col="blue",pch=16,
     cex=1.5, 
     ylim=c(0,5.5),
     las=1)
abline(lm2, col="blue", lwd=2)
abline(a=1,b=0.5,lwd=2,col="black",lty="dashed")
text(1.5, 5, adj=0, 
     cex=0.8,
     expression(hat(y) == 0.023 + 1.16*x))
```
::::
:::

### Residual Diagnostics

Basic questions about the correctness of the model revolve around the properties of $\bepsilon$. The usual assumption
$\bepsilon \sim (\bzero, \sigma^2\bI)$ states that the errors have zero mean, are uncorrelated, and have equal variance. Although $\bepsilon$ is unobservable, we should be able to check those assumptions by looking at the OLS residuals of the fitted model, $\widehat{\bepsilon} = \by - \widehat{\by}$. These are also called the **raw residuals**.

Unfortunately, the properties of $\widehat{\bepsilon}$ match the properties of $\bepsilon$ only partially. Because $\widehat{\bepsilon}$ is the result of fitting a model to data, the fitted residuals obey constraints that do not affect the model errors $\bepsilon$. Because
$$
\bX^\prime \widehat{\bepsilon} = \bX^\prime (\bI-\bH)\by = \bzero
$$
the raw residuals sum to zero across each column of the $\bX$ matrix. In other words, there are only $n-r(\bX)$ degrees of freedom in the raw residual vector. From a statistical perspective, the residuals have zero mean, $\Exp[\widehat{\bepsilon}] = \bzero$ and share this property with the model errors. The variance of $\bepsilon$ and $\widehat{\bepsilon}$ is different, however:

\begin{align*}
    \Var[\bepsilon] &= \sigma^2 \bI \\
    \Var[\widehat{\bepsilon}] &= \sigma^2(\bI - \bH) \\
    \Var[\widehat{\epsilon}_i] &= \sigma^2 (1-h_{ii})
\end{align*}

While the model errors are uncorrelated, the fitted residuals are correlated, $\bI - \bH$ is not a diagonal matrix. The fitted residuals also do not have the same variance; the variance depends on the leverage of the $i$^th^ data point. 

These properties (or lack thereof) should give pause in using the raw residuals to diagnose the assumptions of equal variance or uncorrelated errors. Instead, residual diagnostics use transformations of the raw residuals.

#### Studentized residuals

The unequal variance of the residuals can be handled by **standardizing**, dividing the residual by its standard deviation (the square root of its variance)
$$
\frac{\widehat{\epsilon}_i}{\sigma\sqrt{1-h_{ii}}}
$$
$\sigma$ is unknown and the obvious solution is to substitute an estimator. Statisticians refer to this process, using an estimate to scale a random variable, as **studentization**. The studentized residual is thus
$$
r_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}\sqrt{1-h_{ii}}}
$$
The usual estimator for $\sigma$ is the square root of the estimator of $\sigma^2$, 
$$
\widehat{\sigma} = \sqrt{ \frac{\text{SSE}}{n-r(\bX)}}
$$

#### R-student residuals

A further adjustment can be made to the studentized residuals. Rather than use an estimate of the variance that is derived from all the data, an estimator can be used that does not depend on the $i$^th^ observation. This technique is called **external studentization** in contrast to the **internal studentization** that gives rise to $r_i$. 

Fortunately such an external estimate of the variance $\sigma^2$ that does not rely on the $i$^th^ observation can be computed based on the analysis of all $n$ observations. Not surprisingly, as with PRESS residuals discussed earlier, the leverage plays a role again:
$$
\widehat{\sigma}^2_{-i} = \frac{(n-r(\bX))\widehat{\sigma}^2 - \frac{\widehat{\epsilon}^2_i}{1-h_{ii}}}{n-r(\bX)-1}
$$
The externally studentized residual is called the R-student residual,
$$
t_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}_{-i}\sqrt{1-h_{ii}}}
$$
Since $\widehat{\epsilon}_i$ and $\widehat{\sigma}_{-i}$ are independent, $t_i$ behaves like a $t$-distributed random variable. The R-student residuals are good diagnostics to detect outliers and high-influence points (**hip**s). Outliers are observations unusual in $y$-space. They are not necessarily hips, unless they are also high leverage points (see the previous example).

The $t_i$ work well for outliers and hips because an outlier has large $\widehat{\epsilon}_i$ and a hip has small $\sqrt{1-h_{ii}}$. Both effects increase the value of $t_i$. This is also true for the (internally) studentized residual. In addition, outliers or hips will have a large 
$$
\frac{\widehat{\epsilon}^2_i}{1-h_{ii}}
$$
the adjustment term in the computation of $\widehat{\sigma}^2_{-i}$. For those data points $\widehat{\sigma}^2_{-i} < \widehat{\sigma}^2$ and $t_i$ will be more sensitive than $r_i$.

You can obtain all three sets of residuals easily in `R`:

+ The `residual` vector returned on the `lm` return object contains the $\widehat{\epsilon}_i$.

+ The `rstandard()` function returns the studentized residuals $r_i$ (unfortunate function name)

+ The `rstudent()` function returns the R-student residuals $t_i$


When testing model assumptions such as linearity, equal variance (homoscedasticity), 
and checking for outliers, the R-student residuals are the preferred quantities. 
The threshold $|r_i| > 2$ is often applied to indicate outlying observations. 


::: {.example}
::::{.example-header}
Example: Boston Housing Values
::::
::::{.example-container}
To demonstrate residual analysis in a linear model we use the Boston housing data that is part of the `MASS` library in `R`.
The data set comprises 506 observations on the median value of owner-occupied houses (`medv` in \$000s) in Boston suburbs and 13 variables describing the towns and properties. 

The following statements fit a multiple linear regression model to predict median home value as a function of all but two inputs. The formula syntax `medv ~ .` requests to include all variables in the dataframe as inputs. `medv ~ . -indus -age` requests inclusion of all inputs except `indus` and `age`.


``` {r model_fit}
library(MASS)
fit <- lm(medv ~ . - indus - age, data=Boston)
summary(fit)
```

All 11 input variables are significant in this model; it explains 74\% of the variability in median home values.

A plot of the R-student residuals against the observation number helps to identify
outlying observations. Observations with $t_i$ values outside the 
[-2, +2] interval are outliers. The plot also shows whether the equal variance assumption
is reasonable; if the assumption is met the residuals show as a band of equal width.

``` {r RStudent_plot, out.width="80%", fig.align="center"}
RStudent <- rstudent(fit)
par(mar=c(5.1, 4.1, 2, 3))
plot(RStudent, xlab="Obs no.", las=1,bty="l")
abline(h= 2, lty="dashed")
abline(h=-2, lty="dashed")
```


A plot of the residuals against leverage shows observations that are unusual with respect to $y$
(large absolute value of the residual), with respect to $x$ (large leverage), or both. 
A high leverage point that is also an outlier is a highly influential data point.

``` {r RStudent_vs_leverage, out.width="80%", fig.align="center"}
leverage <- hatvalues(fit)
lev_threshold <- 2*fit$rank/length(fit$residuals)
par(mar=c(5.1, 4.1, 2, 3))
plot(leverage, RStudent, xlab="Leverage")
abline(h= 2, lty="dashed")
abline(h=-2, lty="dashed")
abline(v=lev_threshold, lty="dotted")
```


In a simple linear regression model you can plot the residuals against the input $x$.
In a multiple linear regression model (or a SLR) you plot the residuals against the
fitted values. The residuals should display no obvious trend against $\widehat{y}$.

When plotting the residuals against the fitted value, add the predictions from
a smoothing spline or other nonparametric model to identify trends in the 
residuals. Ideally, the smoothing spline should not show any gross departures
from a flat line at zero. 

The following plot raises concerns. We might not have
the right set of inputs. Inputs might need to be transformed or 
additional/different terms are needed in the model, for example, interactions
between the inputs.

``` {r Rstudent_vs_fit, out.width="80%", fig.align="center"}
yhat <- predict(fit)
plot(yhat,RStudent)
abline(h=2, lty="dashed")
abline(h=-1, lty="dashed")
lines(predict(loess(RStudent ~ yhat)),col="red",lwd=2)
```
::::
:::

#### Partial regression plots

It is tempting to create plots comparing residuals against the values of the input variables. This is meaningful in a simple linear regression with one input and can help suggest transformations of $X$ to achieve linearity and help diagnose heteroscedasticity.

In the following example, the variance of $Y$ increases with the value of $X$ and the relationship between $Y$ and $X$ is a second-degree polynomial. 

```{r, fig.align="center", out.widfth="80%"}
set.seed(543)
x <- seq(1,10,0.1) 
x <- x + rnorm(length(x),0,1)
mn <- 2 - 0.5*x + 0.3*x^2
y <- rnorm(n=length(x), mean=mn, sd=sqrt(x))
lfit <- lm(y ~ x)
ri <- rstudent(lfit)

plot(x,ri,ylab="Rstudent",las=1,bty="l")
abline(h=0,col="red",lty="dashed")

```
The plot of the Rstudent residuals shows the increasing variability in $x$ and a systematic quadratic trend in the residuals.

In a multiple linear regression (MLR) model it is tempting to create plots such as the previous one for all input variables. Unfortunately, such plots can be misleading because in an MLR model values of one input are changing with the values of other inputs. A way around this problem seems to be a plot of residuals versus the fitted values $\widehat{y}_i$ but that is not an optimal solution either; this plot does not tell us anything about the $X$s that could be transformed to improve the model. It mushes together the contributions of all inputs.

So, how can we diagnose whether $X_j$ needs to be transformed to account for non-linearity and visualize the relationship in such a way that accounts for the other inputs in the model? The answer is the **partial regression** plot, also known as the **added-variable** plot.

Suppose we partition the $\bX$ matrix of the MLR model as follows
$$
\bX = [\bX_{-j}, \bx_j]
$$
so that $\bX_{-j}$ contains all inputs (including the intercept) except for the $j$^th^ input. Now consider two new regression models:

- Regress $\by$ on $\bX_{-j}$. Denote the residuals from this regression as $\textbf{e}_{y,-j}$

- Regress $\bx_j$ on $\bX_{-j}$. Denote the residuals from this regression as $\textbf{e}_{x,-j}$

The added variable plot for input $j$ displays $\textbf{e}_{y,-j}$ on the vertical axis and $\textbf{e}_{x,-j}$ on the horizontal axis.

:::{.example}
::::{.example-header}
Example: Auto (Cont'd)
::::
::::{.example-container}
For the four-regressor model in the Auto example, added-variable plots can be constructed with the `avPlots()` function in the `car` package.

```{r}
mlr <- lm(mpg ~ cylinders + displacement + weight + horsepower,
             data=Auto)
car::avPlots(mlr)
```

::::
:::

Take the scatter of points in an added variable plot. This is a regression through the origin (a no-intercept) model of the form
$$
\textbf{e}_{y,-j} = \beta_j \, \textbf{e}_{x,-j} + \bepsilon^*
$$

Using $\beta_j$ to denote the slope in this regression is no accident. The estimate of $\beta_j$ in this regression is the same as the estimate in the multiple linear regression model. 

The added-variable plot is a visual representation of how $X_j$ fares in the $p$-input model even if we are looking only at a two-dimensional plot. The partial regressions show the effect of $X_j$ on $y$ as if it was added last to the model. If the residual point scatter in the added-variable plot suggests nonlinearity, a transformation of $X_j$ is in order. 

Inputs that are highly significant in the multiple linear regression model will have a tight point cloud in the added-variable plot. Inputs that are correctly specified in the model will show non-systematic scatter of the points around the line. Note that we are not looking for horizontal point clouds in the added-variable plots, because the points are arranged around a non-zero line, its slope corresponds to the coefficient estimate.

### Case Deletion Diagnostics

Case deletion diagnostics express how much an aspect of the model changes when 
an observation is removed from the analysis. The RStudent residual is a leave-one-out
diagnostic in this spirit, as it uses an estimator of $\sigma$ that does not incorporate
the $i$th data point.

The two most important case deletion diagnostics are Cook's D and the DFFITS. 
The name DFFITS stands for **d**i**ff**erence in **fit**, **s**tandardized. The 
statistic measures the change in predicted value in units of standard errors when
the $i$th observation is deleted. We are concerned when a DFFIT exceeds $2\sqrt{(p+1)/n}$.

The Cook's D ("D" for distance) statistic measures the change in the parameter estimates $\boldsymbol{\beta}$
when the $i$th observation is removed. If the purpose of modeling is to build 
a model that predicts well, focus more on DFFITS. If the purpose of modeling is
to test hypotheses, then focus more on Cook's D. We are concerned if the D statistic
exceeds 1.

#### Cook's D

Computing Cook's D for our model is easy:

``` {r cooks_d, out.width="85%"}
D <- cooks.distance(fit)
D[which.max(D)]
D[which(D > 0.1)]
```

There are no data points with a D > 1. We conclude that there are no data points
that unduly influence the regression coefficients.

A plot of the D statistic against the observation number shows that there is a
group of data points with much higher values of the D statistic. This group also
coincides with larger residuals in the previous plots.

``` {r cooks_d_plot, out.width="85%", fig.align="center"}
plot(D, xlab="Obs no.")
```

Here are the observations with the larger D values

``` {r large_D}
D[which(D > 0.035)]
```

What do these observations have in common?

``` {r}
subset(Boston, D > 0.035)
```

#### DFFITs

You can calculate the DFFITs statistics with the `dffits()` function. The following
plot shows those statistics along with the threshold.

``` {r dffits}
threshold_dff <- 2 * sqrt(12/506) 
dff <- dffits(fit)
dff[which(dff > threshold_dff)]
```


``` {r dff_plot, out.width="85%", fig.align="center"}
plot(dff,ylab="DFFITs")
abline(h=threshold_dff)
```

The same group of observations with high D values has also high DFFITs. Contrary to
the D values, the observations exceed the threshold for the DFFITs. We conclude that
the data points are not influential on the regression coefficient estimates, but 
they are influential on the predicted values. If the model is used to predict 
median home values, we should consider refining the model or excluding the outlying 
observations and refitting.

### Collinearity Diagnostics {#sec-collinearity-diag}

When inputs are related to each other it has two important consequences

- we cannot interpret the regression coefficient for one input without considering
the other inputs
- with increasing dependence among the inputs, the least squares estimation
procedure becomes increasingly numerically unstable.

The condition when inputs are linearly related is called **collinearity** and it
negatively affects any calculations that involve the $(\textbf{X}^\prime\textbf{X})^{-1}$ 
matrix (which is about all the calculations.) 

The extreme case is when an input variable is linear combination of other inputs, for example
$$
Z = aX_2 + bX_3
$$
Adding $Z$ to a model that contains inputs $X_2$ and $X_3$ leads to a singular (rank-deficient) $\bX$ matrix. The inverse cross-product matrix $(\bXpX)^{-1}$ does not exist and the OLS estimator cannot be computed. Software uses instead a **generalized** inverse matrix $(\bXpX)^{-}$ to find a solution, which happens to be not unique.

::: {.example}
::::{.example-header}
Example: Boston Housing Values
::::
::::{.example-container}
The following code fits a linear model with four inputs. The variable `newvar` is the sum of the `zn` and `nox` variables. With `zn` and `nox` already in the model, `newvar` does not provide any additional information. The $\bX$ matrix is singular and an estimate for the coefficient of `newvar` cannot be found.
```{r}
B <- Boston
B$newvar <- B$zn+B$nox
singular_model <- lm(medv ~ crim + zn + nox + newvar, data=B)
summary(singular_model)
```

`R` indicates the singularity in $\bX$ with `NA` for the coefficient associated with the singularity. Note that this depends on the order in which the variables enter the model. When `newvar` is placed before `zn` and `nox` in the model, the `nox` coefficient is `NA`.
::::
:::

Collinearity is the condition where inputs are highly correlated. They do not follow exact linear dependencies but are close to linearly dependent (a so-called near-linear dependency). The situation is more complex than involving just two input variables. For example, $X_1$ might not be strongly correlated with $X_2$ but can be strongly correlated
with a linear combination of $X_2$, $X_3$, and $X_8$. This condition is called
**multicollinearity**.

When multicollinearity is strong, the OLS estimates of the regression coefficients become unstable, small perturbations in the target values or inputs can lead to large changes in the coefficients. The $\widehat{\beta}_j$ can be of the wrong size and/or sign.

A nontechnical diagnostic for multicollinearity is to compute the matrix of pairwise correlations among the inputs. Large values of $\text{Corr}[X_j,X_k]$ is a sufficient condition for collinearity, but it is not a necessary condition. Even with weak pairwise correlations you can have strong linear dependencies among multiple inputs.

Nevertheless, the pairwise correlation plot is a good place to start.

``` {r corrplot, out.width="75%", fig.align="center"}
library("corrplot")
X <- model.matrix(fit)
corrplot(cor(X[,2:12]), method = "circle", diag=FALSE, tl.col="black")
```

We see some strong **pairwise** relationships between `tax` and `rad`, between `dis` and `nox`, 
and between `lstat` and `rm`. Do we need to worry?

Here are some other, informal, ways to diagnose a multicollinearity problem:


- The $R^2$ statistic indicates the model explains substantial variability in $Y$, but none or few of the inputs show statistically significant $p$-values. Because the $t$-tests are partial tests, the other input variables act as proxy for the variable being tested.

- Different variable selection methods lead to very different models.

- Standard errors of coefficients and/or fitted values are unusually large.

- Slight perturbations of the data, for example, by adding some small Gaussian random noise, change the results dramatically.

A formal diagnosis relies on the computation of **variance inflation factors** (VIFs) or **condition indices*.

#### Variance inflation factors

Each predictor (input) variable in a linear model is associated with a variance inflation factor that quantifies the strength of linear dependencies between this input and all other inputs.

The $j$^th^ VIF measures how many times more variable the variance of the standardized coefficients are due to the involvement of $X_j$ in linear dependencies involving the other $X$s.

You can find the $\text{VIF}_j$ from the $R^2$ statistic of a multiple linear regression of $X_j$ on 
all the other input variables. 
$$ \text{VIF}_j = \frac{1}{1-R^2_j}$$
For example, the VIF for `rad` can be obtained by regressing all other inputs onto `rad`

``` {r vif_rad}
vif_calc <- lm(rad ~ crim + zn + chas + nox + rm + dis + tax + 
                   ptratio + black + lstat, data=Boston)
R2_rad <- summary(vif_calc)$r.squared
VIF_rad <- 1 / (1-R2_rad)
VIF_rad
```

:::{.callout-caution}
When calculating variance inflation factors this way make sure that the response variable does not appear on the right hand side of the model formula. The expression `lm(rad ~ .)` would include `medv` on the right hand side. Variance inflation factors capture relationships among the inputs and are not related to the response.
:::

Notice that a variance inflation factor **does not** depend on $Y$. It is
solely based on relationships among the inputs (predictors). Also, you can 
see from the model equation above that the VIF discovers more than a pairwise
dependence on other variables. It models one input as a function of all other
inputs.

To compute variance inflation factors in `R` directly, use the `vif` function in the `car` package
(Companion to Applied Regression).

``` {r VIF}
library(car)
vif(fit)
```

The VIF for `rad` reported by `vif()` matches the previous calculation.

Another way to compute the variance inflation factors is to fit the linear regression with a scaled and centered $\bX^*$ matrix. The columns of $\bX^*$ are centered at their sample mean and are scaled by dividing by $\sqrt{n-1}$ times their standard error. As a result, the $\bX^{*\prime} \bX^*$ matrix is the matrix of the empirical pairwise correlations of the inputs. The regression coefficients of this model are called the **standardized** coefficients ($beta^*_j$) and the variance inflation factors are 
$$
\text{VIF}_j = \widehat{Var}[\widehat{\beta}^*_j] / \widehat{\sigma}^2 
$$

:::{.example}
::::{.example-header}
Example: Variance Inflation Factors from Scaled-Centered Regression; Boston Data (Cont'd)
::::
::::{.example-container}
To compute the VIFs in the model for the Boston data, we compute first the centered-and-scaled $\bX$ matrix. The `scale()` function in `R` centers and scales the data by default but uses the standard deviation as the scaling factor. We use a custom scaling so that the $\bX^{*\prime}\bX^*$ matrix equals the empirical correlation matrix.

We use `model.matrix()` to extract the $\bX$ matrix from the model object computed earlier. The intercept column is replaced with the target values so we can use this matrix as input to a call to `lm`.

```{r}
X <- model.matrix(fit)
n_1 <- dim(X)[1] - 1
scaled_X <- scale(X,
                  center=apply(X,2,mean),
                  scale =apply(X,2,sd)*sqrt(n_1))
scaled_X[,1] <- Boston$medv
colnames(scaled_X)[1] <- "medv"
ll <- lm(medv ~ ., data=data.frame(scaled_X))
summary(ll)
```

The variance inflation factors are obtained by dividing the square values in the `Std. Error` column with the estimator of the residual variance

```{r}
s <- summary(ll)
vif <- s$coefficients[,2]^2 / s$sigma^2
vif
```
::::
:::

The smallest possible VIF value is 1.0, it indicates that the input is not linearly
related to the other variables. The thresholds are as follows

- 1 < VIF < 10: moderate collinearity
- 10 < VIF < 30: moderate to severe collinearity
- VIF > 30: severe collinearity problem


#### Condition index and condition number

A formal diagnostic for multicollinearity, based on the eigenvalue decomposition of the (scaled-and-centered) $\bX^*$ matrix, examines the spread of the eigenvalues of \bX^{*\prime}\bX$. If $\bX^*$ is a centered and scaled version of $\bX$ such that $\bX^{*\prime}\bX$ is the empirical correlation matrix, its eigen decomposition is
$$
    \bX^{*\prime}\bX = \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^\prime
$$
where $\textbf{Q}$ is a $(p+p)$ orthogonal matrix of eigenvectors and $\boldsymbol{\Lambda}$ is a diagonal matrix with the eigenvalues $\lambda_j$ on its diagonal. The number of eigenvalues close to zero indicates the number of near linear dependencies in $\bX$ (or $\bX^*$). In this centered-and-scaled form the eigenvalues satisfy $\sum_{j=1}^p \lambda_j = p$, so if some eigenvalues get small, others need to get bigger. 

The **condition index** associated with the $j$^th^ eigenvalue is 
$$
\phi_j = \frac{\max(\lambda_j)}{\lambda_j}
$$
and the **condition number** is $\max(\phi_j)$.



```{r}
library(Matrix)

X <- model.matrix(fit)[,2:12]
X_star <- scale(X,
                center=apply(X,2,mean),
                scale=apply(X,2,sd)*sqrt((n_1)))
XpX_scaled <- t(X_star) %*% X_star
eigen_decomp <- eigen(XpX_scaled)
evals <- eigen_decomp$values

evals

cat("Sum of eigenvalues: ", sum(evals),"\n")

cond_index <- max(evals)/evals
cat("Condition indices: ", cond_index,"\n")

cond_number <- max(cond_index)
cat("Condition number: ", cond_number,"\n")
```

Condition indices larger than 900 indicate that near linear dependencies exist.

:::{.callout-caution}
In contrast to variance inflation factors, where the $j$^th^ factor is associated with the $j$^th^ input, the eigenvalue $\lambda_j$ is not associated with a particular input. It is associated with a linear combination of all the $X$s.
:::


An obvious remedy of the multicollinearity is to remove inputs that are associated with high variance inflation factors and to refit the model. If you cannot remove the variables from the model a different estimation method is called for. Regularization methods such as Ridge regression or Lasso regression handle high-dimensional problems and reduce the instability of the least-squares estimates by shrinking their values (suppressing the high variability of the coefficients). At the cost of introducing some bias, these estimators drastically reduce variability for an overall better mean square prediction error.

