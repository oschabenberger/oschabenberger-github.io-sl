<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 16&nbsp; Regression and Classification Trees</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./treesinR.html" rel="next">
<link href="./supportvectors.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./decisiontrees.html">Part IV. Decision Trees</a></li><li class="breadcrumb-item"><a href="./decisiontrees.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification with Random Inputs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Decision Trees</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesinR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Part VI. Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Part VII. Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Part VIII. Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Part IX. Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">16.1</span> Introduction</a></li>
  <li><a href="#partitioning-x-space" id="toc-partitioning-x-space" class="nav-link" data-scroll-target="#partitioning-x-space"><span class="header-section-number">16.2</span> Partitioning <span class="math inline">\(X\)</span>-space</a></li>
  <li><a href="#training-a-tree" id="toc-training-a-tree" class="nav-link" data-scroll-target="#training-a-tree"><span class="header-section-number">16.3</span> Training a Tree</a>
  <ul>
  <li><a href="#growing" id="toc-growing" class="nav-link" data-scroll-target="#growing">Growing</a>
  <ul class="collapse">
  <li><a href="#regression-tree" id="toc-regression-tree" class="nav-link" data-scroll-target="#regression-tree">Regression tree</a></li>
  <li><a href="#classification-tree" id="toc-classification-tree" class="nav-link" data-scroll-target="#classification-tree">Classification tree</a></li>
  </ul></li>
  <li><a href="#sec-trees-pruning" id="toc-sec-trees-pruning" class="nav-link" data-scroll-target="#sec-trees-pruning">Pruning</a></li>
  </ul></li>
  <li><a href="#issues-related-to-decision-trees" id="toc-issues-related-to-decision-trees" class="nav-link" data-scroll-target="#issues-related-to-decision-trees"><span class="header-section-number">16.4</span> Issues Related to Decision Trees</a>
  <ul>
  <li><a href="#categorical-inputs" id="toc-categorical-inputs" class="nav-link" data-scroll-target="#categorical-inputs">Categorical Inputs</a></li>
  <li><a href="#missing-values" id="toc-missing-values" class="nav-link" data-scroll-target="#missing-values">Missing Values</a></li>
  <li><a href="#optimality" id="toc-optimality" class="nav-link" data-scroll-target="#optimality">Optimality</a></li>
  <li><a href="#stability" id="toc-stability" class="nav-link" data-scroll-target="#stability">Stability</a></li>
  <li><a href="#smoothness" id="toc-smoothness" class="nav-link" data-scroll-target="#smoothness">Smoothness</a></li>
  <li><a href="#nonlinearity-and-interactions" id="toc-nonlinearity-and-interactions" class="nav-link" data-scroll-target="#nonlinearity-and-interactions">Nonlinearity and Interactions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./decisiontrees.html">Part IV. Decision Trees</a></li><li class="breadcrumb-item"><a href="./decisiontrees.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-trees" class="quarto-section-identifier"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">16.1</span> Introduction</h2>
<p>We decided to give decision trees its own part in the material for a number of reasons.</p>
<ol type="1">
<li><p>Decision trees can be used in regression and classification settings which could place them in both Parts II and III. Trees are different from the regression approach to classification where one fits a regression-type model first to predict category probabilities and then uses them to classify the result. With decision trees, the tree is set up from the beginning as either a regression tree or a classification tree. The remaining logic of building the tree is mostly the same, apart from differences in loss functions and how predicted values are determined.</p></li>
<li><p>Decision trees are among the statistical learning techniques that are easiest to explain to a non-technical audience. Trees are intrinsically interpretable, a tree diagram reveals completely how an input record is processed to arrive at a prediction or classification. This makes decision trees very popular.</p></li>
<li><p>Decision trees are <strong>non-parametric</strong> in the sense that they do not require a distributional assumption. Whether the target variable is uniform, gamma, Poisson, or Gaussian distributed, a tree-building algorithm works the same way. No explicit assumptions about the nature of model errors, such as equal variance, are made either.</p></li>
<li><p>Decision trees can be applied to many problems—since they can predict and classify—but their performance is often lacking. Trees have a number of shortcomings such as high variability, sensitivity to small changes in the data, a tendency to overfit. You are trading these against advantages such as simplicity, interpretability, and the ability to handle missing values (to some extent).</p></li>
<li><p>Many popular statistical learning techniques, which can perform extremely well, are based on decision trees and have been developed to overcome their shortcomings. Bagged trees, random forests, gradient boosted trees, and extreme gradient boosting are examples of such methods. To understand and appreciate these popular learning approaches, we need a basic understanding of how decision trees work.</p></li>
</ol>
<p>The trees described in this chapter were introduced by <span class="citation" data-cites="Breiman_etal_1984">Breiman et al. (<a href="references.html#ref-Breiman_etal_1984" role="doc-biblioref">1984</a>)</span>, and are known as CART (Classification and Regression Trees). <span class="citation" data-cites="Sutton_2005">Sutton (<a href="references.html#ref-Sutton_2005" role="doc-biblioref">2005</a>)</span> gives an excellent overview of these type of trees, and discusses bagging and boosting as well.</p>
</section>
<section id="partitioning-x-space" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="partitioning-x-space"><span class="header-section-number">16.2</span> Partitioning <span class="math inline">\(X\)</span>-space</h2>
<p>The principle behind building decision trees can be visualized if we consider a problem with a target variable <span class="math inline">\(Y\)</span> and two input variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. <span class="math inline">\(Y\)</span> can be a continuous target or a classification target variable. Trees are frequently introduced with this scenario because we can visualize how observed values <span class="math inline">\(\textbf{x}= [x_1,x_2]\)</span> translate into a prediction or classification.</p>
<p>The basic idea is to carve up the <span class="math inline">\([X_1,X_2]\)</span> space into <span class="math inline">\(J\)</span> non-overlapping regions <span class="math inline">\(R_1,\cdots, R_J\)</span> based on training data. Each region is associated with a representative value, also determined from the training data. A new observation will fall into one of the regions, and the representative value for the region is returned as the predicted value.</p>
<div id="fig-part-2dim" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-part-2dim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/dtree_2dim_part.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-part-2dim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.1: Partitioning a two-dimensional input space through binary splitting.
</figcaption>
</figure>
</div>
<p>An example is shown in <a href="#fig-part-2dim" class="quarto-xref">Figure&nbsp;<span>16.1</span></a>. <span class="math inline">\(J=5\)</span> regions are generated by first splitting <span class="math inline">\(X_2 \le a_1\)</span> followed by <span class="math inline">\(X_1 \le b_2\)</span>. This creates regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. The next split is again applied to <span class="math inline">\(X_2\)</span>, now separating data points with <span class="math inline">\(X_2 \le a_3\)</span>. This creates region <span class="math inline">\(R_3\)</span>. The final split is splitting points where <span class="math inline">\(X_2 \ge a_3\)</span> according to <span class="math inline">\(X_1 \le b_4\)</span>. This creates regions <span class="math inline">\(R_4\)</span> and <span class="math inline">\(R_5\)</span>.</p>
<p>A display such as <a href="#fig-tree-2dim" class="quarto-xref">Figure&nbsp;<span>16.2</span></a> works for trees with <span class="math inline">\(p \le 2\)</span> inputs. In higher dimensions we cannot visualize the regions. The typical display for a decision tree is, well, a tree. However, the tree display resembles more a root system with the trunk at the top and the branches of the tree below; or an upside down tree with the leaves at the bottom. The decision tree implied by the partitioning in <a href="#fig-part-2dim" class="quarto-xref">Figure&nbsp;<span>16.1</span></a> is shown in <a href="#fig-tree-2dim" class="quarto-xref">Figure&nbsp;<span>16.2</span></a>.</p>
<div id="fig-tree-2dim" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-2dim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/dtree_2dim.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-2dim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.2: Decision tree implied by the partitioning in <a href="#fig-part-2dim" class="quarto-xref">Figure&nbsp;<span>16.1</span></a>.
</figcaption>
</figure>
</div>
<p>The segments connecting levels of the tree are called <strong>branches</strong>, the points where the tree splits are the <strong>nodes</strong> (or <strong>internal nodes</strong>) and the end points of the tree are called the <strong>terminal nodes</strong> or the <strong>leaves</strong>. Note that having split on a variable higher up in the tree (closer to the trunk) does not preclude reusing that variable at a later point. This is evident on the right side of the tree where the tree splits on <span class="math inline">\(X_2\)</span> a second time.</p>
<p>The particular form of creating the regions is called <strong>binary splitting</strong>, it leads to rectangular regions rather than irregularly shaped regions or polygons. Binary splitting is not the optimal partitioning strategy, but it leads to trees that are easy to describe and it is easier to find the regions.</p>
<p>Note that the coordinate system in <a href="#fig-part-2dim" class="quarto-xref">Figure&nbsp;<span>16.1</span></a> displays <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and not the target variable <span class="math inline">\(Y\)</span>. Where and when does the target come into the picture? The values of the target variable in the training set are used to determine</p>
<ul>
<li>which variable to split on at any split point of the tree</li>
<li>the location of the splits <span class="math inline">\((a_1, b_2, \cdots)\)</span></li>
<li>the number of regions <span class="math inline">\(M\)</span>.</li>
<li>the representative (predicted) value in each region</li>
</ul>
<p>In the case of a regression tree, <span class="math inline">\(Y\)</span> is continuous and the representative value in region <span class="math inline">\(R_j\)</span> is typically the average of the training data points that lie in <span class="math inline">\(R_j\)</span>. In the case of a classification tree, a majority vote is taken, that is, the most frequently observed category in <span class="math inline">\(R_j\)</span> is returned as the classified value.</p>
</section>
<section id="training-a-tree" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="training-a-tree"><span class="header-section-number">16.3</span> Training a Tree</h2>
<p>Training a tree involves multiple steps. First, we have to decide on a criterion by which to measure the quality of a tree. For a regression tree we could consider a least-squares type residual sum of squares (RSS) criterion such as <span class="math display">\[
\sum_{j=1}^J \sum_{i \in R_j} \left(y_i - \overline{y}_{R_j}\right)^2
\]</span></p>
<p>where <span class="math inline">\(\overline{y}_{R_j}\)</span> is the average of the observations in the <span class="math inline">\(j\)</span>^th region.</p>
<p>Once we have the criterion (the loss function) for the tree, the next step is to <strong>grow</strong> the tree until some termination criterion is met. For example, we might continue splitting nodes until all leaves contain no more than <span class="math inline">\(m\)</span> observations. At this point we have found a tree, but not necessarily a great one. The question is how complex a tree one should build? Too many nodes and leaves with few observations and the tree will overfit and not generalize well to new observations. Too few nodes and the tree will not take advantage of the information in the training data.</p>
<p>Since the predictive accuracy of trees tends to improve initially as nodes are added, the typical process is to grow the tree until a termination criterion is met and in a subsequent step to remove leaves and branches to create a smaller tree that generalizes well. Another reason for initially growing a deep tree is that a best possible split may not reduce the RSS by much, but a subsequent split can produce an appreciable decrease in RSS. By stopping too early one would miss good later splits. The process of removing nodes and branches from a deep tree is known as <strong>pruning</strong> the tree and involves a test data set or cross-validation.</p>
<section id="growing" class="level3">
<h3 class="anchored" data-anchor-id="growing">Growing</h3>
<section id="regression-tree" class="level4">
<h4 class="anchored" data-anchor-id="regression-tree">Regression tree</h4>
<p>To grow a regression tree that minimizes such a criterion we restrict the regions <span class="math inline">\(R_j\)</span> to those obtained by recursive binary splitting as pointed out above. We first find the combination of input variable <span class="math inline">\(X_j\)</span> and split point <span class="math inline">\(c\)</span> such that separating the observations according to <span class="math inline">\(X_j \le c\)</span> leads to the greatest reduction in RSS. Formally, <span class="math display">\[
\min_{j,c} \left \{ \sum_{i:x_i \in R_1(j,c)} (y_i - \overline{y}_{R_1(j,c)})^2 +
                    \sum_{i:x_i \in R_2(j,c)} (y_i - \overline{y}_{R_2(j,c)})^2
            \right \}
\]</span> At each step of the tree-building process, we choose the pair <span class="math inline">\((j,c)\)</span> independent of any future splits further below in the tree. In other words, we only consider the best possible split for <em>this</em> step, and do not look ahead. This strategy is known as a <strong>greedy</strong> algorithm.</p>
<p>Building the tree continues by repeating the above optimization, at each point considering the previously constructed regions instead of the entire input space. Splitting the tree continues until a termination criterion is met: for example as long as all leaves contain at least <span class="math inline">\(m\)</span> observations.</p>
</section>
<section id="classification-tree" class="level4">
<h4 class="anchored" data-anchor-id="classification-tree">Classification tree</h4>
<p>A squared-error loss function works for a regression tree but not for classification problems. Besides the loss function, we also need to compute a representative value to classify observations. Let <span class="math inline">\(\widehat{p}_{jk}\)</span> denote the proportion of training observations in region <span class="math inline">\(R_j\)</span> that fall into class <span class="math inline">\(k\)</span>; this is simply <span class="math display">\[
\widehat{p}_{jk} = \frac{1}{n_j}\sum_{x_i \in R_j} I(y_i = k)
\]</span> The representative value returned from region <span class="math inline">\(R_j\)</span> is the category with the largest proportion, <span class="math inline">\(\mathop{\mathrm{arg\,max}}_k \widehat{p}_{jk}\)</span>. In other words, we take a majority vote and classify a leaf according to its most frequent category.</p>
<p>Based on the <span class="math inline">\(\widehat{p}_{jk}\)</span> we can define several criteria:</p>
<ul>
<li><strong>Misclassification error</strong> <span class="math display">\[ E = 1 - \max_k(\widehat{p}_{jk})\]</span></li>
<li><strong>Gini Index</strong> <span class="math display">\[ G = \sum_{k=1}^K \widehat{p}_{jk}(1-\widehat{p}_{jk})\]</span></li>
<li><strong>Cross-Entropy</strong> <span class="math display">\[ D = - \sum_{k=1}^{K} \widehat{p}_{jk} \text{log}(\widehat{p}_{jk})\]</span></li>
<li><strong>Deviance</strong> <span class="math display">\[ -2 \sum_{j=1}^{J} \sum_{k=1}^K n_{jk} \text{log}(\widehat{p}_{jk})\]</span></li>
</ul>
<p>The misclassification error might seem as the natural criterion for classification problems, but the Gini index and the cross-entropy criterion are preferred. The misclassification error is not sensitive enough to changes in the node probabilities and is not differentiable everywhere, making it less suitable for numerical optimization.</p>
<p>The Gini index and cross-entropy are sensitive to node <strong>purity</strong>, leading to trees having nodes where one class dominates. To see this, consider a Bernoulli(<span class="math inline">\(\pi\)</span>) distribution. The variance of the distribution is <span class="math inline">\(\pi(1-\pi)\)</span> and is smallest when <span class="math inline">\(\pi \rightarrow 0\)</span> or <span class="math inline">\(\pi \rightarrow 1\)</span>; it is highest for <span class="math inline">\(\pi= 0.5\)</span>. Nodes where <span class="math inline">\(\widehat{p}_{jk}\)</span> is near 0 or 1 have high purity. We’d rather base classification on terminal nodes where the decision is “decisive” rather than “wishy-washy”.</p>
<p><span class="citation" data-cites="hastie_ESL">Hastie, Tibshirani, and Friedman (<a href="references.html#ref-hastie_ESL" role="doc-biblioref">2001, 271</a>)</span> give the following example of a two-category problem with 400 observations in each category. A tree with two nodes that splits observations [300,100] and [100,300] has a misclassification rate of 1/4 (200 out of 800) and a Gini index of <span class="math inline">\(2 \times 3/4 1/4 = 3/8\)</span>. A tree with nodes that splits [200,0] and [200,400] has the same misclassification rate but a lower Gini index of 2/9. The second tree would be preferred for classification.</p>
<p>An interesting interpretation of the Gini index is in terms of the total variance over <span class="math inline">\(K\)</span> binary problems, each with a variance of <span class="math inline">\(\widehat{p}_{jk}(1-\widehat{p}_{jk})\)</span>. Another interpretation of the index is to imagine that rather than classifying an observation to the majority category we assign an observation to category <span class="math inline">\(k\)</span> with probability <span class="math inline">\(\widehat{p}_{jk}\)</span>. The Gini index is the training error of this procedure.</p>
<p>The cross-entropy criterion also leads to nodes with higher purity than the misclassification rate. In neural networks (<a href="ann.html" class="quarto-xref"><span>Chapter 31</span></a> and <a href="training_ann.html" class="quarto-xref"><span>Chapter 32</span></a>) for classification, cross-entropy is the loss function of choice.</p>
<p>The deviance function originates in likelihood theory and plays a greater role in pruning decision trees than in training classification trees.</p>
</section>
</section>
<section id="sec-trees-pruning" class="level3">
<h3 class="anchored" data-anchor-id="sec-trees-pruning">Pruning</h3>
<p>As mentioned earlier, simply applying a stop criterion based on the number of observations in the leaves (terminal nodes) does not guarantee that we have arrived at a great decision tree. Trees with many nodes and branches, also called deep trees, tend to overfit. If one were to split the tree all the way down so that each terminal leaf contains exactly one observation, the tree would be a perfect predictor on the training data.</p>
<p>Pruning a tree means to remove splits from a tree, creating a subtree. As subsequent splits are removed, this creates a sequence of trees, each a subtree of the previous ones.</p>
<div id="fig-tree-pruned" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-pruned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/treepruning_before_after.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-pruned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.3: Tree before and after pruning.
</figcaption>
</figure>
</div>
<p>To generate a useful sequence of trees to evaluate, a process called <strong>minimum cost-complexity pruning</strong> is employed, A nested sequence of the original (full) tree is created by lopping off all nodes that emanate from a non-terminal node and the node is chosen for which a criterion such as misclassification error decreases by the smallest amount. In other words, the nodes emanating from the node are pruned that worsens the tree the least. This is known as <strong>weakest-link pruning</strong>. The performance of the subtrees is evaluated by computing a tree complexity measure based on a test data set or <span class="math inline">\(k\)</span>-fold cross-validation.</p>
<p>Formally, let <span class="math inline">\(Q(T)\)</span> be the cost criterion by which tree <span class="math inline">\(T\)</span> is evaluated. <span class="math inline">\(Q(T)\)</span> is RSS for a regression tree and any of the criteria listed above for classification trees (Gini index or deviance, for example). If <span class="math inline">\(|T|\)</span> is the number of terminal nodes in <span class="math inline">\(T\)</span>, then we seek to minimize for each subtree the cost-complexity criterion <span class="math display">\[
C_\alpha(T) = Q(T) + \alpha|T|
\]</span> where <span class="math inline">\(\alpha\)</span> is a hyperparameter. If <span class="math inline">\(\alpha = 0\)</span> we end up with the full tree. As <span class="math inline">\(\alpha\)</span> increases, trees with more terminal nodes are penalized more heavily. For each value of <span class="math inline">\(\alpha\)</span> there is one tree <span class="math inline">\(T_\alpha\)</span> in the sequence of subtrees obtained by weakest-link pruning that minimizes <span class="math inline">\(C_\alpha(T)\)</span>.</p>
<p>With regression trees, the criterion used to grow the trees (squared error) is also used to prune the tree. With classification trees, growing the tree often relies on Gini index or cross-entropy, while pruning a tree uses the deviance or misclassification rate.</p>
</section>
</section>
<section id="issues-related-to-decision-trees" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="issues-related-to-decision-trees"><span class="header-section-number">16.4</span> Issues Related to Decision Trees</h2>
<section id="categorical-inputs" class="level3">
<h3 class="anchored" data-anchor-id="categorical-inputs">Categorical Inputs</h3>
<p>Many introductions to decision trees focus on the case where the <span class="math inline">\(X\)</span>s are quantitative (numeric) variables. Categorical inputs (factors) can be handled easily in trees.</p>
<p>Suppose <span class="math inline">\(X\)</span> is categorical with <span class="math inline">\(k\)</span> levels in the set <span class="math inline">\(D_k\)</span>. The set of possible values can be divided into <span class="math inline">\(2^{k-1}-1\)</span> possible partitions, which is trivial for <span class="math inline">\(k=1\)</span> and leads to many possible partitions as <span class="math inline">\(k\)</span> grows. However, for values of <span class="math inline">\(k &gt; 2\)</span>, if you split the set <span class="math inline">\(D_k\)</span> into two proper subsets <span class="math inline">\(D_s\)</span> and <span class="math inline">\(D_l\)</span>, where <span class="math inline">\(s+l=k\)</span>, then fewer possible splits need to be considered. In other words, at each split of the categorical variable, we try to split off groups containing 1, 2, or a small number of levels from the other set.</p>
</section>
<section id="missing-values" class="level3">
<h3 class="anchored" data-anchor-id="missing-values">Missing Values</h3>
<p>It is often said that “trees can handle missing values” and by extension tree-based techniques such as random forests or gradient boosted trees are somehow <em>robust</em> to the presence of missing values. That is simply not true. So let us look into how missing values can be handled in decision trees.</p>
<p>First, observations with missing values for the target variable are not automatically handled by tree constructing software. These observation are discarded. For input variables with missing values there are two approaches.</p>
<p>With categorical inputs one approach is to create a new <em>missing</em> category. This will be considered in determining splits just like any other value of the input variable. This approach can shed light on whether observations with missing values exhibit a different pattern from complete observations. Another strategy, that applies to categorical and quantitative inputs, is the use of <strong>surrogate splits</strong>. If we come to a split in the tree and the current observation has a missing value we split on a surrogate variable that does not have missing values. In choosing surrogate variables for splits we choose variables whose splits mimic those of the primary split variable—that is, variables that are highly correlated with the primary variable. If the surrogate split variable is missing, then we can have a surrogate for the surrogate variable and so on.</p>
<p>Surrogate splits can be used during training and during prediction of new observations. They “handle” missingness in the sense that the observations do not need to be omitted from the analysis. However, this does not mean that the resulting analysis is OK. Unless the missing value is MCAR (missing completely at random), the analysis can be biased.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Missing Value Processes">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Missing Value Processes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A detailed study of missing values and their impact on statistical analysis is given in <span class="citation" data-cites="LittleRubin">Little and Rubin (<a href="references.html#ref-LittleRubin" role="doc-biblioref">1987</a>)</span>. They define missing value processes as <strong>MAR</strong> (missing at random) and <strong>MCAR</strong> (missing completely at random).</p>
<p>Suppose we collected data on inputs <span class="math inline">\(\textbf{X}\)</span> and target <span class="math inline">\(\textbf{Y}\)</span>. Some of the rows of <span class="math inline">\(\textbf{X}\)</span> contain missing values and <span class="math inline">\(\textbf{X}_{c}\)</span> are the rows with complete data. Define <span class="math inline">\(\textbf{R}\)</span> as a matrix with typical element <span class="math inline">\(r_{ij} = 1\)</span> if the <span class="math inline">\(j\)</span><sup>th</sup> input in row <span class="math inline">\(i\)</span> is missing, and <span class="math inline">\(r_{ij} = 0\)</span> otherwise.</p>
<p>The missing value process is MAR if the distribution of <span class="math inline">\(\textbf{R}\)</span> depends on the data <span class="math inline">\(\textbf{Z}= [\textbf{Y},\textbf{X}]\)</span> only through <span class="math inline">\(\textbf{Z}_{c} = [\textbf{Y}, \textbf{X}_c]\)</span>: <span class="math display">\[
\Pr(\textbf{R}| \textbf{Z}) = \Pr(\textbf{R}| \textbf{Z}_{c})
\]</span></p>
<p>This essentially states that the mechanism that results in missingness is independent of the unobserved value. If furthermore the distribution of <span class="math inline">\(\textbf{R}\)</span> does not depend on either the observed or missing data, the process is MCAR: <span class="math display">\[
\Pr(\textbf{R}| \textbf{Z}) = \Pr(\textbf{R})
\]</span> If a weight measurement is not taken because the scales unexpectedly malfunctioned the missing data is MAR or MCAR. If the measurement is not taken because the animal is too light or heavy for the scales, the missing value process is not MAR or MCAR. If in a longitudinal health study a patient drops out because their condition worsened, the missingness is not MAR or MCAR. We say that the missingness is <strong>informative</strong>.</p>
<p>An example of informative missingness is a model to predict cardiovascular risk. Because the risk is judged low, a cholesterol measurement is not taken, whereas for patients with expected higher risk for heart health such measurements are routinely done. The absence of the cholesterol measurement in electronic health records can have predictive power for a lower cardiovascular risk, its missingness is informative.</p>
</div>
</div>
</div>
</section>
<section id="optimality" class="level3">
<h3 class="anchored" data-anchor-id="optimality">Optimality</h3>
<p>Decision trees constructed with the CART method (or other methods) are not optimal in that they make concessions for reasons of computational expediency and interpretability.</p>
<ul>
<li><p>Recursive binary splitting leads to partitions of the <span class="math inline">\(X\)</span>-space that are simple to interpret but are not optimal partitions in terms of a loss criterion.</p></li>
<li><p>The greedy splitting algorithm only considers a reduction in variability or increase in purity at the current level of the tree, it does not look ahead and gauge the impact of a split on nodes further down in the tree.</p></li>
<li><p>Weakest-link cutting during cost-complexity pruning does not visit all subtrees, only a nested sequence of subtrees.</p></li>
<li><p>With large data sets, multiway splits rather than binary splits could be beneficial. We use binary splitting to prevent fragmentation of the data into small sets too quickly and hope that a subsequent split further down the tree will achieve the same as a multiway split higher up on the tree.</p></li>
</ul>
</section>
<section id="stability" class="level3">
<h3 class="anchored" data-anchor-id="stability">Stability</h3>
<p>While decision trees are easy to interpret, the lack of stability under small changes of the data reduces their interpretability in the sense that firm conclusions based on trees are difficult. If the tree could change greatly with small changes in the data, how can we be confident that inputs that appear high in the tree (early splits) are particularly important?</p>
<p>On the other hand, decision trees with single splits are stable in a different sense. If single variable splits are used, the tree is invariant with respect to monotone transformations of the inputs <span class="citation" data-cites="Sutton_2005">(<a href="references.html#ref-Sutton_2005" role="doc-biblioref">Sutton 2005</a>)</span>. You do not have to worry whether to use log or square root or reciprocal transformations of an input variable in constructing the tree. As long as large values consistenly map to large transformed values or large values map consistenly to smaller values, the tree is invariant.</p>
</section>
<section id="smoothness" class="level3">
<h3 class="anchored" data-anchor-id="smoothness">Smoothness</h3>
<p>The prediction surface of decision trees is not smooth, it changes abruptly at the boundaries of the leaf nodes. In a regression application the underlying relationship <span class="math inline">\(\text{E}[Y] = f(\textbf{x})\)</span> is normally assumed to be a smooth function of the inputs. In classification models, in particular with binary targets, this is less of an issue.</p>
</section>
<section id="nonlinearity-and-interactions" class="level3">
<h3 class="anchored" data-anchor-id="nonlinearity-and-interactions">Nonlinearity and Interactions</h3>
<p>One advantage of trees over linear model is their ability to model complex nonlinear relationships between inputs and target and non-homogeneous response. For example, the response surface can look different in some regions of the <span class="math inline">\(X\)</span>-space compared to others. A linear model would have to use transformations and interactions to capture this in a single function across the space. A tree can easily adjust through the choice of split variables and split values in different regions of the <span class="math inline">\(X\)</span>-space. So much so, that capturing linear relationships is actually difficult with decision trees. <span class="citation" data-cites="hastie_ESL">Hastie, Tibshirani, and Friedman (<a href="references.html#ref-hastie_ESL" role="doc-biblioref">2001, 274</a>)</span> give the following example:</p>
<p>Suppose that <span class="math inline">\(Y = \beta_1 I(X_1 &lt; t_1) + \beta_2 I(X_2 &lt; t_2) + \epsilon\)</span> is the underlying model. To maintain this additive relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\([X_1,X_2]\)</span>, a tree that would first split on <span class="math inline">\(X_1\)</span> near <span class="math inline">\(t_1\)</span> would then have to split both nodes on <span class="math inline">\(X_2\)</span> near <span class="math inline">\(t_2\)</span>. As <span class="citation" data-cites="hastie_ESL">Hastie, Tibshirani, and Friedman (<a href="references.html#ref-hastie_ESL" role="doc-biblioref">2001</a>)</span> put it</p>
<blockquote class="blockquote">
<p><em>…the model is given no special encouragement to find such structure. If there were ten rather than two aditive effects, … the data analyst would be hard pressed to recognize [the additive structure] in the estimated tree.</em></p>
</blockquote>
<p>Interactions can be captured by trees. However, what appears as interaction between inputs can be caused by just correlation. <span class="citation" data-cites="Sutton_2005">Sutton (<a href="references.html#ref-Sutton_2005" role="doc-biblioref">2005</a>)</span> gives the following example:</p>
<p><span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are highly correlated inputs. A tree first splits on <span class="math inline">\(X_3\)</span>, then on <span class="math inline">\(X_1\)</span> in the left side of the tree and on <span class="math inline">\(X_2\)</span> in the right side of the tree. This might suggest the presence of interactions, but it could be simply that a tree split on only <span class="math inline">\(X_3\)</span> and <span class="math inline">\(X_1\)</span> is equivalent to one split only on <span class="math inline">\(X_3\)</span> and <span class="math inline">\(X_2\)</span>. Both of these trees would suggest an additive relationship rather than an interaction, their equivalence stems from the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Breiman_etal_1984" class="csl-entry" role="listitem">
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. <em>Classification and Regression Trees</em>. Wadsworth, Pacific Grove, CA.
</div>
<div id="ref-hastie_ESL" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.
</div>
<div id="ref-LittleRubin" class="csl-entry" role="listitem">
Little, R., and D. Rubin. 1987. <em>Statistical Analysis with Missing Data</em>. Wiley, New York.
</div>
<div id="ref-Sutton_2005" class="csl-entry" role="listitem">
Sutton, Clifton D. 2005. <span>“Classification and Regression Trees, Bagging, and Boosting.”</span> <em>Handbook of Statistics</em> 24: 303–29.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./supportvectors.html" class="pagination-link" aria-label="Support Vectors">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./treesinR.html" class="pagination-link" aria-label="Trees in `R`">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>