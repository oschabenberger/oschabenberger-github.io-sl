<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 37&nbsp; Interpretability and Explainability</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./reinforcement.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./explainability.html">Part IX. Explainability</a></li><li class="breadcrumb-item"><a href="./explainability.html"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification with Random Inputs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Decision Trees</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesinR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesInPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Trees in Python</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Part VI. Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Part VII. Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Part VIII. Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Part IX. Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">37.1</span> Introduction</a>
  <ul>
  <li><a href="#why-do-we-care" id="toc-why-do-we-care" class="nav-link" data-scroll-target="#why-do-we-care">Why Do we Care?</a></li>
  <li><a href="#a-continuum" id="toc-a-continuum" class="nav-link" data-scroll-target="#a-continuum">A Continuum</a></li>
  <li><a href="#the-mind-map" id="toc-the-mind-map" class="nav-link" data-scroll-target="#the-mind-map">The Mind Map</a></li>
  <li><a href="#words-of-caution" id="toc-words-of-caution" class="nav-link" data-scroll-target="#words-of-caution">Words of Caution</a></li>
  <li><a href="#good-explanations" id="toc-good-explanations" class="nav-link" data-scroll-target="#good-explanations">Good Explanations</a></li>
  </ul></li>
  <li><a href="#sec-expplain-modelagnostic" id="toc-sec-expplain-modelagnostic" class="nav-link" data-scroll-target="#sec-expplain-modelagnostic"><span class="header-section-number">37.2</span> Model-agnostic Explainability</a>
  <ul>
  <li><a href="#sec-explain-pdp" id="toc-sec-explain-pdp" class="nav-link" data-scroll-target="#sec-explain-pdp">Partial Dependence Plots (PDP)</a>
  <ul class="collapse">
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages">Advantages</a></li>
  <li><a href="#disadvantages" id="toc-disadvantages" class="nav-link" data-scroll-target="#disadvantages">Disadvantages</a></li>
  </ul></li>
  <li><a href="#sec-explain-ice" id="toc-sec-explain-ice" class="nav-link" data-scroll-target="#sec-explain-ice">Individual Conditional Expectation (ICE)</a>
  <ul class="collapse">
  <li><a href="#advantages-1" id="toc-advantages-1" class="nav-link" data-scroll-target="#advantages-1">Advantages</a></li>
  <li><a href="#disadvantages-1" id="toc-disadvantages-1" class="nav-link" data-scroll-target="#disadvantages-1">Disadvantages</a></li>
  </ul></li>
  <li><a href="#sec-explain-vimp" id="toc-sec-explain-vimp" class="nav-link" data-scroll-target="#sec-explain-vimp">Variable (Feature) Importance</a>
  <ul class="collapse">
  <li><a href="#advantages-2" id="toc-advantages-2" class="nav-link" data-scroll-target="#advantages-2">Advantages</a></li>
  <li><a href="#disadvantages-2" id="toc-disadvantages-2" class="nav-link" data-scroll-target="#disadvantages-2">Disadvantages</a></li>
  </ul></li>
  <li><a href="#surrogate-models" id="toc-surrogate-models" class="nav-link" data-scroll-target="#surrogate-models">Surrogate Models</a>
  <ul class="collapse">
  <li><a href="#advantages-3" id="toc-advantages-3" class="nav-link" data-scroll-target="#advantages-3">Advantages</a></li>
  <li><a href="#disadvantages-3" id="toc-disadvantages-3" class="nav-link" data-scroll-target="#disadvantages-3">Disadvantages</a></li>
  </ul></li>
  <li><a href="#local-interpretable-model-agnostic-explanation-lime" id="toc-local-interpretable-model-agnostic-explanation-lime" class="nav-link" data-scroll-target="#local-interpretable-model-agnostic-explanation-lime">Local Interpretable Model-Agnostic Explanation (LIME)</a>
  <ul class="collapse">
  <li><a href="#advantages-4" id="toc-advantages-4" class="nav-link" data-scroll-target="#advantages-4">Advantages</a></li>
  <li><a href="#disadvantages-4" id="toc-disadvantages-4" class="nav-link" data-scroll-target="#disadvantages-4">Disadvantages</a></li>
  </ul></li>
  <li><a href="#shapley-additive-explanation-shap" id="toc-shapley-additive-explanation-shap" class="nav-link" data-scroll-target="#shapley-additive-explanation-shap">Shapley Additive Explanation (SHAP)</a>
  <ul class="collapse">
  <li><a href="#shapley-values" id="toc-shapley-values" class="nav-link" data-scroll-target="#shapley-values">Shapley Values</a></li>
  <li><a href="#shapley-values-with-iml" id="toc-shapley-values-with-iml" class="nav-link" data-scroll-target="#shapley-values-with-iml">Shapley Values with <code>iml</code></a></li>
  </ul></li>
  <li><a href="#shap" id="toc-shap" class="nav-link" data-scroll-target="#shap">SHAP</a></li>
  </ul></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">37.3</span> Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./explainability.html">Part IX. Explainability</a></li><li class="breadcrumb-item"><a href="./explainability.html"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-explain" class="quarto-section-identifier"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="37.1">
<h2 data-number="37.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">37.1</span> Introduction</h2>
<p>You do not have to look very hard these days to come across terms such as <strong>interpretable machine learning</strong>, <strong>explainable AI</strong> (xAI), <strong>responsible AI</strong>, <strong>ethical AI</strong>, and others in the discourse about data analytics, machine learning, and artificial intelligence. This chapter deals with questions of interpreting and explaining models derived from data, not with the ethical aspects of the discipline. Although one could argue that working with models that we are unable to explain how they work is not a responsible thing to do.</p>
<p>Interpretability and explainability are often used interchangeably. It is worthwhile making a distinction. We draw in this chapter on the excellent online book “Interpretable Machine Learning” <span class="citation" data-cites="Molnar2022">(<a href="references.html#ref-Molnar2022" role="doc-biblioref">Molnar 2022</a>)</span> although by the author’s definition, most of the book is concerned with explaining models.</p>
<div class="definition">
<div class="definition-header">
<p>Definition: Interpretability and Explainability</p>
</div>
<div class="definition-container">
<p>A system is <strong>interpretable</strong> if it is capable of being understood. In such a system the change that follows when a knob is turned is known. Someone trained in the arts can articulate how the system works, how input is transformed into output. The qualifier “trained in the arts” is added because some threshold of knowledge must be assumed. An internal combustion engine is interpretable, it is capable of being understood—but not by everyone.</p>
<p>A system is <strong>explainable</strong> if we can understand how something happened, how it came up with its answers. The difference to interpretability is subtle. Interpreting focuses on the engine of the system, how it transforms input into output. Explaining focuses on the output side of the system, trying to understand what makes the box work without understanding how the box works (<a href="#fig-inter-explain" class="quarto-xref">Figure&nbsp;<span>37.1</span></a>).</p>
<div id="fig-inter-explain" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inter-explain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/Inter_and_explain.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inter-explain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.1: Interpretability and explainability.
</figcaption>
</figure>
</div>
</div>
</div>
<p>An internal combustion engine is interpretable: gas and air are mixed, compressed, and ignited. The force of the ignition moves one or more pistons which turns a crank shaft. In a car or motorcycle, this motion is translated into tire rotation. The internal combustion engine is also explainable: if I step on the gas pedal the engine revs higher and the car goes faster.</p>
<p>A system that is interpretable is always explainable, but not the other way around. Clearly, we prefer interpretable systems over those that are just explainable.</p>
<section id="why-do-we-care" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-care">Why Do we Care?</h3>
<p>Replace “systems” with “models based on data” in the previous paragraphs and you see how concepts of interpretability and explainability apply to statistical models and machine learning. We care about the topic and about the distinction for several reasons:</p>
<ul>
<li><p>Model interpretability is generally on the decline. The need for <strong>transparency</strong>, on the other hand, is on the rise. Decisions have consequences and someone or something has to be accountable for them.</p></li>
<li><p>There is an inverse relationship between model complexity and interpretability. Simple, interpretable models often do not perform well. The measures we take to improve their performance tend to reduce their interpretability. A great example are decision trees. A single tree is highly interpretable, we say it is <strong>intrinsically interpretable</strong>, its very structure as a tree lends its interpretation. However, a single tree often does not perform well, it has high variance and is sensitive to changes in the data. An ensemble of decision trees, such as in a random forest or a gradient boosting machine, can perform extremely well, but now we have sacrificed interpretability. Instead of a single tree, a collection of 50, 100, or 500 trees are generating a predicted value. A single decision rule was replaced by 500 decision rules.</p></li>
<li><p>Any model can be made less interpretable by adding features. A linear regression model is intrinsically interpretable, but add more inputs, their transformations and interactions, and the entire model becomes much less understandable.</p></li>
<li><p>Models are increasingly seen as a source of risk. As with all risks, that means we need to manage it (understand, contain, insure). A risk that is not understood is difficult to guard against. Ironically, the more complex models that perform well carry a larger risk by virtue of being difficult to understand.</p></li>
</ul>
</section>
<section id="a-continuum" class="level3">
<h3 class="anchored" data-anchor-id="a-continuum">A Continuum</h3>
<p>Interpretability is not an all-or-nothing proposition. As mentioned before, any model becomes less interpretable by adding features.</p>
<p>It is best to think of a spectrum of interpretability and the methods we consider to model data fall somewhere on the spectrum—with room for discussion and movement (<a href="#fig-inter-cont" class="quarto-xref">Figure&nbsp;<span>37.2</span></a>). An expert in artificial neural networks might find them to be more interpretable than an occasional user. It is clear from the figure that more contemporary and currently popular analytic methods tend to appear on the right hand side.</p>
<div id="fig-inter-cont" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inter-cont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/InterpretabilityContinuum.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="images/InterpretabilityContinuum.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inter-cont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.2: Continuum of model interpretability.
</figcaption>
</figure>
</div>
<p>On the left hand side of the figure are what we call <strong>intrinsically interpretable</strong> models. Interpretable models are transparent and can be understood. Intrinsically interpretable models can be understood by looking at the model <em>structure</em>. Single decision trees, simple linear regressions, and nonlinear models that are parameterized in terms of domain-specific quantities and relationships are intrinsically interpretable. <span class="citation" data-cites="sankaran_2024">Sankaran (<a href="references.html#ref-sankaran_2024" role="doc-biblioref">2024</a>)</span> calls intrinsically interpretable models <strong>glass boxes</strong> to distinguish them from non-interpretable <strong>black boxes</strong>.</p>
<div class="example">
<div class="example-header">
<p>Example: Mitscherlich Equation</p>
</div>
<div class="example-container">
<p>In <a href="regnlr.html#sec-nlr-starting-values" class="quarto-xref"><span>Section 9.3</span></a> we encountered the Mitscherlich equation, popular in modeling plant and crop yield:</p>
<p><span class="math display">\[
\text{E}[Y] = \lambda + (\xi-\lambda) \exp\left\{ -\kappa x\right\}
\]</span></p>
<p>The Mitscherlich yield equation is intrinsically interpretable. The parameters have a direct interpretation in terms of the subject matter (<a href="#fig-mitsch-data2" class="quarto-xref">Figure&nbsp;<span>37.3</span></a>):</p>
<ul>
<li><span class="math inline">\(\xi\)</span>: the crop yield at <span class="math inline">\(x=0\)</span></li>
<li><span class="math inline">\(\lambda\)</span>: the upper yield asymptote as <span class="math inline">\(x\)</span> increases</li>
<li><span class="math inline">\(\kappa\)</span>: is related to the rate of change, how quickly the yield increases from <span class="math inline">\(\xi\)</span> to <span class="math inline">\(\lambda\)</span></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mitsch-data2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mitsch-data2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-mitsch-data2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mitsch-data2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.3: Simulated yield data and Mitscherlich model
</figcaption>
</figure>
</div>
</div>
</div>
<p>It is also clear how “turning a knob” in the model changes the output. For example, raising or lowering <span class="math inline">\(\lambda\)</span> affects the asymptotic yield. Changing <span class="math inline">\(\kappa\)</span> affects the shape of the yield curve between <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\lambda\)</span>.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: First-order Compartmental Model</p>
</div>
<div class="example-container">
<p>Figure <a href="#fig-explain-theoph" class="quarto-xref">Figure&nbsp;<span>37.4</span></a> shows a first-order compartmental model for the concentration of a drug in patients over time. The concentration <span class="math inline">\(C_t\)</span> at time <span class="math inline">\(t\)</span> is modeled as a function of dose <span class="math inline">\(D\)</span> as <span class="math display">\[
    \text{E}[C_t] = \frac{D k_e k_a}{Cl(k_a - k_e)} \left \{ \exp(-k_e t) - \exp(-k_a t) \right \}
\]</span> The model is intrinsically interpretable. The parameters represent</p>
<ul>
<li><span class="math inline">\(k_e\)</span>: the elimination rate of the drug</li>
<li><span class="math inline">\(k_a\)</span>: the absorption rate of the drug</li>
<li><span class="math inline">\(Cl\)</span>: the clearance of the drug</li>
</ul>
<div id="fig-explain-theoph" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-explain-theoph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/Theophylline.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explain-theoph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.4: First-order compartmental model. The lines represent two doses of the drug.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Linear regression models are intrinsically interpretable. In the simple linear model <span class="math display">\[
\text{E}[Y] = \beta_0 + \beta_1 x
\]</span> <span class="math inline">\(\beta_0\)</span> is the mean response when <span class="math inline">\(x=0\)</span>, <span class="math inline">\(\beta_1\)</span> represents the change in mean response when <span class="math inline">\(X\)</span> increases by one unit. The model is still intrinsically interpretable when more input variables are added, but the interpretation is more nuanced. In the model <span class="math display">\[
\text{E}[Y] = \beta_0 + \beta_1 x_1 + \beta_2x_2 + \beta_3x_3
\]</span> <span class="math inline">\(\beta_j\)</span> (<span class="math inline">\(j=1,2,3)\)</span> is no longer the change in mean response if <span class="math inline">\(x_j\)</span> increases by one unit. It is the change in mean response if <span class="math inline">\(X_j\)</span> increases by one unit <strong>and</strong> all other <span class="math inline">\(X\)</span>s are held fixed. Add more inputs, factors, feature transformations, and interaction terms and the interpretation becomes even more nuanced.</p>
<p>Adding regularization can make a model more interpretable or less interpretable. Ridge (<span class="math inline">\(L_2\)</span>) regularization does not help with interpretability because it assigns non-zero weight to the model coefficients. A model with 1,000 inputs does not become more interpretable by shrinking 1,000 coefficients somewhat. Lasso (<span class="math inline">\(L_1\)</span>) regularization increases interpretability because it shrinks coefficients all the way to zero, combining regularization with feature selection.</p>
<p>Ensemble methods are less interpretable than their non-ensembled counterparts such as a single decision tree (<a href="#fig-explain-tree" class="quarto-xref">Figure&nbsp;<span>37.5</span></a>).</p>
<div id="fig-explain-tree" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-explain-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/DecisionTreeHealth.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" data-glightbox="description: .lightbox-desc-2"><img src="images/DecisionTreeHealth.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explain-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.5: A single decision tree is intrinsically interpretable.
</figcaption>
</figure>
</div>
<p>Highly over-parameterized nonlinear models such as artificial neural networks are completely uninterpretable. These are the proverbial <strong>black boxes</strong> and being able to explain the model output is the best one can hope for. <a href="#fig-inter-alexnet" class="quarto-xref">Figure&nbsp;<span>37.6</span></a> is a schema of AlexNet, a convolutional neural network that won the ImageNet competition in 2012. The schema tells us how AlexNet is constructed but it is impossible to say how exactly it works. We cannot articulate how an input is transformed into the output, we can only describe what happens to it: it is going through a 11 x 11 convolutional layer with 96 kernels, followed by a max pooling layer, and so on.</p>
<div id="fig-inter-alexnet" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inter-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/AlexNet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" data-glightbox="description: .lightbox-desc-3"><img src="images/AlexNet.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inter-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.6: AlexNet, from <a href="https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/">LearnOpenCV</a>.
</figcaption>
</figure>
</div>
</section>
<section id="the-mind-map" class="level3">
<h3 class="anchored" data-anchor-id="the-mind-map">The Mind Map</h3>
<p><a href="#fig-explain-map" class="quarto-xref">Figure&nbsp;<span>37.7</span></a> is our mind map for model interpretability and explainability. We spend most of the time on the explainability side, trying to determine what drives a particular model. For non-interpretable model, trying to explain how a model arrives at an outcome is all that one can hope for.</p>
<div id="fig-explain-map" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-explain-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/ExplainabilityMindMap.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" data-glightbox="description: .lightbox-desc-4"><img src="images/ExplainabilityMindMap.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explain-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.7: Model explainability mind map
</figcaption>
</figure>
</div>
<p>Explainability tools are categorized in two dimensions:</p>
<ul>
<li><p><strong>Model-agnostic</strong> tools and <strong>model-specific</strong> tools. Model-agnostic tools can be applied to any model family, whether it is an artificial neural network, a support vector machine, <span class="math inline">\(k\)</span>-nearest neighbor, or a nonlinear regression. That makes them very popular because you can apply your favorite explainability tool to whatever statistical learning technique was used.<br>
<br>
As the name suggests, model-specific tools are developed for a specific type of model. For example, explainability tools that deal specifically with neural networks or gradient boosting machines. The specialization enables computational tricks and specialized algorithms. For example, TREESHap was developed to compute Shapley-based measures (see below) for random forests and gradient boosting. It takes care of the special structure when ensembling trees and is much faster than a model-agnostic Shapley tool. This is relevant because computing measures of explainability can be very time consuming.</p></li>
<li><p><strong>Global</strong> and <strong>local</strong> methods. The distinction of global and local explainability tools is different from our distinction of global and local models in <a href="regintro.html#sec-local-global-models" class="quarto-xref"><span>Section 6.2</span></a>. A global explainability method focuses on the model behavior overall, across all observations. A local method explains the individual predictions or classifications, at the observation level. Some approaches, for example, Shapley values, can be used in a local and a global context. In most cases, however, aggregating a local measure across all observations does not yield a corresponding global measure.</p></li>
</ul>
</section>
<section id="words-of-caution" class="level3">
<h3 class="anchored" data-anchor-id="words-of-caution">Words of Caution</h3>
<p>An important aspect of explainability is the focus on the outcome of the model and <strong>understanding the model’s drivers</strong>. This cannot be overemphasized. The tools cannot inform us about the true underlying mechanisms that act on the target. They can only inform us about the model we built. Applying explainability tools to a crappy model does not suddenly reveal some deep insight about the underlying process. It reveals insight into what makes the crappy model tick. We are not validating a model, we are simply finding out what the model can tell us about itself.</p>
<p>Explainability <strong>tools are not free of assumptions</strong>. If these are not met the results can be misleading. A common assumption is that features are “uncorrelated”. We put this in quotes as the features in a model are usually not considered random variables, so they cannot have a correlation in the statistical sense. What is meant by uncorrelated features is that the inputs can take on values independently of each other. In most applications, that is an unrealistic assumption; inputs change with each other, they are collinear. The implications of not meeting the assumption are significant.</p>
<p>Suppose you are predicting the value of homes based on two inputs: the living area in ft<sup>2</sup> and the number of bedrooms. The first variable ranges in the data set from 1,200 to 7,000 ft<sup>2</sup> and the number of bedrooms varies from 1–10. The assumption of “uncorrelated features” implies that you can pair values of the variables independently. An explainability method such as partial dependence plots will evaluate the impact of the input variable number of bedrooms by averaging over living areas of 1,200–7,000 ft<sup>2</sup>. In evaluating the impact of living area, the procedure averages across houses with 1–10 bedrooms. A 7,000 ft<sup>2</sup> home with one bedroom is unlikely and a 1,200 ft<sup>2</sup> home with 10 bedrooms is also difficult to find. A method that assumes “uncorrelated features” will behave as if those combinations are valid. You probably would not start the human-friendly explanation to the CEO with</p>
<blockquote class="blockquote">
<p><em>we evaluated the importance of factors affecting home prices by considering mansions with a single bedroom and tiny houses with nothing but sleeping quarters…</em></p>
</blockquote>
<p>but that is exactly what the explainability tool is giving you. Imagine a model developed for the lower 48 states of the U.S. that contains season (Winter, Spring, Summer, Fall) and average daily temperature as inputs and evaluating the impact of summer months for average daily temperature in the freezing range. It makes no sense.</p>
<p>Explainability tools are not free of <strong>parameters and require choices</strong> that affect their performance. These tools use bandwidths, kernel functions, subsampling, surrogate model types, etc. Software has default settings which might or might not apply to a particular model and data combination.</p>
<p>Another issue to look out for is the <strong>data requirement</strong> of the explainability tool itself. Some methods are based on analyzing predictions of a model while others need access to the original training data. If the only way to study a model is by analyzing predictions returned from an API, you are limited to methods that can be carried out without access to the training data.</p>
</section>
<section id="good-explanations" class="level3">
<h3 class="anchored" data-anchor-id="good-explanations">Good Explanations</h3>
<p>Before we dive into the math and applications of the tools itself, let’s remind ourselves that running explainability tools is not the end goal. A business decision maker is not helped by a Shapley summary or ten partial dependence plots any more as they are helped by a list of regression coefficients. While explainability tools can generate nice summaries and visualizations, by themselves they do not provide an explanation.</p>
<p>In the end the data scientist has to convert the output from the tools into a human-consumable form. <span class="citation" data-cites="Molnar2022">Molnar (<a href="references.html#ref-Molnar2022" role="doc-biblioref">2022</a>)</span> discusses the ingredients of human-friendly explanations. We summarize some of his excellent points. Good explanations</p>
<ul>
<li><p>are <strong>contrastive</strong>; they compare a prediction to another instance in the data, they use counterfactuals, what has <strong>not</strong> happened:<br>
<em>Why did the drug not work for my patient?</em><br>
<em>Why was the predicted home price higher than expected?</em></p></li>
<li><p>are <strong>selective</strong> (sparse); focus on the main factors, not all factors. Keep explanations short, giving up to 3 reasons:<br>
<em>The Washington Huskies lost to the Michigan Wolverines because they could not get their usually explosive offense going.</em></p></li>
<li><p>are <strong>social</strong>; the explanation is appropriate for the social context in which it is given. Charlie Munger explained EBITDA (earnings before interest, taxes, depreciation, and amortization) at the 2003 Berkshire Hathaway annual meeting as follows:<br>
<em>“You wold understand any presentation using the words EBITDA, if every time you saw that word you just substituted the phrase bullshit earnings.”</em></p></li>
<li><p>focus on the <strong>abnormal</strong> in the explanation if abnormal features impact the outcome:<br>
<em>The predicted price of the house was high because it has 16 balconies</em>.</p></li>
<li><p>are <strong>general</strong>; in the absence of abnormal features that drive the explanation good explanations are probable:<br>
<em>The credit score is low for individuals who carry a lot of debt.</em><br>
<em>The house is expensive because it is big.</em></p></li>
</ul>
</section>
</section>
<section id="sec-expplain-modelagnostic" class="level2" data-number="37.2">
<h2 data-number="37.2" class="anchored" data-anchor-id="sec-expplain-modelagnostic"><span class="header-section-number">37.2</span> Model-agnostic Explainability</h2>
<section id="sec-explain-pdp" class="level3">
<h3 class="anchored" data-anchor-id="sec-explain-pdp">Partial Dependence Plots (PDP)</h3>
<p>Partial dependence plots (PDP) are a global method that summarizes the <strong>marginal</strong> effects of input variables across a data set. The plots are typically 1-D or 2-D dependence plots, meaning that they show the marginal effect of one variable or of two variables together. 1-D plots are most common; they display the average response across the daa set as one input variable takes on different values.</p>
<p>Suppose a model contains <span class="math inline">\(p\)</span> features <span class="math inline">\(X_1, \cdots, X_p\)</span>. In the one-dimensional case we choose one of the features, <span class="math inline">\(X_j\)</span> say, and all other features form a complement set <span class="math inline">\(X_\mathcal{C}\)</span>. The partial dependence function for <span class="math inline">\(X_j\)</span> is defined as <span class="math display">\[
f(X_j) = \text{E}_{X_\mathcal{C}}\left[f(X_j,X_\mathcal{C})\right]
\]</span> where the expectation is taken with respect to the joint distribution of the input variables in <span class="math inline">\(X_\mathcal{C}\)</span> (assuming that the <span class="math inline">\(X\)</span>s are random). The partial dependence function is not observable but can be estimated as <span class="math display">\[
\widehat{f}(X_j) = \frac{1}{n} \sum_{i=1}^n f(X_j,x_{i\mathcal{C}})
\]</span> where <span class="math inline">\(x_{i\mathcal{C}}\)</span> are the values of the complement features in the training data. In practice you vary the values of the feature of interest (<span class="math inline">\(X_j\)</span>) over the observed range (or the observed values). For each value <span class="math inline">\(x_j\)</span> the average above is computed, substituting the observed values for all other inputs. The final result is presented as a plot of the <span class="math inline">\(\widehat{f}(x_j)\)</span> versus <span class="math inline">\(x_j\)</span> values.</p>
<p>In the two-dimensional case, the complement set <span class="math inline">\(X_\mathcal{C}\)</span> contains all but two of the input variables, <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span> are the features of interest for the PDP, and the averages are calculated as <span class="math display">\[
\widehat{f}(X_j, X_k) = \frac{1}{n} \sum_{i=1}^n f(X_j,X_k,x_{i\mathcal{C}})
\]</span> The results are presented as a three-dimensional plot (image plot, contour, etc.) of <span class="math inline">\(\widehat{f}_{X_j,X_k}\)</span> against a grid of <span class="math inline">\(X_j, X_k\)</span> values.</p>
<div class="example">
<div class="example-header">
<p>Example: Banana Quality</p>
</div>
<div class="example-container">
<p>The data for this example can be found on <a href="https://www.kaggle.com/datasets/l3llff/banana">kaggle</a> and comprises observations on the quality of bananas (“Good”, “Bad”) and seven attributes. It was used previously in this material to demonstrate bagging and support vector machines.</p>
<p>We classify the observations in the training data set here with a random forest with 500 trees.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>ban_train <span class="ot">&lt;-</span> <span class="fu">duckload</span>(<span class="st">"banana_train"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ban_test <span class="ot">&lt;-</span> <span class="fu">duckload</span>(<span class="st">"banana_test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">54</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="fu">as.factor</span>(Quality) <span class="sc">~</span> . , </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data=</span>ban_train, </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">importance=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#fig-varimp-banana" class="quarto-xref">Figure&nbsp;<span>37.8</span></a> shows that the most important features with respect to improving model accuracy are <code>Softness</code>, <code>Weight</code> and <code>HarvestTime</code>. Two of the three are also most effective in increasing tree node purity.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-varimp-banana" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-varimp-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-varimp-banana-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-varimp-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.8: Variable importance from randomForest analysis for banana data.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>rf.predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf,<span class="at">newdata=</span>ban_test)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>rf.predict.cm <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(rf.predict, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                                        <span class="fu">as.factor</span>(ban_test<span class="sc">$</span>Quality))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>rf.predict.cm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  Bad Good
      Bad  1930   58
      Good   64 1948
                                          
               Accuracy : 0.9695          
                 95% CI : (0.9637, 0.9746)
    No Information Rate : 0.5015          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.939           
                                          
 Mcnemar's Test P-Value : 0.6508          
                                          
            Sensitivity : 0.9679          
            Specificity : 0.9711          
         Pos Pred Value : 0.9708          
         Neg Pred Value : 0.9682          
             Prevalence : 0.4985          
         Detection Rate : 0.4825          
   Detection Prevalence : 0.4970          
      Balanced Accuracy : 0.9695          
                                          
       'Positive' Class : Bad             
                                          </code></pre>
</div>
</div>
<p>The confusion matrix for the test data set shows excellent accuracy of 96.95 % and high sensitivity and specificity.</p>
<p>How does the predicted probability of banana quality depend on the most important features? To answer this question we compute partial dependence plots for <code>Sweetness</code>, <code>Weight</code>, and <code>HarvestTime</code> with the <code>iml</code> package in <code>R</code>.</p>
<p>The <code>iml</code> package is based on R6 classes which gives it an object-oriented flavor. The first step is to set up a prediction container. Once this object is in place we can pass it to various functions to compute explainability measures. For example, the <code>FeatureEffect</code> class implements accumulated local effect plots, partial dependence plots, and individual conditional expectation curves. The <code>FeatureImp</code> class computes permutation-based feature importance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(iml)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>ban.X <span class="ot">&lt;-</span> ban_train[,<span class="fu">which</span>(<span class="fu">names</span>(ban_train) <span class="sc">!=</span> <span class="st">"Quality"</span>)]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> Predictor<span class="sc">$</span><span class="fu">new</span>(rf,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data=</span>ban.X,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">y=</span>ban_train<span class="sc">$</span>Quality)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Partial dependence plot for the three continuous inputs are requested with the <code>FeatureEffects</code> class and <code>method="pdp"</code>. The plot methods of the result objects are based on <code>ggplot2</code> and can be customized by adding <code>ggplot2</code> functions.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>pdp <span class="ot">&lt;-</span> FeatureEffects<span class="sc">$</span><span class="fu">new</span>(model,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">features =</span> <span class="fu">c</span>(<span class="st">"Softness"</span>, <span class="st">"Weight"</span>, <span class="st">"HarvestTime"</span>),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">method=</span><span class="st">"pdp"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>pdp<span class="sc">$</span><span class="fu">plot</span>() <span class="sc">+</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Random Forest"</span>, <span class="at">subtitle=</span><span class="st">"Banana Quality"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-pdp-banana" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pdp-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-pdp-banana-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pdp-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.9: Partial dependence plots for softness, weight, and harvest time features in banana quality prediction.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-pdp-banana" class="quarto-xref">Figure&nbsp;<span>37.9</span></a> shows the partial dependence plots for the three inputs. The vertical axis displays the predicted probabilities, the probabilities for <code>Good</code> and <code>Bad</code> outcomes are complements of each other. Focusing on the PDP for <code>Weight</code>, we can see that the predicted probability for good banana quality initially does not change much for small values of the (scaled) weight, then increases quickly with increasing banana weight.</p>
<p>Each point on the PDP plots is calculated by averaging over the values of the other input variables for all 4,000 observations. The computational effort of the PDP plots is substantial.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What do PDP plots look like for a multiple linear regression model with only main effects?</li>
<li>How do interactions between the features change the PDP plot?</li>
</ul>
</div>
</div>
<section id="advantages" class="level4">
<h4 class="anchored" data-anchor-id="advantages">Advantages</h4>
<p>Partial dependence plots are intuitive, the concept of an average prediction at an input value is easy to grasp. They are also easy to implement. The partial dependence analysis reveals the impact of a feature on the average predicted value.</p>
<p>The partial dependence plots show how model predictions behave for a set of data. It does not matter whether this is training data or test data in the sense that the PDP shows what the model learned from the training data. The (average) predictions change with the values of the inputs in a certain way. This is in contrast to explainability tools that explicitly depend on a measure of model loss, such as feature importance summaries (see <a href="#sec-explain-vimp" class="quarto-xref"><span>Section 37.2.3</span></a>).</p>
</section>
<section id="disadvantages" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages">Disadvantages</h4>
<p>Studying the partial dependence mostly focuses on individual features, rather than the joint impact of features. More than two features of interest cannot be visualized.</p>
<p>Care should be taken not to place too much emphasis on regions of the input space where data are sparse. The rug plot along the horizontal axis shows where data are dense and helps with interpretation of the PDP.</p>
<p>Interactions between inputs can mask the marginal effect. It might appear in a PDP that a particular feature has little impact on the predictions, when in fact it interacts with another feature in such a way that the average effect across the features is nil. The feature is still an important driver of the model, although a marginal plot does not reveal it.</p>
<p>The PDP is based on the assumption that the feature <span class="math inline">\(X_j\)</span> for which the plot is computed is uncorrelated with the other features. In the banana quality example, the PDP variables show only small to modest pairwise correlations with the other features</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(ban.X[,<span class="st">"Softness"</span>]   , ban.X),<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Size Weight Sweetness Softness HarvestTime Ripeness Acidity
[1,] 0.166 -0.181    -0.104        1       0.209   -0.237  -0.153</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(ban.X[,<span class="st">"Weight"</span>]     , ban.X),<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Size Weight Sweetness Softness HarvestTime Ripeness Acidity
[1,] -0.184      1     0.413   -0.181      -0.084   -0.049   0.444</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(ban.X[,<span class="st">"HarvestTime"</span>], ban.X),<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Size Weight Sweetness Softness HarvestTime Ripeness Acidity
[1,] 0.573 -0.084      -0.2    0.209           1    0.112  -0.084</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-explain-ice" class="level3">
<h3 class="anchored" data-anchor-id="sec-explain-ice">Individual Conditional Expectation (ICE)</h3>
<p>Individual Conditional Expectation (ICE) plots are a <strong>local</strong> version of the partial dependence display. They show predictions of <span class="math inline">\(f(X_j,X_\mathcal{S})\)</span> on an observation basis. This is one local method that can be meaningfully aggregated to a global method: when averaged across all observations, ICE plots are partial dependence plots. It is thus common to overlay the plots at the observation level with the PDP.</p>
<div class="example">
<div class="example-header">
<p>Example: Abalone Age</p>
</div>
<div class="example-container">
<p>Abalone is a common name for a group of marine snails (genus <em>Haliotis</em>), found worldwide in colder waters. The flesh of the abalone is widely considered a delicacy and their shells are used for jewelry. Per <a href="https://en.wikipedia.org/wiki/Abalone">Wikipedia</a>, the shell of abalone is particularly strong and made up of stacked tiles with a clingy substance between the tiles (<a href="#fig-abalone-shell" class="quarto-xref">Figure&nbsp;<span>37.10</span></a>). When the shell is struck the tiles move rather than shatter.</p>
<div id="fig-abalone-shell" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-shell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/abalone_red.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-shell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.10: Red Abalone shells.
</figcaption>
</figure>
</div>
<p>The <a href="https://archive-beta.ics.uci.edu/dataset/1/abalone">abalone data</a> we use is available from the UC Irvine Machine Learning Repository and was originally collected by <span class="citation" data-cites="Nash_et_al_1994">Nash et al. (<a href="references.html#ref-Nash_et_al_1994" role="doc-biblioref">1994</a>)</span>.</p>
<p>The problem tackled here is to predict the age of abalone from physical measurements. Abalone are aged by the number of rings seen through a microscope. The process is time consuming, the shell needs to be cut through the cone, stained, and the rings are counted under a microscope. These rings appear in abalone shells after 1.5 years, so the age of the animal in years is 1.5 plus the number of rings.</p>
<p>The data comprise the following measurements on 4,177 animals:</p>
<ul>
<li>Sex: M, F, and I (infant)</li>
<li>Length: Longest shell measurement (in mm)</li>
<li>Diameter: perpendicular to length (in mm)</li>
<li>Height: with meat in shell (in mm)</li>
<li>Whole_Weight: whole abalone (in grams)</li>
<li>Shucked_Weight: weight of meat (in grams)</li>
<li>Viscera_Weight: gut weight after bleeding (in grams)</li>
<li>Shell_Weight: after being dried (in grams)</li>
<li>Rings: +1.5 gives the age in years</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>abalone <span class="ot">&lt;-</span> <span class="fu">duckload</span>(<span class="st">"abalone"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>abalone<span class="sc">$</span>Sex <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(abalone<span class="sc">$</span>Sex)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(abalone)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Sex Length Diameter Height Whole_Weight Shucked_Weight Viscera_Weight
1   M  0.455    0.365  0.095       0.5140         0.2245         0.1010
2   M  0.350    0.265  0.090       0.2255         0.0995         0.0485
3   F  0.530    0.420  0.135       0.6770         0.2565         0.1415
4   M  0.440    0.365  0.125       0.5160         0.2155         0.1140
5   I  0.330    0.255  0.080       0.2050         0.0895         0.0395
6   I  0.425    0.300  0.095       0.3515         0.1410         0.0775
  Shell_Weight Rings
1        0.150    15
2        0.070     7
3        0.210     9
4        0.155    10
5        0.055     7
6        0.120     8</code></pre>
</div>
</div>
<p>The following statements fit a model to predict the number of rings using a gradient boosting machine for Poisson data, using all input variables in the data frame. A Poisson distribution seems reasonable as we are dealing with count data and the average count is not large enough to justify a Gaussian approximation. Setting the interaction depth to 2 allows for two-way interactions between the input variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4876</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>gbm_fit <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Rings <span class="sc">~</span> ., </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">data=</span>abalone,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">interaction.depth=</span><span class="dv">2</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">distribution=</span><span class="st">"poisson"</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">cv.folds =</span> <span class="dv">5</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="fu">gbm.perf</span>(gbm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 100</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-abalone-gbm" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-gbm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-gbm-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-gbm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.11: Training and cross-validation error for gradient boosting machine.
</figcaption>
</figure>
</div>
</div>
</div>
<p>5-fold cross-validation is used to produce an estimate of the test error. Both test and training error are decreasing up until 100 boosting iterations (<a href="#fig-abalone-gbm" class="quarto-xref">Figure&nbsp;<span>37.11</span></a>). Predictions will be based on all 100 trees.</p>
<p>The following statements set up a prediction object for the gradient boosting machine in <code>iml</code> and request ICE plots with PDP overlay for three of the input variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X_data <span class="ot">&lt;-</span> abalone[,<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>aba_model <span class="ot">&lt;-</span> Predictor<span class="sc">$</span><span class="fu">new</span>(gbm_fit,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data=</span>X_data,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">y=</span>abalone<span class="sc">$</span>Rings)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>ice_pdp <span class="ot">&lt;-</span> FeatureEffects<span class="sc">$</span><span class="fu">new</span>(aba_model,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">features =</span> <span class="fu">c</span>(<span class="st">"Diameter"</span>,<span class="st">"Height"</span>,<span class="st">"Whole_Weight"</span>),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">method=</span><span class="st">"pdp+ice"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>ice_pdp<span class="sc">$</span><span class="fu">plot</span>() <span class="sc">+</span> </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Gradient Boosting"</span>, <span class="at">subtitle=</span><span class="st">"Abalone - ICE + PDP"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-abalone-ice-pdp" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-ice-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-ice-pdp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-ice-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.12: ICE plots for diameter, height, and whole animal weight, abalone data.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-abalone-ice-pdp" class="quarto-xref">Figure&nbsp;<span>37.12</span></a> shows one issue with ICE plots, they become very busy for large data sets. Each plot is showing 4,177 trends and the overlaid PDP plot. Notice that for <code>Height</code>, the majority of the observations are below 0 and 0.3 mm, with a few very large values. The ICE and PDP plots cover the entire range of the observed data. One should limit the interpretation of these plots to the dense areas of the observed data; the rug dot plots help with that. Since both ICE and PDP plots assume uncorrelated features, this is especially important because observations are paired with observed values for the other inputs. Averaging predicted values for values that are rare over values for other variables that are extreme is dangerous.</p>
<p>For categorical features, ICE and partial dependence plots are generated for each level of the feature. <a href="#fig-abalone-ice-pdp1" class="quarto-xref">Figure&nbsp;<span>37.13</span></a> shows the plots for the three-level <code>Sex</code> variables. There is considerable variation in the predicted number of rings, with the average predicted number being slightly smaller for infants (<code>Sex</code>=“I”) and about equal for male and female animals.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>ice_pdp1 <span class="ot">&lt;-</span> FeatureEffects<span class="sc">$</span><span class="fu">new</span>(aba_model,</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">features =</span> <span class="fu">c</span>(<span class="st">"Sex"</span>),</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">method=</span><span class="st">"pdp+ice"</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ice_pdp1<span class="sc">$</span><span class="fu">plot</span>() <span class="sc">+</span> </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Gradient Boosting"</span>, <span class="at">subtitle=</span><span class="st">"Abalone - ICE + PDP"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-abalone-ice-pdp1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-ice-pdp1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-ice-pdp1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-ice-pdp1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.13: ICE plots for factor variable <code>Sex</code>, with three levels.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Another useful feature of the individual conditional expectation plot is centering the predicted responses at a particular value of the input variable. <a href="#fig-abalone-ice-pdp2" class="quarto-xref">Figure&nbsp;<span>37.14</span></a> shows the predictions for <code>Diameter</code> centered at a value of 0.25 mm. The vertical axis displays the deviation from <span class="math inline">\(\widehat{f}(X_j=0.25)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>FeatureEffects<span class="sc">$</span><span class="fu">new</span>(aba_model,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">features =</span> <span class="fu">c</span>(<span class="st">"Diameter"</span>),</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method=</span><span class="st">"ice"</span>,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">center.at=</span><span class="fl">0.25</span>) <span class="sc">%&gt;%</span> <span class="fu">plot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-abalone-ice-pdp2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-ice-pdp2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-ice-pdp2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-ice-pdp2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.14: ICE plots for input variable <code>Diameter</code>, centered at diameter of 0.3 mm.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<section id="advantages-1" class="level4">
<h4 class="anchored" data-anchor-id="advantages-1">Advantages</h4>
<p>An advantage of ICE plots over partial dependence plots is the ability to visualize interactions through the observation-level trends that might get washed out in the PDP. The ICE plots for the abalone data do not suggest interactions, the individual trends and the average trend have the same shape.</p>
<p>A second advantage of the ICE plots is that they aggregate to a meaningful summary, namely the partial dependence plot.</p>
<p>Because they show individual predictions, ICE plots give you a sense of the variability of the predicted values for a given value of a feature. For example, the <code>Diameter</code> plot in <a href="#fig-abalone-ice-pdp" class="quarto-xref">Figure&nbsp;<span>37.12</span></a> shows that the number of rings for an animal with diameter 0.4 mm varies by almost 2x, from about 1.7 to 3.2 rings.</p>
</section>
<section id="disadvantages-1" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages-1">Disadvantages</h4>
<p>Like the PDP, the ICE plots assume that inputs can vary independently of each other. For the abalone data, this is hardly the case. All of the inputs show large correlations above 0.7–0.8. You cannot pair arbitrary lengths with arbitrary heights or weights of the animals.</p>
<p>For large data sets the ICE plots become very busy, and overlaying thousands of lines might not tell you much.</p>
</section>
</section>
<section id="sec-explain-vimp" class="level3">
<h3 class="anchored" data-anchor-id="sec-explain-vimp">Variable (Feature) Importance</h3>
<p>Ensemble methods such as random forests, adaptive boosting, or gradient boosting machines have their versions of measuring (and displaying) the importance of input variables as drivers of the model. These are based on concepts such as reduction in loss function, increase in purity, frequency of variable selection in trees, etc.</p>
<p>A model-agnostic way of measuring variable importance is based on the simple idea of data permutation. Suppose you have target <span class="math inline">\(Y\)</span> and used inputs <span class="math inline">\(X_1, \cdots, X_p\)</span> to derive a model to predict or classify <span class="math inline">\(Y\)</span>. If <span class="math inline">\(X_j\)</span> is an important variable in the model, then disrupting (destroying) its association with the target variable should lead to a less performant model. On the other hand, if <span class="math inline">\(X_j\)</span> is not important, then breaking its relationship to <span class="math inline">\(Y\)</span> should not be of great consequence.</p>
<p>Permutation-based importance analysis of <span class="math inline">\(X_j\)</span> destroys the feature’s information on the target by randomly shuffling the values for the feature in the data. All other features remain the same. Compare a loss criterion based on the original data with the loss calculated after shuffling <span class="math inline">\(X_j\)</span>. Important variables will show a large increase in loss when shuffled. Unimportant variables will not change the loss as much.</p>
<div class="example">
<div class="example-header">
<p>Example: Abalone Age (Cont’d)</p>
</div>
<div class="example-container">
<p>The <code>summary</code> of a <code>gbm</code> object displays the relative variable importance as measured by the effect attributable to each input variable in reducing the loss function. For the abalone data, the <code>Shell_Weight</code> and the <code>Shucked_Weight</code> are the most important variables (<a href="#fig-abalone-vimp" class="quarto-xref">Figure&nbsp;<span>37.15</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gbm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                          var    rel.inf
Shell_Weight     Shell_Weight 66.0432484
Shucked_Weight Shucked_Weight 16.9001305
Height                 Height  6.2752199
Sex                       Sex  4.2170913
Length                 Length  2.1421675
Diameter             Diameter  1.8884405
Whole_Weight     Whole_Weight  1.7766336
Viscera_Weight Viscera_Weight  0.7570682</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-abalone-vimp" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-vimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-vimp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-vimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.15: Relative variable importance in gradient boosting machine.
</figcaption>
</figure>
</div>
</div>
</div>
<p>How does this compare to a permutation-based variable importance analysis? The <code>FeatureImp</code> class in the <code>iml</code> package computes feature importance based on a permutation analysis. The following code computes and displays the feature importance based on shuffling each variable twenty times.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>FeatureImp<span class="sc">$</span><span class="fu">new</span>(aba_model, </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">loss=</span><span class="st">"mse"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">n.repetitions=</span><span class="dv">20</span>) <span class="sc">%&gt;%</span> <span class="fu">plot</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-abalone-vimp2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abalone-vimp2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-abalone-vimp2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abalone-vimp2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.16: Permutation-based variable importance for abalone gradient boosting machine.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Interestingly, <code>Shell_Weight</code> is the most important driver of the predictions in the permutation-based analysis as well. This is followed by several variables with equal–and large–importance (<a href="#fig-abalone-vimp2" class="quarto-xref">Figure&nbsp;<span>37.16</span></a>). <code>Shucked_Weight</code>, judged second-most important in the boosting model based on the reduction in MSE during training, is considered least important based on the permutation analysis.</p>
<p>The horizontal bars around some of the data points in <a href="#fig-abalone-vimp2" class="quarto-xref">Figure&nbsp;<span>37.16</span></a> show the variability of the results for the input variable across the 20 repetitions.</p>
</div>
</div>
<section id="advantages-2" class="level4">
<h4 class="anchored" data-anchor-id="advantages-2">Advantages</h4>
<p>Feature importance based on permutation is easy to understand: destroy the association between target and feature by shuffling the data and measure the change in loss.</p>
<p>The plot provides a compressed, global insight into the model.</p>
</section>
<section id="disadvantages-2" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages-2">Disadvantages</h4>
<p>The plot should be interpreted with caution when features are correlated. Breaking the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span> does not reveal the relative importance of <span class="math inline">\(X_j\)</span> if its effect can be picked up by other variables with which it is correlated.</p>
<p>Shuffling the data at random can produce unlikely combinations of inputs.</p>
<p>To perform permutation-based variable importance you need access to the original <span class="math inline">\(Y\)</span> values in the training data. When a model is evaluated based on its predictions alone, that information is probably not available.</p>
<p>The results depend on random processes and can vary greatly from run to run.</p>
<p>Feature importance is based on changes in the loss function and thus depend on whether they are applied to training data or test data. When using it on the training data, it tells us which features the model relies on in making predictions. Features are only “important” in this sense. An overfit model relies on certain features too much.</p>
</section>
</section>
<section id="surrogate-models" class="level3">
<h3 class="anchored" data-anchor-id="surrogate-models">Surrogate Models</h3>
<p>Surrogate modeling applies an interpretable method to shed light on the predictions of an un-interpretable method. For example, you can train a decision tree on the predictions of an artificial neural network.</p>
<p>This is a global method where the performance of the black-box model does not affect the training of the surrogate model. The steps in training a surrogate are as follows:</p>
<ol type="1">
<li><p>Choose the data set for which predictions are desired, it can be the original training data set or a test data set.</p></li>
<li><p>Compute the predictions from the black-box model.</p></li>
<li><p>Train an interpretable model on those predictions and the data set from step 1.</p></li>
<li><p>Measure how well the predictions of the surrogate model in 3. agree with the predictions of the black-box model in 2.</p></li>
<li><p>If satisfied with the agreement, base interpretation of the black-box model on the interpretable surrogate.</p></li>
</ol>
<p>The agreement between black-box and surrogate model is calculated for a qualitative target based on the confusion matrix. For quantitative targets the agreement is based on <span class="math inline">\(R^2\)</span>-type measures that express how much of the variability in the black-box prediction is explained by the surrogate predictions <span id="eq-surrogate-agreement"><span class="math display">\[ 1 - \frac{\sum_{i=1}^n \left ( \widehat{y}_i^* - \widehat{y}_i\right )^2}
    {\sum_{i=1}^n \left( \widehat{y}_i - \overline{\widehat{y}} \right )^2}
\tag{37.1}\]</span></span></p>
<p>In <a href="#eq-surrogate-agreement" class="quarto-xref">Equation&nbsp;<span>37.1</span></a> <span class="math inline">\(\widehat{y}_i^*\)</span> is the prediction for the <span class="math inline">\(i\)</span><sup>th</sup> observation in the surrogate model, <span class="math inline">\(\widehat{y}_i\)</span> is the prediction from the black-box model, and <span class="math inline">\(\overline{\widehat{y}}\)</span> is the average black-box prediction.</p>
<div class="example">
<div class="example-header">
<p>Example: US Crime Data (MASS)</p>
</div>
<div class="example-container">
<p>We are fitting a model to the US crime data in the <code>MASS</code> library using Bayesian Model Averaging (see <a href="bma.html" class="quarto-xref"><span>Chapter 22</span></a>). This creates an ensemble model where predicted values are a weighted average of the models considered in BMA. Can we find a surrogate model that explains which features are driving the predictions?</p>
<p>Note that this data contains aggregate data on 47 U.S. states from 1960 and that the variables have been scaled somehow. We are not going to overinterpret the results in a contemporary context. The example is meant to highlight the steps in creating a surrogate model.</p>
<p>The following statements fit a regression model by Bayesian Model Averaging to the log crime rate per population. With the exception of the second input variable, an indicator for a Southern state, all other inputs are also log-transformed.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BMA)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(UScrime)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> UScrime[,<span class="sc">-</span><span class="dv">16</span>]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">log</span>(UScrime[,<span class="dv">16</span>])</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>x[,<span class="sc">-</span><span class="dv">2</span>]<span class="ot">&lt;-</span> <span class="fu">log</span>(x[,<span class="sc">-</span><span class="dv">2</span>])</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>crimeBMA <span class="ot">&lt;-</span> <span class="fu">bicreg</span>(x,y,<span class="at">strict=</span><span class="cn">FALSE</span>,<span class="at">OR=</span><span class="dv">20</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>predBMA <span class="ot">&lt;-</span> <span class="fu">predict</span>(crimeBMA, <span class="at">newdata=</span>x)<span class="sc">$</span>mean</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(crimeBMA)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
bicreg(x = x, y = y, strict = FALSE, OR = 20)


  115  models were selected
 Best  5  models (cumulative posterior probability =  0.2039 ): 

           p!=0    EV        SD       model 1    model 2    model 3  
Intercept  100.0  -23.45301  5.58897  -22.63715  -24.38362  -25.94554
M           97.3    1.38103  0.53531    1.47803    1.51437    1.60455
So          11.7    0.01398  0.05640      .          .          .    
Ed         100.0    2.12101  0.52527    2.22117    2.38935    1.99973
Po1         72.2    0.64849  0.46544    0.85244    0.91047    0.73577
Po2         32.0    0.24735  0.43829      .          .          .    
LF           6.0    0.01834  0.16242      .          .          .    
M.F          7.0   -0.06285  0.46566      .          .          .    
Pop         30.1   -0.01862  0.03626      .          .          .    
NW          88.0    0.08894  0.05089    0.10888    0.08456    0.11191
U1          15.1   -0.03282  0.14586      .          .          .    
U2          80.7    0.26761  0.19882    0.28874    0.32169    0.27422
GDP         31.9    0.18726  0.34986      .          .        0.54105
Ineq       100.0    1.38180  0.33460    1.23775    1.23088    1.41942
Prob        99.2   -0.24962  0.09999   -0.31040   -0.19062   -0.29989
Time        43.7   -0.12463  0.17627   -0.28659      .       -0.29682
                                                                     
nVar                                      8          7          9    
r2                                      0.842      0.826      0.851  
BIC                                   -55.91243  -55.36499  -54.69225
post prob                               0.062      0.047      0.034  
           model 4    model 5  
Intercept  -22.80644  -24.50477
M            1.26830    1.46061
So             .          .    
Ed           2.17788    2.39875
Po1          0.98597      .    
Po2            .        0.90689
LF             .          .    
M.F            .          .    
Pop         -0.05685      .    
NW           0.09745    0.08534
U1             .          .    
U2           0.28054    0.32977
GDP            .          .    
Ineq         1.32157    1.29370
Prob        -0.21636   -0.20614
Time           .          .    
                               
nVar           8          7    
r2           0.838      0.823  
BIC        -54.60434  -54.40788
post prob    0.032      0.029  </code></pre>
</div>
</div>
<p>The BMA ensemble comprise 115 individual regression models where the input variables contribute to different degrees. Mean years of schooling (<code>Ed</code>) appears in all models whereas the labor force participation rate (<code>LF</code>) in only 6% of the models. Next we train a decision tree on the predictions to see if we can explain the overall predictions across the 115 models.</p>
<p>Because the data frame has only 47 observations, we set the <code>minsplit</code> parameter of the recursive partitioning algorithm to 10 (the default is 20).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>surrogate_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">predicted=</span>predBMA,<span class="at">X=</span>x)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87654</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>t1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(predicted <span class="sc">~</span> . , </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">data=</span>surrogate_data,</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">control=</span><span class="fu">rpart.control</span>(<span class="at">minsplit=</span><span class="dv">10</span>,<span class="at">maxcompete=</span><span class="dv">2</span>),</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">method=</span><span class="st">"anova"</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(t1,<span class="at">roundint=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-surrogate-tree" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surrogate-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-surrogate-tree-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surrogate-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.17: Full decision tree trained on BMA predictions of U.S. Crime data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The tree in <a href="#fig-surrogate-tree" class="quarto-xref">Figure&nbsp;<span>37.17</span></a> is quite deep and is based on seven variables. How well do the predictions of the tree and the BMA regression agree? The following code chunk computes the <span class="math inline">\(R^2\)</span> between the two prediction vectors using the <code>lm</code> function.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(predBMA <span class="sc">~</span> <span class="fu">predict</span>(t1)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = predBMA ~ predict(t1))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31638 -0.09604  0.00086  0.08052  0.28606 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 4.190e-15  3.691e-01    0.00        1    
predict(t1) 1.000e+00  5.481e-02   18.25   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1296 on 45 degrees of freedom
Multiple R-squared:  0.8809,    Adjusted R-squared:  0.8783 
F-statistic: 332.9 on 1 and 45 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span>predBMA,<span class="at">y=</span><span class="fu">predict</span>(t1),<span class="at">type=</span><span class="st">"p"</span>, </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"BMA prediction"</span>,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Decision tree prediction"</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">las=</span><span class="dv">1</span>,<span class="at">bty=</span><span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>The decision tree explains 88% of the variability in the BMA predictions. The relationship between the two prediction vectors appears linear. The big question is whether we should be satisfied with this level of agreement? And if so, does the surrogate decision tree provided a reasonable explanation of what drives the BMA model?</p>
<p>For example, one could argue that <code>PO1</code> and <code>PO2</code>, the policing expenditures in two successive years are probably measuring the same thing. Also, <code>U1</code> and <code>U2</code> are both unemployment rates, just in two different age groups. Maybe the tree in <a href="#fig-surrogate-tree" class="quarto-xref">Figure&nbsp;<span>37.17</span></a> can be simplified.</p>
<p>The cross-validation analysis of <code>rpart</code> shows that a much smaller tree minimizes the cross-validation error.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plotcp</span>(t1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-surrogate-tree2" class="quarto-xref">Figure&nbsp;<span>37.18</span></a> shows the trees with two and three terminal nodes.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>t2 <span class="ot">&lt;-</span> <span class="fu">prune</span>(t1,<span class="at">cp=</span><span class="fl">0.1</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>t3 <span class="ot">&lt;-</span> <span class="fu">prune</span>(t1,<span class="at">cp=</span><span class="fl">0.23</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(predBMA <span class="sc">~</span> <span class="fu">predict</span>(t2)))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6058538</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(predBMA <span class="sc">~</span> <span class="fu">predict</span>(t3)))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5034857</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(t2,<span class="at">roundint=</span><span class="cn">FALSE</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(t3,<span class="at">roundint=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-surrogate-tree2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surrogate-tree2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-surrogate-tree2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surrogate-tree2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.18: Pruned decision trees.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The simpler trees explain only 50% and 60% of the variability in the BMA predictions, yet are much simpler to explain. The trees essentially convey that what drives the BMA predictions are the policing expenditures.</p>
</div>
</div>
<section id="advantages-3" class="level4">
<h4 class="anchored" data-anchor-id="advantages-3">Advantages</h4>
<p>The surrogate model approach is flexible, you can use any interpretable model type and apply it to any black-box model. The approach is intuitive, straightforward, and easy to implement.</p>
</section>
<section id="disadvantages-3" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages-3">Disadvantages</h4>
<p>The disadvantages of the surrogate model approach outweigh its advantages:</p>
<ul>
<li><p>You are replacing one model with another, it can overfit or underfit, and needs to be trained appropriately.</p></li>
<li><p>It is not clear what level of agreement between surrogate and black-box predictions is adequate. Is an <span class="math inline">\(R^2\)</span> of 0.7 enough to rely on the surrogate model for explanations?</p></li>
<li><p>You can introduce new input variables in the surrogate model on which the black-box model does not depend.</p></li>
<li><p>The surrogate model can work well on one subset of the data and perform poorly on another subset.</p></li>
<li><p>The surrogate model never sees the target values in the training data; it is trained on predictions.</p></li>
<li><p>There is potential for misuse by guiding the surrogate model toward explanations that fit a narrative.</p></li>
<li><p>The surrogate model can give the illusion of interpretability. Remember that you are explaining the predictions of the model, not the underlying process that generated the data. Close agreement between surrogate and black-box predictions for a poorly fitting black-box model means you found a way to reproduce bad predictions.</p></li>
</ul>
</section>
</section>
<section id="local-interpretable-model-agnostic-explanation-lime" class="level3">
<h3 class="anchored" data-anchor-id="local-interpretable-model-agnostic-explanation-lime">Local Interpretable Model-Agnostic Explanation (LIME)</h3>
<p>Local interpretable model-agnostic explanations (LIME) is a specific local approach to surrogate modeling. It is based on the idea of fitting locally to individual observations interpretable models. Rather than assuming a single global surrogate, a different surrogate is fit to each observation, the data for the local fit comprises small perturbations of the original data in the neighborhood of <span class="math inline">\(\textbf{x}_i\)</span>.</p>
<p>The idea is intuitive. It is as if a complex global model is approximated in the neighborhood of an input vector <span class="math inline">\(\textbf{x}_i\)</span> by a simple local model. The approach is reminiscent of fitting local models in <a href="regintro.html#sec-local-global-models" class="quarto-xref"><span>Section 6.2</span></a> with the distinction that we are not using the original data in a neighborhood of <span class="math inline">\(\textbf{x}_i\)</span>, but perturbed samples. <span class="citation" data-cites="Molnar2022">Molnar (<a href="references.html#ref-Molnar2022" role="doc-biblioref">2022</a>)</span> explains:</p>
<blockquote class="blockquote">
<p><em>LIME tests what happens to the predictions when you give variations of your data into the machine learning model. LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest.</em></p>
</blockquote>
<p>In this sense LIME does not require the training data, as is the case for global surrogate models. It just requires a mechanism to generate data locally near <span class="math inline">\(\textbf{x}_i\)</span> to which the surrogate model can be fit. For example, the surrogate model might be a multiple linear regression in just two of the inputs.</p>
<p>The steps to train a local surrogate are as follows:</p>
<ol type="1">
<li><p>Select the observation <span class="math inline">\(\textbf{x}_0\)</span> for which you want to explain the prediction of the black-box model.</p></li>
<li><p>Perturb the data around <span class="math inline">\(\textbf{x}_0\)</span> and obtain the black-box prediction for the perturbed data.</p></li>
<li><p>Train a weighted, interpretable model to the local data, weighing data points by proximity to <span class="math inline">\(\textbf{x}_0\)</span>.</p></li>
<li><p>Explain the black-box prediction using the prediction from the local surrogate model.</p></li>
</ol>
<p>In practice, the local models are frequently linear regression models and the perturbations are performed by adding random noise to the inputs. The neighborhood weights are determined by kernel functions with fixed bandwidth. The software requires to select a priori <span class="math inline">\(p^*\)</span> the number of inputs in the local surrogate model. Which <span class="math inline">\(p^*\)</span> inputs are chosen is then determined by a feature selection process (variable selection or Lasso with a penalty that leads to <span class="math inline">\(p^*\)</span> non-zero coefficients).</p>
<section id="advantages-4" class="level4">
<h4 class="anchored" data-anchor-id="advantages-4">Advantages</h4>
<p>The same surrogate model approach can be used for explanations, regardless of the type of black-box model.</p>
<p>LIME can be applied to tabular data, text, and image data.</p>
<p>It is possible to use different inputs or transformations of the inputs in the local model. For example, one could generate LIME models based on the original variables when interpreting components in a PCA.</p>
</section>
<section id="disadvantages-4" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages-4">Disadvantages</h4>
<p>The practical implementation of LIME makes several important choices that have effect on the analysis: for example, the definition of the neighborhood in terms of kernel functions and bandwidths and how to generate samples in the neighborhood.</p>
<p>Generating data sets for the local surrogate model should take into account the covariance structure of the inputs but that is typically ignored.</p>
<p>The LIME explanations are somewhat unstable. Like the global surrogate models, the explanations can be manipulated to fit a narrative. For example, one can choose the predictor variables for the local surrogate. This ensures that all explanations are given in terms of those variables.</p>
<div class="example">
<div class="example-header">
<p>Example: Boston Housing Data</p>
</div>
<div class="example-container">
<p>The approach to local surrogate modeling taken in the <code>iml</code> package is similar to the original LIME paper, with some differences:</p>
<ul>
<li><p><code>iml</code> uses the Gower distance by default instead of a kernel based on Euclidean distance. You can choose other distance metrics with <code>dist.fun=</code>.</p></li>
<li><p><code>iml</code> samples from the data rather than from a normal distribution</p></li>
</ul>
<p>We start by fitting a random forest to the Boston housing data and creating an <code>iml</code> predictor object.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"Boston"</span>, <span class="at">package =</span> <span class="st">"MASS"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> Boston[,<span class="fu">which</span>(<span class="fu">names</span>(Boston) <span class="sc">!=</span> <span class="st">"medv"</span>)]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">98</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data=</span>Boston, <span class="at">ntree=</span><span class="dv">50</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>rf_mod <span class="ot">&lt;-</span> Predictor<span class="sc">$</span><span class="fu">new</span>(rf, <span class="at">data=</span>X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Explain the first instance of the data set with the <code>LocalModel</code> class using two predictors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>lemon <span class="ot">&lt;-</span> LocalModel<span class="sc">$</span><span class="fu">new</span>(rf_mod, <span class="at">x.interest=</span>X[<span class="dv">1</span>,], <span class="at">k=</span><span class="dv">2</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>lemon</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Interpretation method:  LocalModel 


Analysed predictor: 
Prediction task: unknown 


Analysed data:
Sampling from data.frame with 506 rows and 13 columns.


Head of results:
            beta x.recoded    effect x.original feature feature.value
rm     3.1487444     6.575 20.702995      6.575      rm      rm=6.575
lstat -0.2836397     4.980 -1.412526       4.98   lstat    lstat=4.98</code></pre>
</div>
</div>
<p>The <code>beta</code> column shows the coefficients for the local model at the particular <span class="math inline">\(\textbf{x}\)</span> location, here we have a linear regression model with <span class="math inline">\(medv = \beta_0 +\)</span> 3.1487 <span class="math inline">\(*6.575 +\)</span> -0.2836 <span class="math inline">\(*4.98\)</span>. The <code>effect</code> is the product of the coefficient with the <span class="math inline">\(x\)</span>-value. So, <span class="math inline">\(medv = \beta_0 +\)</span> 20.703 + -1.4125. Unfortunately, the intercept is not shown, but you can infer it by predicting from the local model at <span class="math inline">\(x\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lemon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  prediction
1   25.96882</code></pre>
</div>
</div>
<p>The <code>plot</code> function shows the predictions in terms of effect sizes <span class="math inline">\(\beta_jx_j\)</span>, see <a href="#fig-boston-lime" class="quarto-xref">Figure&nbsp;<span>37.19</span></a>. The local model predicts the median housing value for the first observation as 25.95, the black-box model predicted a value of 24.75. The local prediction is mostly driven by a positive effect from variable <code>rm</code> with a negative effect from <code>lstat</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lemon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-boston-lime" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boston-lime-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-boston-lime-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boston-lime-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.19: Results from local surrogate prediction for the first observation.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The <code>iml</code> package supports reuse of objects. After creating a new model, you can use it to explain other observations of interest. Here is a local surrogate with <span class="math inline">\(p^* = 3\)</span> and explanations for the first three observations.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>lemon <span class="ot">&lt;-</span> LocalModel<span class="sc">$</span><span class="fu">new</span>(rf_mod, <span class="at">x.interest=</span>X[<span class="dv">1</span>,], <span class="at">k=</span><span class="dv">3</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>lemon<span class="sc">$</span>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              beta x.recoded    effect x.original feature feature.value
rm       4.4211584     6.575 29.069117      6.575      rm      rm=6.575
ptratio -0.5263060    15.300 -8.052482       15.3 ptratio  ptratio=15.3
lstat   -0.4142289     4.980 -2.062860       4.98   lstat    lstat=4.98</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>lemon<span class="sc">$</span><span class="fu">explain</span>(X[<span class="dv">2</span>,])</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>lemon<span class="sc">$</span>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              beta x.recoded    effect x.original feature feature.value
rm       4.2555509     6.421 27.324892      6.421      rm      rm=6.421
ptratio -0.5373882    17.800 -9.565510       17.8 ptratio  ptratio=17.8
lstat   -0.4217386     9.140 -3.854691       9.14   lstat    lstat=9.14</code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>lemon<span class="sc">$</span><span class="fu">explain</span>(X[<span class="dv">3</span>,])</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>lemon<span class="sc">$</span>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              beta x.recoded    effect x.original feature feature.value
rm       4.3786461     7.185 31.460572      7.185      rm      rm=7.185
ptratio -0.5306840    17.800 -9.446175       17.8 ptratio  ptratio=17.8
lstat   -0.4173738     4.030 -1.682016       4.03   lstat    lstat=4.03</code></pre>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="shapley-additive-explanation-shap" class="level3">
<h3 class="anchored" data-anchor-id="shapley-additive-explanation-shap">Shapley Additive Explanation (SHAP)</h3>
<p>SHAP is an approach to explainability that has gained much momentum. It is based on <strong>Shapley values</strong>, introduced in a game-theoretic consideration on how to fairly distribute the payout among players in a coalition based on their individual contributions <span class="citation" data-cites="Shapley_1953">(<a href="references.html#ref-Shapley_1953" role="doc-biblioref">Shapley 1953</a>)</span>.</p>
<p>Besides the grounding in theory, SHAP has other appealing properties: the local Shapley values on which it is based can be aggregated into meaningful global values. Shapley values are the foundation for partial dependence, feature importance and other analyses. And because it can be applied locally and globally, SHAP provides many of the previously discussed explainability tools on a single foundation.</p>
<p>Before we can understand how SHAP works, we need to understand what Shapley values are. Otherwise we would be explaining a black box model with black box measures—that would be ironic.</p>
<section id="shapley-values" class="level4">
<h4 class="anchored" data-anchor-id="shapley-values">Shapley Values</h4>
<p><span class="citation" data-cites="Shapley_1953">Shapley (<a href="references.html#ref-Shapley_1953" role="doc-biblioref">1953</a>)</span> considered the following problem: a group (coalition) of players are playing a game, and as part of the game play there is a gain for the group. How should the gain be paid out fairly among the players in accordance with their contributions toward the gain?</p>
<p>How does this consideration translate to predictions in statistical models?</p>
<ul>
<li>The <strong>game</strong> is making a prediction using a statistical model.</li>
<li>The <strong>players</strong> in the game are the features (inputs) of the model.</li>
<li>The <strong>coalition</strong> is a particular collection of features, not all of which may be present.</li>
<li>The <strong>gain</strong> is the difference between the prediction and some baseline.</li>
<li>The <strong>payout</strong> is the distribution of the gain among the features according to their contribution toward the prediction.</li>
</ul>
<p>Shapley values quantify the contribution each feature makes to the model’s prediction.</p>
<div class="definition">
<div class="definition-header">
<p>Definition: Shapley Value of a Feature</p>
</div>
<div class="definition-container">
<p>The <strong>Shapley value</strong> of a feature is the (weighted) average of its marginal contributions to all possible coalitions.</p>
<ul>
<li><p>The <strong>marginal contribution</strong> of a feature is the difference between a prediction in a model with and without the feature present.</p></li>
<li><p>A <strong>coalition</strong> is a combination of features. In a model with <span class="math inline">\(p\)</span> features (inputs) there are <span class="math inline">\(2^p\)</span> possible coalitions (all possible subsets).</p></li>
<li><p>The <strong>weight</strong> of the marginal contribution to a model with <span class="math inline">\(m\)</span> features is the reciprocal of the number of possible marginal contributions to all models with <span class="math inline">\(m\)</span> features.</p></li>
</ul>
</div>
</div>
<p>An example adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span> will make these concepts tangible.</p>
<p>Suppose we want to predict a person’s income based on their age, gender, and job. The eight possible models—the eight coalitions—are</p>
<div id="fig-shapley-ex1" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shapley-ex1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/Shapley_Example1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" data-glightbox="description: .lightbox-desc-5"><img src="images/Shapley_Example1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shapley-ex1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.20: The eight coalitions. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>Imagine that we fit the eight different models to the training data and for some input vector <span class="math inline">\(\textbf{x}_0\)</span> we perform the predictions. The predicted values are shown in <a href="#fig-shapley-ex2" class="quarto-xref">Figure&nbsp;<span>37.21</span></a>.</p>
<div id="fig-shapley-ex2" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shapley-ex2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/Shapley_Example2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" data-glightbox="description: .lightbox-desc-6"><img src="images/Shapley_Example2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shapley-ex2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.21: Predicting under eight models. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>Next we compute the marginal contributions of the features. The calculation is detailed for Age in <a href="#fig-shapley-ex3" class="quarto-xref">Figure&nbsp;<span>37.22</span></a>.</p>
<div id="fig-shapley-ex3" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shapley-ex3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/Shapley_Example3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" data-glightbox="description: .lightbox-desc-7"><img src="images/Shapley_Example3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shapley-ex3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.22: Computing the marginal contributions of the features. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>To do this for feature Age, we consider the possible coalitions where age is added to a model as you move from one stage of the hierarchy to the next. Starting from the empty model (no inputs), there is one model of the single-input model that differs by the presence of Age. The difference in predicted value is <span class="math inline">\(-\$10K\)</span>.</p>
<p>Similarly, when moving from the single-predictor models to the two-predictor models there are two models that differ in the presence of the Age variable. The change in predicted value by adding Age as a predictor is <span class="math inline">\(-\$9K\)</span> and <span class="math inline">\(-\$15K\)</span>. Finally, there is a single model that adds Age to the two-predictor model, with a change in predicted value of <span class="math inline">\(-\$12K\)</span>.</p>
<p>The weight assigned to the changes in predicted value is the reciprocal of the number of green lines (dotted and solid) at each level of the hierarchy.</p>
<p>Putting it all together gives the Shapley value for Age: <span class="math display">\[
\text{Shap}(Age) =\frac13 (-\$10K) + \frac16 (-\$9K) + \frac16 (-\$15K) + \frac13 (-\$12K) = -\$11.33K
\]</span> Similar calculations for the other features yield <span class="math display">\[
\text{Shap}(Gender) = -\$2.33K
\]</span> <span class="math display">\[
\text{Shap}(Job) = \$46.66K
\]</span> These are the <strong>payout</strong> contributions to the features. If you add them up you obtain the <strong>total payout</strong> of <span class="math display">\[
\text{Shap}(Age) + \text{Shap}(Gender) + \text{Shap}(Job) = -\$11.33K - \$2.33K +  \$46.66K = \$33K
\]</span> This total payout is also the difference between the null model without inputs and the full model with three inputs—the gain of the prediction.</p>
<hr>
<p>In practice we do not fit all possible (<span class="math inline">\(2^p\)</span>) models. Several modifications are necessary to make the Shapley values computationally feasible in our context:</p>
<ul>
<li>The gain is calculated as the difference between <span class="math inline">\(\widehat{y}(\textbf{x}_0)\)</span> and the average of all predictions.</li>
<li>Features not in the coalition are replaced by randomly selected values rather than considering all possible coalitions.</li>
</ul>
<p>Additional approximations and sampling are put in place to make the Shapley values computationally feasible (reasonable).</p>
</section>
<section id="shapley-values-with-iml" class="level4">
<h4 class="anchored" data-anchor-id="shapley-values-with-iml">Shapley Values with <code>iml</code></h4>
<div class="example">
<div class="example-header">
<p>Example: Banana Quality (Cont’d)</p>
</div>
<div class="example-container">
<p>We return to the banana quality data to compute the Shapley values for a model trained with a support vector machine.</p>
<p>The following statements compute a SVM with radial kernel based on all input variables except <code>Weight</code> and generates a matrix of predicted probabilities.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>ban.svm <span class="ot">&lt;-</span> <span class="fu">svm</span>(<span class="fu">as.factor</span>(Quality) <span class="sc">~</span> Size <span class="sc">+</span> Sweetness <span class="sc">+</span> Softness <span class="sc">+</span> </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                                    HarvestTime <span class="sc">+</span> Ripeness <span class="sc">+</span> Acidity,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">data=</span>ban_train, </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">kernel=</span><span class="st">"radial"</span>,</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">probability=</span><span class="cn">TRUE</span>,</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">cost=</span><span class="dv">3</span>,</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">gamma=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">7</span>,</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale=</span><span class="cn">TRUE</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>ban.svm.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ban.svm,<span class="at">newdata=</span>ban_test,<span class="at">probability=</span><span class="cn">TRUE</span>)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The next code chunk sets up the <code>iml</code> predictor object and computes the variable importance plot based on permutation. The loss criterion for the classification model is cross-entropy (<code>loss="ce"</code>). <code>Softness</code>, <code>Sweetness</code>, and <code>HarvestTime</code> emerge as the most important variables. Note, however, the substantial variation in the importance measures.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>ban.X <span class="ot">=</span> ban_train[,<span class="fu">which</span>(<span class="fu">names</span>(ban_train) <span class="sc">!=</span> <span class="st">"Quality"</span>)]</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>ban.X <span class="ot">=</span> ban.X[,<span class="fu">which</span>(<span class="fu">names</span>(ban.X) <span class="sc">!=</span> <span class="st">"Weight"</span>)]</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>ban.model <span class="ot">=</span> Predictor<span class="sc">$</span><span class="fu">new</span>(<span class="at">model=</span>ban.svm,</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data=</span>ban.X,</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">y=</span>ban_train<span class="sc">$</span>Quality)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">876</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>FeatureImp<span class="sc">$</span><span class="fu">new</span>(ban.model,</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">loss=</span><span class="st">"ce"</span>,</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">n.repetitions=</span><span class="dv">25</span>) <span class="sc">%&gt;%</span> <span class="fu">plot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-banana-svm-vimp" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-banana-svm-vimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-banana-svm-vimp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-banana-svm-vimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.23: Permutation-based variable importance for support vector machine.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The following code computes the Shapley values of the SVM classification for the first observation:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6543</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>ban.shap <span class="ot">&lt;-</span> Shapley<span class="sc">$</span><span class="fu">new</span>(ban.model,<span class="at">x.interest=</span>ban.X[<span class="dv">1</span>,])</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>ban.shap<span class="sc">$</span>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       feature class   phi    phi.var         feature.value
1         Size   Bad -0.04 0.03878788        Size=1.7066437
2    Sweetness   Bad  0.08 0.09454545  Sweetness=-4.4634495
3     Softness   Bad  0.00 0.08080808   Softness=-1.5100472
4  HarvestTime   Bad -0.48 0.25212121 HarvestTime=4.5640225
5     Ripeness   Bad  0.00 0.08080808 Ripeness=-0.041712634
6      Acidity   Bad -0.04 0.03878788     Acidity=4.3266883
7         Size  Good  0.04 0.03878788        Size=1.7066437
8    Sweetness  Good -0.08 0.09454545  Sweetness=-4.4634495
9     Softness  Good  0.00 0.08080808   Softness=-1.5100472
10 HarvestTime  Good  0.48 0.25212121 HarvestTime=4.5640225
11    Ripeness  Good  0.00 0.08080808 Ripeness=-0.041712634
12     Acidity  Good  0.04 0.03878788     Acidity=4.3266883</code></pre>
</div>
</div>
<p>The <code>phi</code> column displays the difference in predicted probabilities from the baseline, which is the average prediction across the data set.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>ban.shap<span class="sc">$</span>y.hat.average</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Bad    Good 
0.50725 0.49275 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(ban.shap<span class="sc">$</span>results[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>,<span class="dv">3</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.44</code></pre>
</div>
</div>
<p>The predicted probabilities for the first observation are</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(ban.svm.pred,<span class="st">"probabilities"</span>)[<span class="dv">1</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Good        Bad 
0.96399502 0.03600498 </code></pre>
</div>
</div>
<p>The <code>phi</code> values for <code>class="Bad"</code> are the mirror image of those for <code>class="Good"</code>. <code>HarvestTime</code> has the largest contribution to the difference between the predicted probability and the average prediction. Next is <code>Sweetness</code> which increases the probability of good banana qualityl</p>
<p>The <code>plot</code> method displays the Shapley values graphically (<a href="#fig-banana-shap1" class="quarto-xref">Figure&nbsp;<span>37.24</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ban.shap)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-banana-shap1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-banana-shap1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-banana-shap1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-banana-shap1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.24: Shapley values for first observation in SVM for banana quality.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the second observation the predicted probability of Good banana quality is even higher than for the first observation. The Shapley values reveal that <code>Size</code> is the primary driver of the predicted probability for that instance, rather than <code>HarvestTime</code> (<a href="#fig-banana-shap2" class="quarto-xref">Figure&nbsp;<span>37.25</span></a>)</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(ban.svm.pred,<span class="st">"probabilities"</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Good         Bad
1 0.9639950 0.036004976
2 0.9969397 0.003060289</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>ban.shap<span class="sc">$</span><span class="fu">explain</span>(ban.X[<span class="dv">2</span>,])</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ban.shap)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-banana-shap2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-banana-shap2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-banana-shap2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-banana-shap2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.25: Shapley values for the second observation in SVM for banana quality.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="shap" class="level3">
<h3 class="anchored" data-anchor-id="shap">SHAP</h3>
<p><span class="citation" data-cites="LundbergLee_SHAP">Lundberg and Lee (<a href="references.html#ref-LundbergLee_SHAP" role="doc-biblioref">2017</a>)</span> introduced <strong>SH</strong>apley <strong>A</strong>dditive ex<strong>P</strong>lanation (SHAP) values as a unified approach to explaining model predictions and feature importance.</p>
<p>SHAP is based on Shapley values and represents Shapley value explanation as an linear model of feature attributions, creating a connection to LIME models with local linear structure. Because of the computational demand to compute Shapley values, <span class="citation" data-cites="LundbergLee_SHAP">Lundberg and Lee (<a href="references.html#ref-LundbergLee_SHAP" role="doc-biblioref">2017</a>)</span> propose several approximations, some model-agnostic and some model-specific. The model-agnostic SHAP approximation combines LIME with Shapely values and is known as KernelSHAP. In a later paper <span class="citation" data-cites="LundbergErionLee">Lundberg, Erion, and Lee (<a href="references.html#ref-LundbergErionLee" role="doc-biblioref">2018</a>)</span> proposed the model-specific TreeSHAP for tree-based methods.</p>
<p>Another feature of SHAP is a useful collection of tools to aggregate and visualize measures based on Shapley values, such as force plots, Shapley summaries, dependence plots, etc.</p>
<p>An implementation of TreeSHAP for XGBoost in<code>R</code> is available in the <code>SHAPforxgboost</code> package.</p>
<div class="example">
<div class="example-header">
<p>Example: Glaucoma Database</p>
</div>
<div class="example-container">
<p>We return to the Glaucoma data analyzed in the chapter on Boosting (<a href="boosting.html" class="quarto-xref"><span>Chapter 21</span></a>). The following code chunks recreate the training and test data from that chapter and fit an XGBoost model for binary classification.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"TH.data"</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>( <span class="st">"GlaucomaM"</span>, <span class="at">package=</span><span class="st">"TH.data"</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">5333</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>trainindex <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(GlaucomaM),<span class="fu">nrow</span>(GlaucomaM)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>glauc_train <span class="ot">&lt;-</span> GlaucomaM[trainindex,]</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>glauc_test <span class="ot">&lt;-</span> GlaucomaM[<span class="sc">-</span>trainindex,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>Glaucoma <span class="ot">=</span> <span class="fu">ifelse</span>(GlaucomaM<span class="sc">$</span>Class<span class="sc">==</span><span class="st">"glaucoma"</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>glauc_train<span class="sc">$</span>Glaucoma <span class="ot">&lt;-</span> Glaucoma[ trainindex]</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>glauc_test<span class="sc">$</span>Glaucoma <span class="ot">&lt;-</span> Glaucoma[<span class="sc">-</span>trainindex]</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>dataX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(glauc_train[,<span class="dv">1</span><span class="sc">:</span><span class="dv">62</span>])</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>xgdata <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data=</span>dataX,</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>                      <span class="at">label=</span>glauc_train<span class="sc">$</span>Glaucoma)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>xgtest <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data=</span>dataX,</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>                      <span class="at">label=</span>glauc_test<span class="sc">$</span>Glaucoma)</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">765</span>)</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>xgb_fit <span class="ot">&lt;-</span> <span class="fu">xgb.train</span>(<span class="at">data=</span>xgdata,</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>                     <span class="at">watchlist=</span><span class="fu">list</span>(<span class="at">test=</span>xgtest),</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>                     <span class="at">max.depth=</span><span class="dv">3</span>,</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>                     <span class="at">eta=</span><span class="fl">0.05</span>,  </span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>                     <span class="at">early_stopping_rounds=</span><span class="dv">15</span>,</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>                     <span class="at">subsample=</span><span class="fl">0.5</span>,</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>                     <span class="at">objective=</span><span class="st">"binary:logistic"</span>,</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>                     <span class="at">nrounds=</span><span class="dv">100</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>                     )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] test-logloss:0.691143 
Will train until test_logloss hasn't improved in 15 rounds.

[2] test-logloss:0.691066 
[3] test-logloss:0.690617 
[4] test-logloss:0.690792 
[5] test-logloss:0.690083 
[6] test-logloss:0.690760 
[7] test-logloss:0.692218 
[8] test-logloss:0.694467 
[9] test-logloss:0.696919 
[10]    test-logloss:0.699999 
[11]    test-logloss:0.701853 
[12]    test-logloss:0.704680 
[13]    test-logloss:0.709163 
[14]    test-logloss:0.716283 
[15]    test-logloss:0.717489 
[16]    test-logloss:0.720253 
[17]    test-logloss:0.722769 
[18]    test-logloss:0.727558 
[19]    test-logloss:0.735394 
[20]    test-logloss:0.744886 
Stopping. Best iteration:
[5] test-logloss:0.690083</code></pre>
</div>
</div>
<p><code>shap.values</code> extracts the SHAP values from the xgboost object. The <code>mean_shap_score</code> vector contains the averaged SHAP values for each feature, ranked from largest to smallest (absolute value). It also functions as a ranking of feature importance. Seven of the features have average SHAP values different from zero, many of the features in the Glaucoma database were not used in the XGBoost model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SHAPforxgboost)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>shap_values <span class="ot">&lt;-</span> <span class="fu">shap.values</span>(<span class="at">xgb_model=</span>xgb_fit, <span class="at">X_train=</span>dataX)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>shap_values<span class="sc">$</span>mean_shap_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       vari         tms        phci        phcg          mv        phcn 
0.127955418 0.103766854 0.073036527 0.023943328 0.015380792 0.014420251 
       mhcg          ag          at          as          an          ai 
0.008273273 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
        eag         eat         eas         ean         eai        abrg 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       abrt        abrs        abrn        abri         hic        mhct 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       mhcs        mhcn        mhci        phct        phcs         hvc 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       vbsg        vbst        vbss        vbsn        vbsi        vasg 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       vast        vass        vasn        vasi        vbrg        vbrt 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       vbrs        vbrn        vbri        varg        vart        vars 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       varn         mdg         mdt         mds         mdn         mdi 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
        tmg         tmt         tmn         tmi          mr         rnf 
0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 
       mdic         emd 
0.000000000 0.000000000 </code></pre>
</div>
</div>
<hr>
<p>Before calling the visualization functions in the <code>SHAPforxgboost</code> package, the SHAP values are converted into a long-format data frame. Only the 10 most important features are being kept in this application, as we have already seen that most features did not contribute to the XGBoost model.</p>
<p>The summary plot shows the SHAP values for these variables (<a href="#fig-shap-summary" class="quarto-xref">Figure&nbsp;<span>37.26</span></a>). The plot combines local information (the SHAP values for the individual observations) with global information (the ranking of features by importance based on the aggregated SHAP values.) The coloring of the points in the summary plot relates to the value of the specific feature. For example, low values of the <code>vari</code> input variable increase the SHAP value, and large values decrease the SHAP value. The opposite is true for the <code>tms</code> variable. These are the two most important drivers of predictions in the model, followed by <code>phci</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>shap_long <span class="ot">&lt;-</span> <span class="fu">shap.prep</span>(<span class="at">xgb_model=</span>xgb_fit, </span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">X_train =</span> dataX,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">top_n=</span><span class="dv">10</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="fu">shap.plot.summary</span>(shap_long)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-shap-summary" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shap-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-shap-summary-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shap-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.26: SHAP summary plot for xgboost model, Glaucoma data
</figcaption>
</figure>
</div>
</div>
</div>
<p>The next series of figures displays dependence plots for the four most important features. The feature values appear on the horizontal axis, the SHAP values on the vertical axis. These plots are the SHAP-based versions of the partial dependence plots discussed earlier. They show how the SHAP values change as a function of the feature values. The color of the points is varied by another feature, selected as the feature that minimizes the variance of the SHAP values given both features. The feature chosen for coloring suggests a strong interaction with the input featured in the dependence plot.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) {</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>     x <span class="ot">&lt;-</span> <span class="fu">shap.importance</span>(shap_long, <span class="at">names_only =</span> <span class="cn">TRUE</span>)[i]</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>     p <span class="ot">&lt;-</span> <span class="fu">shap.plot.dependence</span>(</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>         shap_long, </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">x =</span> x, </span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">color_feature =</span> <span class="st">"auto"</span>,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">smooth =</span> <span class="cn">FALSE</span>, </span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">jitter_width =</span> <span class="fl">0.01</span>, </span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">alpha =</span> <span class="fl">0.7</span>) </span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>     <span class="fu">print</span>(p)</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-37-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-37-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-37-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="explainability_files/figure-html/unnamed-chunk-37-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-shap-force" class="quarto-xref">Figure&nbsp;<span>37.27</span></a> displays a <strong>force plot</strong>; it shows how various features contribute to the overall SHAP value for an observation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">shap.prep.stack.data</span>(<span class="at">shap_contrib=</span>shap_values<span class="sc">$</span>shap_score,</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">top_n=</span><span class="dv">5</span>, </span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">n_groups=</span><span class="dv">10</span>)</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="fu">shap.plot.force_plot</span>(plot_data, </span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">zoom_in_group=</span><span class="dv">4</span>,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">y_parent_limit =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>,<span class="fl">0.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-shap-force" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shap-force-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="explainability_files/figure-html/fig-shap-force-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shap-force-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37.27: Force plot for xgboost model for Glaucoma data
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="takeaways" class="level2" data-number="37.3">
<h2 data-number="37.3" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">37.3</span> Takeaways</h2>
<p>We summarize some of the main takeaways from this chapter.</p>
<ul>
<li><p>Explainability tools are a great asset to the data scientist. They can help us understand better how a model works, what drives it, and help formulate a human-friendly explanation.</p></li>
<li><p>Explainability tools are not free of assumptions and not free of user choices that affect their performance.</p></li>
<li><p>There is an ever increasing list of methods. Know their pros and cons. What is needed changes from application to application. Global explanations can be appropriate in one setting while observation-wise (local) explanations can be called for in another setting.</p></li>
<li><p>Methods based on Shapley values have several nice properties</p>
<ul>
<li>Grounded in (game) theory</li>
<li>Feature contributions add up to deviations from the average</li>
<li>Changes to a model that increase the marginal contribution of a feature also increase the features’ Shapley values</li>
<li>Combines local and global information</li>
<li>Addresses multiple aspects: dependence, variable importance, effect force</li>
</ul></li>
</ul>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure&nbsp;37.2: Continuum of model interpretability.</span>
<span class="glightbox-desc lightbox-desc-2">Figure&nbsp;37.5: A single decision tree is intrinsically interpretable.</span>
<span class="glightbox-desc lightbox-desc-3">Figure&nbsp;37.6: AlexNet, from <a href="https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/">LearnOpenCV</a>.</span>
<span class="glightbox-desc lightbox-desc-4">Figure&nbsp;37.7: Model explainability mind map</span>
<span class="glightbox-desc lightbox-desc-5">Figure&nbsp;37.20: The eight coalitions. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.</span>
<span class="glightbox-desc lightbox-desc-6">Figure&nbsp;37.21: Predicting under eight models. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.</span>
<span class="glightbox-desc lightbox-desc-7">Figure&nbsp;37.22: Computing the marginal contributions of the features. Adapted from <span class="citation" data-cites="Mazzanti_2020">Mazzanti (<a href="references.html#ref-Mazzanti_2020" role="doc-biblioref">2020</a>)</span>.</span>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-LundbergErionLee" class="csl-entry" role="listitem">
Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. 2018. <span>“Consistent Individualized Feature Attribution for Tree Ensembles.”</span> <a href="https://arxiv.org/abs/1802.03888">https://arxiv.org/abs/1802.03888</a>.
</div>
<div id="ref-LundbergLee_SHAP" class="csl-entry" role="listitem">
Lundberg, Scott M., and Su-In Lee. 2017. <span>“A Unified Approach to Interpreting Model Predictions.”</span> <em>31st Conference on Neural Information Processing Systems (NIPS)</em>. <a href="https://arxiv.org/abs/1705.07874">https://arxiv.org/abs/1705.07874</a>.
</div>
<div id="ref-Mazzanti_2020" class="csl-entry" role="listitem">
Mazzanti, Samuele. 2020. <span>“SHAP Values Explained Exactly How You Wished Someone Explained to You. Making Sense of the Formula Used for Computing SHAP Values.”</span> <em>Medium</em>. <a href="https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30">https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30</a>.
</div>
<div id="ref-Molnar2022" class="csl-entry" role="listitem">
Molnar, Christoph. 2022. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>. 2nd ed. <a href="https://christophm.github.io/interpretable-ml-book">https://christophm.github.io/interpretable-ml-book</a>.
</div>
<div id="ref-Nash_et_al_1994" class="csl-entry" role="listitem">
Nash, Warwick J., Tracy L. Sellers, Simon R. Talbot, Andrew J. Cawthorn, and Wes B Ford. 1994. <span>“The Population Biology of Abalone (*Haliotis* Species) in Tasmania. I. Blacklip Abalone (*h. Rubra*) from the North Coast and Islands of Bass Strait.”</span>
</div>
<div id="ref-sankaran_2024" class="csl-entry" role="listitem">
Sankaran, Kris. 2024. <span>“Data Science Principles for Interpretable and Explainable AI.”</span> <em>Journal of Data Science</em>, 1–27. <a href="https://doi.org/10.6339/24-JDS1150">https://doi.org/10.6339/24-JDS1150</a>.
</div>
<div id="ref-Shapley_1953" class="csl-entry" role="listitem">
Shapley, L. 1953. <span>“A Value for <span class="math inline">\(n\)</span>-Peson Games.”</span> In <em>Contributions to the Theory of Games II</em>, edited by N. Kuhn and A. Tucker, 307–17. Princeton University Press, Princeton.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./reinforcement.html" class="pagination-link" aria-label="Reinforcement Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","descPosition":"bottom","openEffect":"zoom","loop":false,"closeEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>