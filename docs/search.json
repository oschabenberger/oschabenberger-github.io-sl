[
  {
    "objectID": "deeplearning.html",
    "href": "deeplearning.html",
    "title": "33  Deep Learning",
    "section": "",
    "text": "33.1 Introduction\nWe can at best provide an introduction into the vast topic of deep learning in this material. Our goal is to continue the discussion of neural networks and move from the generic ANN to specialized neural networks that are used for specific types of data and to specialized forms of learning from data. Figure 33.1 categorizes important approaches in deep learning.\nConvolutional neural networks process data that appears on a grid (Section 33.2). The most frequent case is two-dimensional data in images that are arranged into rows and columns of pixels. These networks take advantage of the spatial layout of the pixels and train neural networks for tasks that are relevant in computer vision: object detection, object classification, edge detection, face detection and recognition, optical character recognition (OCR) and so forth.\nRecurrent neural networks process data that are sequential in nature such as text or time series (Section 33.3).\nDeep neural networks are one manifestation of deep learning.\nSince 2017, when the architecture was described in Vaswani et al. (2017), and in particular since 2022, when Chat GPT 3.5 was released, transformer architectures have revolutionized the approach to natural language modeling and to artificial intelligence in general. We give a brief introduction into transformer architectures as an evolution of recurrent encoder/decoder networks in Section 33.4.\nReinforcement learning also belongs to the deep learning technologies (Chapter 34). It is a very different form of learning that does not fall neatly into the delineation of supervised and unsupervised methods of learning. In reinforcement learning, an agent reacts to feedback from the environment and chooses actions to optimize future positive feedback.\nDeep learning flipped the narrative about artificial intelligence applying to tasks that are easy for computers but difficult for humans. Automating things that are intuitive for us, such as sensing the world, reading, writing, playing games, understanding language, were long thought to be out of reach for computers. That changed with the rise of deep learning in the early 2000s and was marked by watershed moments such as a reinforcement-trained system beating the world’s best Go player 4:1.\nThe idea of deep learning is to represent solutions as a hierarchy of related concepts. The hierarchy looks like a layered graph, hence the name deep learning. Instead of engineering features and trying to predict or classify based on carefully chosen features, deep learning starts with the obvious inputs (pixels, audio, text) and learns concepts about them from the data. Instead of providing to the system representation of the concepts through features, we let the system discover representations. The definition of deep learning according to Goodfellow, Bengio, and Courville (2016) reflects this view:\nFigure 33.2, adapted from Goodfellow, Bengio, and Courville (2016), depicts this concept for the case of object classification with images. To build a model that can classify the content of the image directly from the pixel information is difficult at best. However, we can train a model to learn different representations about the image content, first colors, then edges, and so forth. The combination of the abstract representations then allows the algorithm to assign “probabilities” with the possible object categories. Based on the probabilities we choose a possible explanation.\nA fundamental issue with artificial intelligence for tasks that are intuitive for humans is how to get the required knowledge about the world into the machine? A computer does not look at a photo the same way we do. It sees pixels, we see concepts. Once we know what a tomato looks like we can conceptualize tomatoness and recognize a different tomato variety immediately. How do we teach computers about tomatoes?\nThe deep learning solution is to extract knowledge from data through machine learning. Several implicit assumptions are at work here:\nThis raises many fundamental questions. For example, by scanning all the text on the internet, does a machine really understand language? Is language not more—much more—than just data? Is a computer vision algorithm that has 95% accuracy in object classification not just a pixel pattern-matching system and does not really understand the concepts it is classifying? The answer to both questions is an emphatic “Yes!”\nThe quintessential tool for deep learning is the deep neural network.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#introduction",
    "href": "deeplearning.html#introduction",
    "title": "33  Deep Learning",
    "section": "",
    "text": "Figure 33.1: Deep learning mind map.\n\n\n\n\n\n\n\n\n\n\n\n\nDeep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe put probabilities in quotation marks because the last step of the\n\n\n\n\n\n\n\n\nFigure 33.2: Deep learning as a hierarchy of concepts. Adapted from Goodfellow, Bengio, and Courville (2016)\n\n\n\n\n\n\nthe knowledge required to understand a system is actually in the data\nthe knowledge can be extracted\nthere is enough data to make sure the machine can learn and learn the right thing",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-cnn",
    "href": "deeplearning.html#sec-dl-cnn",
    "title": "33  Deep Learning",
    "section": "33.2 Convolutional Neural Networks",
    "text": "33.2 Convolutional Neural Networks\nConvolutional neural networks (CNN) have become the dominant architecture in computer vision applications—until transformer architectures, that is. A CNN is a neural network for data with a grid-like topology, such as the array of pixels on an image. In contrast to a generic ANN with fully connected layers, a CNN takes advantage of the spatial arrangement of the data. CNNs can also be applied to types of time series data such as samples at regular time intervals. It turns out that another class of deep learning networks, the recurrent neural networks (RNNs), are better suited to analyze sequential data.\nCNNs take advantage of the rich structure of image data to\n\nreduce the number of parameters\nincrease opportunities for parallel processing\nanswer questions that are pertinent to this type of data, such as object detection and classification (Figure 33.3 and Figure 33.4), facial recognition (Figure 33.5), etc.\n\n\n\n\n\n\n\nFigure 33.3: Object detection and classification\n\n\n\n\n\n\n\n\n\nFigure 33.4: Object detection and classification.\n\n\n\n\n\n\n\n\n\nFigure 33.5: Facial detection and recognition\n\n\n\nIn Section 30.4 we took a first look at the MNIST Image classification data. Recall that each image is a 3-dimensional tensor that consists of a 28 x 28 array of pixels and a grayscale value. The two-layer fully connected AN with 128 and 64 neurons in the hidden layers had 109,386 parameters. This approach does not scale to larger images. A 1-megapixel color image would have \\(3 \\times 10^6\\) features if we analyze it like the MNIST data in Section 30.4 and Section 32.2.3. The classification accuracy of the fully connected ANN of about 95% looks good at first on paper, but might not be sufficient in practical applications. An optical character recognition (OCR) system that reads numbers from hand-written checks cannot get 5 out of every 100 digits wrong.\nSomehow, images need to be processed differently than with a fully connected neural network. Also, the network architectures for image processing should have other desirable properties:\n\nTranslation invariance: In early layers the network should respond similarly to the same pattern, regardless of where it appears. What Waldo looks like does not depend on where Waldo is located (Figure 33.6). Shifting the input should shift the output.\nLocality: early layers of the network should concentrated on local features. Local representations can be aggregated to make predictions at the level of the entire image. Deeper layers should capture longer-ranging features.\n\n\n\n\n\n\n\nFigure 33.6: Where is Waldo?\n\n\n\n\nParameter Sharing: A fully connected network uses each element of the weight matrix exactly once when computing the output of the layer. To reduce the number of parameters, weights should be reused in some form.\n\nConvolutional networks accomplish these goals through a special layer architecture, using convolutions and pooling.\n\nConvolutions\n\nDefinition\nMathematically, convolutions of continuous or discrete functions are defined as follows.\n\n\nDefinition: Convolution\n\n\nThe convolution \\((f\\ast g)(t)\\) of functions \\(f\\) and \\(g\\) describes the process—as well as the result—of computing the integral of one function after the other function is shifted and reflected about the \\(y\\)-axis \\[\n    (f \\ast g)(t) = \\int_{-\\infty}^\\infty f(x)\\,g(t-x)\\,dx\n\\] We can interpret this as averaging \\(f\\) with the weighting (kernel) function \\(g\\) or as measuring the overlap between \\(f\\) and \\(g\\) when one function is “flipped” and shifted by \\(t\\).\nIf \\(f\\) and \\(g\\) are defined on the integers, the discrete convolution is \\[\n(f \\ast g)(i) = \\sum_{m=-\\infty}^\\infty f(m)g(i-m)\n\\]\nIn two dimensions, the discrete convolution is \\[\n    (f \\ast g)(i,j) = \\sum_{m}\\sum_{n} f(m,n) \\, g(i-m,j-n)\n\\]\nBecause convolution is commutative, you can equivalently write \\[\n\\begin{align*}\n    (f \\ast g)(t) &= \\int_{-\\infty}^\\infty f(t-x)\\,g(x)\\,dx \\\\\n    (f \\ast g)(i) &= \\sum_{m=-\\infty}^\\infty f(i-m)g(m)\\\\\n    (f \\ast g)(i,j) &= \\sum_{m}\\sum_{n} f(i-m, j-n) \\, g(m,n)\n\\end{align*}\n\\]\n\n\n\n\nHow it works\nIn processing images in deep learning, what is called a convolution is mathematically more akin to the concept of the cross-correlation between two functions: \\[\n(f \\ast g)(i,j) = \\sum_m \\sum_n f(i+m,j+n)g(m,n)\n\\] From a practical perspective it does not matter whether it is a genuine convolution or a cross-correlation. What matters is that we are taking weighted combinations of the cells of an array with the elements of some discrete kernel functions. Figure 33.7 and Figure 33.8 show this process for the convolution of a 3 x 4 image with a 2 x 2 kernel with stride 1.\n\n\n\n\n\n\nFigure 33.7: Convolving the first row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\n\n\n\n\n\n\n\n\n\nFigure 33.8: Convolving the second row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\n\n\n\nThe operations are not matrix multiplications, they are a form of dot product between the values in the array and the values of the kernel. Once a group of cells has been convolved, the kernel shifts to a new location and processes the next \\(2 \\times 2\\) set of cells. The kernel can shift by one cell, creating overlap with the previous convolution or by two cells, avoiding overlap. This shift is known as the stride of the convolution. The figures above show the process for a stride of 1.\nThe result of the convolution is an array of size \\(2 \\times 3\\). The size of the array has decreased and the values in the output cells are weighted combinations of the values in the input cells.\nRather than having separate weights (and biases) for all cells of the array, the parameters of the convolution are the weights in the kernel array. The same kernel is applied to all sub-arrays as it slides across the input array. This is a form of parameter sharing. Instead of estimating parameters associated with all input cells, only the weights in the kernel need to be estimated—a much smaller number. On the other hand, we have introduced a number of new decisions that affect the architecture of the convolution layer:\n\nthe size of the kernel\nthe stride of the kernel\nthe handling of the boundary values\n\n\n\nSparse connectivity\nThe effect of the convolution is sparse connectivity between input and output layers. An input unit does not affect all output units and an output unit is not affected by all input units. In a fully connected network all input units are connected to all output units. The receptive field of a unit are the neurons in the previous layer that contribute to it. In a CNN, only the neurons covered by the kernel window fall into the receptive field, providing locality. This allows the layer to focus on pixels in the vicinity of the input rather than receiving contributions from all pixels across the image. Similarly, when viewed from the input layer (viewed from below), a neuron affects only those output units that include it in their receptive fields. Figure 33.9, adapted from Goodfellow, Bengio, and Courville (2016), displays the concepts of sparse and full connectivity.\n\n\n\n\n\n\nFigure 33.9: Sparse and full connectivity, adapted from Goodfellow, Bengio, and Courville (2016, 336–37)\n\n\n\n\n\nHandling boundary values\nConsider again the convolution example in Figure 33.8. If we had chosen a stride of 2, how could we have processed the second row of the \\(3 \\times 4\\) array, there is no fourth row to apply the kernel. Handling the boundary values of grids can be done with several methods:\n\nPadding: assume constant values (zeros or other values) beyond the border.\nExtending: assume that the values in boundary cells extend beyond the border cells.\nMirroring: to read at \\(m\\) pixels outside image use the value from \\(-m\\) pixels inside image.\nWrapping: the image is wrapped on edge or corner.\nCropping: limit convolutions to interior cells that fully overlay the kernel. This means no padding and is the keras method padding=\"valid\".\n\nHow far you have to pad, extend, mirror, or wrap depends on the image size, the kernel size and the stride. Different amounts of padding in the horizontal and vertical direction makes sense for rectangular-shaped input arrays.\n\n\nFiltering example\nTo see how kernels can extract features of arrays, we consider a special case where the kernel weights are predetermined to create a desired effect. Suppose you are dealing with grayscale images and you subtract from a cell the value of the neighboring pixel to the right (or left). This operation will reveal vertical edges. A slightly more sophisticated application is the Sobel filter for finding edges. The filter consists of two kernels, \\(G_x\\) for edges in the \\(x\\)-direction (horizontal) and its transpose, \\(G_y\\) for the detection of edges in the \\(y\\)-direction: \\[\nG_x = \\left [ \\begin{array}{rrr} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{array} \\right ]\n    \\qquad\nG_y = \\left [ \\begin{array}{rrr} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{array} \\right ]\n\\]\nFigure 33.10 depicts the application of \\(G_x\\) to the first cell of the array after extending the bounday by one cell. Figure 33.11 shows the result of the convolution for all cells.\n\n\n\n\n\n\nFigure 33.10: Edge detection with Sobel filter \\(G_x\\).\n\n\n\n\n\n\n\n\n\nFigure 33.11: Edge detection with Sobel filter \\(G_x\\).\n\n\n\nFilters such as the Sobel filter are applied in classical image processing where known transformations are applied to the pixels. When fitting convolutional neural networks we do not specify which filters to apply. Instead, we let the algorithm learn the best filters for a layer by estimating the weights of the kernel.\nAfter convolving the cells, the values pass through an activation function as in other neural networks—also called the detector stage. ReLU activation is common these days, sigmoid functions were popular prior to the discovery of the rectified linear unit. The convolution and activation are often combined into a single step, sometimes they are shown as separate processing stages (separate layers).\n\n\n\nPooling\nThe pooling stage of a convolutional layer—often shown as its own layer—takes the output of a convolved and activated layer and replaces its values with summaries of nearby values. Similar to applying a kernel, a pooling layer has a window of cells to consider and a stride by which it shifts across the array. Unlike the convolution layer, there are no parameters associated with pooling. The effect of pooling is a reduction in the dimension of the output; the goal is to perform this reduction with minimal loss of information. The pooling layer is also called a downsample layer for this reason.\nAdding a pooling layer serves two purposes: to mitigate the sensitivity of the convolution to location and to downsample the representation, reducing the number of parameters for subsequent layers.\nFigure 33.12 shows a full convolutional layer with all three phases: convolution, activation (here, ReLU), and pooling. Two pooling operations are shown for \\(2 \\times 2\\) pooling with stride 2: using the average of the values and using the maximum value in the pooling window.\n\n\n\n\n\n\nFigure 33.12: Convolutional layer with pooling.\n\n\n\nThe most common pooling operations are taking the max value (max-pooling) and taking the average, with a preference of the former because it confers more invariance to the output.\n\n\nConsiderations\n\nChannels and feature maps\nWhen working with images and CNNs you encounter the terms channels and feature maps. The terms are used interchangeably and refer to the number of input or output features. When processing color images, channels at the input layer refer to the RGB color values, the input layer has three channels or feature maps. In the case of the MNIST digit data set, there is a single channel, the grayscale value.\nAfter the input layer, the number of channels or feature maps is determined by the number of filters. A convolutional layer with 16 filters has 16 output channels, one per filter. Each channel has a different set of weights, but the weights within a channel that convolve the image are the same (parameter sharing).\nWe think of the first layer of the CNN as consuming the color information and other layers creating new channels of information about the image.\nEach layer of a CNN has input and output features, known as feature maps.\nAfter the input layer, the number of feature maps is determined by the number of filters. After the input layer, these are also referred to as the channels. At the input layer, the channels for RGB images refer to the three color values.\n\n\nOther considerations\nThere are many architectural choices that define a convolutional neural network. The number and arrangement of convolutional and pooling layers. Strides of kernels and pooling windows. The handling of boundary values. The choice of activation functions.\nHere are some general recommendations:\n\nHaving multiple convolutional layers before a pooling layer effectively increases the dimension of the filter.\nMax poling is used more frequently than average pooling.\nReLU is the most important activation function in CNNs today.\nThe number of filters increases for the deeper layers of the network as feature maps get smaller due to previous pooling.\nWhen the feature maps are sufficiently reduced the network is flattened, treating the pixels as separate units. This is followed by one or more fully connected layer(s) and an output layer.\nYou can have multiple convolutional layers before a pooling layer. In fact, the pooling effect of reducing the output size can also be achieved with a convolutional layer without padding and stride greater than 1.\nYou can add dropout layers in CNNs just like in ANNs to reduce the number of neurons and to avoid overfitting. However, dropout layers are less common in CNNs because the number of parameters can be managed through pooling layers and padding/stride of convolutional layers. In the fully connected parts near the end of CNNs dropout can make sense.\n\n\n\n\nLeNet\nIt is common in image processing tasks to apply a previously defined network. Famous network architectures are AlexNet, VGG, GoogLeNet, and others. These can be seen as improvements over the first CNN that solved an important practical problem and found commercial application: LeNet\nLeNet is a famous example of a CNN, developed by Yann LeCun, a founding father of modern deep learning, between 1989 and 1995. Various iterations of LeNet exist, we describe and train LeNet-5 in this chapter. This network outperformed other approaches for hand-written digit classification and started the deep learning revolution.\nIt is remarkable that LeNet was developed and used for character recognition as early as 1989. Computing resources were extremely limited compared to today. In fact, reducing the number of parameters that needed to be estimated while increasing accuracy was part of the motivation of developing specialized networks for specialized applications.\nHere is a video of Yann LeCun demonstrating the first LeNet for character recognition using a 486 PC with a DSP (digital signal processing card).\n\n\nNetwork structure\nThe network structure of LeNet-5 for the MNIST data starts with a single channel of 28 x 28 pixel images (Figure 33.13).\n\n\n\n\n\n\nFigure 33.13: LetNet-5 architecture. Source\n\n\n\nThe first convolution layer (C1) has 6 filters (kernels) of size 5 x 5. Its result is passed to a 2 x 2 pooling layer with stride 2. This reduces the dimension of the output to 14 x 14. The next convolutional layer C2 applies 16 filters of size 5 x 5, leading to output channels of size 10 x 10. These pass through another pooling layer to reduce the output to size 5 x 5. After flattening, two fully connected layers follow with 120 and 84 units, respectively. The final dense output layer has 10 units corresponding to the digits “0”–“9”. The convolutional and fully connected layers used sigmoid activation functions. The original LeNets used a Gaussian decoder rather than the softmax output function.\nSetting up LeNet-5 in keras is pretty straightforward. You build up the layers sequentially as just described.\n\nlibrary(keras)\nreticulate::use_condaenv(condaenv = \"r-tensorflow\")\n\nlenet5 &lt;- keras_model_sequential() %&gt;%\n    layer_conv_2d(kernel_size=c(5,5),\n                  filters    =6,\n                  padding    =\"same\",\n                  activation =\"sigmoid\",\n                  input_shape=c(28,28,1),\n                  name       =\"Conv_1\"\n                  ) %&gt;%\n    layer_average_pooling_2d(pool_size=c(2,2),\n                             name     =\"Pool_1\") %&gt;%\n    layer_conv_2d(kernel_size=c(5,5),\n                  filters    =16,\n                  activation =\"sigmoid\",\n                  name       =\"Conv_2\") %&gt;%\n    layer_average_pooling_2d(pool_size=c(2,2),\n                             name=\"Pool_2\") %&gt;%\n    layer_flatten(name=\"Flatten\") %&gt;%\n    layer_dense(units     =120,\n                activation=\"sigmoid\",\n                name      =\"FC_1\") %&gt;%\n    layer_dense(units     =84,\n                activation=\"sigmoid\",\n                name      =\"FC_2\") %&gt;%\n    layer_dense(units     =10,\n                activation=\"softmax\",\n                name      =\"Output\")\n\nThe only difference to the original LeNets is that we use softmax in the output layer.\nThe new layer types compared to Section 32.2.3 are\n\nlayer_conv_2d: a 2-d convolutional layer\nlayer_average_pooling_2d: a pooling layer for 2-d data\nlayer_flatten: a flattening layer\n\nLeNet-5 is much deeper than the 2-hidden layer ANN in Section 32.2.3. Recall that ANN had 109,386 parameters. How many parameters are in LeNet-5?\n\nsummary(lenet5)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n Conv_1 (Conv2D)                    (None, 28, 28, 6)               156         \n Pool_1 (AveragePooling2D)          (None, 14, 14, 6)               0           \n Conv_2 (Conv2D)                    (None, 10, 10, 16)              2416        \n Pool_2 (AveragePooling2D)          (None, 5, 5, 16)                0           \n Flatten (Flatten)                  (None, 400)                     0           \n FC_1 (Dense)                       (None, 120)                     48120       \n FC_2 (Dense)                       (None, 84)                      10164       \n Output (Dense)                     (None, 10)                      850         \n================================================================================\nTotal params: 61706 (241.04 KB)\nTrainable params: 61706 (241.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nNotice that pooling and flattening are parameter-free operations, like the dropout layer.\nLeNet-5 has only 56% of the parameters of the 2-layer ANN. Does it perform better than the larger ANN on this data?\n\n\nTraining MNIST data with keras\nAll the data pre-processing steps are the same as in Section 32.2.3. We also use the same optimization setup.\n\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x\ng_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x\ng_test &lt;- mnist$test$y\n\ny_train &lt;- to_categorical(g_train, 10)\ny_test &lt;- to_categorical(g_test, 10)\n\n\nlenet5 %&gt;% compile(loss=\"categorical_crossentropy\",\n                   optimizer=optimizer_rmsprop(), \n                   metrics=c(\"accuracy\")\n                   )\n\nNow we are ready to go. The final step is to supply training data, and fit the model.\n\nhistory &lt;- lenet5 %&gt;%\n      fit(x_train, \n          y_train, \n          epochs=15, \n          batch_size=128,\n          validation_data= list(x_test, y_test)\n          )\n\nEpoch 1/15\n469/469 - 7s - loss: 0.9813 - accuracy: 0.7165 - val_loss: 0.3185 - val_accuracy: 0.9123 - 7s/epoch - 14ms/step\nEpoch 2/15\n469/469 - 6s - loss: 0.2571 - accuracy: 0.9237 - val_loss: 0.1866 - val_accuracy: 0.9437 - 6s/epoch - 12ms/step\nEpoch 3/15\n469/469 - 6s - loss: 0.1746 - accuracy: 0.9468 - val_loss: 0.1348 - val_accuracy: 0.9578 - 6s/epoch - 12ms/step\nEpoch 4/15\n469/469 - 6s - loss: 0.1326 - accuracy: 0.9597 - val_loss: 0.1036 - val_accuracy: 0.9695 - 6s/epoch - 12ms/step\nEpoch 5/15\n469/469 - 6s - loss: 0.1068 - accuracy: 0.9668 - val_loss: 0.0890 - val_accuracy: 0.9720 - 6s/epoch - 12ms/step\nEpoch 6/15\n469/469 - 6s - loss: 0.0900 - accuracy: 0.9728 - val_loss: 0.0731 - val_accuracy: 0.9767 - 6s/epoch - 13ms/step\nEpoch 7/15\n469/469 - 6s - loss: 0.0790 - accuracy: 0.9751 - val_loss: 0.0654 - val_accuracy: 0.9786 - 6s/epoch - 13ms/step\nEpoch 8/15\n469/469 - 6s - loss: 0.0699 - accuracy: 0.9785 - val_loss: 0.0595 - val_accuracy: 0.9802 - 6s/epoch - 13ms/step\nEpoch 9/15\n469/469 - 6s - loss: 0.0634 - accuracy: 0.9803 - val_loss: 0.0510 - val_accuracy: 0.9832 - 6s/epoch - 12ms/step\nEpoch 10/15\n469/469 - 6s - loss: 0.0581 - accuracy: 0.9820 - val_loss: 0.0518 - val_accuracy: 0.9822 - 6s/epoch - 12ms/step\nEpoch 11/15\n469/469 - 6s - loss: 0.0536 - accuracy: 0.9834 - val_loss: 0.0482 - val_accuracy: 0.9837 - 6s/epoch - 13ms/step\nEpoch 12/15\n469/469 - 6s - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.0591 - val_accuracy: 0.9823 - 6s/epoch - 13ms/step\nEpoch 13/15\n469/469 - 6s - loss: 0.0473 - accuracy: 0.9851 - val_loss: 0.0560 - val_accuracy: 0.9826 - 6s/epoch - 12ms/step\nEpoch 14/15\n469/469 - 6s - loss: 0.0440 - accuracy: 0.9861 - val_loss: 0.0422 - val_accuracy: 0.9859 - 6s/epoch - 12ms/step\nEpoch 15/15\n469/469 - 6s - loss: 0.0418 - accuracy: 0.9870 - val_loss: 0.0508 - val_accuracy: 0.9840 - 6s/epoch - 12ms/step\n\nplot(history, smooth=FALSE)\n\n\n\n\n\n\n\n\nAfter only 15 epochs the LeNet model has achieved a classification accuracy of 98.64%, considerably higher than the fully connected network, and with fewer parameters. You can improve the model to 99% accuracy by continuing to train. Also, I encourage you to play with different activation functions (try the hyperbolic tangent), batch sizes, layer structure, etc.\nHere are some of the predicted values. Remember that the first “5” in position 9 of the test data set was notoriously difficult to classify for the ANN.\n\npredcl &lt;- lenet5 %&gt;% predict(x_test) %&gt;% k_argmax() \n\n313/313 - 1s - 537ms/epoch - 2ms/step\n\ng_test[1:36]\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\nas.numeric(predcl[1:36])\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\n\n\nplotIt &lt;- function(id=1) {\n    im &lt;- mnist$test$x[id,,]\n    im &lt;- t(apply(im, 2, rev)) \n    image(1:28, 1:28, \n          im, \n          col=gray((0:255)/255), \n          xaxt='n', \n          main=paste(\"Image label: \",\n                     g_test[id], \n                     \" Predicted: \", \n                     as.numeric(predcl[id])))\n}\n\nplotIt(1)\n\n\n\n\n\n\n\nplotIt(9)\n\n\n\n\n\n\n\n\n\n\n\nMerlin Bird ID\nIf you love technology and the outdoors, like me, then you might get a kick out of the Merlin Bird ID app from Cornell University. It can identify birds based on sound captured on your device. Simply stand outside, use Sound ID in the app, and Merlin will recognize the birds within “hearing” distance.\nFigure 33.14 shows part of my “Life List” of birds identified by sound through the app at our home in Blacksburg, VA.\n\n\n\n\n\n\nFigure 33.14: Part of my Life List in Merlin Bird ID.\n\n\n\nWhy bring this up here? Convolutional neural networks are not just for image data. As your phone records sounds, Sound ID converts it into a spectral representation of frequencies over time. The spectogram is the input to a convolutional neural network that maps spectograms to bird species. In addition to the spectral signature, Sound ID also uses spatial and temporal information to improve the accuracy of the bird species prediction. Over 1,300 bird species are supported, that is a pretty big softmax output layer.\nFigure 33.15 shows the spectogram of a sound signature identified as that of a Carolina Wren.\n\n\n\n\n\n\nFigure 33.15: Spectogram in Merlin Bird ID.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-rnn",
    "href": "deeplearning.html#sec-dl-rnn",
    "title": "33  Deep Learning",
    "section": "33.3 Recurrent Neural Networks",
    "text": "33.3 Recurrent Neural Networks\nRecurrent neural networks (RNNs) process sequential data, data in which the order of the information is important. Examples of such data types are\n\nWritten documents: summarize, capture sentiment, translate language\nRecordings: summarize, transcribe, assess quality, classify speech, music, and other sounds\nTime series: forecast future values, predict financial indices\nHandwriting: digitization, optical character recognition\n\nUntil generative transformers appeared on the scene, RNNs were the go-to architecture in natural language processing.\nWhile CNNs take advantage of the spatial arrangement of data, RNNs take advantage of the serial nature of data. Like CNNs, they use parameter sharing to reduce the number of parameters, to generalize models, to require fewer input samples and to gain statistical strength across sequences of different lengths. The parameter sharing in CNNs happens in the convolutional layers by using kernels with the same weights across all subarrays of an image. RNNs share parameters in a different way: Each member of the output\n\nis a function of the previous members of the output\nis produced using the same update rules applied previously\n\nThe challenge of processing order-dependent data is to imbue the system with some form of “memory”. When processing a word in a sentence, the system must have some notion of how the word relates to previous words. Locating the year in the sentence “I was born in 1983” and “In 1983, I was born” should not require different sets of parameters. Recurrent neural networks accomplish that by replacing the hidden layer in ANN with a special layer that maintains the internal state of past layer inputs—this state is called the hidden state of the network.\nThe input is processed sequentially—one word or token at a time—and the hidden state is updated at each step. Output is generated based on the internal state and the new input value.\nTo make these ideas more concrete, suppose that we are processing some text. First, we use an encoding mechanism to transform the text in a series of input vectors. For example, using one-hot encoding with a dictionary of 1,000 words, the input is a sequence \\(\\textbf{x}^{(1)}, \\cdots, \\textbf{x}^{(n)}\\) of \\(n\\) vectors of size \\(1000 \\times 1\\). Each vector has a 1 in the dictionary position where the word is found and 999 zeros in all other positions. One can also encode the input with a word embedding, where each word is represented by a \\(m\\)-dimensional vector of real numbers, called an embedding. The embeddings can be learned as separate layers or you can use pre-trained embeddings such as word2vec. GloVe, fastText, ELMo, and others.\nLet \\(\\textbf{h}^{(t)}\\) denote the state of the system at “time” \\(t%\\). Time is simply an indication of the position in the system, not necessarily wall clock time. A dynamic (recurrent) system is defined as \\[\n\\textbf{h}^{(t)} = f(\\textbf{h}^{(t-1)},\\textbf{x}^{(t)};\\boldsymbol{\\theta})\n\\]\nA recurrent neural network learns \\(\\textbf{h}^{(t)}\\) from \\([\\textbf{x}^{(t)},\\textbf{x}^{(t-1)},\\textbf{x}^{(t-2)},\\cdots,\\textbf{x}^{(2)},\\textbf{x}^{(1)}]\\) by using at each step the same transition function \\(f\\) and the same parameters \\(\\boldsymbol{\\theta}\\).\nThe traditional way of depicting neural networks as connected neurons with activations does not work well for RNNs. Instead, two ways of representing recurrence in networks are popular, known as the folded and unfolded views (Figure 33.16)\n\n\n\n\n\n\nFigure 33.16: Folded and unfolded RNN representation.\n\n\n\nFigure 33.17 shows the full view of an RNN in both representation, adding target, loss functions and weight matrices \\(\\textbf{U}\\), \\(\\textbf{W}\\), and \\(\\textbf{V}\\). These matrices represent the weights for the input-to-hidden state connections, the hidden state-to-hidden state connections, and the hidden state-to-output connections. These weight matrices \\(\\textbf{U}\\), \\(\\textbf{W}\\), and \\(\\textbf{V}\\) are the parameters of the RNN that need to be estimated.\n\n\n\n\n\n\nFigure 33.17: Complete representation of an RNN.\n\n\n\nNotice that a recurrent neural network can generate output at every step of the recurrence. This is relevant in language translation, for example. In other applications, such as sentiment analysis, only the output at the final stage of the sequence matters.\nThe same weights are used at each step, this is where the parameter sharing comes in: \\[\n\\textbf{h}^{(t)} = \\sigma\\left( \\bU\\textbf{x}^{(t)} + \\textbf{W}\\textbf{h}^{(t-1)} + \\textbf{b}\\right)\n\\]\nThe hidden layers of the network, represented by the hidden state at each step, are fed by activations from the previous layer and by the input data for this step.\nAs in other neural network architectures, the size of the layers is a hyperparameter that needs to be tuned. In ANNs and RNNs it corresponds to the number of units in a hidden layer, in CNNs it is a function of the kernel size and the number of channels (feature maps). If the hidden state of an RNN is chosen too small the model will underfit and the network lacks expressive power. If the hidden state is chosen too big the model will overfit and not generalize well to new input sequences.\n\nVanishing Gradients\nThe vanishing gradient problem, first discussed in Section 31.3.2.1, is particular punishing for recurrent neural networks, because the same weight matrices are used at all steps of the sequence. Exploding gradients, the opposite problem, when repeated multiplication leads to very large values, is also an issue with RNNs.\nSuppose that we repeatedly multiply the square symmetric matrix \\(\\textbf{A}\\) with itself. Apply the eigenvalue decomposition \\[\n\\textbf{A}= \\textbf{Q}\\boldsymbol{\\lambda}\\textbf{Q}^\\prime\n\\] where \\(\\boldsymbol{\\lambda}\\) is the diagonal matrix of eigenvalues and \\(\\textbf{Q}\\) is an orthogonal matrix. You can now write the \\(k\\)-fold product in terms of the decomposition: \\[\n\\textbf{A}\\times \\textbf{A}\\times \\cdots \\times \\textbf{A}= \\textbf{Q}\\boldsymbol{\\lambda}^k\\textbf{Q}^\\prime\n\\] The \\(k\\)-fold product involves the \\(k\\)th power of the eigenvalues. If \\(\\lambda_i &lt; 0\\), then \\(\\lambda_i^k\\) will vanish. If \\(\\lambda_i &gt; 1\\), then \\(\\lambda_i^k\\) will explode for large values of \\(k\\).\nBecause of this problem, training RNNs can be difficult. Vanishing gradients slow down the learning of the network, making it difficult to learn long-term dependencies. Signals that occur early in the sequence get washed out by the time they have propagated through the chain. Short-term dependencies will receive more weight. Exploding gradients can make the training numerically unstable.\nAs a result, RNNs are limited to learning relatively short sequences (&lt; 20). To overcome these issues, related architectures have been proposed, for example, Gated RNNs and Long Short-Term Memory (LSTM) models.\n\n\nLong Short-Term Memory Models (LSTM)\nThe name long short-term memory appears strange at first. What is long short-term memory? The basic idea of the LSTM becomes clear when its goals are considered: the idea is to create paths through time that do not vanish. Rather than trying to hold on to all past information through the common weights, an LSTM model adds components to the recurrence that allow the network to decide which states to remember and which states to forget.\nAn LSTM provides short-term memory for RNNs that can last many steps in the sequence—hence the name. How it accomplishes this is by way of gates, units that control other elements of the network. Each gate is itself a network with weights, biases, and an activation function. A memory cell of an LSTM (Figure 33.18) consists of\n\nInternal state \\(s^{(t)}\\)\nHidden state \\(h^{(t)}\\)\nInput node \\(C_i^{(t)}\\)\nInput gate \\(i_i{(t)}\\): determines whether an input should affect the internal state\nForget gate \\(f_i^{(t)}\\): determines whether the internal state should be flushed to zero\nOutput gate \\(o_i^{(t)}\\): determines whether the internal state should impact the cell’s output\n\nThe gates are modeled with learned weights and a sigmoid activation function. \\[\n\\begin{align*}\n    f_i^{(t)} &= \\sigma\\left(b_i^f + \\sum_j U_{ij}^f \\,x_j^{(t)} + \\sum_j W_{ij}^f \\,h_j^{(t-1)} \\right ) \\\\\n    i_i^{(t)} &= \\sigma\\left(b_i^i + \\sum_j U_{ij}^i \\,x_j^{(t)} + \\sum_j W_{ij}^i \\,h_j^{(t-1)} \\right ) \\\\\n    o_i^{(t)} &= \\sigma\\left(b_i^o + \\sum_j U_{ij}^o \\,x_j^{(t)} + \\sum_j W_{ij}^o \\,h_j^{(t-1)} \\right )\n\\end{align*}\n\\]\nInput node \\[\n    C_i^{(t)} = \\sigma \\left(b_i + \\sum_j U_{ij} \\,x_j^{(t)} + \\sum_j W_{ij} \\, h_j^{(t-1)} \\right )\n\\]\nInternal state update \\[\n    s_i{(t)} = f_i^{(t)} \\, s_i^{(t-1)} + i_i^{(t)} \\, C_i^{(t)}\n\\] Hidden state update \\[\n    h_{(t)} = \\tanh\\left( s_i^{(t)}\\right) o_i^{(t)}\n\\]\nIf the forget gate \\(f_i^{(t)} = 1\\) and the input gate \\(i_i^{(t)} = 0\\), the cell’s internal state \\(s _i^{(t)}\\) will not change. If the forget gate \\(f_i^{(t)} &lt; 1\\) and the input gate \\(i_i^{(t)} &gt; 0\\), the cell’s internal state \\(s _i^{(t)}\\) will be perturbed by the inputs. If the output gate \\(o_i^{(t)} \\approx 1\\), then the cell’s internal state impacts the subsequent layers fully. If the output gate \\(o_i^{(t)} \\approx 0\\), then the memory is prevented from impacting other layers at the current time step.\n\n\n\n\n\n\nFigure 33.18: LSTM Memory Cell",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-transformers",
    "href": "deeplearning.html#sec-dl-transformers",
    "title": "33  Deep Learning",
    "section": "33.4 Transformers",
    "text": "33.4 Transformers\n\nIntroduction\nFigure 33.19 displays the major neural network types discussed so far.\n\n\n\n\n\n\nFigure 33.19: Basic neural network types.\n\n\n\nFor three decades the basic neural network architecture remained the same, despite many advances. CNNs became the de-factor standard in computer vision and LSTMs became the standard in natural language processing. Rather than fundamental architectural breakthroughs, the progress in deep learning was due to greater computational resources and the availability of more data. Until transformers, that is.\nIn 2017, Vaswani et al. (2017) proposed in the paper “Attention is all you need” a new network architecture for sequence-to-sequence data where the input and output is sequential (as in a RNN). The goal was to address issues with long-range dependencies and contextual understanding in RNN-style models. All recurrent layers were replaced with a new type of network layer, the self-attention layer. Besides improved performance in language translation tasks, a key difference between recurrent and attention architectures was the opportunities for parallel training in the latter. Because recurrent networks process data in a sequential order, opportunities for parallel execution are limited to within-layer operations. Transformer architecture introduced an additional encoding—positional encoding—to capture positional information.\nThe paper by Vaswani et al. (2017) has been cited more than 120,000 times (by July 2024). For statisticians used to read methodological papers, it is noteworthy that the paper does not contain a single theorem or proof. It introduces self-attention mechanism as an extension of the attention mechanism introduced in Bahdanau, Cho, and Bengio (2014) and makes recommendations how to build self-attention-based encoder-decoder models. This work fundamentally changed work on neural network architectures and led to the foundation large-language models such as GPT, Bert, LLama, and others. The revolution was on.\nTo summarize what makes transformer architectures special, here are a few points:\n\nUsing self-attention mechanism instead of recurrence allows the model to consider the entire sequence simultaneously. This allows processing of longer sequences compared to RNN-style models.\nEliminates the need for recurrence or hidden states. Instead, positional encoding maintains information about the sequential nature of the data.\nTraining in parallel allows building of larger networks\nSelf-supervised learning: unsupervised learning is followed by supervised fine-tuning. A good example are GPT and Chat GPT. GPT is the foundation model that provides language understanding. Chat GPT is the question-answer application trained on top of the GPT foundation model.\nTransformers excel not only in natural language processing, but also computer vision, audio processing, etc. Vision transformers, for example, convert images into a sequential format and apply a transformer architecture. Transformer models is the new hammer deep learning had been waiting for.\n\n\n\nSelf Attention\nConsider the two sentences in Figure 33.20. It seems obvious to us that “it” in the first sentence refers to the cup and in the second sentence refers to the pitcher. How would a computer algorithm figure that out?\n\n\n\n\n\n\nFigure 33.20\n\n\n\nAttention mechanisms use weights to focus the algorithm on the elements of the sequence that matter most (Figure 33.21).\n\n\n\n\n\n\nFigure 33.21\n\n\n\n\n\n\nFigure 33.1: Deep learning mind map.\nFigure 33.2: Deep learning as a hierarchy of concepts. Adapted from Goodfellow, Bengio, and Courville (2016)\nFigure 33.3: Object detection and classification\nFigure 33.6: Where is Waldo?\nFigure 33.7: Convolving the first row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\nFigure 33.8: Convolving the second row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\nFigure 33.9: Sparse and full connectivity, adapted from Goodfellow, Bengio, and Courville (2016, 336–37)\nFigure 33.12: Convolutional layer with pooling.\nFigure 33.13: LetNet-5 architecture. Source\nFigure 33.14: Part of my Life List in Merlin Bird ID.\nFigure 33.15: Spectogram in Merlin Bird ID.\nFigure 33.16: Folded and unfolded RNN representation.\nFigure 33.17: Complete representation of an RNN.\nFigure 33.18: LSTM Memory Cell\nFigure 33.19: Basic neural network types.\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. https://api.semanticscholar.org/CorpusID:11212020.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  }
]