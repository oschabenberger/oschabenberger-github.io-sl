[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#origin-story",
    "href": "index.html#origin-story",
    "title": "Statistical Learning",
    "section": "Origin Story",
    "text": "Origin Story\nAnother treatise on statistical learning, data science, and machine learning. Sigh. Isn’t there enough material already?\nThe genesis of this material is a two-semester graduate-level data science methods sequence I am teaching at Virginia Tech. The courses are taken primarily by students from non-statistics programs.\nThey need to understand methods of supervised and unsupervised learning and apply them in the real world. They work on research problems much more complex than Palmer Penguins or Fisher’s Iris data. They are getting ready to take a role as a data scientist or statistical programmer at an organization. They want to know about classical and modern methods. They need to know how discriminant analysis is different from support vector machines but they do not know either of them yet.\nAt the same time, their background in foundational aspects of data science—statistics and probability, linear algebra, computer science—varies considerably. That is to be expected, many of us come to data science from a non-statistical, non-computer science path.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "Statistical Learning",
    "section": "Philosophy",
    "text": "Philosophy\nSo I asked myself: suppose you have two semesters to convey data science methods at the M.S. level to this audience; what needs to be covered and at what depth? This material reflects the balance I strike between mathematical hand waving, theorems and derivations, algorithms, and details of software implementation.\nExamples of this philosophy are hopefully evident throughout the parts and chapters:\n\nBeing able to implement all methods with software is just as important to me as their underpinnings in statistics, probability, and computer science.\nKnowing how to build a multilayer neural network with Keras is as important as understanding how activation functions in hidden layers introduce nonlinearity.\nRepresenting linear mixed models in terms of vectors and matrices of constants and random variables is relevant to match model components to software syntax.\nModeling overdispersed count data becomes a lot easier if mechanisms that lead to overdispersion are framed in terms of finite mixtures of random processes.\nGeneralized linear models are a natural extension of the classical linear model that can be understood without knowing the details of the exponential family of distributions. If you want to approach them from a theoretical perspective, knowing exponential family properties is not negotiable.\nNonlinear models and numerical optimization are under-served topics.\n\n\nMind Maps\nI believe that an important aspect of learning material is to organize it and structure it. How to best compartmentalize a topic is personal. Unfortunately, someone else’s treatment enforces their structure on the material.\nMany chapters contain mind maps that reflect my organization of a topic. For example, Figure 1 categorizes statistical modeling approaches in data science and Figure 2 is my high-level mind map for deep learning. Clicking on those and other images in the text will zoom into them for a close-up view.\n\n\n\n\n\n\nFigure 1: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\n\n\n\n\n\n\nFigure 2: Deep learning mind map.\n\n\n\nThese were drawn with the free version of Excalidraw and the source files are available on GitHub. I encourage you to modify these mind maps to suit your personal views of a topic.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#statistical-learning",
    "href": "index.html#statistical-learning",
    "title": "Statistical Learning",
    "section": "Statistical Learning",
    "text": "Statistical Learning\nStatistical learning is a blend of statistical modeling and machine learning. It draws on a data-generating mechanism as in statistical modeling and it is inspired by a computing-oriented approach to solving problems with data. It leans more on predictive inference (prediction and classification) than confirmatory inference (hypothesis testing). It leans more on observational data where models need to be derived rather than the analysis of experiments where the experiment determines the model.\nStatistical learning, unlike machine learning, is keenly interested in the uncertainty that transfers from the data-generating mechanism into the quantities computed from data and ultimately, into the decisions based on those.\nStatistical learning, unlike statistical modeling, is open to algorithmic approaches to derive insights from data and recognizes data as a resource for learning. Working with data does not have to start with an hypothesis. It can start with the data.\nIf these remarks apply to you and this philosophy and approach appeals to you, then the material might work for you. If not, then there are oodles of other resources.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Statistical Learning",
    "section": "Software",
    "text": "Software\nThe programming language used almost entirely throughout this material is R. Wait, what? Yes, I spent the majority of my non-academic career at SAS—19 years—writing statistical software, developing distributed analytic platforms, leading R&D teams, and doing other stuff.\nWhen I was in graduate school 30+ years ago, SAS was everywhere. Today, the reality of statistics and data science is that almost all of it is done in R or Python. You did not have to explain back then why teaching material is based on SAS. How things have changed.\nRegarding the R-vs-Python debate, it is my experience that those coming to data science from a statistical path prefer R, those coming from a different path are more familiar with Python as a general programming language. R was developed a a statistical programming language–based on the S language. And it shows. It is highly efficient in interacting with data, in formulating statistical models, and R packages tend to make the results available that matter to statisticians. For example, not computing standard errors for estimated quantities is a head-scratcher for me. Those should matter in machine learning as well. The R code for statistical learning is short compared to the lengthy and wordy Python code I am writing for the same task. I must be doing something wrong.\nWriting material that caters to multiple tools or programming language is a nightmare for the author and the reader. My approach is to use a single framework where possible. Once you know the basics you can map that to other languages, IDEs, and tools.\nThe source for this material is available on GitHub. I would very much welcome a Python version of it.\n\nThis material is accompanied by\n\nFoundations of Data Science–Beyond the Numbers (Foundations) and\nStatistical Programming (StatProgramming).\n\nFoundations covers fundamental concepts in data science and the data science project life cycle. StatProgramming is a short introduction into programming from a statistical perspective—using mostly R.\nAll of these are written in Quarto, because it handles multiple programming languages in the same document, works in RStudio like RMarkdown on steroids, incorporates \\(\\LaTeX\\) beautifully, and creates great-looking and highly functional documents. To learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\nFigure 1: Structuring and categorizing input, algorithm, and output in data science models.\nFigure 2: Deep learning mind map.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is in it?\nThe material is organized in several parts.\nPart I covers foundation topics. Chapters 3  Linear Algebra Review and 4  Parameter Estimation are mostly for self study and serve as reference material.\nParts II and III discuss methods for supervised learning from the perspective of regression and classification. There is of course overlap between the two, a technique that predicts probabilities can be used for classification. Beyond the classical linear model, Part II also introduces nonlinear regression and a first look at regression problems with a discrete target variable. Generalized models, linear and additive ones, are picked up again in more theoretical detail in Chapters 27  Generalized Linear Models and 28  Generalized Additive Models of Part VII.\nDecision trees are regression and classification methods, they could go in Part II and Part III. I decided to give them their own section. They are simple, intrinsically interpretable, but by themselves perform pretty poorly. Yet they are important building blocks of methods that perform extremely well in many situations, such as random forests and gradient boosting machines.\nEnsemble learning (Part V) is a powerful approach to combine basic methods into highly performant prediction or classification engines. They are underrated in statistics and maybe overrated in machine learning. They are not without pitfalls, though.\nMethods for unsupervised learning are discussed in Part VI. Principal Component Analysis (PCA) is in its own chapter. It is a classical statistical method that is of great importance in modern data science workflows.\nPart VII revisits generalized linear models introduced earlier in greater depth. The discussion of the exponential family of distribution is here. Many sampling or design structures lead to correlated data: subsampling, hierarchical treatment assignments, time series, longitudinal and spatial data, etc. Ignoring correlations in the target variables has a negative effect on the analysis and decisions. Furthermore, correlations allow us to make certain conclusions more precise, for example, statements about change and growth. We first introduce correlated error models in 29  Correlated Data and then cover (linear) mixed models for longitudinal data in 30  Mixed Models for Longitudinal Data.\nPart VIII moves from more traditional statistical techniques into modern territory. Artificial neural networks have changed what algorithms trained on data are capable of. This part journeys from single-layer and multi-layer fully connected networks to convolutional and recurrent neural networks to an introduction to transformer architectures. We are not able to give more than an introduction into deep learning here, but feel strongly that a statistician or data scientist today needs to have at least a basic understanding of these technologies.\nInterpretability and explainability of models trained on data has risen greatly in importance over the past decade. Things we do to make models perform better tend to have a negative effect on interpretability and explainability. A linear regression model is highly interpretable, a regularized Ridge regression with 1,000 inputs is less so. A single decision tree is intrinsically interpretable, a random forest of 500 trees is not. This topic appears at the end of the material not because it is an afterthought but because we emphasize model-agnostic explainability tools that apply independently of how a model was initially trained.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#possible-organization-into-courses",
    "href": "intro.html#possible-organization-into-courses",
    "title": "Introduction",
    "section": "Possible Organization into Courses",
    "text": "Possible Organization into Courses\nIn a two-semester sequence I cover in the first semester\n\nFoundation\nRegression\nClassification\nDecision Trees\n\nand in the second semester\n\nEnsemble Methods\n\n\nUnsupervised Learning\nAdvanced Topics\nNeural Networks and Deep Learning\nExplainability\n\nI have found that the material in Chapter 11  Local Models can be a bit much in the first semester. Depending on the class, I might move basis expansions, regression and smoothing splines to the second semester. Also, a first semester course that covers decision trees might dip into bagging and boosting to improve performance.\nIf you are an instructor, you will find different ways to break up or subset the material.\nLet’s get started.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "statmodels.html",
    "href": "statmodels.html",
    "title": "1  Statistical Models",
    "section": "",
    "text": "1.1 What is in a Model?\nThe term model is pervasive in our field, and we likely come to this conversation with different notions of what constitutes a model. Before going any further in the discussion, let’s discuss the concept of a model in the context of data science.\nFrom the 30,000 foot view a model is simply a mechanism to process some input and produce a corresponding output (Figure 1.1).\nThe input to drive the model algorithm is almost always some form of data. The algorithm that processes the inputs can be based on data, but that is not necessarily so. Suppose the problem we are trying to solve is to ascertain an individuals annual federal income tax. The problem is solved with a model that takes as input the individuals financial situation. This information is typically known without error as information about income, property taxes, expenses, etc. is well documented. The algorithm processing this input is a translation of the relevant information in the federal income tax laws into machine instructions. The output is the amount of money owed to the government or expected as a refund.\nNow suppose that for some reason the input data in the tax problem is not known without error. For example, income from tips, medical expenses or charitable contributions might be best guesses rather than exact amounts. Income data could be noisy because foreign income is converted at fluctuating exchange rates. If the input data is the realization of stochastic (random) influences, should we modify the algorithm?\nWhen the input data to an algorithm is the result of observing random variation, we are looking to the algorithms of the model to find the signal in the data, to de-noise it. The signal located in the data is then transformed into the model output. Most models we build in data science are of this kind because the data we work with is inherently noisy. The reasons for the random variations are many: selecting observation from a larger population at random, applying treatments to randomly chosen experimental units, variations in measurement instruments and procedures, variations in the environment in which a phenomenon is observed, and so on.\nThe algorithms we use depend on the goals of the analysis, properties of the data, assumptions we are willing to make, attributes we look for in competitive models, and personal preferences.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statmodels.html#what-is-in-a-model",
    "href": "statmodels.html#what-is-in-a-model",
    "title": "1  Statistical Models",
    "section": "",
    "text": "Figure 1.1: A simple representation of a model that processes inputs with algorithmic logic and produces output.\n\n\n\n\n\n\n\n\n\nTwo Cultures\nWhile everyone agrees that models in data science incorporate uncertainty, and that uncertainty is expressed in terms of randomly varying model elements, there is not agreement how to incorporate randomness into the model formulation and analysis. In an influential 2001 paper, “Statistical Modeling: The Two Cultures”, Leo Breiman contrasted two schools of thoughts: statistical (data) modeling and algorithmic modeling (Breiman 2001).\nStatistical (data) modeling assumes that the data are generated by a stochastic data model. The model captures the essence of the random processes that gave the rise to the observed data. The data set at hand is one particular realization of those random processes. If the study were repeated, another data set would be realized, containing different numbers, but with the same stochastic properties. According to Breiman, 98% of statisticians subscribe to this approach.\nAlgorithmic modeling, on the other hand, makes no assumption about the underlying data model, treats the data mechanism as unknown, and is more common in fields outside of statistics. In Breiman’s words:\n\nPerhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians.\n\nThe goal of algorithmic models is more predictive accuracy than confirmatory inference and hypothesis testing. The model is supposed to approximate an unknown relationship between inputs and outputs well enough to provide satisfactory accuracy in predicting outputs of previously unseen inputs. Neural networks, decision trees, and support vector machines are examples of algorithmic tools that found rapid adoption outside of statistics. Machine learning as it emerged from computer science is a manifestation of algorithmic modeling.\nIn data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself: starting values, optimization, convergence behavior, parallel execution, hyperparameter tuning, and so on.\nBreiman’s article was widely discussed—the invited comments by leading statisticians at the end of the paper give a sample of opinions.\nBoth views, algorithmic and statistical, are correct and useful, and taking different views based on the situation does not cause cognitive dissonance. When we are working with large data sets, appealing to a random mechanism that generated the data is not necessarily straightforward (or meaningful):\n\nHow do we describe the mechanism that yields the web-scraped corpus of text on which large language models (LLMs) are trained?\nWhat random elements are at work when you mine the database of customer interactions of a company? The database contains all transactions with all customers.\nIs a sampling mechanism adequate to capture variability in images collected on the internet?\nWhen we are concerned with apples randomly selected from randomly trees in an orchard, it is natural to consider the hierarchical sampling mechanism to model the between-tree and within-tree variability.\nWhen a medical treatment and a placebo are assigned to two groups of randomly chosen subjects, we are\nable to make causal statements about the treatment effects because other effects not accounted for are balanced out by the randomization. We will take full advantage of that probability mechanism in analyzing the experimental data.\n\nOverall, our approach is a statistical modeling approach, models contain explicit random elements that allow us to study the statistical properties of quantities derived from the trained model and make uncertainty statements—under the assumption that the probabilistic assumptions made are justifiable.\nGeorge E.P. Box is credited with coining the much-used phrase “all models are wrong, but some are useful” (Box 1976). The phrase appears partially (“all models are wrong”) twice in his 1976 paper on Science and Statistics:\n\nSince all models are wrong the scientist cannot obtain a “correct” one by excessive elaboration.\nSince all models are wrong the scientist must be alert to what is importantly wrong.\n\nThe takeaway is that any model is an abstraction of a phenomenon and we strive to find a useful abstraction. The model does not attempt to reproduce the phenomenon. The tax algorithm converts the essence of the tax code into machine instructions, it is not an electronic copy of the entire law. The purpose is to accurately calculate an entity’s tax, anything else can be stripped away. An algorithm processing noisy data that reproduces the data is uninteresting. The goal is to abstract the data in such a way to allow separating the signal from the noise and to convert the signal into the desired output.\nThe first G.E.P. Box quote instructs us not to overdo it in building models; this translates to the problem of overfitting in data science, crafting a model that follows the training data too closely and as a result does not generalize well to new data points. If the goal is to predict, classify, or cluster the unseen then generalizability of the model is key. A model to forecast stock prices or trading volumes is judged by how well it can predict the future, not by how well it can predict the past. The adequate level of generalization for that model must be wrung from current and past stock prices. Finding the appropriate level of abstraction is resolved by striking the right balance in the bias-and-variance tradeoff (see Chapter 2).\nThe second G.E.P. Box quote instructs us that models are abstracting away features of the phenomenon. If these are important features, the model is not useful. In the best case this model does not meet its goal and is revised or abandoned. In the worst case the model can lead to bad decisions and harmful outcomes.\nNo matter how complex the model, we need to strive to understand how it works (interpret the model), not just what it does. If a model is not intrinsically interpretable then we need to strive to explain the forces that drive the model, keeping in mind that we are then making statements about the model and not about the underlying phenomenon we have abstracted.\n\n\nStatistical Learning and Machine Learning\nMuch is being made of the difference between statistical models and machine learning models, or to be more precise, between statistical learning (SL) and machine learning (ML).\n\n\nDefinition: Statistical Learning\n\n\nStatistical Learning is the process of understanding data through the application of tools that describe structure and relationships in data. Models are formulated based on the structure of data to predict outcomes from inputs, to test hypothesis about relationships, to group data, or to reduce the dimensionality of a problem.\n\n\nStatistical learning emphasizes prediction more than the testing of hypothesis, as compared to statistical modeling. Many model classes used in statistical learning are the same models one uses to test hypothesis about patterns and relationships in data. Emphasis of prediction over hypothesis testing—or vice versa—flows from the nature of the problem we are trying to solve. The same model can be developed with focus on predictive capability or with focus on interpretability. We do not want to overdo the distinction between statistical learning and s tatistical modeling: statistical learning uses statistical models.\nLearning is the process of converting experience into knowledge and machine learning is an automated way of learning by using computers. Rather than directly programming computers to perform a task, machine learning is used when the tasks are not easily described and communicated (e.g., driving, reading, image recognition) or when the tasks exceed human capabilities (e.g., analyzing large and complex data sets). Modern machine learning discovered data as a resource for learning and that is where statistical learning and machine learning meet.\nSL and ML have more in common, than what separates them:\n\nThe input to a learning algorithm is data; the raw material is the same.\nThe data are thought of as randomly generated, there is some sense of variability in the data that is attributed to random sources.\nBoth disciplines distinguish supervised and unsupervised forms of learning\nThey use many of the same models and algorithms for regression, classification, clustering, dimension reduction, etc.\n\nMachine learning uses observed data to describe relationships and “causes”; the emphasis is on predicting new and/or future outcomes. There is comparatively little emphasis on experimentation and hypothesis testing.\nA key difference between SL and ML is what Breiman describes as the difference between data modeling and algorithmic modeling. The difference aligns closely with statistical and machine learning thinking. In data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself. Consequently, statisticians are concerned with the asymptotic distributional behavior of estimators and methods as \\(n \\rightarrow \\infty\\). Machine learning focuses on finite sample properties and ask what accuracy can be expected based on the available data.\nThe strong assumptions statisticians make about the stochastic data-generating mechanism that produced the data set in hand as a realization are not found in machine learning. That does not mean that machine learning models are free of stochastic elements and assumptions—quite the contrary. It means that statisticians use the data-generating mechanism as the foundation for conclusions rather than the data alone.\nWhen you look at a p-value in a table of parameter estimates, you rely on all assumptions about distributional properties of the data, correctness of the model, and (asymptotic) distributional behavior of the estimator. They flow explicitly from the data-generating mechanism or implicitly from somewhere else. Otherwise, the p-value does not make much sense. (Many argue that p-values are not very helpful and possibly even damaging to decision making but this is not the point of this discussion.)\nIf you express the relationship between a target variable \\(Y\\) and inputs \\(x_1, \\cdots, x_p\\) as\n\\[\nY = f(x_1,\\cdots,x_p) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is a random variable, it does not matter whether you perform data modeling or algorithmic modeling. We need to think about \\(\\epsilon\\) and its properties. How does \\(\\epsilon\\) affect the algorithm, the prediction accuracy, the uncertainty of statements about \\(Y\\) or \\(f(x_1, \\cdots, x_p)\\)? That is why all data professionals need to understand about stochastic models and statistical models.\n\n\nStochastic and Statistical Models\n\n\nDefinition: Stochastic Model\n\n\nA stochastic model describes the probability distribution of outcomes by allowing one or more of the model elements to be random variables.\n\n\nSuppose we are charged with developing a model to predict recurrence of cancer. There are many possible aspects that influence the outcome:\n\nAge, gender\nMedical history\nLifestyle factors (nutrition, exercise, smoking, …)\nType of cancer\nSize of the largest tumor\nSite of cancer\nTime since diagnostic, time from treatment\nType of treatment\nand so on\n\nIf we were to try and build a deterministic model that predicts cancer recurrence perfectly, all influences would have to be taken into account and their impact on the outcome would have to be incorporated correctly. That would be an incredibly complex model, and impractical.\nBy taking a stochastic approach we acknowledge that there are processes that affect the variability in cancer recurrence we observe from patient to patient. The modeling can now focus on the most important factors and how they drive cancer recurrence. The other factors are included through random effects. If the model captures the salient factors and their impact correctly, and the variability contributed by other factors is not too large, and not systematic, the model is very useful. It possibly is much more useful than an inscrutably complex model that tries to accommodate all influences perfectly.\nThe simplest stochastic model for cancer recurrence is to assume that the outcome is a Bernoulli (Binary) random variable taking on two states (cancer recurs, cancer does not recur) with probabilities \\(\\pi\\) and \\(1-\\pi\\). If we code the two states numerically, cancer recurs as 1, cancer does not recur as 0, the probability mass function of cancer recurrence is that of the random variable \\(Y\\),\n\\[\n\\Pr(Y=y) = \\left \\{ \\begin{array}{cl} \\pi & y=1 \\\\ 1-\\pi & y = 0\\end{array} \\right .\n\\]\n\n\nDefinition: Statistical Model\n\n\nA statistical model is a stochastic model that contains unknown constants, called parameters. Parameters are estimated based on data. Parameters are constants, not random variables. The estimator of a parameter that depends on data is a random variable since the data are random.\n\n\nThe parameter in our cancer model is \\(\\pi\\), the probability that \\(Y\\) takes on the value 1. In statistics, this probability is often called the “success” probability and its complement is called the “failure” probability. We prefer to call them the “event” and “non-event” probabilities instead. The event is the binary outcome coded as a 1.\nBecause we cannot visit with all cancer patients, a sample of patients is used to estimate \\(\\pi\\). This process introduces uncertainty into the estimator of \\(\\pi\\), a larger sample will lead to a more precise (a less uncertain) estimator.\nThe model is overly simplistic in that it captures all possible effects on cancer recurrence in the single quantity \\(\\pi\\). Regardless of age, gender, type of cancer, etc., we would predict a randomly chosen cancer patient’s likelihood to experience a recurrence as \\(\\pi\\). To incorporate input variables that affect the rate of recurrence we need to add structure to \\(\\pi\\). A common approach in statistical learning and in machine learning is that inputs have a linear effect on a transformation of the probability \\(\\pi\\):\n\\[\ng(\\pi) = \\beta_0 + \\beta_1 x_1+\\cdots + \\beta_p x_p\n\\]\nWhen \\(g(\\pi)\\) is the logit function\n\\[\\log\\left \\{ \\frac{\\pi}{1-\\pi} \\right\\}\\]\nthis is called a logistic regression model. \\(x_1,\\cdots,x_p\\) are the inputs of the model, \\(\\beta_0, \\cdots, \\beta_p\\) are the parameters of the model. \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\cdots , \\beta_p\\) are the coefficients associated with the input variables. The logit transform maps the problem onto a scale where the effects of the inputs are linear. In other words, a unit change in \\(x_j\\) leads to a \\(\\beta_j\\)-change in the logit of the event probability—it does not lead to a \\(\\beta_j\\)-change in the event probability.\nIf we accept that the basic structure of the logistic model applies to the problem of predicting cancer occurrence, we use our sample of patient data to\n\nestimate the parameters \\(\\beta_0, \\cdots, \\beta_p\\);\ndetermine which inputs and how many inputs are adequate: we need to determine \\(p\\) and the specific input variables;\ndetermine whether the logit function is the appropriate transformation to linearity.\n\nThe effect of the inputs is called linear on \\(g(\\pi)\\), if \\(g(\\pi)\\) is a linear function of the parameters. To test whether this is the case, take derivatives of the function with respect to all parameters. If the derivatives do not depend on parameters, the effect is linear.\n\\[\\begin{align*}\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{0}} &= 1 \\\\\n\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{1}} &= x_{1}\\\\\n\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{p}} &= x_{p}\n\\end{align*}\\]\nNone of the derivatives depends on any of the \\((\\beta_{0},\\ldots,\\beta_{p})\\); \\(g(\\pi)\\) is linear in the parameters. A non-linear function is non-linear in at least one parameter.\n\n\nExample: Plateau (hockey stick) Model\n\n\nA plateau model reaches a certain amount of output and remains flat afterwards. When the model prior to the plateau is a simple linear model, the plateau model is also called a hockey-stick model.\n\n\n\n\n\nThe point at which the plateau is reached is called a change point. Suppose the change point is denoted \\(\\alpha\\). The hockey-stick model can be written as\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\left\\{\n\\begin{matrix}\n\\beta_{0} + \\beta_{1}x      & x \\leq \\alpha \\\\\n\\beta_{0} + \\beta_{1}\\alpha & x &gt; \\alpha\n\\end{matrix} \\right.\n\\]\nIf \\(\\alpha\\) is an unknown parameters that is estimated from the data, this is a non-linear model.\n\n\n\n\nModel Types\nMuch of data science methodology is to select the right approach (algorithm) based on input data, learning methodology (supervised, unsupervised, semi-supervised, self-supervised) and analysis goal (prediction, recommendation, classification, clustering, dimension reduction, sequential decisioning), to train the model, and to deploy the model. Figure 1.2 is an attempt at structuring the input, algorithm, and output components of a model in the data science context. The diagram is complex and yet woefully incomplete and is intended to give you an idea of the diversity of methods and the many ways we can look at things. For example, in discussing input data we could highlight how data are stored, how fast it is moving, the degree to which the data is structured, the data types, and so forth. There are many other categorizations of data one could have listed.\nThe categorization of algorithms-—what many consider the models in the narrow sense—-leaves out semi-supervised learning, self-supervised learning, transfer learning, and other learning methods. Volumes of books and papers have been written about every item in the list of algorithms and many algorithms are represented by a simple description. Multilayer networks, for example, include artificial neural networks, deep networks such as convolutional and recurrent networks, and transformer architectures such as GPT.\n\n\n\n\n\n\nFigure 1.2: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nIt might seem like a daunting task to command the plethora of complexity displayed in the previous figure, understand all the pros and cons, grok the idiosyncrasies of software implementations, write code to train the model(s), communicate the results, and possibly implement a solution within a business context. That is what we are here for; let us get started.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "statmodels.html#sec-model-components",
    "href": "statmodels.html#sec-model-components",
    "title": "1  Statistical Models",
    "section": "1.2 Model Components",
    "text": "1.2 Model Components\nThe expression for the logistic regression model\n\\[g(\\pi) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p\\]\nlooks quite different from the model introduced earlier,\n\\[Y = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\\]\nWhere is the connection?\nThe error term \\(\\epsilon\\) is a random variable and we need to specify some of its distributional properties to make progress. At a minimum we provide the mean and variance of \\(\\epsilon\\). If the model is correct—correct on average—then the error terms should have a mean of zero and not depend on any input variables (whether those in the model or other inputs). A common assumption is that the variance of the errors is a constant and not a function of other effects (fixed or random). The two assumptions are summarized as \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\); read as \\(\\epsilon\\) follows a distribution with mean 0 and variance \\(\\sigma^{2}\\).\n\nMean function\nNow we can take the expected value of the model and find that\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\text{E}\\left\\lbrack f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon \\right\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right)\n\\]\nBecause the errors have zero mean and because the function \\(f\\left( x_1,\\ldots, x_p \\right)\\) does not contain random variables, \\(f\\left( x_1,\\ldots, x_p \\right)\\) is the expected value (mean) of \\(Y\\). \\(f\\left( x_1,\\ldots,x_p \\right)\\) is thus called mean function of the model.\n\n\nExample: Curvilinear models\n\n\nPolynomial models such as a quadratic model \\[Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\epsilon\\] or cubic model \\[Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\epsilon\\] have a curved appearance when \\(Y\\) is plotted against \\(x\\). They are linear models, however.\nTo test this, take derivatives of the mean function with respect to the parameters. For the quadratic model the partial derivatives with respect to \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\) are 1, \\(x\\), and \\(x^{2}\\), respectively. The model is linear in the parameters.\nTo emphasize that the models are not just straight lines in \\(x\\), a linear model with curved appearance is called curvilinear.\n\n\nWhat does the mean function look like in the logistic regression model? The underlying random variable \\(Y\\) has a Bernoulli distribution. Its mean is\n\\[\\text{E}\\lbrack Y\\rbrack = \\sum y\\, \\Pr(Y = y) = 1 \\times \\pi + 0 \\times (1 - \\pi) = \\pi\\]\nThe logit function \\[g(\\pi) = \\log \\left\\{ \\frac{\\pi}{1 - \\pi} \\right\\}\\] is invertible and the model \\[g(\\pi) = \\beta_0 + \\beta_1 x_{1} + \\ldots + \\beta_p x_p\\]\ncan be written as\n\\[\\text{E}\\lbrack Y\\rbrack = \\pi = g^{- 1}\\left( \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p} \\right)\\]\nThe mean function of the logistic model is also a function of the inputs. It is easy to show that if \\(g(\\pi)\\) is the logit function the mean function is\n\\[\\pi = \\frac{1}{1 + \\exp\\left\\{ - \\beta_0 - \\beta_1 x_1 - \\cdots - \\beta_p x_p \\right\\}}\\] The logistic regression model is linear in the parameters on the logit scale (\\(g(\\pi)\\)). It is a non-linear model on the probability scale; also called the response scale. Logistic regression models belong to a larger class of statistical models, the generalized linear models (GLM). The function that maps the mean of the data onto a scale where effects are linear is called the link function. It is thus common to refer to the link scale when interpreting the regression coefficients \\(\\beta_0, \\cdots, \\beta_p\\) and the response scale when interpreting the mean of the data.\n\n\nSystematic component\nThe mean functions \\[f\\left( x_{1},\\ldots,x_{p} \\right)\\] and \\[\\frac{1}{1 + \\exp\\left\\{ - \\beta_{0} - \\beta_{1}x_{1} - \\ldots - \\beta_p x_p \\right\\} }\\] look rather different, except for the input variables \\(x_{1},\\ldots,x_{p}\\).\nFor the model \\(Y = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\\) we left it open how the mean function depends on parameters. There are three general approaches.\nThe systematic component has the form of a linear predictor, that is, a linear combination of the inputs. The linear predictor is frequently denoted as \\(\\eta\\):\n\\[\\eta = \\beta_{0} + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\]\nThe parameter \\(\\beta_0\\) is called the intercept of the linear predictor. Although optional, it is included in most models to capture the effect on the mean if no input variables are present. Models with a linear predictor and an intercept have \\(p + 1\\) parameters in the mean function.\nThe logistic regression model also contains a linear predictor. Depending on whether you write the model in terms of \\(g(\\pi)\\) or \\(\\pi\\), the expressions are\n\\[g(\\pi) = \\eta\\]\n\\[\\pi = \\frac{1}{1 + \\exp\\{ - \\eta \\}}\\]\nThe mean function can be a general non-linear function of the parameters. The number of input variables and the number of parameters can be quite different.\nThe Mitscherlich model is popular in agricultural studies of plant growth as a function of an input such as a fertilizer. The plant species is a commercial crop. If \\(Y\\) denotes plant yield and \\(x\\) the amount of input, the Mitscherlich model is\n\\[Y = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\exp\\left\\{ - \\kappa x \\right\\} + \\epsilon\\]\nThe mean function \\(f\\)() depends on one input variable \\(x\\) and three parameters \\((\\xi,\\lambda,\\kappa)\\). Taking derivatives, it is easily established that the mean function is non-linear. For example,\n\\[\\frac{\\partial f(x,\\xi,\\lambda,\\kappa)}{\\partial\\xi} = \\exp\\{ - \\kappa x \\}\\]\nThe derivative with respect to \\(\\xi\\) depends on the \\(\\kappa\\) parameter.\nNon-linear models like the Mitscherlich equation are appealing because they are intrinsically interpretable. The parameters have meaning in terms of the subject domain:\n\n\\(\\xi\\) is the crop yield if no fertilizer is applied, the mean of \\(Y\\) at \\(x = 0\\). This is the baseline yield.\n\\(\\lambda\\) is the upper yield asymptote as \\(x\\) increases.\n\\(\\kappa\\) relates to a rate of change, how quickly the yield increases from \\(\\xi\\) and reaches \\(\\kappa\\).\n\nFigure 1.3 shows the Mitscherlich model fitted to a set of plant yield data, the input variable is the nitrogen rate applied (in kg/ha). Visual estimates for the baseline yield and the asymptotic yield are \\(\\widehat{\\xi} = 40\\) and \\(\\widehat{\\lambda} = 80\\).\n\n\n\n\n\n\nFigure 1.3: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\n\n\n\nInterpretability of the parameters enables mapping of research questions to the model:\n\nIs the asymptotic yield greater than 75?\nThis can be answered with a confidence interval for the estimate of \\(\\lambda\\).\nAt what level of \\(x\\) does yield achieve 75% of the maximum?\nThis is an inverse prediction problem. Set yield to 75% of \\(\\lambda\\) and solve the model for \\(x\\).\nThe rate of change in yield is less than ½ unit once \\(x = 100\\) are applied.\nThis can be answered with a hypothesis test for \\(\\kappa\\).\n\nThe third method of specifying the systematic component is to not write it as a function of inputs and parameters. This is common for non-parametric methods such as smoothing splines, local regression, generalized additive models, and kernel methods. These models still have parameters, but the relationship between inputs and parameters is implied through the method of training the models.\nThe systematic component of a smoothing spline in one input variable, for example, can be written as\n\\[\nf(x)= \\sum_{m=1}^M \\beta_m h_m(x)\n\\]\nwhere the \\(h_m(x)\\) functions are based on natural cubic splines, B-splines, or other spline expansions.\nLOESS is a local polynomial regression method that fits a weighted model to data within a window. Data points near the center of the window receive more weight than data points further away. For example, a LOESS model of degree 2 fits a weighted quadratic polynomial model to the data captured in the window. Within window \\(k\\), the model takes the form\n\\[Y = \\beta_{0k} + \\beta_{1k}x + \\beta_{2k}x^2 + \\epsilon\\]\nAs the window moves across the range of \\(x\\), different observations are captured in the window. The underlying model in each window is a quadratic polynomial but the values of the parameter estimates change from window to window. We do not write the systematic component of the model as a single function that applies to the entire range of \\(x\\).\n\n\nRandom component\nThe random components of a statistical model are the stochastic elements that describe the distribution of the target variable \\(Y\\). By now we are convinced that most data we work with are to some degree the result of random processes and that incorporating randomness into models makes sense. The model does not need to be correct for every observation, but it needs to be correct on average—an additive zero-mean random error is OK. Even if all influences on the output \\(Y\\) were known, it might be impossible to measure them, or to include them correctly into the model. Randomness is often introduced deliberately by sampling observations from a population or by randomly assigning treatments to experimental units. Finally, stochastic models are often simpler and easier to explain than other models. Among competing explanations, the simpler one wins (Occam’s Razor).\nWe have encountered so far two ways to reflect randomness in a statistical model:\n\nBy adding an additive error term to a mean function\nBy describing the distribution of the target variable\n\nThe Mitscherlich model is an example of the first type of specification:\n\\[Y = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\exp\\left\\{ - \\kappa x \\right\\} + \\epsilon\\]\nUnder the assumption that \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\), it follows that \\(Y\\) is randomly distributed with mean \\(f(x,\\xi,\\lambda,\\kappa)\\) and variance \\(\\sigma^{2}\\); \\(Y \\sim \\left( f(x,\\xi,\\lambda,\\kappa),\\sigma^{2} \\right)\\). If the model errors were normally distributed, \\(\\epsilon \\sim N\\left( 0,\\sigma^{2} \\right)\\), then \\(Y\\) would also be normally distributed. Randomness is contagious.\nThe logistic regression model is an example of the second type of specification:\n\\[g\\left( \\text{E}\\lbrack Y\\rbrack \\right) = \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\\]\nand \\(Y\\) follows a Bernoulli distribution.\n\n\n\n\n\n\nImportant\n\n\n\nIt does not make sense to write the model with an additive error term unless the target variable is continuous.\n\n\nModels can have more than one random element. In the cancer recurrence example, suppose we want to explicitly associate a random effect with each patient, \\(b_{i} \\sim \\left( 0,\\sigma_{b}^{2} \\right)\\), say. The modified model is now\n\\[g\\left( \\pi\\ |\\ b_{i} \\right) = \\beta_{0} + b_{i} + \\ \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\\]\nConditional on the patient-specific value of \\(b_{i}\\) the model is still a logistic model with intercept \\(\\beta_{0} + b_{i}\\). Because the parameters \\(\\beta_{0},\\ \\cdots,\\beta_{p}\\) are constants (not random variables), they are also referred to as fixed effects. Models that contain both random and fixed effects are called mixed models.\nMixed models occur naturally when the sampling process is hierarchical.\nFor example, you select apples on trees in an orchard to study the growth of apples over time. You select at random 10 trees in the orchard and chose 25 apples at random on each tree. The apple diameters are then measured in two-week intervals. To represent this data structure, we need a few subscripts.\nLet \\(Y_{ijk}\\) denote the apple diameter at the \\(k\\)th measurement of the \\(j\\)th apple from the \\(i\\)th tree. A possible decomposition of the variability of the \\(Y_{ijk}\\) could be\n\\[Y_{ijk} = \\beta_{0} + a_{i} + \\eta_{ijk} + \\epsilon_{ijk}\\]\nwhere \\(\\beta_0\\) is an overall (fixed) intercept, \\(a_i \\sim \\left( 0,\\sigma_{a}^{2} \\right)\\) is a random tree effect, \\(\\eta_{ijk}\\) is an effect specific to apple and measurement time, and \\(\\epsilon_{ijk} \\sim \\left( 0,\\sigma_{\\epsilon}^{2} \\right)\\) are the model errors. This is a mixed model because we have multiple random effects (\\(a_{i}\\) and \\(\\epsilon_{ijk}\\)). In addition, we need to decide how to parameterize \\(\\eta_{ijk}\\). Suppose that a simple linear regression trend is reasonable for each apple over time. Estimating a separate slope and intercept for each of the 10 x 25 apples would result in a model with over 500 parameters. A more parsimonious parameterization is to assume that the apples share a tree-specific (fixed) intercept and slope and to model the apple-specific deviations from the tree-specific trends with random variables:\n\\[\\eta_{ijk} = \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk}\\]\n\\(t_{ijk}\\) is the time that a given apple on a tree is measured. The apple-specific intercept offsets from the tree-specific intercepts \\(\\beta_{0i}\\) are model as random variables \\(b_{0ij} \\sim \\left( 0,\\sigma_{b_{0}}^{2} \\right)\\). Similarly, \\(b_{1ij} \\sim \\left( 0,\\sigma_{b_{1}}^{2} \\right)\\) models the apple-specific offset for the slopes as random variables. Putting everything together we obtain\n\\[Y_{ijk} = \\beta_{0} + \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk} + \\epsilon_{ijk}\\]\nNote that \\(a_{i}\\) was no longer necessary in this model, that role is now played by \\(\\beta_{0i}\\).\nThe total number of parameters in this model is 24 (1 overall intercept, 10 tree-specific intercepts, 10 tree-specific slopes, and 3 variances (\\(\\sigma_{\\epsilon}^{2}, \\sigma_{b_{0}}^{2}\\), \\(\\sigma_{b_{1}}^{2}\\)).\nThis is a relatively complex model and included here only to show how the sampling design can be incorporated into the model formulation to achieve interpretable and parsimonious models and how this naturally leads to multiple random effects.\nA further refinement of this model is to recognize that the measurements over time for each apple are likely not independent. Furthermore, diameter measurements on the same apple close in time are more strongly correlated than measurements further apart. Incorporating this correlation structure into the models leads to a mixed model with correlated errors.\n\n\nResponse (Target) variable\nA model has inputs that are processed by an algorithm to produce an output. When the output is a variable to be predicted, classified, or grouped, we refer to it with different—but interchangeable—names as the response variable, or the target variable, or the dependent variable. We are not particular about what you call the variable, as long as we agree on what we are talking about—the left-hand side of the model.\nThe target variable is a random variable and can be of different types. This matters greatly because we have to match distributional assumptions to the natural type of the target. Applying an analytic method designed for continuous variables that can take on infinitely many values to a binary variable that takes on two values is ill advised. However, it happens. A lot.\nThe first distinction is whether the target variable is continuous or discrete.\n\nContinuous: the number of possible values of the variable is not countable. Typical examples are physical measurements such as weight, height, length, pressure, temperature. If the values of a variable are countable but the cardinality is high, applying methods for continuous data can make sense—for example, number of days since birth.\nDiscrete: the number of possible values is countable. Even if the number of possible values is infinite, the variable is still discrete. The number of fish caught per day does not have a theoretical upper limit, although it is highly unlikely that a weekend warrior will catch 1,000 fish. A commercial fishing vessel might.\n\nDiscrete variables are further divided into the following groups:\n\nCount Variables: the values are true counts, obtained by enumeration. There are two types of counts:\n\nCounts per unit: the count relates to a unit of measurement, e.g., the number of fish caught per day, the number of customer complaints per quarter, the number of chocolate chips per cookie, the number of cancer incidences per 100,000.\nProportions (Counts out of a total): the count can be converted to a proportion by dividing it with a maximum value. Examples are the number of heads out of 10 coin tosses, the number of larvae out of 20 succumbing to an insecticide,\n\nCategorical Variables: the values consist of labels, even if numbers are used for labeling.\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)\n\n\n\n\n\nFigure 1.2: Structuring and categorizing input, algorithm, and output in data science models.\nFigure 1.3: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Models</span>"
    ]
  },
  {
    "objectID": "biasvariance.html",
    "href": "biasvariance.html",
    "title": "2  Bias Variance Tradeoff",
    "section": "",
    "text": "2.1 A Simulation\nTo illustrate the concept, let’s start with a simulated example where we know the true function and collect multiple samples.\nThe following figure shows the relationship between an input variable \\(X\\) and some output function \\(f(x)\\). The function depicts the true relationship, the dots mark design points at which we collect observations. Because the data is inherently variable our sample observations will not fall on the black line. If the sample is unbiased, they should spread evenly about the true trend.\nSuppose that we repeat the sampling process four times, drawing eleven observations each time.\nThis is an unrealistic situation. In real life, we do not know the solid function \\(f(x)\\) and we draw only one set of data, for example, we would work with only the black triangles or the blue dots in the previous figure.\nNext, we train a model on the data and are considering two types of methods: a linear regression model and a smoothing spline.\nThe linear regression model is not flexible. It has only two parameters, the intercept of the vertical line at \\(x = 0\\) and the slope of the line. The lines do not follow the curved trend in the function \\(f(x)\\). Because of this rigidity, the four lines are somewhat similar to each other, they do not show a high degree of variability from sample to sample.\nThe splines show more flexibility than the linear regression lines and follow the observed data more closely. The curviness of the true function \\(f(x)\\) is echoed in the curviness of the splines, but some splines seem to try to connect the dots more than they are picking up the true trend. Because the splines follow the observed data more closely, the four functions show more variability from sample to sample than the linear regression lines.\nSuppose the task is to develop a model that predicts a new observation well, one that did not participate in fitting the model. The model needs to generalize to previously unseen data. Should we choose linear regression or smoothing splines as our method? A method that is highly variable because it follows the data too closely will not generalize well—its predictions will be off because they are highly variable. A method that is not flexible enough also does not generalize well—its predictions will be off because the model is not correct.\nMathematically, we can express the problem of predicting a new observation as follows. Since the true function is unknown, it is also unknown at the new data location \\(x_{0}\\). However, we observed a value \\(y\\) at \\(x_{0}\\). Based on the model we choose the function can be predicted at \\(x_{0}\\). But since we do not know the true function \\(f(x)\\), we can only measure the discrepancy between the value we observe and the value we predicted; this quantity is known as the error of prediction.\nMultiple components contribute to the prediction error: the variability of the data \\(y\\), the discrepancy between \\(f\\left( x_{0} \\right)\\) and \\(\\widehat{f}\\left( x_{0} \\right)\\), and the variability of the function \\(\\widehat{f}\\left( x_{0} \\right)\\). The variability of \\(y\\) is also called the irreducible variability or the irreducible error because the observations will vary according to their natural variability. Once we have decided which attribute to observe, how to sample it, and how to measure it, this variability is a given. The other two sources relate to the accuracy and precision of the prediction; or, to use statistical terms, the bias and the variance.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#a-simulation",
    "href": "biasvariance.html#a-simulation",
    "title": "2  Bias Variance Tradeoff",
    "section": "",
    "text": "Figure 2.1: A response function of one input variable.\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Four random samples of size eleven.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Linear regression models fit to the four data sets.\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Smoothing splines with 6 degrees of freedom fit to the four data sets.\n\n\n\n\n\n\n\nComponents that contribute to bias and variance of an estimator. The last column designates whether the quantity can be measured in data science applications.\n\n\n\n\n\n\n\nQuantity\nMeaning\nMeasurable\n\n\n\n\n\\(f(x)\\)\nThe true but unknown function\nNo\n\n\n\\(f\\left( x_{0} \\right)\\)\nThe value of the function at a data point \\(x_0\\) that was not part of fitting the model\nNo\n\n\n\\(\\widehat{f}\\left( x_{0} \\right)\\)\nThe estimated value of the function at the new data point \\(x_{0}\\)\nYes\n\n\n\\(f\\left( x_{0} \\right) - \\widehat{f}\\left( x_{0} \\right)\\)\nThe function discrepancy\nNo\n\n\n\\(y -\\widehat{f}\\left( x_{0} \\right)\\)\nThe error of prediction\nYes",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#accuracy-and-precision",
    "href": "biasvariance.html#accuracy-and-precision",
    "title": "2  Bias Variance Tradeoff",
    "section": "2.2 Accuracy and Precision",
    "text": "2.2 Accuracy and Precision\nIn the context of measuring devices, accuracy and precision are defined as\n\nAccuracy: How close are measurements to the true value\nPrecision: How close are measurements to each other\n\nTo demonstrate the difference between accuracy and precision, the dart board bullseye metaphor is helpful. The following figure shows four scenarios of shooting four darts each at a dart board. The goal is to hit the bullseye in the center of the board; the bullseye represents the true value we are trying to measure. A is the result of a thrower who is neither accurate nor precise. The throws vary greatly from each other (lack of precision), and the average location is far from the bullseye. B is the result of a thrower who is inaccurate but precise. The throws group tightly together (high precision) but the average location misses the bullseye (the average distance from the bullseye is not zero). The thrower with pattern C is not precise, but accurate. The throws vary widely (lack of precision) but the average distance of the darts from the bullseye is close to zero—on average the thrower hits the bullseye. Finally, the thrower in D is accurate and precise; the darts group tightly together and are centered around the bullseye.\n\n\n\n\n\n\nFigure 2.5: Accuracy and precision—the dart board bullseye metaphor.\n\n\n\nWe see that both accuracy and precision describe not a single throw, but a pattern over many replications. In statistical terms, this long-run behavior is the expected value.\nWhat is the connection between accuracy and precision and expectations of random variables? The accuracy of a statistical estimator is the proximity of its expected value from the target value.\n\n\n\n\n\n\nNote\n\n\n\nWe use the term target here to describe the quantity we are interested in estimating. Please do not confuse this with the target variable in a statistical model. A function of the target variable such as its mean at \\(x_0\\), \\(\\text{E}[Y | x_0]\\) might well be the target we are trying to estimate.\n\n\nAn estimator that is not accurate is said to be biased.\n\n\nDefinition: Bias\n\n\nAn estimator \\(h\\left( \\textbf{Y}\\right)\\) of the parameter \\(\\theta\\) is said to be biased if its expected value does not equal \\(\\theta\\).\n\\[\\text{Bias}\\left\\lbrack h\\left( \\textbf{Y}\\right);\\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) - \\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) \\right\\rbrack - \\theta\\]\n\n\nThe last equality in the definition follows because the expected value of a constant is identical to the constant. In the dartboard example, \\(\\theta\\) is the bullseye and \\(h\\left( \\textbf{Y}\\right)\\) is the distance of the dart from the bullseye. The bias is the expected value of that distance, the average across many repetitions (dart throws).",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#mean-squared-error",
    "href": "biasvariance.html#mean-squared-error",
    "title": "2  Bias Variance Tradeoff",
    "section": "2.3 Mean Squared Error",
    "text": "2.3 Mean Squared Error\nWith these definitions in place, let’s return to the question whether to favor the linear regression or the smoothing spline to predict a new observation at \\(x_0\\)? The model can be written as\n\\[Y = f(x) + \\epsilon\\]\nwhere \\(\\epsilon\\) is a random variable with mean 0 and variance \\(\\sigma^{2}\\), the irreducible variability. The observational model for \\(n\\) observed data points is\n\\[Y_{i} = f\\left( x_{i} \\right) + \\epsilon_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ i = 1,\\ldots,n\\]\nThe \\(Y_{i}\\) are observed unless there are missing values. However, for a new observation this might not be the case. The model for the new observation is no different than the previous model\n\\[Y_{0} = f\\left( x_0 \\right) + \\epsilon\\]\nbut only \\(x_0\\) is known.\nThere are two possible targets for prediction: \\(f\\left( x_{0} \\right)\\) and \\(f\\left( x_{0} \\right) + \\epsilon\\). The former is the expected value of \\(Y_{0}\\): \\[\\text{E}\\left\\lbrack Y_{0} \\right\\rbrack = f\\left( x_0 \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{0} \\right)\\]\nThis is a fixed quantity (a constant), not a random variable. The latter is a random variable. Interestingly, the estimator of both quantities is the same, \\(\\widehat{f}\\left( x_0 \\right)\\). The difference comes into play when we consider the uncertainty associated with estimating \\(f\\left( x_0 \\right)\\) or predicting \\(f\\left( x_0 \\right) + \\epsilon\\)—more on this later.\nWe need a way to express the discrepancy between the estimator and the target that incorporates the estimator’s accuracy and precision—this is the mean-squared error.\n\n\nDefinition: Mean-squared error (MSE)\n\n\nThe mean-squared error of estimator \\(h\\left( \\textbf{Y}\\right)\\) for target \\(\\theta\\) is the expected value of the squared differences between estimator and target\n\\[\n    \\text{MSE}\\left\\lbrack h\\left( \\textbf{Y}\\right);\\ \\theta \\right\\rbrack = \\text{E}\\left\\lbrack \\left( h\\left( \\textbf{Y}\\right) - \\theta \\right)^{2} \\right\\rbrack\n\\]\n\n\nThe mean-squared error is the expected square deviation between the estimator and its target. Expanding the right hand side and arranging terms we can write the MSE as the sum of two components\n\\[\\begin{align*}\n        \\text{MSE}(h(\\textbf{Y});\\theta) &= \\text{E} \\left [ \\left( h(\\textbf{Y}) - \\theta \\right)^2\\right] \\\\\n        &= \\text{E} \\left [h(\\textbf{Y})^2 - 2 h(\\textbf{Y}) \\theta + \\theta^2\\right ]\\\\\n        &= \\text{E} \\left [h(\\textbf{Y})^2 - \\mu^2 + \\mu^2 - 2 h(\\textbf{Y}) \\theta + \\theta^2\\right ]\\\\\n        &= \\text{E} \\left [h(\\textbf{Y})^2 \\right ] - \\mu^2  + \\mu^2 - 2 \\mu \\theta + \\theta^2 \\\\\n        &= \\text{E} \\left [h(\\textbf{Y})^2 \\right ] - \\mu^2  + (\\mu - \\theta)^2 \\\\\n        &= \\text{Var}[h(\\textbf{Y})] + \\text{Bias}(h(\\textbf{Y});\\theta)^2\n\\end{align*}\\]\n\n\n\n\n\n\nTip\n\n\n\nWe use a common trick in the third line of this derivation, to add and subtract the same quantity: \\(\\mu^2\\). This allows completion of the squares that lead to the variance and the squared bias terms.\n\n\nThe mean-squared error decomposes into the variance of the estimator, \\(\\text{Var}[h(\\textbf{Y})]\\), and the squared bias between the estimator and the target it is trying to estimate. The MSE equals the variance only if the estimator is unbiased. The bias enters in squared terms because the variance is measured in squared units and because negative and positive bias discrepancies should not balance out.\nIf we apply the MSE definition to the problem of using estimator \\(\\widehat{f}\\left( x_0 \\right)\\) to predict \\(f\\left( x_0 \\right)\\),\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right)\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2}\\]\nwe see how the variability of the estimator and its squared bias contribute to the overall MSE. Similarly, if the goal is to predict a new observation, rather than its mean, the expression becomes\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) + \\epsilon\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2} + \\sigma^{2}\\]\nYou now see why \\(\\sigma^{2}\\) is called the irreducible error. Even if the estimator \\(\\widehat{f}\\left( x_{0} \\right)\\) would have no variability and be unbiased, the mean-squared error in predicting \\(Y_{0}\\) can never be smaller than \\(\\sigma^{2}\\).\n\n\nExample: \\(k\\)-Nearest Neighbor Regression\n\n\nThe \\(k\\)-nearest neighbor (\\(k\\)-NN for short) regression estimator is a simple estimator of the local structure between a target variable \\(y\\) and an input variable \\(x\\). The value \\(k\\) represents the number of values in the neighborhood of some input \\(x_{0}\\) that are used to predict \\(y\\). The extreme case is \\(k = 1\\), the value of \\(f\\left( x_{0} \\right)\\) is predicted as the \\(y\\)-value of the observation closest to \\(x_{0}\\).\nSuppose our data come from a distribution with mean \\(f(x)\\) and variance \\(\\sigma^{2}\\). The mean-square error decomposition for the \\(k\\)-NN estimator is then\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = }\\frac{\\sigma^{2}}{k}{+ \\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum_{}^{}Y_{(i)} \\right\\rbrack}^{2} + \\sigma^{2}\\]\nwhere \\(y_{(i)}\\) denotes the \\(k\\) observations in the neighborhood of \\(x_{0}\\).\nThe three components of the MSE decomposition are easily identified:\n\n\\(\\sigma^{2}/k\\) is the variance of the estimator, \\(\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack\\). Not surprisingly, it is the variance of the sample mean of \\(k\\) observations drawn at random from a population with variance \\(\\sigma^{2}\\).\n\\(\\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum Y_{(i)} \\right\\rbrack^{2}\\) is the squared bias component of the MSE.\n\\(\\sigma^2\\) is the irreducible error, the variance in the population from which the data are drawn.\n\nWhile we cannot affect the irreducible error \\(\\sigma^{2}\\), we can control the magnitude of the other components through the choice of \\(k\\). The variance contribution will be largest for \\(k = 1\\), when prediction relies on only the observation closest to \\(x_{0}\\). The bias contribution for this 1-NN estimator is \\(\\left\\lbrack f\\left( x_{0} \\right) - Y_{(1)} \\right\\rbrack^{2}\\).\nAs \\(k\\) increases, the variance of the estimator decreases. For a large enough value of \\(k\\), all observations are included in the “neighborhood” and the estimator is equal to \\(\\overline{Y}\\). If \\(f(x)\\) changes with \\(x\\), the nearest neighbor method will then have smallest variance but large bias.\n\n\nIf we want to minimize the mean-squared error, we can strive for estimators with low bias and low variance. If we cannot have both, how do we balance between the bias and variance component of an estimator? That is the bias-variance tradeoff.\nStatisticians resolve the tension with the UMVUE principle. Uniformly minimum-variance unbiased estimation requires to first identify unbiased estimators, those for which \\(\\text{Bias}\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\rbrack = 0\\), and then to select the estimator with the smallest variance among the unbiased estimators. According to UMVUE you will never consider a biased estimator. It is comforting to know that on average the estimator will be on target. This principle would select estimator C in the dartboard example over estimator B because the latter is biased. If you have only one dart left and you need to get as close to the bullseye as possible, would you ask player B or player C to take a shot for the team?\nUMVU estimators are not necessarily minimum mean-squared error estimators. It is possible that a biased estimator has a sharply reduced variance so that the sum of variance and squared bias is smaller than the variance of the best unbiased estimator. If we want to achieve a small mean-square error, then we should consider estimators with some bias and small variance. Resolving the bias-variance tradeoff by eliminating all biased estimators does not lead to the “best” predictive models. Of course, this depends on our definition of “best”.\nIn practice, \\(f\\left( x_{0} \\right)\\) is not known and the bias component \\(\\text{Bias}\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\rbrack\\) cannot be evaluated by computing the difference of expected values. For many modeling techniques we can calculate—or at least estimate— \\(\\text{Var}\\lbrack \\widehat{f}\\left( x_{0} \\right) \\rbrack\\), the variance component of the MSE. Those derivations depend on strong assumptions about distributional properties and the correctness of the model. So, we essentially need to treat the MSE as an unknown quantity. Fortunately, we can estimate it from data.\n\n\nDefinition: Mean-squared prediction error (MSPE)\n\n\nThe mean-squared prediction error (MSPE) is the average squared prediction error in a sample of \\(n\\) observations,\n\\[\\text{MSPE} = \\frac{1}{n}\\sum_{i=1}^n\\left( y_i - \\widehat{f}\\left( x_i \\right) \\right)^{2}\\]\n\n\nTaking the sample average replaces taking formal expectations over the distribution of \\(( Y - \\widehat{f}(x) )^2\\).\nBack to choosing between the regression and spline models. If we denote the two approaches \\(\\widehat{f}_{r}(x)\\) and \\(\\widehat{f}_{s}(x)\\), respectively, selecting the winning model based on the mean-squared prediction error reduces to picking the model with the smaller MSPE:\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{f}}_{r}\\left( x_{i} \\right) \\right)^{2}\\]\nor\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{f}}_{s}\\left( x_{i} \\right) \\right)^{2}\\]\nAs we will see, this is not without problems. These expressions are calculating the MSPE by averaging over the data points used in training the model; we call this the MSPE of the training set or MSETr for short. To identify models that generalize well to new observations, it is recommended to calculate the MSPE across a test set of observations that was not used to fit the model; this is called the MSPE of the test set or the MSETe for short.\nWe will discuss training, test, and validation data sets in more detail below.\nWhether you are working with MSPE in a regression context or MCR in a classification problem, the goal is to develop a model that is neither too complex nor too simple. We want to avoid over- and underfitting the model.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#overfitting-and-underfitting",
    "href": "biasvariance.html#overfitting-and-underfitting",
    "title": "2  Bias Variance Tradeoff",
    "section": "2.4 Overfitting and Underfitting",
    "text": "2.4 Overfitting and Underfitting\nThe preceding discussion might suggest that flexible models such as the smoothing spline have high variability and that rigid models such as the simple linear regression model have large bias. This generalization does not necessarily hold although in practice it often works out this way. The reason for this is not that simple linear regression models are biased—they can be unbiased. The reason why flexible models tend to have high variance and low bias and rigid models tend to have low variance and high bias has to do with overfitting and underfitting.\nAn overfit model follows the observed data \\(Y_{i}\\) too closely and does not capture the mean trend \\(f(x)\\). The overfit model memorizes the training data too much. When you predict a new observation with an overfit model that memory causes high variability. Remember that the variability we are focusing on here is the variability across repetitions of the sample process. Imagine drawing 1,000 sets of \\(n\\) observations, repeating the model training and predicting from each model at the new location \\(x_{0}\\). We now have 1,000 predictions at \\(x_{0}\\). Because the overfit model follows the training data too closely, its predictions will be variable at \\(x_{0}\\).\nAn underfit model, on the other hand, lacks the flexibility to capture the mean trend \\(f(x)\\). Underfit models result, for example, when important predictor variables are not included in the model.\nThe most extreme case of overfitting a model is the saturated model. It perfectly predicts the observed data. Suppose you collect only two pairs of \\((x,y)\\) data: (1,0) and (2,1). A two-parameter straight line model will fit these data perfectly. The straight line has an intercept of –1 and a slope of +1. It passes through the observed points and the mean-squared prediction error is zero.\n\n\n\n\n\n\nFigure 2.6: A straight line model saturates a data set with two $(x,y)$ pairs. The difference between observed values (the dots) and the predicted values (values on the line) is zero at each point. The saturated model has a MSPE of zero.\n\n\n\nSaturated models are not very interesting, they are just a re-parameterization of the data, capturing both signal \\(f(x)\\) and noise \\(\\epsilon\\). A useful model separates the signal from the noise. Saturated models are used behind the scenes of some statistical estimation methods, for example to measure how much of the variability in the data is captured by a model—this type of model metric is known as the deviance. Saturated models are never the end goal of data analytics.\nOn the other extreme lies the constant model; it does not use any input variables. It assumes that the mean of the target variable is the same everywhere:\n\\[Y_{i} = \\mu + \\epsilon_{i}\\]\nThis model, also known as the intercept-only model, is slightly more useful than the saturated model. It is rarely the appropriate model in data science applications; it expresses the signal as a flat line, the least flexible model of all.\nIn our discussion of the model building process during the data science project life cycle we encountered an example of pharmacokinetic data, 500 observations on how a drug is absorbed and eliminated by the body over time (\\(t\\)). The data are replayed in the next figure along with the fit of the constant model. The constant model underpredicts the drug concentration between times \\(t = 3\\) and \\(t = 12\\) and overpredicts everywhere else.\n\n\n\n\n\n\nFigure 2.7: Concentration of a drug in patient’s bodies over time.\n\n\n\nSuppose we draw 1,000 sets of \\(n = 500\\) observations, fit the constant model to each, and predict at the new time \\(t_{0}\\). Because the constant model does not depend on time, we get the same predicted value regardless of the value of \\(t_{0}\\). In each sample of size \\(n\\), the predicted value will be the sample mean, \\(\\overline{y} = \\frac{1}{500}\\sum_{}^{}y_{i}\\). The variability of the 1,000 predictions will be small; it is the variance of the sample mean:\n\\[\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_0 \\right) \\right\\rbrack = \\frac{\\sigma^2}{500}\\]\nIf the true model does depend on \\(t\\)—and the plot of the data suggests this is the case—the bias of the predictions will be large. The mean-squared prediction error is dominated by the squared bias component in this case.\nSomewhere between the two extremes of a hopelessly overfit saturated model and a hopelessly underfit constant model are models that capture the signal \\(f(x)\\) well enough without chasing the noisy signal \\(f(x) + \\epsilon\\) too much. Those models permit a small amount of bias if that results in a reduction of the variance of the predictions.\nTo summarize,\n\nOverfit models do not generalize well because they follow the training data too closely. They tend to have low bias and a large variance.\nUnderfit models do not generalize well because they do not capture the salient trend (signal) in the data. They tend to have high bias and low variance.\nA large mean-squared prediction error can result in either case but is due to a different cause.\nFor a small mean-squared prediction error you need to have small bias and small variance.\nIn practice, zero-bias methods with high variance are rarely the winning approaches. The best MSPE is often achieved by allowing some bias to substantially decrease the variance.\n\nThe danger of overfitting is large when models contain many parameters, and when the number of parameters \\(p\\) is large relative to the sample size \\(n\\). When many attributes (inputs) are available and you throw them all into the model, the result will likely be an overfit model that does not generalize well. It will have a large prediction error. In other words, there is a cost to adding unimportant information to a model. Methods for dealing with such high-dimensional problems play an important role in statistics and machine learning and are discussed in detail in a more advanced section. We mention here briefly:\n\nFeature Selection: Structured approaches that use algorithms to determine which subset of the inputs should be in the model. The decision is binary in that an input is either included or excluded. Also known as variable selection.\nRegularization: Deliberately introducing some bias in the estimation through penalty terms that control the variability of the model parameters which in turn controls the variability of the predictions. The parameters are shrunk toward zero in absolute value compared to an unbiased estimator—regularization is thus also known as shrinkage estimation. The Lasso methods can shrink parameters to zero and thus combines regularization with feature selection. The Ridge regression methods also applies a shrinkage penalty but allows all inputs to contribute.\nEnsemble Methods: Ensemble methods combine multiple methods into an overall, averaged prediction or classification. Ensembles can be homogeneous, where the methods are the same, or heterogeneous. An example of a homogeneous ensemble is a bagged decision tree, where several hundred individual trees are trained independently and the predictions from the trees are averaged to obtain an overall predicted value. Due to averaging, the variance of the ensemble estimator is smaller than any individual estimator. Bagging and boosting are common ensemble methods to reduce variance.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#sec-train-test-validate",
    "href": "biasvariance.html#sec-train-test-validate",
    "title": "2  Bias Variance Tradeoff",
    "section": "2.5 Training, Testing, and Validation",
    "text": "2.5 Training, Testing, and Validation\nTraining, testing, and validation refers to different stages of the model building process and also to different types of data used in the model building process.\n\nTraining Data\nTraining data is the set of \\(n\\) observations used to train the model. The training data is useful to diagnose whether model assumptions are met, for example,\n\ndoes the model adequately describe the mean trend in the (training) data,\nare distributional assumptions such as normality of the model errors met,\nis it reasonable to assume that the data points are uncorrelated (or even independent)\n\nWe can also use the training data after the model fit to detect data points that have a high influence of the analysis—that is, the presence of those points substantially affects an important aspect of the model. And based on the training data we can study the interdependence of the model inputs and whether those relationships affect the model performance negatively.\nThe diagnostic techniques just mentioned rely on\n\nResidual diagnostics\nCase-deletion and influence diagnostics\nCollinearity diagnostics\n\nThese diagnostics are all very helpful, but they do not answer an important question: how well does the model generalize to observations not used in training the model; how well does the model predict new observations? We also need to figure out, given a single training data set, how to select the values for the hyperparameters of the various techniques.\n\n\nDefinition: Hyperparameter\n\n\nA hyperparameter is a variable that controls the overall configuration of a statistical model or machine learning technique. Hyperparameters are sometimes referred to as external parameters, whereas the parameters of the model function (slopes, intercepts, etc.) are called the internal parameters.\n\n\nHyperparameters are not directly derived from the data, they need to be set to values before the model can be trained; their values can greatly impact the performance of the model. The process of determining the values for hyperparameters given a particular data set is called hyperparameter tuning.\nHyperparameters include, for example,\n\nThe number of terms in a polynomial model\nThe smoothing parameters in non-parametric regression models\nThe bandwidth in kernel-based estimation methods such as LOESS, kernel regression, local polynomial regression\nThe shrinkage penalty in Lasso, Ridge regression, smoothing splines\nThe depth of decision trees\nThe number \\(k\\) in \\(k\\)-nearest neighbor methods\nThe convergence rate and other tolerances in numerical optimization\nThe learning rate, number of nodes, and number of layers in neural networks\n\nWe can calculate the MSPE or MCR of the trained model, depending on whether we are dealing with a regression or a classification problem. Doing so for the training data has some serious drawbacks. We have seen earlier that saturated models have no prediction error since they perfectly connect the dots in the data. Trying to minimize the MSPE based on the training data (MSETr) invariably leads to overfit models since you can always drive MSETr toward zero.\n\n\nTest Data\nTo measure the true predictive performance of a model we need to apply the model to a different set of observations; a set that was not used in training the model. This set of observations is called the test data set. With a test data set we can measure how well the model generalizes and we can also use it to select the appropriate amount of flexibility of the model. The following graph shows the general behavior of test and train mean-squared prediction error as a function of model flexibility and complexity.\nThe MSPE of the test data set is on average higher than the MSPE of the training data set. Since these are random variables, it can happen in a particular application that the test error is lower than the training error, but this is rare. The model complexity/flexibility is measured here by the number of inputs in the model. As this number increases, the MSETr decreases toward zero. The MSETe, on the other hand, first decreases, reaches a minimum, and increases again. The MSETe is high for models with few parameters because of bias, it increases with model flexibility past the minimum because of variability. The two contributors to the MSE work at different ends of the spectrum—you find models that balance bias and variance somewhere in-between.\n\n\n\n\n\n\nFigure 2.8: MSETr and MSETe as a function of model flexibility (complexity).\n\n\n\nThe big question is: where do we get the test data?\n\n\nValidation Data\nBefore discussing ways to obtain test data sets, a few words about another type of data set, the validation data. The terms test data and validation data are often used interchangeably, but there is a difference. Test data represents new data that should otherwise be representative of the training data. A test data set drawn at random from the training data set typically satisfies that.\nValidation data can be a separate data set with known properties, for example, a benchmark data set. Such a data set can be used to compare approaches from different model families, for example, a random forest and a neural network. It can be used to measure model performance against known conditions (typical and atypical) to ensure a model works properly.\n\n\nExample: Computer Vision\n\n\nImageNet is a data set of images organized according to the WordNet hierarchy. ImageNet provides an average of 1,000 images for each meaningful concept in WordNet. The data set is used as a benchmark for object categorization algorithms and currently contains over 14 million images that are labeled and annotated by humans.\nThe most used subset of ImageNet data is the Large Scale Visual Recognition Challenge (ILSVRC) data set. It is used to evaluate object classification algorithms since 2010. The data sets for the challenges are themselves broken down into training, test, and validation sets.\nThe IARPA Janus Benchmark (IJB) datasets contain images and videos used in face detection and face recognition challenges. There are several data sets, for example IJB-B consists of 1,845 subjects with human-labeled face bounding boxes, eye & nose location, and metadata such as skin tone and facial hair for 21,798 still images and 55,026 video frames. The collection methodology for the IJB-B data set is documented .\n\n\nTest data tells us how well a model performs, validation data tells us which model is best.\n\n\nExample: Programming Competition\n\n\nSuppose we want to send one student from a group of students to a programming competition. The goal is to win the competition. In training the students encounter problems from past programming competitions.\nStudents that do well during training are not necessarily the best candidates for the competition. We need to find out whether a student does well because they memorized the solution or whether they truly understand how to solve the programming problem. To answer this a validation step is used and a set of new programming problems is presented, specifically designed to test student’s ability to apply general concepts in problem solving. At the end of the validation step we have identified the best student to represent the group at the competition.\nWe are not done, however. Does the best student in the group have a chance in the competition? We now enter the testing phase to answer the question: how well will the best student perform? After administering a real test with new problems, we find out that the student scores above 90%: they are ready for the competition. If, however, we find out that the student scores below 25%, we will not send them to the competition. Instead, we return to the drawing board with a new training procedure and/or a set of new training problems.\n\n\nValidation and test data are often used interchangeably because the test data is often used as the validation data. The questions “which model is best?” and “how well does the model perform?” are answered simultaneously: the best model is the one that achieves the best metric on the test data set. Often that results in choosing the model with the lowest MSETe or MCRTe.\n\n\nHold-out Sample\nLet’s return to the important question: where do we find the test data set?\nMaybe you just happen to have a separate set of data lying around that is just like the training data, but you did not use it. Well, that is highly unlikely.\nTypically, we use the data collected, generated, or available for the study to carve out observations for training and testing. This is called a hold-out sample, a subset of the observations is held back for testing and validation. If we start with \\(n\\) observations, we use \\(n - m\\) observation to train the model (the training data set), and \\(m\\) observations to test/validate the model.\nIn Python you can create this train:test split with the train_test_split() function in sklearn. The following statements load the fitness data from DuckDB into a Pandas DataFrame and split it into two frames of 15 and 16 observations.\n\nimport pandas as pd\nimport duckdb\n\ncon = duckdb.connect(database=\"ads.ddb\")\nfit = con.sql(\"SELECT * FROM fitness\").df()\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(fit,random_state=235,train_size=0.5)\n\nThe random_state= parameter sets the seed for the random number generator. By setting this to a non-zero integer, the random number generator starts to produce numbers with that seed value. This makes the selection reproducible, subsequent runs of the program will produce identical—yet random—results. The train_size= parameter specifies the proportion of observations in the training set—if the value is between 0 and 1—or the number of observations in the training set—if the value is an integer &gt; 1.\n\ntrain.shape\n\n(15, 7)\n\n\n\ntrain.describe()\n\n             Age     Weight     Oxygen  ...  RestPulse   RunPulse    MaxPulse\ncount  15.000000  15.000000  15.000000  ...  15.000000   15.00000   15.000000\nmean   49.666667  75.539333  47.693067  ...  52.733333  171.00000  174.133333\nstd     4.654747   8.076112   4.516180  ...   7.731814   10.96097    9.210760\nmin    40.000000  59.080000  39.203000  ...  40.000000  148.00000  155.000000\n25%    48.000000  70.760000  45.215500  ...  48.000000  166.00000  169.000000\n50%    51.000000  76.320000  46.672000  ...  51.000000  170.00000  172.000000\n75%    53.000000  80.400000  49.772000  ...  58.500000  178.00000  180.500000\nmax    57.000000  91.630000  59.571000  ...  67.000000  186.00000  188.000000\n\n[8 rows x 7 columns]\n\n\n\ntest.shape\n\n(16, 7)\n\n\n\ntest.describe()\n\n             Age     Weight     Oxygen  ...  RestPulse    RunPulse    MaxPulse\ncount  16.000000  16.000000  16.000000  ...  16.000000   16.000000   16.000000\nmean   45.812500  79.230625  47.078375  ...  54.125000  168.375000  173.437500\nstd     5.140931   8.415590   6.125977  ...   7.701731    9.721968    9.408994\nmin    38.000000  61.240000  37.388000  ...  45.000000  146.000000  155.000000\n25%    43.750000  73.285000  43.665750  ...  48.000000  162.000000  167.500000\n50%    44.500000  80.170000  47.023500  ...  53.500000  169.000000  174.000000\n75%    48.500000  86.295000  50.040750  ...  59.000000  174.500000  180.000000\nmax    57.000000  91.630000  60.055000  ...  70.000000  186.000000  192.000000\n\n[8 rows x 7 columns]\n\n\nThe two data sets have very similar properties as judged by the descriptive statistics. If the goal is to develop a model that can predict the difficult to measure oxygen intake from easy to measure attributes such as age, weight, and pulse, then we would use the 15 observations in the train frame to fit the model and the 16 observations in the test frame to evaluate the model.\nIf we cull the test data from the overall data, how should we determine an appropriate size for the test data? The previous example used a 50:50 split, would it have mattered if we had taken a 20:80 or a 90:10 split? For the two data sets to serve their respective functions, you need enough observations in the training data set to fit the model well enough so it can be tested, and you need enough observations in the test data set to produce a stable estimate of MSETe. In practice splits that allocate between 50 and 90% of the observations to the training data set are common.\nWith small training proportions you run the risk that the model cannot be fit and/or that the data does not support the intended model. For example, with a 10:90 train:test split in the fitness example, the training data contains only 3 observations and evaluating the effect of all input variables on oxygen intake is not possible—the model is saturated after three inputs are in the model. With categorical inputs, you need to make sure that the training and test data sets contain all the categories. For example, if you categorize age into four age groups and only three groups are present in the training data after the split, the resulting model no longer applies to a population with four age groups.\nFrom this discussion we can glean the general advantages and disadvantages of hold-out test samples.\n\nAdvantages and disadvantages of hold-out samples generated by random train:test splits.\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nEasy to do\nInvolves a random selection; results change depending on which observations selected\n\n\nNo separate test data set needed\nPotentially large variability from run to run, especially for noisy data\n\n\nA general method that can be applied regardless of how model performance is measured\nMust decide how large to make the training (test) set\n\n\nReproducible if fixing random number seed\nAn observation is used either for testing or for training\n\n\n\nTends to overestimate the test error compared to cross-validation methods\n\n\n\nThe last two disadvantages in the table weigh heavily. Since we cannot rely on the training error for model selection, we are sacrificing observations by excluding them from training. At least we expect then a good estimate of the test error. The reason for overestimating the true test error with a train:test hold-out sample is that models tend to perform worse when trained on fewer observations. Reducing the size of the training data set results in less precise parameter estimates which in turn increases the variability of predictions.\nTo compare the variability of the hold-out sample method with other techniques, we draw on the Auto data set from ISLR2 (An Introduction to Statistical Learning by James et al.). The data comprise information on fuel mileage and other vehicle attributes of 392 automobiles. Suppose we want to model mileage as a function of horsepower. The next figure shows the raw data and fits of a linear and quadratic model\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_{1}\\text{hp}_{i} + \\epsilon_{i}\\]\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_{1}\\text{hp}_{i} + {\\beta_{2}\\text{hp}_{i}^{2} + \\epsilon}_{i}\\]\n\n\n\n\n\n\nFigure 2.9: Simple linear and quadratic polynomial fit for miles per gallon versus horsepower in Auto data set.\n\n\n\nA simple linear regression—the red line in the figure—does not seem appropriate. The model does not pick up the curvature in the underlying trend. A quadratic model seems more appropriate. Can this be quantified? What about a cubic model\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_{1}\\text{hp}_{i} + {\\beta_{2}\\text{hp}_{i}^{2} + \\beta_{3}\\text{hp}_{i}^{3} + \\epsilon}_{i}\\]\nFigure 2.10 shows the hold-out test errors for all polynomial models up to degree 10. The simple linear regression (SLR) model has degree 1 and is shown on the left. The test error is large for the SLR model and for the 10-degree polynomial. The former is biased as can be seen from the previous graph. The latter is too wiggly and leads to a poor test error because of high variability. The test error is minimized for the quadratic model but we note that the test error is also low for degrees 7—9.\n\n\n\n\n\n\nFigure 2.10: Hold-out test errors for polynomial models from first to tenth degree. The horizontal line marks the minimum, achieved at degree 2.\n\n\n\nBased on this result we would probably choose the second-degree polynomial. To what extent is this decision the result of having selected the specific 196 observations in the 50:50 split? We can evaluate this by repeating the sampling process a few more times. The next graph shows the results of 9 other 50:50 random splits.\nThe variability in the results is considerable. Most replications would select a second-degree polynomial as the model with the lowest MSETe, but several replications achieve a smallest MSETe for much higher degree polynomials (5th degree, 7th degree, etc.).\n\n\n\n\n\n\nFigure 2.11: Test errors in ten hold-out samples, 50:50 splits. The errors from the previous graph are shown in red.\n\n\n\nHaving spent time, energy, resources, money to build a great data set, it seems wasteful to use some observations only for training and the others only for testing. Is there a way in which we can use all observation for training and testing and still get a good estimate (maybe even a better estimate) of the test error?\nHow about the following proposal:\n\nSplit the data 50:50 into sets \\(t_1\\) and \\(t_2\\)\nUse \\(t_1\\) as the training data set and determine the mean-squared prediction error from \\(t_{2}\\), call this MSETe(\\(t_{2}\\))\nReverse the roles of \\(t_1\\) and \\(t_2\\), using \\(t_2\\) to train the model and \\(t_1\\) to compute the test error MSETe(\\(t_1\\))\nCompute the overall test error as the average MSETe = 0.5 x (MSETe(\\(t_1\\)) + MSETe(\\(t_2\\)))\n\nEach observation is used once for training and once for testing. Because of averaging, the combined estimate of test error is more reliable than the individual test errors.\nThis proposal describes a special case of cross-validation, namely 2-fold cross-validation.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#sec-cross-validation",
    "href": "biasvariance.html#sec-cross-validation",
    "title": "2  Bias Variance Tradeoff",
    "section": "2.6 Cross-validation",
    "text": "2.6 Cross-validation\nCross-validation is a general method to measure the performance of a model. It is commonly used for predictive models to evaluate how well a model generalizes to new observations, but it can also be used to, for example, select hyperparameters. Cross-validation extends the concept of the hold-out sample to address the drawbacks of train:test splits. It is also general in that you are not limited to MSE or MCR as performance measurements. So, first, a few words about loss functions.\n\nLoss Functions\n\n\nDefinition: Loss function\n\n\nA loss function or cost function maps an event to a real number that reflects some loss or cost incurred from the event.\nIn data analytics, loss functions measure the discrepancy between observed and predicted values and the losses are typically referred to as errors.\n\n\nThe following table displays common loss functions in data science.\n\nLoss functions common in data science applications. \\(y\\) and \\(\\widehat{y}\\) denote observed and predicted value, respectively.\\(\\widehat{p}_j\\) denotes the sample proportion in category \\(j\\) of a classification problem with \\(k\\) categories.\n\n\n\n\n\n\n\nLoss Function\nExpression\nApplication Example\n\n\n\n\nSquared Error\n\\(\\left( y - \\widehat{y} \\right)^{2}\\)\nRegression with continuous response\n\n\nZero-one (0—1)\n\\(I\\left( y \\neq \\widehat{y} \\right)\\)\nClassification\n\n\nAbsolute Value\n\\(\\left| y - \\widehat{y} \\right|\\)\nRobust regression\n\n\nMisclassification\n\\(1 - \\max_{j}{\\widehat{p}}_{j}\\)\nPruning of decision trees\n\n\nGini Index\n\\(\\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\left( 1 - {\\widehat{p}}_{j} \\right)}\\)\nGrowing of decision trees, neural networks\n\n\nCross-entropy (deviance)\n\\(- 2\\sum_{j = 1}^{k}{{n_{j}\\log}{\\widehat{p}}_{j}}\\)\nGrowing of decision trees, neural networks\n\n\nEntropy\n\\(- \\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\log{\\widehat{p}}_{j}}\\)\nGrowing of decision trees\n\n\n\nSquared error and zero-one loss dominate data science work in regression and classification problems. For specific methods you will find additional loss functions used to optimize a particular aspect of the model, for example, growing and pruning of decision trees.\nSuppose the loss associated with an observation is denoted \\(\\mathcal{l}_{i}\\). Cross-validation estimates the average loss for each of \\(k\\) sets of observations and averages the \\(k\\) estimates into an overall cross-validation estimate of the loss.\nSuppose we create two random sets of (near) equal size for the 31 observations in the fitness data set; \\(k = 2\\). The sets will have \\(n_1 = 15\\) and \\(n_2 = 16\\) observations. This leads to one cross-validation estimate of the loss function for each set:\n\\[CV_1\\left( \\mathcal{l} \\right) = \\frac{1}{n_1}\\sum_{i = 1}^{n_1}\\mathcal{l}_{i}\\]\n\\[CV_2\\left( \\mathcal{l} \\right) = \\frac{1}{n_2}\\sum_{i = 1}^{n_1}\\mathcal{l}_i\\]\nThe overall cross-validation loss is the average of the two:\n\\[CV\\left( \\mathcal{l} \\right) = \\frac{1}{2}\\left( CV_{1}\\left( \\mathcal{l} \\right) + CV_{2}\\left( \\mathcal{l} \\right) \\right)\\]\nThis is a special case of \\(k\\)-fold cross-validation; the sets are referred to as folds. The other special case is leave-one-out cross-validation.\n\n\n\\(K\\)-fold Cross-validation\nThe set of \\(n\\) observations is divided randomly into \\(k\\) groups of (approximately) equal size. The groups are called the \\(k\\) folds. The model is fit \\(k\\) times, holding out a different fold each time. After computing the loss in each fold\n\\[{CV}_{j}\\left( \\mathcal{l} \\right) = \\frac{1}{n_{j}}\\sum_{i = 1}^{n_{j}}\\mathcal{l}_{i}\\]\nthe overall loss is calculated as the average\n\\[CV\\left( \\mathcal{l} \\right) = \\frac{1}{k}\\sum_{j = 1}^{k}{{CV}_{j}\\left( \\mathcal{l} \\right)}\\]\nFigure 2.12 shows 5-fold cross-validation for \\(n = 100\\) observations. The observations are randomly divided into 5 groups of 20 observations each. The model is trained five times. The first time around, observations in fold 1 serve as the test data set, folds 2—5 serve as the training data set. The second time around, fold 2 serves as the test data set and folds 1, 3, 4, and 5 are the training data set; and so forth. Each time, the average loss is calculated for the 20 observations not included in training. At the end, five average cross-validation losses are averaged to calculate the overall loss.\n\n\n\n\n\n\nFigure 2.12: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\n\nAdvantages and disadvantages of \\(k\\)-fold cross-validation.\n\n\n\n\n\n\n\nAdvantages\nDisadvantgages\n\n\n\n\n\nNot as variable as the train:test hold-out sample\nStill has a random element due to randomly splitting the data into \\(k\\) sets\n\n\n\nLess bias in test error than train:test hold-out sample\nCan be computationally intensive if the model must be fit \\(k\\) times\n\n\n\nNot as computationally intensive as leave-one-out cross-validation (see below)\nMust decide on the number of folds\n\n\n\nEvery observation is used for training (\\(k - 1\\) times) and testing (once)\n\n\n\n\nReproducible if fixing random number seed\n\n\n\n\nA general method that can be applied regardless of how model performance is measured\n\n\n\n\n\nThe most common values for \\(k\\) found in practice are 5, 10, and \\(n\\). \\(k = n\\) is a special case, called leave-one-out cross-validation; see below. Values of 5 and 10 have shown to lead to good estimates of loss while limiting the variability of the results. The averaging of the losses from the folds has a powerful effect of stabilizing the results.\nFor the Auto data set, the following figures show the results of repeating 5-fold and 10-fold cross-validation ten times. The results vary considerably less than the ten repetitions of the 50:50 hold-out sample in Figure 2.11.\n\n\n\n\n\n\nFigure 2.13: Ten repetitions of 5-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\n\n\n\n\n\n\nFigure 2.14: Ten repetitions of 10-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\nAlso, the 10-fold cross-validation shows less variability than the 5-fold CV. This is the effect of averaging 10 quantities rather than 5. In other words, the effect of averaging the results from the folds is larger than the averaging of observations within the folds. But if training a model is computationally intensive, 5-fold cross-validation is a good solution.\n\n\nLeave-One-Out Cross-validation\nAbbreviated LOOCV, this method takes the random element out of selecting observations into the folds. Instead, each observation is used once as a test set of size 1 and the model is fit to the remaining \\(n - 1\\) observations. The observation is put back and the next observation is removed from the training set.\nLOOCV thus estimates the model \\(n\\) times, each time removing one of the observations. It is a special case of \\(k\\)-fold cross-validation where \\(k = n\\).\nA pseudo-algorithm for LOOCV is as follows:\n\nStep 0: Set \\(i = 1\\)\nStep 1: Set the index of the hold-out observation to \\(i\\)\nStep 2. Remove observation \\(i\\) and fit the model to the remaining \\(n - 1\\) observations\nStep 3. Compute the loss \\(\\mathcal{l}_i\\) for the held-out observation\nStep 4. Put the observation back into the data. If \\(i = n\\), go to Step 5. Otherwise, increment \\(i\\) and return to Step 1.\nStep 5. Compute the LOOCV loss as the average of the \\(n\\) losses: \\(CV\\left( \\mathcal{l} \\right) = \\frac{1}{n}\\sum_{i} mathcal{l}_i\\)\n\n\nAdvantages and disadvantage of leave-one-out cross-validation.\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nNo randomness involved. Identical results upon repetition.\nCan become computationally intensive if fitting a model is expensive and no closed-form expressions (or approximations) are available to compute the loss per observation based on a single fit\n\n\nEvery observation is used in training (\\(n - 1\\) times) and in testing (once)\n\n\n\nA general method that can be applied to any loss function and model\n\n\n\nGood estimate of test error\n\n\n\n\nThe results of LOOCV for the Auto data set are shown in Figure 2.15. LOOCV selects the seventh-degree polynomial.\n\n\n\n\n\n\nFigure 2.15: Leave-one-out cross-validation for polynomials in the Auto data set.\n\n\n\nFortunately, the leave-one-out cross-validation error can be computed for some model classes without fitting the model \\(n\\) times. For linear regression models, formulas exist to compute the LOO prediction error from information available after just training the model once on all observations. Wait, what?\nSuppose we are predicting the target value of the \\(i\\)th observation in the LOO step when that observation is not in the training set and denote this predicted value as \\(\\widehat{y}_{-i}\\). The LOO cross-validation error using a squared error loss function is then\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{y}}_{- i} \\right)^{2}\\]\nThe sum in this expression is called the PRESS statistic (for prediction sum of squares). The interesting result is that\\({\\widehat{\\ y}}_{- i}\\) can be calculated as\n\\[y_i - \\widehat{y}_{-i} = \\frac{y_i - \\widehat{y}_i}{1 - h_{ii}}\\]\nwhere \\(h_{ii}\\) is called the leverage of the \\(i\\)th observation. We will discuss the leverage in more detail in the context of linear model diagnostics. At this point it is sufficient to note that the leverage measures how unusual an observation is with respect to the input variables of the model and that \\(0 &lt; h_{ii} &lt; 1\\).\nThe term in the numerator is the regular residual for \\(y_{i}\\). In other words, we can calculate the leave-one-out prediction error from the difference between observed and predicted values in the full training data by adjusting for the leverage. Since \\(0 &lt; h_{ii} &lt; 1\\), it follows that\n\\[y_i - \\widehat{y}_{-i} &gt; y_i - \\widehat{y}_i\\]\nPredicting an observation that was not used in training the model cannot be more precise than predicting the observation if it is part of the training set.\n\n\n\nFigure 2.5: Accuracy and precision—the dart board bullseye metaphor.\nFigure 2.12: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "linalg.html",
    "href": "linalg.html",
    "title": "3  Linear Algebra Review",
    "section": "",
    "text": "3.1 Basics\nCommand of linear algebra is essential in data science, models and estimators are often expressed in terms of tensors, matrices, and vectors. Using scalar-based arithmetic becomes tedious very quickly as models become more complex. For example, the simple linear regression model and a straight line through the intercept model can be written as\n\\[Y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\\]\n\\[Y_{i} = \\beta_{1}x_{i} + \\epsilon_{i}\\]\nUsing scalar algebra, the estimates of the slope are quite different:\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{\\left( y_{i} - \\overline{y} \\right)\\left( x_{i} - \\overline{x} \\right)} \\right)}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{y_{i}x_{i}} \\right)}{\\sum_{i = 1}^{n}x_{i}^{2}}\\]\nThe formulas get messier as we add another input variable to the model. Using matrix—vector notation, the estimator of all the regression coefficients takes the same form, regardless of the size of the model:\n\\[\\widehat{\\boldsymbol{\\beta}} = \\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\]\nA scalar is a single real number, a vector is an array of scalars arranged in a single column (a column vector) or a row (a row vector). A matrix is a two-dimensional array of scalars, a tensor is a multi-dimensional array.\nThe order of a vector or matrix is specified as (rows x columns) and is sometimes used as a subscript for clarity. For example,\\(\\textbf{A}_{(3 \\times 5)}\\) denotes a matrix with 3 rows and 5 columns. It can be viewed as a concatenation} of five \\((3 \\times 1)\\) column vectors:\n\\[\\textbf{A}_{(3 \\times 5)}=\\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n1 \\\\\n1\n\\end{matrix} & \\begin{matrix}\n9.0 \\\\\n3.2 \\\\\n4.1\n\\end{matrix} & \\begin{matrix}\n\\begin{matrix}\n6.2 \\\\\n1.4 \\\\\n- 0.6\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n0 \\\\\n0\n\\end{matrix} & \\begin{matrix}\n0 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\n\\(\\textbf{a}_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{2} = \\begin{bmatrix} 9.0 \\\\ 3.2 \\\\ 4.1 \\end{bmatrix}\\ \\ \\ \\ \\textbf{a}_{3} = \\begin{bmatrix} 6.2 \\\\ 1.4 \\\\ - 0.6 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{4} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{5} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nA matrix with as many rows as columns is called a square matrix.\nBold symbols are common, lowercase for vectors and uppercase for matrices, but there are some exceptions. When dealing with vectors of random variables, bold uppercase notation is used for a vector of random variables and bold lowercase notation is used for a vector of the realized values. For example, if \\(Y_{1},\\cdots,Y_{n}\\) is a random sample of size \\(n\\), the vector of random variables is\n\\[\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\nand the vector of realized values is\n\\[\\textbf{y}_{(n \\times 1)} = \\begin{bmatrix}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\\]\nThe difference is significant because \\(\\textbf{Y}\\) is a random variable and \\(\\textbf{y}\\) is a vector of constants. \\(\\textbf{Y}\\) has a multi-variate distribution with mean and variance, \\(\\textbf{y}\\) is just a vector of numbers.\nWe follow the convention that all vectors are column vectors, so that \\(\\textbf{y}_{(n)}\\) serves as a shorthand for \\(\\textbf{y}_{(n \\times 1)}\\).\nThe typical element of a matrix is written as a scalar with subscripts that refer to rows and columns. For example, the statement\n\\[\\textbf{A}= \\left\\lbrack a_{ij} \\right\\rbrack\\]\nsays that matrix \\(\\textbf{A}\\) consists of the scalars \\(a_{ij}\\); for example, \\(a_{23}\\) is the scalar in row 2, column 3 of the matrix.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#special-matrices",
    "href": "linalg.html#special-matrices",
    "title": "3  Linear Algebra Review",
    "section": "3.2 Special Matrices",
    "text": "3.2 Special Matrices\nA few special matrices, common in statistics and machine learning are\n\n\\(\\textbf{1}_{n} = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\), the unit vector of size \\(n\\); all its elements are 1.\n\\(\\textbf{0}_{n} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), the zero vector of size \\(n\\); all its elements are 0.\n\\(\\textbf{0}_{(n \\times k)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ \\vdots & \\cdots & \\vdots \\\\ 0 & 0 & 0 \\end{bmatrix}\\), the zero matrix of order \\((n \\times k)\\). All its elements are 0.\n\\(\\textbf{J}_{(n \\times k)} = \\begin{bmatrix} 1 & 1 & 1 \\\\ \\vdots & \\cdots & \\vdots \\\\ 1 & 1 & 1 \\end{bmatrix}\\), the unit matrix of size \\((n \\times k)\\). All its elements are 1.\n\\(\\textbf{I}_{n} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\ddots & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), the identity matrix of size \\((n \\times n)\\) with 1s on the diagonal and 0s elsewhere.\n\nIf the order of these matrices is obvious from the context, the subscripts tend to be omitted.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#basic-operations-on-matrices-and-vectors",
    "href": "linalg.html#basic-operations-on-matrices-and-vectors",
    "title": "3  Linear Algebra Review",
    "section": "3.3 Basic Operations on Matrices and Vectors",
    "text": "3.3 Basic Operations on Matrices and Vectors\nThe basic operations on matrices and vectors are addition, subtraction, multiplication, transposition, and inversion. These are standard operations in manipulating matrix and vector equations. Decompositions such as Cholesky roots, eigenvalue and singular value decompositions are more advanced operations that are important in solving estimation problems in statistics.\n\nTranspose\nThe transpose of a matrix is obtained by exchanging rows and columns. If \\(a_{ij}\\) is the element in row \\(i\\), column \\(j\\) of matrix \\(\\textbf{A}\\), the transpose of \\(\\textbf{A}\\), denoted \\(\\textbf{A}^\\prime\\), has typical element \\(a_{ji}\\). In case of the \\((3\\  \\times 5)\\) matrix shown previously, its transpose is\n\\[\\textbf{A}^\\prime_{(5 \\times 3)} = \\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n9.0 \\\\\n\\begin{matrix}\n6.2 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n3.2 \\\\\n\\begin{matrix}\n1.4 \\\\\n0 \\\\\n1\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n4.1 \\\\\n\\begin{matrix}\n- 0.6 \\\\\n0 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\nThe transpose of a column vector is a row vector:\n\\[\\textbf{a}^{\\prime} = \\begin{bmatrix}\na_{1} \\\\\n\\vdots \\\\\na_{n}\n\\end{bmatrix}^\\prime = \\left\\lbrack a_{1},\\cdots,a_{n} \\right\\rbrack\\]\nTransposing a transpose produces the original matrix, \\(\\left( \\textbf{A}^{\\prime} \\right)^{\\prime}\\ = \\textbf{A}\\).\nA matrix is symmetric if it is equal to its transpose, \\(\\textbf{A}^\\prime = \\textbf{A}\\). Symmetric matrices are square matrices (same numbers of rows and columns). The matrices \\(\\textbf{A}^\\prime\\textbf{A}\\) and \\(\\textbf{A}\\textbf{A}^\\prime\\) are always symmetric. A symmetric matrix whose off-diagonal elements are zero is called a diagonal matrix.\n\n\nAddition and Subtraction\nThe sum (difference) of two matrices is the matrix of the elementwise sums (differences) of their elements. These operations require that the matrices being summed or subtracted have the same order:\n\\[\\textbf{A}_{(n \\times k)} + \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} + b_{ij} \\right\\rbrack\\]\n\\[\\textbf{A}_{(n \\times k)} - \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} - b_{ij} \\right\\rbrack\\]\nSuppose, for example, that \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(\\textbf{B}=\\begin{bmatrix} - 1 & - 2 & - 3 \\\\ - 4 & - 5 & - 6 \\end{bmatrix}\\). Then,\n\\[\\textbf{A}+ \\textbf{B}= \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\]\n\\[\\textbf{A}- \\textbf{B}= \\begin{bmatrix}\n2 & 4 & 6 \\\\\n8 & 10 & 12\n\\end{bmatrix}\\]\nSince addition (subtraction) are elementwise operations, they can be combined with transposition:\n\\[\\left( \\textbf{A}+ \\textbf{B}\\right)^\\prime = \\textbf{A}^{\\prime} + \\textbf{B}^{\\prime}\\]\n\n\nMultiplication\nTwo matrices conform for addition (subtraction) if they have the same order, that is, the same number of rows and columns. Multiplication of matrices requires a different type of conformity; two matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) can be multiplied as \\(\\text{AB}\\) (or \\(\\textbf{A}\\text{×}\\textbf{B}\\)), if the number of columns in \\(\\textbf{A}\\) equals the number of columns in \\(\\textbf{B}\\). We say that in the product \\(\\textbf{A}\\text{×}\\textbf{B}\\), \\(\\textbf{A}\\) is post-multiplied by \\(\\textbf{B}\\) or that \\(\\textbf{A}\\) is multiplied into \\(\\textbf{B}\\). The result of multiplying a \\((n \\times k)\\) matrix into a \\((k \\times p)\\) matrix is a \\((n \\times p)\\) matrix.\nBefore examining the typical elements in the result of multiplication, let’s look at a special case, the inner product of two \\((k \\times 1)\\) vectors \\(\\textbf{A}\\) and \\(\\textbf{B}\\), also called the dot product or the scalar product, is the result of multiplying the transpose of \\(\\textbf{A}\\) into \\(\\textbf{B}\\), a scalar value\n\\[\\textbf{A}^\\prime\\textbf{B}= \\left\\lbrack a_{1}, \\cdots,a_{k} \\right\\rbrack\\begin{bmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{k}\n\\end{bmatrix} = a_{1}b_{1} + \\cdots a_{k}b_{k} = \\sum_{i = 1}^{k}{a_{i}b_{i}}\\]\nThe square root of the dot product of a vector with itself is the Euclidean \\({(L}_{2})\\) norm of the vector,\n\\[\\left| \\left| \\textbf{a}\\right| \\right| = \\sqrt{\\textbf{a}^\\prime\\textbf{a}} = \\sum_{i = 1}^{k}a_{i}^{2}\\]\nThe \\(L_{2}\\) norm plays an important role as a loss function in statistical models. The vector for which the norm is calculated is then often a vector of model errors.\nNow let’s return to the problem of multiplying the \\((n \\times k)\\) matrix \\(\\textbf{A}\\) into the \\((k \\times p)\\) matrix \\(\\textbf{B}\\) and introduce one more piece of notation: the \\(i\\)th row of \\(\\textbf{A}\\) is denoted \\(\\mathbf{\\alpha}_{i}\\) and the \\(j\\)th column of \\(\\textbf{B}\\) is denoted \\(\\textbf{B}_{j}\\). Now we can finally write the product \\(\\textbf{A}\\text{×}\\textbf{B}\\) as a matrix whose typical element is the inner product of \\(\\mathbf{\\alpha}_{i}\\) and \\(\\textbf{B}_{j}\\):\n\\[\\textbf{A}_{(n \\times k)} \\times \\textbf{B}_{(k \\times p)} = \\left\\lbrack \\boldsymbol{\\alpha}_{i}^\\prime\\ \\textbf{b}_{j} \\right\\rbrack_{(n \\times p)}\\ \\]\nAs an example, let \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 0 \\\\ 3 & 1 & - 3 \\\\ 4 & 1 & 2 \\end{bmatrix}\\) and \\(\\textbf{B}= \\begin{bmatrix} 1 & 0 \\\\ 2 & 3 \\\\ 2 & 1 \\end{bmatrix}\\). The product \\(\\textbf{A}\\times\\textbf{B}\\) is a \\((3 \\times 2)\\) matrix with elements\n\\[\\textbf{A}\\times\\textbf{B}= \\begin{bmatrix}\n1 \\times 1 + 2 \\times 2 + 0 \\times 2 & 1 \\times 0 + 2 \\times 3 + 0 \\times 1 \\\\\n3 \\times 1 + 1 \\times 2 - 3 \\times 2 & 3 \\times 0 + 1 \\times 3 - 3 \\times 1 \\\\\n4 \\times 1 + 1 \\times 2 + 2 \\times 2 & 4 \\times 0 + 1 \\times 3 + 2 \\times 1\n\\end{bmatrix} = \\begin{bmatrix}\n5 & 6 \\\\\n- 1 & 0 \\\\\n10 & 5\n\\end{bmatrix}\\]\nHere are a few helpful rules for matrix multiplication:\n\n\\(c\\textbf{A}= \\left\\lbrack ca_{ij} \\right\\rbrack\\)\n\\(c\\left( \\textbf{A}+ \\textbf{B}\\right) = c\\textbf{A}+ c\\textbf{B}\\)\n\\(\\textbf{C}\\left( \\textbf{A}+ \\textbf{B}\\right) = \\textbf{C}\\textbf{A}+ \\textbf{C}\\textbf{B}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)\\textbf{C}= \\textbf{A}(\\textbf{B}\\textbf{C})\\)\n\\(\\left( \\textbf{A}+ \\textbf{B}\\right)\\left( \\textbf{C}+ \\mathbf{D} \\right) = \\textbf{A}\\textbf{C}+ \\textbf{A}\\mathbf{D} + \\mathbf{BC} + \\mathbf{BD}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^\\prime = \\textbf{B}^\\prime\\textbf{A}^\\prime\\)\n\\(\\left( c\\textbf{A}\\right)^\\prime = c\\textbf{A}^\\prime\\)\n\n\n\nInversion and Rank\nIn scalar algebra, division and multiplication are inverse operations, dividing a non-zero scalar by itself yields the multiplicative identity: \\(\\frac{a}{a} = 1\\). What is the equivalent of this operation for matrices? First, inversion of a matrix does not reduce it to a scalar, the multiplicative identity for matrices is the identity matrix \\(\\textbf{I}\\), a diagonal matrix with 1s on the diagonal. Second, the inversion is only defined for square matrices. If \\(\\textbf{A}\\) is an \\((n \\times n)\\) matrix, the matrix \\(\\textbf{B}\\) for which\n\\[\\textbf{A}\\textbf{B}= \\textbf{I}\\]\nis called the inverse of \\(\\textbf{A}\\), denoted as \\(\\textbf{A}^{- 1}\\).\nInverse matrices do not have to exist, even for square matrices. If \\(\\textbf{A}\\) has an inverse matrix, then \\(\\textbf{A}\\) is called a non-singular matrix. In that case, \\(\\textbf{A}^{- 1}\\textbf{A}= \\textbf{A}\\textbf{A}^{- 1} = \\text{I}\\).\nFor the inverse of a square matrix to exist, for the matrix to be non-singular, the matrix must be of full rank. The rank of a matrix, denoted \\(r(\\textbf{A})\\), is the number of its linearly independent columns. What does that mean? Suppose we are dealing with a \\((n \\times k)\\) matrix \\(\\textbf{B}\\) and its column vectors are \\(\\textbf{B}_{1},\\cdots,\\textbf{B}_{k}\\). A linear combination of the columns of \\(\\textbf{B}\\) is\n\\[c_{1}\\textbf{b}_{1} + c_{2}\\textbf{b}_{2} + \\cdots + c_{k}\\textbf{b}_{k} = q\\]\nIf you can find a set of scalars \\(c_{1},\\cdots,c_{k}\\) such that \\(q = 0\\), then the columns of \\(\\textbf{B}\\) are linearly dependent. If the only set of scalars that yields \\(q = 0\\) is\n\\[c_{1} = c_{2} = \\cdots = c_{k} = 0\\]\nthen the columns of \\(\\textbf{B}\\) are not linearly dependent and the rank of \\(\\textbf{B}\\) is \\(k\\).\nHere are a few more useful results about the rank of a matrix:\n\n\\(r\\left( \\textbf{A}\\right) = r\\left( \\textbf{A}^\\prime \\right) = r\\left( \\textbf{A}^\\prime\\textbf{A}\\right) = r\\left( \\textbf{A}\\textbf{A}^{\\prime} \\right)\\)\n\\(r\\left( \\textbf{A}\\textbf{B}\\right) \\leq \\min\\left\\{ r\\left( \\textbf{A}\\right),r\\left( \\textbf{B}\\right) \\right\\}\\)\n\\(r\\left( \\textbf{A}+ \\textbf{B}\\right) \\leq r\\left( \\textbf{A}\\right) + r(\\textbf{B})\\)\n\nThe first two results are particularly important in statistical models. In models with linear structures, it is common to collect the \\(p\\) input variables in a linear model, including the intercept as a column of ones, into a matrix \\(\\textbf{X}_{(n\\  \\times p + 1)}\\):\n\\[\\textbf{X}_{(n\\  \\times p + 1)} = \\begin{bmatrix}\n1 & x_{11} & \\begin{matrix}\n\\cdots & x_{1p}\n\\end{matrix} \\\\\n\\vdots & \\vdots & \\begin{matrix}\n\\ddots & \\vdots\n\\end{matrix} \\\\\n1 & x_{n1} & \\begin{matrix}\n\\cdots & x_{np}\n\\end{matrix}\n\\end{bmatrix}\\]\nSuppose we want to solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\) for \\(\\textbf{c}\\). Start by pre-multiplying both sides of the equation with the transpose of \\(\\textbf{X}\\):\n\\[\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^{\\prime}\\textbf{X}\\textbf{c}\\]\nIf we had an inverse of \\(\\textbf{X}^\\prime\\textbf{X}\\), then we can now pre-multiply both sides with that inverse and isolate \\(\\text{c}\\):\n\\[{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{\\mathbf{- 1}}\\textbf{X}}^{\\prime}\\textbf{Y}= \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{X}\\textbf{c}= \\textbf{I}\\textbf{c}= \\textbf{c}\\]\nWe have a solution to the system, namely \\({\\textbf{c}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\), only if the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. And that requires this \\((p + 1) \\times (p + 1)\\) matrix is of full rank \\(r\\left( \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} \\right) = p + 1\\). This, in turn is equivalent to saying that \\(\\textbf{X}\\) has full rank \\(p + 1\\) because of property (i).\nHere are some useful results about inverse matrices:\n\n\\(\\left( \\textbf{A}^{- 1} \\right)^\\prime = \\left( \\textbf{A}^\\prime \\right)^{- 1}\\)\n\\(\\left( \\textbf{A}^{- 1} \\right)^{- 1} = \\textbf{A}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^{- 1} = \\textbf{B}^{-1}\\textbf{A}^{-1}\\)\n\\(r\\left( \\textbf{A}^{- 1} \\right) = r(\\textbf{A})\\)\n\nIf the matrix \\(\\textbf{X}\\) is of less than full rank, it is called a rank-deficient matrix. Can we still solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\)? Not by using a (regular) inverse matrix, but there is a way out, by using a generalized inverse matrix. If a matrix \\(\\textbf{A}^{-}\\) can be found that satisfies\n\\[\\textbf{A}\\textbf{A}^{-}\\textbf{A}= \\textbf{A}\\]\nthen it is called the generalized inverse (or pseudo-inverse or g-inverse) of \\(\\textbf{A}\\). Suppose we can find such a generalized inverse \\(\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\)f or \\(\\textbf{X}^\\prime\\textbf{X}\\). What if we use that in the solution of the linear system,\n\\[\\textbf{c}= {\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\textbf{X}}^{\\prime}\\textbf{Y}\\]\nUnfortunately, whereas regular inverses are unique, there are (infinitely) many generalized inverses that satisfy the condition \\((\\textbf{X}^\\prime\\textbf{X})\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}^\\prime\\textbf{X}= \\textbf{X}^\\prime\\textbf{X}\\). So, there will be infinitely many possible solutions to the linear system. Fortunately, it turns out that generalized inverses have some nice properties, for example, \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}\\) is invariant to the choice of the generalized inverse. Even if the solution \\(\\textbf{c}\\) is not unique, \\(\\textbf{X}\\textbf{c}\\) is unique. This result is important in linear models with rank-deficient design matrices, a condition that is common when the model contains classification variables. While the parameter estimates in such a model are not unique, because we need to use a generalized inverse to derive the estimates, the predicted values are the same, no matter which generalized inverse we choose.\n\n\nDeterminant\nThe rank reduces a matrix to a single scalar value, the number of linearly independent columns of the matrix. Another value that reduces a square matrix to a single scalar is the determinant, written as \\(det(\\textbf{A})\\) or \\(|\\textbf{A}|\\). The determinant has a geometric interpretation which is not that relevant for our discussion. What matters more is that the determinant appears frequently in expressions of multivariate probability distributions and knowing how to manipulate the determinants.\n\n\\(|\\textbf{A}| = |\\textbf{A}^\\prime|\\)\n\\(|\\textbf{I}| = 1\\)\n\\(\\left| c\\textbf{A}\\right| = c^{n}\\mathbf{|A}\\mathbf{|}\\)\nIf \\(\\textbf{A}\\) is singular, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf each element of a row (column) of \\(\\textbf{A}\\) is zero, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf two rows (column) of \\(\\textbf{A}\\) are identical, then \\(\\left| \\textbf{A}\\right| = 0\\)\n\\(\\left| \\textbf{A}\\textbf{B}\\right| = \\left| \\textbf{A}\\right|\\ \\left| \\textbf{B}\\right|\\)\n\\(\\left| \\textbf{A}^{- 1} \\right| = 1/|\\textbf{A}|\\)\nIf \\(\\textbf{A}\\) is a triangular matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\nIf \\(\\textbf{A}\\) is a diagonal matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\n\n\n\nTrace\nThe trace operator, \\(tr(\\textbf{A})\\), applies only to square matrices. The trace of an \\(\\textbf{A}_{(n \\times n)}\\) matrix is the sum of its diagonal elements:\n\\[tr\\left( \\textbf{A}\\right) = \\sum_{i = 1}^{n}a_{ii}\\]\nThe trace plays an important role in statistics in determining expected values of quadratic forms of random variables, for example, sums of squares in linear models. An important property of the trace is its invariance under cyclic permutations,\n\\[tr\\left( \\mathbf{ABC} \\right) = tr\\left( \\mathbf{BCA} \\right) = tr(\\mathbf{CAB})\\]\nprovided the matrices conform to multiplication.\nSome other useful properties of the trace are\n\n\\(tr\\left( \\textbf{A}+ \\textbf{B}\\right) = tr\\left( \\textbf{A}\\right) + tr\\left( \\textbf{B}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = tr\\left( \\textbf{A}^\\prime \\right)\\)\n\\(\\textbf{Y}^\\prime\\text{Ay} = tr\\left( \\textbf{Y}^\\prime\\text{Ay} \\right)\\)\n\\(tr\\left( c\\textbf{A}\\right) = c \\times tr\\left( \\textbf{A}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = r(\\textbf{A})\\) if \\(\\textbf{A}\\) is symmetric and idempotent (\\(\\textbf{A}\\textbf{A}= \\textbf{A}\\) and \\(\\textbf{A}= \\textbf{A}^\\prime\\))",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#random-vectors",
    "href": "linalg.html#random-vectors",
    "title": "3  Linear Algebra Review",
    "section": "3.4 Random Vectors",
    "text": "3.4 Random Vectors\nIf the elements of a vector are random variables, the vector object itself is a random variable. You can think of random vectors as a convenient mechanism to collect random variables. Suppose we draw a random sample \\(Y_{1},\\cdots,Y_{n}\\), then we can collect the \\(n\\) random variables in a single random vector\n\\[\\textbf{Y}= \\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\n\nExpected Value\nSince each \\(Y_{i}\\) has a probability distribution, a mean (expected value) \\(\\text{E}\\left\\lbrack Y_{i} \\right\\rbrack\\), a variance \\(\\text{Var}\\left\\lbrack Y_{i} \\right\\rbrack\\), and so forth, the same applies to their collection. The expected value (mean) of a random vector is the vector of the expected values of its elements:\n\\[\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\begin{bmatrix}\n\\text{E}\\left\\lbrack Y_{1} \\right\\rbrack \\\\\n\\vdots \\\\\n\\text{E}\\left\\lbrack Y_{n} \\right\\rbrack\n\\end{bmatrix}\\]\nSuppose that \\(\\textbf{A},\\ \\textbf{B},\\ \\textbf{c}\\) are matrices and vectors of constants, respectively, and that \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are random vectors. The following are useful expectation operations in this situations:\n\n\\(\\text{E}\\left\\lbrack \\textbf{A}\\right\\rbrack = \\textbf{A}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AYB} + \\mathbf{C} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{B}+ \\textbf{C}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{c} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{c}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{B}\\ \\text{E}\\lbrack\\mathbf{U}\\rbrack\\)\n\n\n\nCovariance Matrix\nWhile the distribution of \\(Y_{i}\\) is univariate, \\(\\textbf{Y}\\) has a multivariate (\\(n\\)-variate) distribution. The mean of the distribution is represented by a vector. The variance of the distribution is represented by a matrix, the variance-covariance matrix, a special case of a covariance matrix.\nThe covariance matrix between random vectors \\(\\textbf{Y}_{(k \\times 1)}\\) and \\(\\mathbf{U}_{(p \\times 1)}\\) is a \\((k \\times p)\\) matrix whose typical elements are the covariances between the elements of \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\):\n\\[\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\left\\lbrack \\text{Cov}(Y_{i},U_{j}) \\right\\rbrack\\]\nThe covariance matrix can be written in terms of expected values of \\(\\textbf{Y}\\), \\(\\mathbf{U}\\), and \\(\\textbf{Y}\\mathbf{U}^\\prime\\)\n\\[\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\mathbf{U} - \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\mathbf{U}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack^{\\prime}\\]\nSome useful rules to manipulate covariance matrices are:\n\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{U} \\right\\rbrack = \\textbf{A}\\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\\(\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{BU} \\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\ \\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U},c\\mathbf{W} + d\\textbf{V}\\right\\rbrack = ac\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{W} \\right\\rbrack + bc\\text{Cov}\\left\\lbrack \\mathbf{U},\\mathbf{W} \\right\\rbrack + ad\\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{V}\\right\\rbrack + bd\\text{Cov}\\lbrack\\mathbf{U},\\textbf{V}\\rbrack\\)\n\n\n\nVariance-covariance Matrix\nThe variance-covariance matrix (or variance matrix for short) of a random vector \\(\\textbf{Y}\\) is the covariance matrix of \\(\\textbf{Y}\\) with itself.\n\\[\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{Y}\\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\textbf{Y}-\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\textbf{Y}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^{\\prime}\\]\nThe diagonal entries of the variance-covariance matrix contain the variances of the \\(Y_{i}\\). The off-diagonal cells contain the covariances \\(\\text{Cov}\\left\\lbrack Y_{i},Y_{j} \\right\\rbrack\\). If the variance matrix is diagonal, the elements of random vector \\(\\textbf{Y}\\) are uncorrelated. Two random vectors \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are uncorrelated if their variance matrix is block-diagonal:\n\\[\\text{Var}\\begin{bmatrix}\n\\textbf{Y}_{1} \\\\\n\\textbf{Y}_{2}\n\\end{bmatrix} = \\begin{bmatrix}\n\\text{Var}\\lbrack\\textbf{Y}_{2}\\rbrack & \\textbf{0}\\\\\n\\textbf{0}& \\text{Var}\\lbrack\\textbf{Y}_{1}\\rbrack\n\\end{bmatrix}\\]\nA very special variance-covariance matrix in statistical models is the scaled identity matrix, \\(\\sigma^{2}\\textbf{I}\\). This is the variance matrix of uncorrelated observations drawn from the same distribution—a common assumption for the error terms in models.\nThe rules for working with covariances extend to working with variance matrices:\n\n\\(\\text{Var}\\left\\lbrack \\mathbf{AY} \\right\\rbrack = \\textbf{A}\\ \\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}^\\prime\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{Y}+ \\textbf{A}\\right\\rbrack = \\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{A}^\\prime\\textbf{Y}\\right\\rbrack = \\textbf{A}^\\prime\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}\\right\\rbrack = a^{2}\\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U} \\right\\rbrack = a^{2}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack + b^{2}\\text{Var}\\left\\lbrack \\mathbf{U} \\right\\rbrack + 2ab\\ \\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\nFinally, an important result about expected values of quadratic forms, heavily used to in decomposing variability is\n\\[\\text{E}\\left\\lbrack \\textbf{Y}^\\prime\\mathbf{AY} \\right\\rbrack = tr\\left( \\textbf{A}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right) + \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^\\prime\\textbf{A}\\ \\text{E}\\lbrack\\textbf{Y}\\rbrack\\]",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#sec-matrix-differentiation",
    "href": "linalg.html#sec-matrix-differentiation",
    "title": "3  Linear Algebra Review",
    "section": "3.5 Matrix Differentiation",
    "text": "3.5 Matrix Differentiation\nEstimation of parameters in statistical models often requires minimization or maximization of an objective function. For example, the ordinary least squares (OLS) principle finds the OLS estimator as the function of the data that minimizes the error sum of squares of the model. Maximum likelihood finds estimators of the parameters as the functions of the data that maximizes the joint likelihood (the joint distribution function) of the data.\nThe parameters of the models appear as elements of vectors and matrices. Finding estimators of the parameters thus requires calculus on vectors and matrices. Consider matrix \\(\\textbf{A}\\), whose elements depend on a scalar parameter \\(\\theta\\), \\(\\textbf{A}= \\left\\lbrack a_{ij}(\\theta) \\right\\rbrack\\). The derivative of \\(\\textbf{A}\\) with respect to \\(\\theta\\) is the matrix of the derivatives of the typical elements \\(a_{ij}(\\theta)\\) with respect to \\(\\theta\\). We write this formally as\n\\[\\frac{\\partial\\textbf{A}}{\\partial\\theta} = \\left\\lbrack \\frac{\\partial a_{ij}(\\theta)}{\\partial\\theta} \\right\\rbrack\\]\nThe derivative of a function \\(f(\\boldsymbol{\\theta})\\) with respect to the vector \\(\\boldsymbol{\\theta}_{(p \\times 1)}\\) is the vector of the partial derivatives of the function\n\\[\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\boldsymbol{\\theta}} = \\begin{bmatrix}\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{1}} \\\\\n\\vdots \\\\\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{p}}\n\\end{bmatrix}\\]\nHere are some useful results from vector and matrix calculus where \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are functions of \\(\\theta\\) and vector \\(\\textbf{X}\\) does not depend on \\(\\theta\\):\n\n\\(\\frac{{\\partial ln}\\left| \\textbf{A}\\right|}{\\partial\\theta} = \\frac{1}{\\left| \\textbf{A}\\right|}\\frac{\\partial\\left| \\textbf{A}\\right|}{\\partial\\theta} = tr\\left( \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{A}^{- 1}}{\\partial\\theta} = - \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta\\ }\\textbf{A}^{- 1}\\)\n\\(\\frac{\\partial tr\\left( \\mathbf{AB} \\right)}{\\partial\\theta} = tr\\left( \\frac{\\mathbf{\\partial}\\textbf{A}}{\\partial\\theta}\\textbf{B}\\right) + tr\\left( \\textbf{A}\\frac{\\mathbf{\\partial}\\textbf{B}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}^{- 1}\\textbf{X}}{\\partial\\theta} = - \\textbf{X}^\\prime\\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta}\\textbf{A}^{- 1}\\textbf{X}\\)\n\\(\\frac{\\partial\\textbf{X}^{\\prime}\\mathbf{Ax}}{\\partial\\textbf{X}} = 2\\mathbf{Ax}\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}}{\\partial\\textbf{X}} = \\frac{\\partial\\textbf{A}^\\prime\\textbf{X}}{\\partial\\textbf{X}} = \\textbf{A}\\)",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#sec-idempotent",
    "href": "linalg.html#sec-idempotent",
    "title": "3  Linear Algebra Review",
    "section": "3.6 Idempotent Matrices",
    "text": "3.6 Idempotent Matrices\nA matrix is called idempotent if multiplying the matrix by itself yields the matrix–\\(\\textbf{A}\\textbf{A}= \\textbf{A}\\). Because \\(\\textbf{A}\\) must conform to itself for multiplication, idempotent matrices are square matrices. Idempotent matrices that are also symmetric play an important role in statistical estimation. Idempotent matrices are projection matrices, that means they map a vector from a space to a sub-space. Symmetric idempotent matrices are orthogonal projection matrices.\nAmong some of the interesting properties of idempotent matrices are the following:\n\nThe trace of an idempotent matrix is equal to its rank.\nThe trace of an idempotent matrix is an integer.\nThe eigenvalues of an idempotent matrix are either 0 or 1.\n\n\nProjections\nFor example, suppose we want to find a solution for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}_{(n \\times 1)} = \\textbf{X}\\boldsymbol{\\beta}_{(p + 1 \\times 1)} + \\mathbf{\\epsilon}\\). The vector \\(\\textbf{Y}\\) is a vector in \\(n\\)-dimensional space \\(\\mathbb{R}^{n}\\) and the model places a restriction on the predicted values \\(\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\): the predicted values are confined to a \\((p + 1)\\)-dimensional sub-space of \\(\\mathbb{R}^{n}\\). Regardless of how we choose the estimator \\(\\widehat{\\boldsymbol{\\beta}}\\), we are dealing with projecting vector \\(\\textbf{Y}\\) onto a sub-space of \\(\\mathbb{R}^{n}\\).\nWe can thus think of the problem of finding the best estimator in this model as the problem of finding the best projection onto the space generated by the columns of \\(\\textbf{X}\\). In that case, why not choose the projection that minimizes the distance between the observed values \\(\\textbf{Y}\\) and the predicted values \\(\\widehat{\\textbf{Y}}\\). This is achieved by projecting \\(\\textbf{Y}\\) perpendicular (orthogonal) onto the sub-space generated by \\(\\textbf{X}\\). In other words, our solution is the vector \\(\\widehat{\\boldsymbol{\\beta}}\\) that satisfies\n\\[\\left( \\textbf{Y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\right)^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = 0\\]\nMultiplying out and rearranging terms yields\n\\[{\\widehat{\\boldsymbol{\\beta}}}^{\\prime}\\textbf{X}^{\\prime}\\textbf{Y}= {\\widehat{\\boldsymbol{\\beta}}}^{\\prime}\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\nwhich implies that\n\\[\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) exists and we can solve:\n\\[\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\]\n\n\nThe “Hat” Matrix\nThe ordinary least squares estimator is the orthogonal projection of \\(\\textbf{Y}\\) onto the sub-space created by the columns of \\(\\textbf{X}\\). To see the projection matrix at work, compute the predicted values, \\(\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\):\n\\[\\textbf{X}\\widehat{\\boldsymbol{\\beta}}=\\textbf{X}\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}= \\widehat{\\textbf{Y}}\\]\nThe matrix \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\) has a very special role in regression analysis, it is often called the “hat” matrix and denoted \\(\\textbf{H}\\), because pre-multiplying \\(\\textbf{Y}\\) with \\(\\textbf{H}\\) puts the hats on \\(\\textbf{Y}\\):\n\\[\\textbf{H}\\textbf{Y}= \\widehat{\\textbf{Y}}\\]\nLet’s verify that \\(\\textbf{H}\\) is indeed a projection matrix, which requires that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\):\n\\[\\textbf{H}\\textbf{H}= \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} \\times \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime=\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} = \\textbf{H}\\]\nMatrices with the property that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\) are called idempotent matrices, these are projection matrices. If, in addition, \\(\\textbf{H}\\) is symmetric, \\(\\textbf{H}^\\prime = \\textbf{H}\\), the matrix is called symmetric idempotent—these are orthogonal projection matrices. (An idempotent matrix that is not symmetric is called an oblique projector.)\nThe hat matrix in the regression model is a symmetric idempotent matrix.\nHere are some results about (symmetric) idempotent matrices that come in handy when working out the properties of estimators in regression-type models:\n\nProjection matrices are typically not of full rank. If an \\((n \\times n)\\) idempotent matrix is of rank \\(n\\), then it is the identity matrix \\(\\textbf{I}\\).\nIf \\(\\textbf{A}\\) is (symmetric) idempotent, then \\(\\textbf{I}- \\textbf{A}\\) is (symmetric) idempotent.\nIf \\(\\mathbf{P}\\) is non-singular, then \\(\\mathbf{PA}\\mathbf{P}^{-1}\\) is an idempotent matrix.\n\nYou can use these properties to show that in the linear regression model with uncorrelated errors and equal variance the variance matrix of the model residuals \\(\\textbf{Y}- \\widehat{\\textbf{Y}}\\) is\n\\[\\text{Var}\\left\\lbrack \\textbf{Y}- \\widehat{\\textbf{Y}} \\right\\rbrack = \\sigma^{2}(\\textbf{I}- \\textbf{H})\\]\n\n\nA Special Case\nConsider the special case where \\(\\textbf{X}= \\textbf{1}_{n}\\), a column vector of ones. The corresponding linear model is \\(\\textbf{Y}= \\textbf{1}\\beta + \\boldsymbol{\\epsilon}\\), an intercept-only model. The hat matrix for this model is\n\\[\\textbf{1}\\left( \\textbf{1}^{\\prime}\\textbf{1}\\right)^{- 1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{J}= \\begin{bmatrix}\n\\frac{1}{n} & \\cdots & \\frac{1}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{n} & \\cdots & \\frac{1}{n}\n\\end{bmatrix}\n\\]a matrix filled with \\(\\frac{1}{n}\\). The projection of \\(\\textbf{Y}\\) onto the space generated by \\(\\textbf{1}_{n}\\) is\n\\[\\frac{1}{n}\\mathbf{JY} = \\begin{bmatrix}\n\\overline{Y} \\\\\n\\vdots \\\\\n\\overline{Y}\n\\end{bmatrix}\\]\nThe predicted values are all the same, the sample mean. In other words, \\(\\beta = \\overline{Y}\\). Since the projector is idempotent, deriving the variance of the predicted value in the iid case is simple:\n\\[\\text{Var}\\left\\lbrack \\textbf{H}\\textbf{Y}\\right\\rbrack = \\textbf{H}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{H}^\\prime = \\sigma^{2}\\textbf{H}\\textbf{H}^{\\prime} = \\sigma^{2}\\textbf{H}= \\sigma^{2}\\frac{1}{n}\\textbf{J}=\\begin{bmatrix}\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n}\n\\end{bmatrix}\\]",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "linalg.html#sec-multi-gaussian",
    "href": "linalg.html#sec-multi-gaussian",
    "title": "3  Linear Algebra Review",
    "section": "3.7 Multivariate Gaussian Distribution",
    "text": "3.7 Multivariate Gaussian Distribution\n\nDefinition\nA scalar random variable \\(Y\\) has a Gaussian distribution function with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) if its probability density function is given by\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\\]\nWe also say that \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The shorthand expressions \\(Y \\sim G(\\mu,\\sigma^{2})\\) or \\(Y \\sim N(\\mu,\\sigma^{2})\\) are common.\nThe generalization from a scalar random variable \\(Y\\) to a random vector \\(\\textbf{Y}_{(n \\times 1)}\\) with a multivariate Gaussian distribution is as follows. \\(\\textbf{Y}_{(n \\times 1)}\\) has a multivariate Gaussian (normal) distribution with mean \\(\\boldsymbol{\\mu}\\) and variance matrix \\(\\textbf{V}\\), if its density is given by\n\\[f\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right) \\right\\}\\]\nThis is denoted with the shorthand \\(\\textbf{Y}\\sim G_{n}(\\boldsymbol{\\mu},\\textbf{V})\\) or \\(\\textbf{Y}\\sim N_{n}(\\boldsymbol{\\mu},\\textbf{V}\\)\\). If the dimension of the distribution is clear from context, the subscript \\(n\\) can be omitted. A special case is the standard multivariate Gaussian distribution with mean \\(\\textbf{0}\\) and variance matrix \\(\\textbf{I}\\) :\n\\[f\\left( \\textbf{Y}\\right)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\textbf{Y}^{\\prime}\\textbf{Y}\\right\\} = \\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\sum_{i}^{n}y_{i}^{2} \\right\\}\\]\nBut this is just the product of the \\(n\\) univariate densities of \\(N(0,1)\\) random variables:\n\\[f\\left( \\textbf{Y}\\right) = f\\left( y_{1} \\right) \\times \\cdots \\times f\\left( y_{n} \\right)\\]\nwhere\n\\[f\\left( y_{i} \\right) = \\frac{1}{(2\\pi)^{1/2}}\\exp\\left\\{ - \\frac{1}{2}y^{2} \\right\\}\\]\nIf the variance matrix is diagonal—that is, the \\(Y_{i}\\) are uncorrelated—the multivariate normal distribution is the product of the univariate distributions. The random variables are independent.\n\n\nProperties\nGaussian distributions have amazing (magical) properties.\n\nLinear combinations are Gaussian\nFor example, a linear combination of Gaussian random variables also follows a Gaussian distribution. Formally, this can be expressed as follows: if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\) and \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are a matrix and vector of constants (not random variables), respectively, then \\(\\mathbf{AY} + \\textbf{B}\\) follows a \\(G(\\textbf{A}\\boldsymbol{\\mu},\\mathbf{AVA})\\) distribution.\nA special case of this result is that if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\), \\(\\textbf{Y}- \\boldsymbol{\\mu}\\) has a \\(G\\left( 0,\\textbf{V}\\right)\\) distribution.\nBecause a linear function of a Gaussian random variable is Gaussian distributed, you can define all multivariate Gaussian distributions as linear transformations of the standard multivariate Gaussian distribution. If \\(\\textbf{Z}\\sim G_{n}(\\textbf{0},\\textbf{I})\\), and \\(\\textbf{V}= \\textbf{C}^\\prime\\textbf{C}\\), then \\(\\textbf{Y}= \\mathbf{C}^\\prime\\textbf{Z}+ \\boldsymbol{\\mu}\\) has a \\(G(\\boldsymbol{\\mu},\\textbf{V})\\) distribution.\n\n\nZero covariance implies independence\nAnother unusual property of Gaussian random variables is that if they are uncorrelated, they are also stochastically independent. We derived this above for the special case of \\(\\textbf{Y}\\sim G(\\textbf{0},\\sigma^{2}\\textbf{I})\\).\nYou cannot in general conclude that random variables are independent based on their lack of correlation. For Gaussian random variables you can. This result can be extended to Gaussian random vectors. Suppose \\(\\textbf{Y}_{(n \\times 1)} \\sim G(\\boldsymbol{\\mu},\\ \\textbf{V})\\) is partitioned into two sub-vectors of size \\(s\\) and \\(k\\), where \\(n = s + k\\). Then we can similarly partition the mean vector and variance matrix:\n\\[\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\n\\textbf{Y}_{1(s \\times 1)} \\\\\n\\textbf{Y}_{2(k \\times 1)}\n\\end{bmatrix},\\ \\ \\boldsymbol{\\mu}= \\begin{bmatrix}\n\\boldsymbol{\\mu}_{1} \\\\\n\\boldsymbol{\\mu}_{2}\n\\end{bmatrix},\\ \\ \\ \\ \\ \\ \\textbf{V}= \\begin{bmatrix}\n\\textbf{V}_{11} & \\textbf{V}_{12} \\\\\n\\textbf{V}_{21} & \\textbf{V}_{22}\n\\end{bmatrix}\\]\nIf \\(\\textbf{V}_{12} = \\textbf{0}\\), then \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\) are independent. Also, each partition is Gaussian distributed, for example, \\(\\textbf{Y}_{1} \\sim G(\\boldsymbol{\\mu}_{1},\\ \\textbf{V}_{11})\\). We call the distribution of \\(\\textbf{Y}_{1}\\) the marginal distribution.\nIt follows immediately that each element of \\(\\textbf{Y}\\) follows a (univariate) Gaussian distribution, \\(Y_{i} \\sim G(\\mu_{i},V_{ii})\\)—all marginal univariate distributions are Gaussian.\n\n\nConditionals are Gaussian}\nThe conditional distribution of \\(\\textbf{Y}_{1}\\) given \\(\\textbf{Y}_{2}\\) is also a Gaussian distribution, specifically:\n\\[\\textbf{Y}_{1}|\\textbf{Y}_{2} \\sim G\\left( \\boldsymbol{\\mu}_{1}\\mathbf{+}\\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\left( \\textbf{Y}_{2} - \\boldsymbol{\\mu}_{2} \\right),\\ \\textbf{V}_{11} - \\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\textbf{V}_{12}^\\prime \\right)\\]\nThis result plays an important role when predicting Gaussian random variables, for example in mixed models.\nNotice that the variance matrix of the conditional distribution does not depend on the particular value \\(\\textbf{Y}_{2} = \\textbf{Y}_{2}\\) on which the distribution is conditioned. However, the mean of the conditional distribution does depend on \\(\\textbf{Y}_{2}\\) unless \\(\\textbf{V}_{12} = \\textbf{0}\\), a condition established earlier for independence of \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\).",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra Review</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "4  Parameter Estimation",
    "section": "",
    "text": "4.1 Introduction\nStatistical models contain two types of unknown quantities, parameters and hyperparameters. Parameters describe the distributional properties of the data; they are part of the mean function or the variance-covariance structure of the model. They are estimated from the data.\nThis chapter is not concerned with tuning hyperparameters but with the principles we use to estimate parameters of a model’s mean function from data—that is, estimating the internal parameters of the model.\nFinding parameter estimates can be expressed as a numerical problem: find the values that minimize some metric of discrepancy between data and the model. The discrepancy can be some measure of loss such as squared error between observed and predicted target values \\[\\left( y_i - \\widehat{f}_i(\\textbf{x};\\boldsymbol{\\theta}) \\right)^2\\] or the misclassification error \\[\nI(y_i \\ne \\widehat{f}_i(\\textbf{x};\\boldsymbol{\\theta}))\n\\] and the loss for the entire data set is summed over all observations, for example \\[\n\\ell(\\textbf{y};\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left( y_i - \\widehat{y}_i \\right)^2\n\\]\nThe solution to the minimization problem \\[\\mathop{\\mathrm{arg\\,min}}_\\boldsymbol{\\theta}\\, \\ell(\\textbf{y};\\boldsymbol{\\theta})\\] is the estimator \\(\\widehat{\\boldsymbol{\\theta}}\\) of the parameters \\(\\boldsymbol{\\theta}\\).\nIn some situations, this minimization problem has a closed-form solution that can be computed directly. In other cases we have to rely on numerical procedures to find a solution iteratively or approximations to simplify a complex or intractable problem. The solution \\(\\widehat{\\boldsymbol{\\theta}}\\) is unique for some problems and might be one of many solutions, not all equally good.\nExpressing parameter estimation as a general minimization problem does not reveal the\nfoundations of important principles in parameter estimation, in particular, least squares and maximum likelihood estimation. We introduce these principles based on geometric and probabilistic considerations, the relationship to function minimization will be obvious.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#introduction",
    "href": "estimation.html#introduction",
    "title": "4  Parameter Estimation",
    "section": "",
    "text": "Note\n\n\n\nAll optimization tasks will be presented as minimization problem. Finding the maximum of a function can be turned into a minimization of its negative value.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#least-squares-estimation",
    "href": "estimation.html#least-squares-estimation",
    "title": "4  Parameter Estimation",
    "section": "4.2 Least Squares Estimation",
    "text": "4.2 Least Squares Estimation\nLeast squares estimation is arguably one of the most important estimation principles and rests on a geometric concept. Suppose we have a model with additive errors, \\(\\textbf{Y}= \\mathbf{f}(\\textbf{x}; \\boldsymbol{\\theta}) + \\boldsymbol{\\epsilon}\\). The function \\(\\mathbf{f}()\\) is an \\((n \\times 1)\\) vector of the mean function evaluations, the \\(i\\)th value is \\(f(\\textbf{x}_i;\\boldsymbol{\\theta})\\). The least squares estimate \\(\\widehat{\\boldsymbol{\\theta}}\\) of \\(\\boldsymbol{\\theta}\\) is the value that is closest to \\(\\textbf{Y}\\) among all possible values \\(\\tilde{\\boldsymbol{\\theta}}\\).\n\nOrdinary Least Squares (OLS)\nConsider the identity\n\\[\\textbf{Y}= \\textbf{f}(\\textbf{x};\\tilde{\\boldsymbol{\\theta}}) + \\left(\\textbf{Y}- \\textbf{f}(\\textbf{x};\\tilde{\\boldsymbol{\\theta}}) \\right)\\]\nwhich expresses the observed data \\(\\textbf{Y}\\) as the sum of fitted values and residuals.\nBy the Pythagorean theorem, the solution \\(\\widehat{\\boldsymbol{\\theta}}\\) with the smallest vector of residuals is the orthogonal projection of \\(\\textbf{Y}\\) onto \\(\\textbf{f}\\), which implies that \\[ \\left(\\textbf{Y}- \\textbf{f}(\\textbf{x};\\widehat{\\boldsymbol{\\theta}}) \\right)^\\prime \\textbf{f}(\\textbf{x};\\widehat{\\boldsymbol{\\theta}}) = \\textbf{0}\\] When \\(\\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\) is linear in the parameters, it is common to denote the coefficients (parameters) as \\(\\boldsymbol{\\beta}\\) instead of the more generic \\(\\boldsymbol{\\theta}\\). We thus write the standard linear (regression) as \\[\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\] and the orthogonality criterion becomes \\[\\left(\\textbf{Y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\right)^\\prime \\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{0}\\] It is easy to show that this implies \\[\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{X}^\\prime\\textbf{Y}\\] and if the cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) is of full rank, the (ordinary) least squares estimator is \\[\n\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] Expressed as the problem of minimizing a loss function, ordinary least squares (OLS) is approached as follows. Suppose that we measure loss as squared-error loss \\((y_i - \\widehat{y}_i)^2\\). In the linear model where \\(\\widehat{y}_i = \\textbf{x}_i^\\prime \\widehat{\\boldsymbol{\\beta}}\\), the loss function is the residual (error) sum of squares \\[\\text{SSE}(\\boldsymbol{\\beta}) = \\boldsymbol{\\epsilon}^\\prime\\boldsymbol{\\epsilon}= \\left( \\textbf{Y}-\\textbf{X}^\\prime\\boldsymbol{\\beta}\\right)^\\prime \\left(\\textbf{Y}-\\textbf{X}^\\prime\\boldsymbol{\\beta}\\right)\\] and the minimization problem is \\[\\mathop{\\mathrm{arg\\,min}}_\\boldsymbol{\\beta}\\, \\text{SSE}(\\boldsymbol{\\beta})\\]\nTo find the minimum we set to zero the derivative of \\(\\text{SSE}(\\boldsymbol{\\beta})\\) with respect to \\(\\boldsymbol{\\beta}\\). Expanding the residual sum of squares yields \\[SSE(\\boldsymbol{\\beta}) = \\textbf{Y}^\\prime\\textbf{Y}- 2\\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{Y}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}\\]\nThe derivative (Section 3.5) with respect to \\(\\boldsymbol{\\beta}\\) is \\[\\frac{\\partial\\,\\text{SSE}(\\boldsymbol{\\beta})}{\\partial\\boldsymbol{\\beta}} = -2\\textbf{X}^\\prime\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}\\]\nSetting the derivative to zero and solving yields the normal equations as above: \\[\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{X}^\\prime\\textbf{Y}\\] and the OLS estimator \\[\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\\] The vector of fitted values, \\(\\widehat{\\textbf{y}}\\) is obtained by pre-multiplying with the data matrix\n\\[\\begin{align*}\n    \\widehat{\\textbf{Y}} &= \\textbf{X}^\\prime\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\\\\\n                  &= \\textbf{H}\\textbf{Y}\n\\end{align*}\\]\nThe matrix \\(\\textbf{H}\\) is called the “Hat” matrix, because pre-multiplying \\(\\textbf{Y}\\) with \\(\\textbf{H}\\) “puts hats on the \\(y\\)s” (Section 3.6.2). The last equation shows that the OLS estimator is a linear estimator, \\(\\widehat{y}_i\\) is a linear combination of all \\(y_i\\), the weights of the linear combination are given by the entries of the hat matrix.\nThe statistical properties of the OLS estimator depend on the nature of the random error process. The most common assumption is that the \\(\\epsilon_i\\) have zero mean and are iid, independently and identically distributed, formally, \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0},\\sigma^2\\textbf{I})\\).\n\n\n\n\n\n\nNote\n\n\n\nThe statement \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0},\\sigma^2\\textbf{I})\\) is technically weaker than stating independence, it implies zero correlations among the observations. Independent random variables are uncorrelated, but the reverse is not necessarily true. You can conclude independence from lack of correlations for Gaussian (normal) random variables, but not generally.\n\n\nIn the iid case, the OLS estimator is unbiased, \\(\\text{E}[\\widehat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) and has variance \\(\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2(\\textbf{X}^\\prime\\textbf{X})^{-1}\\). In fact, it is a BLUE (best linear unbiased estimator) in this situation. No other unbiased estimator has smaller variance than the OLS estimator. However, it is possible that other estimators have smaller mean squared error than the OLS estimator, if the introduction of bias is more than offset by a reduction of the variance of the estimator. This is important in high-dimensional problems where the number of predictors (\\(p\\)) is large. As \\(p\\) increases, the OLS estimator becomes more unstable, especially if the predictors are highly related to each other (a condition known as multi-collinearity). The values of \\(\\widehat{\\boldsymbol{\\beta}}\\) then have a tendency to vary widely. Estimators that limit the variability of the model coefficients through regularization techniques, such as Ridge or Lasso regression, can have considerably lower variance at the expense of some bias, leading to better mean squared error.\n\n\n\n\n\n\nNote\n\n\n\nA Gaussian distribution (normality) assumption is not a requirement of the linear model or of least squares estimation. The OLS estimator has desirable properties even if the errors are not normally distributed. However, making statements about the significance of the \\(\\beta_j\\) requires additional assumptions such as \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\).\nWhen \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\), the OLS estimator is not only BLUE but a minimum variance unbiased estimator (MVUE), best among all unbiased estimators, not only those estimators linear in \\(\\textbf{Y}\\).\n\n\n\nLeast squares from scratch\nTo understand statistical computing, it is a good idea to implement some algorithms from scratch. That also helps to identify the numbers reported by statistical software. Here we implement the OLS estimator from scratch in R using the fitness data set. The data comprise measurements of aerobic capacity and other attributes on 31 men involved in a physical fitness course at N.C. State University.\nAerobic capacity is the ability of the heart and lungs to provide the body with oxygen. It is a measure of fitness and expressed as the oxygen intake in ml per kg body weight per minute. Measuring aerobic capacity is expensive and time consuming compared to attributes such as age, weight, and pulse. The question is whether aerobic capacity can be predicted from the easily measurable attributes. If so, a predictive equation can reduce time and effort to assess aerobic capacity.\nThe variables are\n\nAge: age in years\nWeight: weight in kg\nOxygen: oxygen intake rate (ml per kg body weight per minute)\nRunTime: time to run 1.5 miles (minutes)\nRestPulse: heart rate while resting\nRunPulse: heart rate while running (same time Oxygen rate measured)\nMaxPulse: maximum heart rate recorded while running\n\nThe linear model we have in mind is \\[\n\\text{Oxygen}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\beta_2\\text{Weight}_i + \\beta_3\\text{RunTime}_i + \\beta_4\\text{RestPulse}_i + \\beta_5\\text{RunPulse}_i + \\beta_6\\text{MaxPulse}_i + \\epsilon_i\n\\] and the \\(\\epsilon_i\\) are assumed zero-mean random variables with common variance \\(\\sigma^2\\).\nThe following code loads the data from the ads DuckDB database.\n\nsuppressWarnings(library(\"duckdb\"))\n\nLoading required package: DBI\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nfit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\n\nThe target variable for the linear model is Oxygen, the remaining variables are inputs to the regression. The next statements create the \\(\\textbf{y}\\) vector and the \\(\\textbf{X}\\) matrix for the model. Note that the first column of \\(\\textbf{X}\\) is a vector of ones, representing the “input” for the intercept \\(\\beta_0\\).\n\ny &lt;- as.matrix(fit[,which(names(fit)==\"Oxygen\")])\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\nhead(X)\n\n     Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\n[1,]      1  44  89.47   11.37        62      178      182\n[2,]      1  40  75.07   10.07        62      185      185\n[3,]      1  44  85.84    8.65        45      156      168\n[4,]      1  42  68.15    8.17        40      166      172\n[5,]      1  38  89.02    9.22        55      178      180\n[6,]      1  47  77.45   11.63        58      176      176\n\n\nNext we are building the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix and compute its inverse, \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\), with the solve() function. t() transposes a matrix and %*% indicates that we are performing matrix multiplication rather than elementwise multiplication.\n\nXpX &lt;- t(X) %*% X\nXpXInv &lt;- solve(XpX)\n\nWe can verify that XpxInv is indeed the inverse of XpX by multiplying the two. This should yield the identity matrix\n\nround(XpX %*% XpXInv,3)\n\n          Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\nIntcpt         1   0      0       0         0        0        0\nAge            0   1      0       0         0        0        0\nWeight         0   0      1       0         0        0        0\nRunTime        0   0      0       1         0        0        0\nRestPulse      0   0      0       0         1        0        0\nRunPulse       0   0      0       0         0        1        0\nMaxPulse       0   0      0       0         0        0        1\n\n\nNext we compute the OLS estimate of \\(\\boldsymbol{\\beta}\\) and the predicted values \\(\\widehat{\\textbf{y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\).\n\nbeta_hat &lt;- XpXInv %*% t(X) %*% y\nbeta_hat\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\ny_hat &lt;- X %*% beta_hat\n\nThe estimate of the intercept is \\(\\widehat{\\beta}_0\\) = 102.9344795, the estimate of the coefficient for Age is \\(\\widehat{\\beta}_1\\) = -0.2269738 and so on.\nThe residuals \\(\\widehat{\\boldsymbol{\\epsilon}} = \\textbf{y}- \\widehat{\\textbf{y}}\\), the error sum of squares\n\\[\\text{SSE} = (\\textbf{y}- \\widehat{\\textbf{y}} )^\\prime (\\textbf{y}- \\widehat{\\textbf{y}}) = \\sum_{i=1}^n \\left(y_i - \\widehat{y}_i\\right)^2\\] and the estimate of the residual variance \\[\\widehat{\\sigma}^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\\]\nare computed as\n\nlibrary(\"Matrix\")\n\nresiduals &lt;- y - y_hat\nSSE &lt;- sum(residuals^2)\nn &lt;- nrow(fit)\nrankX &lt;- rankMatrix(XpX)[1]\nsigma2_hat &lt;- SSE/(n - rankX)\n\nSSE\n\n[1] 128.8379\n\nsigma2_hat\n\n[1] 5.368247\n\n\nWe used the rankMatrix function in the Matrix package to compute the rank of \\(\\textbf{X}\\), which is identical to the rank of \\(\\textbf{X}^\\prime\\textbf{X}\\). With these quantities available, the variance-covariance matrix of \\(\\widehat{\\boldsymbol{\\beta}}\\), \\[\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\textbf{X}^\\prime\\textbf{X})^{-1}\\] can be estimated by substituting \\(\\widehat{\\sigma}^2\\). The standard errors of the regression coefficient estimates are the square roots of the diagonal values of this matrix.\n\nVar_beta_hat &lt;- sigma2_hat * XpXInv\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nse_beta_hat\n\n     Intcpt         Age      Weight     RunTime   RestPulse    RunPulse \n12.40325810  0.09983747  0.05459316  0.38456220  0.06605428  0.11985294 \n   MaxPulse \n 0.13649519 \n\n\nNow let’s compare our results to the output from the lm() function in R.\n\nlinmod &lt;- lm(Oxygen ~ ., data=fit)\nsummary(linmod)\n\n\nCall:\nlm(formula = Oxygen ~ ., data = fit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4026 -0.8991  0.0706  1.0496  5.3847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 102.93448   12.40326   8.299 1.64e-08 ***\nAge          -0.22697    0.09984  -2.273  0.03224 *  \nWeight       -0.07418    0.05459  -1.359  0.18687    \nRunTime      -2.62865    0.38456  -6.835 4.54e-07 ***\nRestPulse    -0.02153    0.06605  -0.326  0.74725    \nRunPulse     -0.36963    0.11985  -3.084  0.00508 ** \nMaxPulse      0.30322    0.13650   2.221  0.03601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.317 on 24 degrees of freedom\nMultiple R-squared:  0.8487,    Adjusted R-squared:  0.8108 \nF-statistic: 22.43 on 6 and 24 DF,  p-value: 9.715e-09\n\n\nBased on the quantities calculated earlier, the following code reproduces the lm summary.\n\ntvals &lt;- beta_hat/se_beta_hat\npvals &lt;- 2*(1-pt(abs(tvals),n-rankX))\nresult &lt;- cbind(beta_hat, se_beta_hat, tvals, pvals)\ncolnames(result) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"Pr(&gt;|t|)\")\nround(result,5)\n\n           Estimate Std. Error  t value Pr(&gt;|t|)\nIntcpt    102.93448   12.40326  8.29899  0.00000\nAge        -0.22697    0.09984 -2.27343  0.03224\nWeight     -0.07418    0.05459 -1.35873  0.18687\nRunTime    -2.62865    0.38456 -6.83544  0.00000\nRestPulse  -0.02153    0.06605 -0.32600  0.74725\nRunPulse   -0.36963    0.11985 -3.08401  0.00508\nMaxPulse    0.30322    0.13650  2.22145  0.03601\n\ncat(\"\\nResidual standard error: \", sqrt(sigma2_hat),\" on \", n-rankX, \"degrees of freedom\\n\")\n\n\nResidual standard error:  2.316948  on  24 degrees of freedom\n\nSST &lt;- sum( (y -mean(y))^2 )\ncat(\"Multiple R-squared: \", 1-SSE/SST, \n    \"Adjusted R-squared: \", 1 - (SSE/SST)*(n-1)/(n-rankX), \"\\n\")\n\nMultiple R-squared:  0.8486719 Adjusted R-squared:  0.8108399 \n\nFstat &lt;- ((SST-SSE)/(rankX-1)) / (SSE/(n-rankX))\ncat(\"F-statistic: \", Fstat, \"on \", \n    rankX-1, \"and\", n-rankX, \"DF, p-value:\", 1-pf(Fstat,rankX-1,n-rankX))\n\nF-statistic:  22.43263 on  6 and 24 DF, p-value: 9.715305e-09\n\n\n\n\n\nWeighted and Generalized Least Squares\nAnother interesting feature of the OLS estimator is that it does not depend on the variability of the model errors. Whether \\(\\sigma^2\\) is large or small, the OLS estimator is only a function of \\(\\textbf{Y}\\) and \\(\\textbf{X}\\). However, the error variance affects the variability of \\(\\widehat{\\boldsymbol{\\beta}}\\).\nWhen the error distribution is more complex than the iid case, the variance and covariances of the errors must be taken into account for least squares estimators to retain their optimality. The first case is that of uncorrelated errors that have unequal variances, a situation known as heteroscedasticity. The variance-covariance matrix of the \\(\\boldsymbol{\\epsilon}\\) is then a diagonal matrix. Let’s call the inverse of the variance-covariance matrix \\(\\textbf{W}\\). \\(\\textbf{W}\\) is a \\((n \\times n)\\) diagonal matrix with \\(1/\\sigma^2_i\\), the inverse of the variance of the \\(i\\)th observation, in the \\(i\\)th diagonal cell. The weighted least squares estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{WLS} = (\\textbf{X}^\\prime\\textbf{W}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{W}\\textbf{Y}\n\\] is the optimal estimator in this situation.\n\n\n\n\n\n\nTip\n\n\n\nA weighted analysis is the correct approach when the weights are inversely proportional to the variance of the observations. This makes sense if we think of the weights as expressing how strongly the analysis should depend on a particular observation. A larger variance means that we are less certain about the observed value and thus should give the observation less weight.\n\n\nIn the weighted model the variance-covariance matrix of the errors are diagonal, the observations have unequal variances but are uncorrelated. If the errors are correlated, the variance-covariance matrix is not diagonal. Suppose that \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0}, \\textbf{V})\\), the optimal least squares estimator is the generalized least squares estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{GLS} = (\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{Y}\n\\]\nThis seems like a small change from the weighted case, replacing \\(\\textbf{W}\\) with \\(\\textbf{V}\\). So what is the big deal? In weighted analyses the weights are often known, at least up to a multiple. For example, when the variability of the target variable increases proportionally with one of the inputs, \\(x_2\\) say, then \\(\\textbf{W}\\) is essentially known. In situations where we apply GLS estimation, \\(\\textbf{V}\\) is often not known and depends itself on parameters. The overall model then composes a model for the mean function that depends on \\(\\boldsymbol{\\beta}\\) and a model for the error structure that depends on \\(\\boldsymbol{\\theta}\\):\n\\[\\begin{align*}\n    \\textbf{Y}&= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\\\\n    \\boldsymbol{\\epsilon}& \\sim (\\textbf{0}, \\textbf{V}(\\boldsymbol{\\theta}))\n\\end{align*}\\]\nand both sets of parameters must be derived from the data. This is somewhat of a cat-and-mouse game. You need to know \\(\\boldsymbol{\\beta}\\) to estimate \\(\\boldsymbol{\\theta}\\) and the estimates of \\(\\boldsymbol{\\theta}\\) depend on \\(\\boldsymbol{\\beta}\\). This tension is resolved by the estimated generalized least squares principle. Given an estimate of \\(\\boldsymbol{\\theta}\\), you compute the estimated GLS estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{EGLS} =  (\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{Y}\n\\]\nWith an updated estimate of \\(\\boldsymbol{\\beta}\\) you use a different estimation principle to compute an updated estimate of \\(\\boldsymbol{\\theta}\\). This is the principle behind restricted maximum likelihood, a likelihood-based estimation principle important for mixed models.\n\n\nNonlinear Least Squares\nThe linear structure of the model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) leads to a closed form solution of the least squares problem \\[\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\\]\nWhen the model is nonlinear in the parameters, \\(\\textbf{Y}= \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta}) + \\boldsymbol{\\epsilon}\\), finding the solution that minimizes\n\\[\n\\text{SSE} = \\left(\\textbf{Y}- \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\right)^\\prime \\left(\\textbf{Y}- \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\right)\n\\tag{4.1}\\]\nis not so straightforward, it requires an iterative approach. Starting from some initial guess for \\(\\boldsymbol{\\theta}\\), call it \\(\\widehat{\\boldsymbol{\\theta}}^{(0)}\\), we iteratively update the guess until we have arrived at step \\(t\\) at \\(\\widehat{\\boldsymbol{\\theta}}^{(t)}\\) such that \\[\n\\frac{\\partial \\,\\text{SSE}}{\\partial\\boldsymbol{\\theta}} \\lvert_{\\widehat{\\boldsymbol{\\theta}}^{(t)}} = \\textbf{0}\n\\] The left hand side of the previous expression is read as the derivative of SSE with respect to \\(\\boldsymbol{\\theta}\\), evaluated at \\(\\widehat{\\boldsymbol{\\theta}}^{t}\\).\nCommon iterative approaches to solve this optimization problem involve the Gauss-Newton and Newton-Raphson algorithms. We introduce the Gauss-Newton method here. The basic idea is that we can approximate the nonlinear mean function with a linear version that depends on some current values for \\(\\boldsymbol{\\theta}\\). Linear least squares can be applied to the linearized form to compute an update of the estimate for \\(\\boldsymbol{\\theta}\\). With the updated estimate the approximation can be refined and another least squares step is performed. This sequence is repeated until some convergence criterion is met.\nWe start by approximating \\(\\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\) with a first-order Taylor series about \\(\\boldsymbol{\\theta}^{(0)}\\) \\[\n    \\textbf{f}(\\textbf{x}; \\boldsymbol{\\theta}) \\approx \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta}^{(0)}) + \\textbf{F}^{(0)}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{(0)})\n\\] where \\(\\textbf{F}^{(0)}\\) is a matrix of derivatives of \\(\\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\) evaluated at the value \\(\\boldsymbol{\\theta}^{(0)}\\). The residual \\(\\textbf{y}- \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta})\\) can now be approximated as \\(\\textbf{r}(\\boldsymbol{\\theta}^{(0)}) = \\textbf{y}- \\textbf{f}(\\textbf{x};\\boldsymbol{\\theta}^{(0)}) - \\textbf{F}^{(0)}(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})\\). Substitute this expression into Equation 4.1 we get an approximate error sums of squares\n\\[\n\\text{SSE} \\approx \\textbf{r}(\\boldsymbol{\\theta}^{(0)})^\\prime \\textbf{r}(\\boldsymbol{\\theta}^{(0)}) - 2 \\textbf{r}(\\boldsymbol{\\theta}^{(0)})\\textbf{F}^{(0)}(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)}) +\n(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})^\\prime \\textbf{F}^{(0)\\prime}\\textbf{F}^{(0)} (\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})\n\\] Taking derivatives with respect to \\(\\boldsymbol{\\theta}\\) and setting to zero results in the following condition \\[\n(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)}) = \\left(\\textbf{F}^{(0)\\prime}\\textbf{F}^{(0)}\\right)^{-1}\\textbf{F}^{(0)\\prime}\\textbf{r}(\\boldsymbol{\\theta}^{(0)})\n\\] This is a really interesting expression. Imagine replacing on the right hand side \\(\\textbf{F}^{(0)}\\) with \\(\\textbf{X}\\) and \\(\\textbf{r}(\\boldsymbol{\\theta}^{(0)})\\) with \\(\\textbf{y}\\). The right hand side is an ordinary least squares solution in a linear model where the \\(x\\)-matrix is given by the derivatives of the nonlinear model and the target variable is the difference between the actual \\(y\\)-values and the approximated mean function. The left hand side of the equation is the difference between the parameter estimate and our current guess. This suggests the following iterative updates \\[\n    \\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} + \\left(\\textbf{F}^{(t)\\prime}\\textbf{F}^{(t)}\\right)^{-1}\\textbf{F}^{(t)\\prime}\\textbf{r}(\\boldsymbol{\\theta}^{(t)})\n\\]\nand is known as the Gauss-Newton algorithm.\n\nGauss-Newton from scratch\nJust like with the OLS problem, we are going to implement a nonlinear least squares solution from scratch, based on the linear algebra presented in this section. The nonlinear model chosen for this exercise has two parameters, \\(\\boldsymbol{\\theta}= [\\theta_1, \\theta_2]\\), one input variable \\(x\\), and mean function \\[\n    f(x; \\boldsymbol{\\theta}) = 1 - \\theta_1 \\exp \\{-x^{\\theta_2}\\}\n\\] We fit this model to a tiny data set with just five observations\n\ny &lt;- c(0.1, 0.4, 0.6, 0.9)\nx &lt;- c(0.2, 0.5, 0.7, 1.8)\n\nThe derivatives of the mean function with respect to the parameters are given by\n\\[\\begin{align*}\n  \\frac{\\partial f(x;\\boldsymbol{\\theta})}{\\partial \\theta_1} &= -\\exp\\{-x^{\\theta_2}\\} \\\\\n  \\frac{\\partial f(x;\\boldsymbol{\\theta})}{\\partial \\theta_2} &= \\theta_1 \\log(x) x^{\\theta_2} \\exp\\{-x^{\\theta_2}\\}\n\\end{align*}\\]\nand we can write a simple R function to compute the derivative matrix \\(\\textbf{F}\\) based on a current estimate of \\(\\boldsymbol{\\theta}\\). The function getres computes \\(\\textbf{r}(\\boldsymbol{\\theta}^{(t)})\\).\n\ngetF &lt;- function(x, theta) {\n    exterm &lt;- exp(-x^theta[2])\n    der1 &lt;- -exterm\n    der2 &lt;- theta[1] * log(x) * (x^theta[2]) * exterm\n    return(cbind(der1,der2))\n}\n\ngetres &lt;- function(y,x,theta) {\n    fx &lt;- 1 - theta[1] * exp(-x^theta[2])\n    return(y - fx)\n}\n\nNow all we need is starting values \\(\\boldsymbol{\\theta}^{(0)}\\) and a loop that iterates until the estimation routine has converged. It is a good idea to not take a full Gauss-Newton step in the updates since there is no guarantee that SSE is lower at iterate \\(t+1\\) than at iterate \\(t\\). We thus multiply the update to the current value by the hyperparameter \\(\\alpha &lt; 1\\). In machine learning, this step size is known as the learning rate. A full implementation of the Gauss-Newton algorithm could determine \\(\\alpha\\) dynamically at each iteration through a line search algorithm.\n\nmaxiter &lt;- 50          # the max number of iterations\nalpha &lt;- 0.5           # the learning rate\ntheta &lt;- c(1, 1.3)     # the starting values\ntol &lt;- 1e-6            # the convergence tolerance\n\nfor (iter in 1:maxiter) {\n    X &lt;- getF(x,theta)\n    r &lt;- getres(y,x,theta)\n    \n    # The linear least squares update\n    new_theta &lt;- theta + alpha * (solve(t(X) %*% X) %*% t(X) %*% r)\n\n    # Now we check a convergence criterion. We take the maximum relative\n    # change in the parameter estimates. If that is less than some tolerance\n    # the algorithm is considered converged.\n    crit &lt;- max(abs(new_theta-theta)/abs(theta))\n    if (crit &lt; tol) {\n        cat( \"Algorithm converged after\", iter,\" iterations! SSE =\", sum(r^2), \"\\n\" )\n        print (new_theta)\n        break\n    } else {\n        theta &lt;- new_theta\n        print (crit)\n    }\n}\n\n[1] 0.03047302\n[1] 0.01661031\n[1] 0.008692942\n[1] 0.004446789\n[1] 0.002248043\n[1] 0.001129919\n[1] 0.000566354\n[1] 0.0002835054\n[1] 0.0001418301\n[1] 7.093331e-05\n[1] 3.547098e-05\n[1] 1.773652e-05\n[1] 8.868505e-06\n[1] 4.434311e-06\n[1] 2.21717e-06\n[1] 1.108588e-06\nAlgorithm converged after 17  iterations! SSE = 0.01569114 \n          [,1]\nder1 0.9366938\nder2 1.2791919\n\n\nAfter 17 iterations with a learning rate (step size) of \\(\\alpha=0.5\\) the algorithm converged on parameter estimates \\(\\widehat{\\theta}_1\\) = 0.9366 and \\(\\widehat{\\theta}_2\\) = 1.2791.\nWe can validate these results with the nls function from the nls2 package.\n\nlibrary(nls2)\n\nLoading required package: proto\n\nnonlin_data &lt;- data.frame(cbind(y=y,x=x))\nf_x &lt;- y ~ 1 - theta1 * exp(-x^theta2)\nnls(f_x, \n    start=list(theta1=1, theta2=1.3), \n    data=nonlin_data)\n\nNonlinear regression model\n  model: y ~ 1 - theta1 * exp(-x^theta2)\n   data: nonlin_data\ntheta1 theta2 \n0.9367 1.2792 \n residual sum-of-squares: 0.01569\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 6.109e-06",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#maximum-likelihood-estimation",
    "href": "estimation.html#maximum-likelihood-estimation",
    "title": "4  Parameter Estimation",
    "section": "4.3 Maximum Likelihood Estimation",
    "text": "4.3 Maximum Likelihood Estimation\nMaximum likelihood estimation (MLE) is an intuitive and important estimation principle in statistics. It is based on the distributional properties of the data, hence it applies to stochastic data modeling. If the observed data \\(\\textbf{y}\\) are the realization of a data-generating random mechanism, then it makes sense to examine the probability distribution of the data and choose as parameter estimates those values that make it most likely to have observed the data. In other words, we use the probability distribution to find the most likely explanation for the data–hence the name maximum likelihood.\nMaking progress with MLE requires that we know the joint distribution of the random vector \\(\\textbf{Y}\\), an \\(n\\)-dimensional distribution. The distribution depends on a vector \\(\\boldsymbol{\\theta}\\) of unknown parameters and we denote it as \\(f(\\textbf{y}; \\boldsymbol{\\theta})\\).\n\n\n\n\n\n\nNote\n\n\n\nWhenever you calculate a maximum likelihood estimator, you are making assumptions about the distribution of the data. If software packages report MLEs, check the documentation regarding distributional assumptions.\n\n\nWhen the observations are independent, the joint density is the product of the individual densities, \\[f(\\textbf{y}; \\boldsymbol{\\theta}) = \\prod_{i=1}^n \\, f(y_i; \\boldsymbol{\\theta})\\]\nFor example, the joint mass function of \\(n\\) iid Bernoulli(\\(\\pi\\)) random variables is \\[\n    f(\\textbf{y}; \\pi) = \\prod_{i=1}^n \\, \\pi^{y_i} \\, (1-\\pi)^{1-y_i}\n\\]\nThe likelihood function is the joint density or mass function of the data, but we interpret it as a function of the parameters evaluated at the data, whereas the density (mass) function is a function of the data evaluated at the parameter values. The log-likelihood function is the natural log of the likelihood function, denoted \\(\\mathcal{l}(\\boldsymbol{\\theta}; \\textbf{y})\\). The parameters that maximize the likelihood function also maximize the log of the function. Logarithms are much easier to work with since they turn products into summations and exponents into multipliers.\n\n\nExample: Likelihood Function for Poisson Data\n\n\nIf \\(Y_1, \\cdots, Y_n\\) are a random sample from a Poisson(\\(\\lambda\\)) distribution, the log-likelihood function is\n\\[\n    \\mathcal{l}(\\lambda; \\textbf{y}) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right ) =\n    \\log(\\lambda)\\sum_{i=1}^n y_i - n\\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\] Setting the derivative with respect to \\(\\lambda\\) to zero yields \\[\n    \\frac{1}{\\lambda}\\sum_{i=1}^n y_i = n\n\\] The MLE of \\(\\lambda\\) is \\(\\widehat{\\lambda} = \\overline{y}\\), the sample mean.\nSuppose that \\(n=4\\) and we observe \\(\\textbf{y}= [3, 4, 2, 2]\\). The sample mean is \\(\\overline{y} = 2.75\\). The following R code computes the log-likelihood function \\(\\mathcal{l}(\\lambda; \\textbf{y})\\) for different values of \\(\\lambda\\). The log-likelihood function has a maximum at \\(\\overline{y} = 2.75\\).\n\ny &lt;- c(3, 4, 2, 2)\nn &lt;- length(y)\nsumy &lt;- sum(y)\nsumlogfac &lt;- sum(log(factorial(y)))\nlambda &lt;- seq(0.1, 5, 0.1)\nloglike &lt;- log(lambda)*sumy - n*lambda - sumlogfac\n\nplot(lambda,loglike,type=\"l\",bty=\"l\",lwd=1.5,\n     xlab=expression(lambda),\n     ylab=\"log likelihood\")\nabline(v=mean(y),lty=\"dotted\",lwd=1.5,col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nExample: MLE for iid Bernoulli Experiments\n\n\nTo find the maximum likelihood estimator of \\(\\pi\\) in \\(n\\) iid Bernoulli(\\(\\pi\\)) experiments, we need to find the value \\(\\widehat{\\pi}\\) that maximizes the log-likelihood function\n\\[\\begin{align*}\n    \\mathcal{l}(\\pi; \\textbf{y}) &= \\log\\left(\\prod_{i=1}^n \\, \\pi^{y_i} \\, (1-\\pi)^{1-y_i}\\right) \\\\\n                          &= \\sum_{i=1}^n \\, y_i\\log(\\pi) + (1-y_i)\\log(1-\\pi)\n\\end{align*}\\]\n\\(\\mathcal{l}(\\pi; \\textbf{y}) = \\sum_i y_i\\log(\\pi) + (1-y_i)\\log(1-\\pi)\\). The derivative with respect to \\(\\pi\\) is\n\\[\\begin{align*}\n    \\frac{\\partial \\mathcal{l}(\\pi; \\textbf{y})}{\\partial \\pi} &= \\frac{1}{\\pi}\\sum_{i=1}^n y_i - \\frac{1}{1-\\pi}\\sum_{i=1}^n(1-y_i) \\\\\n    &= \\frac{1}{\\pi} n\\overline{y} - \\frac{1}{1-\\pi}(n - n\\overline{y})\n\\end{align*}\\]\nSetting the derivative to zero and rearranging terms we get\n\\[\\begin{align*}\n    \\frac{1-\\widehat{\\pi}}{\\widehat{\\pi}} &= \\frac{n-n\\overline{y}}{n\\overline{y}} \\\\\n    \\frac{1}{\\widehat{\\pi}} &= \\frac{n - n\\overline{y}}{n\\overline{y}} + 1 \\\\\n    \\widehat{\\pi} &= \\overline{y}\n\\end{align*}\\]\nThe MLE of \\(\\pi\\) is the sample mean.\n\n\nMaximum likelihood estimation is popular because it is an intuitive principle if we accept a random data-generating mechanism. MLEs have very appealing properties, for example, they are invariant estimators. If \\(\\widehat{\\theta}\\) is the MLE of \\(\\theta\\), then \\(g(\\widehat{\\theta})\\) is the maximum likelihood estimator of \\(g(\\theta)\\)\n\n\nExample: MLEs of Confidence Intervals\n\n\nIn generalized linear models, a linear predictor is related to a transformation of the mean through the link function \\[\n    g(\\mu) = \\eta = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_p x_p\n\\] For example, if the data are Poisson random variables, \\(g(\\cdot)\\) is typically the log function (the log link). The coefficient estimates have a linear interpretation on the logarithmic scale. Suppose \\(\\boldsymbol{\\beta}\\) is estimated by maximum likelihood and is used to construct a 95% confidence interval \\([\\widehat{\\eta}_{.025},\\widehat{\\eta}_{.975}]\\) for \\(\\eta\\).\nYou can transform from \\(\\eta\\) to \\(\\mu\\) by inverting the link function, \\(\\mu = g^{-1}(\\mu)\\). Thus, \\[\n    \\left[ \\exp\\{\\widehat{\\eta}_{.025}\\}, \\exp\\{\\widehat{\\eta}_{.975}\\} \\right]\n\\] is a 95% confidence interval for \\(\\mu\\).\n\n\nFor a finite sample size, MLEs are not necessarily optimal estimators but they have appealing properties as the sample size grows. As \\(n \\rightarrow \\infty\\), maximum likelihood estimators\n\nare consistent, that means they converge in probability to the true value\nare normally distributed\nare efficient, that means no other estimator has asymptotically smaller mean squared error\n\n\nLinear Model with Gaussian Errors\nSuppose we want to find an estimator for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) where the model errors follow a Gaussian distribution with mean \\(\\textbf{0}\\) and variance \\(\\textbf{V}\\). \\(\\textbf{Y}\\) then follows a Gaussian distribution because it is a linear function of \\(\\boldsymbol{\\epsilon}\\) (see Section 3.7). The probability density function of \\(\\textbf{Y}\\) is\n\\[f\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\\]\nThis joint distribution of the data can be used to derive the maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\beta}\\). Maximum likelihood estimation considers this as a function of \\(\\boldsymbol{\\beta}\\) rather than a function of \\(\\textbf{Y}.\\) Maximizing this likelihood function \\(\\mathcal{l}(\\boldsymbol{\\beta};\\textbf{Y})\\) is equivalent to maximizing its logarithm and working on the log scale is much simpler. The log-likelihood function for this problem is given by\n\\(\\log\\mathcal{l}\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right\\} = l\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right) = - \\frac{1}{2}\\log\\left( \\left| \\textbf{V}\\right| \\right) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\\)\nFinding the maximum of this function with respect to \\(\\boldsymbol{\\beta}\\) is equivalent to minimizing the quadratic form \\[\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{-1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\\] with respect to \\(\\boldsymbol{\\beta}\\). Applying the results about matrix differentiation from Section 3.5 leads to\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) &= \\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left\\{ \\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{Y}- 2\\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\right\\} \\\\\n\n  &= - 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\\]\nThe derivative is zero when \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\).\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\) is non-singular and its inverse exists. Pre-multiplying both sides of the equation with that inverse yields the solution\n\\[\\begin{align*}\n    \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\\\\n\n    \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= {\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\\\\n\n    \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\widehat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\nThe maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\) is the generalized least squares estimator\n\\[\\widehat{\\boldsymbol{\\beta}}_{GLS} = \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}\\]\nA special case arises when the model errors \\(\\boldsymbol{\\epsilon}\\) are uncorrelated and the variance matrix \\(\\textbf{V}\\) is a diagonal matrix:\n\\[\\textbf{V}= \\begin{bmatrix}\n\\sigma_{1}^{2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_{n}^{2}\n\\end{bmatrix}\\]\nSince the errors are Gaussian distributed, we know that the errors are then independent. The MLE of \\(\\boldsymbol{\\beta}\\) is the weighted least squares estimator\n\\[\n\\widehat{\\boldsymbol{\\beta}}_{WLS} = \\left(\\textbf{X}^\\prime\\textbf{W}\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{W}\\textbf{Y}\n\\] where \\(\\textbf{W}= \\textbf{V}^{-1}\\).\nA further special case arises when the diagonal entries are all the same,\n\\[\\textbf{V}= \\begin{bmatrix}\n\\sigma^{2}\\  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma^{2}\n\\end{bmatrix} = \\sigma^{2}\\textbf{I}\\]\nWe can write the error distribution in this case as \\(\\boldsymbol{\\epsilon}\\sim G\\left(\\textbf{0},\\sigma^{2}\\textbf{I}\\right)\\) and the model for \\(\\textbf{Y}\\) as \\(\\textbf{Y}\\sim G\\left( \\textbf{X}\\boldsymbol{\\beta},\\sigma^{2}\\textbf{I}\\right)\\).\nUnder this iid assumption for the Gaussian linear model we can substitute \\(\\sigma^{2}\\textbf{I}\\) for \\(\\textbf{V}\\) in the formula for \\(\\widehat{\\boldsymbol{\\beta}}\\). The maximum likelihood estimator for \\(\\boldsymbol{\\beta}\\) is the ordinary least squares estimator:\n\\[\\widehat{\\boldsymbol{\\beta}}_{OLS} = \\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{Y}\\]\nNotice that \\(\\sigma^{2}\\) cancels out of the formula; the value of the OLS estimator does not depend on the intrinsic variability of the data. However, the variability of the OLS estimator does depend on \\(\\sigma^{2}\\) (and on \\(\\textbf{X}\\)).\nNow that we have found the MLE of \\(\\boldsymbol{\\beta}\\), the parameters in the mean function, we can also derive the MLE of the parameters in the variance-covariance structure of the multi-variate Gaussian. Let’s do this for the iid case, \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\). The joint density of the data can then be written as \\[\n    f(\\textbf{Y}) = \\prod_{i=1}^n (2 \\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\beta})^2\\right\\}\n\\] Taking logs and arranging terms, the log-likelihood function for \\(\\sigma^2\\) is \\[\n\\mathcal{l}(\\sigma^2; \\textbf{y}) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\textbf{x}_i\\prime\\boldsymbol{\\beta})^2\n\\]\nTaking the derivative of \\(\\mathcal{l}(\\sigma^2; \\textbf{y})\\) with respect to \\(\\sigma^2\\), setting it to zero and arranging terms results in \\[\n    \\frac{1}{\\sigma^4}\\sum_{i=1}^n(y_i - \\textbf{x}_i\\prime\\boldsymbol{\\beta})^2 = \\frac{n}{\\sigma^2}\n\\] The MLE of \\(\\sigma^2\\) in this case is \\[\n    \\widehat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\textbf{x}_i\\prime\\boldsymbol{\\beta})^2\n\\] The MLE of \\(\\sigma^2\\) looks similar to the estimator we used in the OLS case, \\[\n    \\frac{1}{n-r(\\textbf{X})} \\sum_{i=1}^n(y_i - \\textbf{x}_i\\prime \\widehat{\\boldsymbol{\\beta}})^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\n\\] with two important differences. The divisor in the MLE is \\(n\\) instead of \\(n-r(\\textbf{X})\\) and the MLE uses \\(\\boldsymbol{\\beta}\\) rather than the OLS estimator \\(\\widehat{\\boldsymbol{\\beta}}\\) in the sum-of-squares term. In practice, we would substitute the MLE for \\(\\boldsymbol{\\beta}\\) to compute \\(\\widehat{\\sigma}^2\\), so that the least squares-based estimator and the maximum likelihood estimator differ only in the divisor. Consequently, they cannot be both unbiased estimators of \\(\\sigma^2\\). Which should we choose?\nWe can think of the divisor \\(n-r(\\textbf{X})\\) as accounting for the actual degrees of freedom, the amount of information if you will, in the estimator. Since we are using an estimate of \\(\\boldsymbol{\\beta}\\) to compute SSE, we have “used up” information in the amount of the rank of \\(\\textbf{X}\\). This is the same rationale that computes the estimate of the sample variance as \\[\n    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(y_i - \\overline{y}\\right)^2\n\\] because we are using the sample mean \\(\\overline{y}\\) in the computation rather than the true mean. Once the sample mean is known, only \\((n-1)\\) of the \\(y_i\\) can be chosen freely, the value of the last one is determined.\nThe reason the MLE divides by \\(n\\) instead of \\(n-r(\\textbf{X})\\) is that it uses \\(\\boldsymbol{\\beta}\\) and does not need to account for information already “used up” in the estimation of the mean function. If you substitute \\(\\widehat{\\boldsymbol{\\beta}}\\) for \\(\\boldsymbol{\\beta}\\), the MLE of \\(\\sigma^2\\) is a biased estimator.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "learningtypes.html",
    "href": "learningtypes.html",
    "title": "5  Types of Statistical Learning",
    "section": "",
    "text": "5.1 Supervised Learning\nSupervised learning is characterized by the presence of a target variable, also called a dependent variable, response variable, or output variable. This is the attribute we wish to model. The training and test data sets contain values for the target variable, in machine learning these values are often called the labels and are described as the “ground truth”. All other variables in the data set are potentially input variables. In short, we know the values of the target variable, now we need to use it in analytical methods to learn how outputs and inputs connect (Figure 5.1).\nThe goals of supervised learning can be to\nStudies can pursue one or more of these goals. For example, you might be interested in understanding the relationship between target and input variables and use that relationship for predictions and/or to test hypotheses.\nThe name supervised learning comes from thinking of learning in an environment that is supervised by a teacher. The teacher asks questions for which they know the correct answer (the ground truth) and judge a student’s response to the questions. The goal is to increase the students knowledge as measured by the quality of their answers. But we do not want students to just memorize answers, we want to teach them to be problem solvers, to apply the knowledge to new problems, to generalize.\nThe parallel between the description of supervised learning in a classroom and training an algorithm on data is obvious: the problems asked by the teacher, the learning algorithm, are the data points, \\(Y\\) is the correct answer, the inputs \\(x_1,\\cdots,x_p\\) are the information used by the students to answer the question. The discrepancy between question and answer is measured by \\((y - \\widehat{y})^2 = (y - \\widehat{f}\\left( x_1,\\cdots x_p \\right))^2\\) or some other error metric. The training of the model stops when we found a model that generalizes well to previously unseen problems. We are not interested in models that follow the observed data too closely.\nHere is a non-exhaustive list of algorithms and models you find in supervised learning.\nThere is a lot to choose from, and for good reason. The predominant application of data analytics is supervised learning with batch (or mini-batch) data. In batch data analysis the data already exist as a historical data source in one place. We can read all records at once or in segments (called mini-batches). If we have to read the data multiple times, for example, because an iterative algorithm passes through the data at each iteration, we can do so.\nBatch-oriented learning contrasts with online learning where the data on which the model is trained is generated and consumed in real time.",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Statistical Learning</span>"
    ]
  },
  {
    "objectID": "learningtypes.html#supervised-learning",
    "href": "learningtypes.html#supervised-learning",
    "title": "5  Types of Statistical Learning",
    "section": "",
    "text": "Definition: Supervised Learning\n\n\nSupervised learning trains statistical learning models through a target variable.\n\n\n\n\n\nPredict the target variable from input variables.\nDevelop a function that approximates the underlying relationship between inputs and outputs.\nUnderstand the relationship between inputs and outputs.\nClassify observations into categories of the target variable based on the input variables.\nGroup the observations into sets of similar data based on the values of the target variable and based on values of the inputs.\nReduce the dimensionality of the problem by transforming target and inputs from a high-dimensional to a lower-dimensional space.\nTest hypotheses about the target variable.\n\n\n\n\n\n\nA sampling of supervised learning methods.\n\n\n\n\n\n\n\nLinear regression\nNonlinear regression\nRegularized regression\n(Lasso, Ridge, Elastic nets)\n\n\nLocal polynomial regression (LOESS)\nSmoothing splines\nKernel methods\n\n\nLogistic regression (binary & binomial)\nMultinomial regression (nominal and ordinal)\nPoisson regression (counts and rates)\n\n\nDecision trees\nRandom forests\nBagged trees\n\n\nAdaptive boosting\nGradient boosting machine\nExtreme gradient boosting\n\n\nNaïve Bayes classifier\nNearest-neighbor methods\nDiscriminant analysis (linear and quadratic)\n\n\nPrincipal component regression\nPartial least squares\nGeneralized linear models\n\n\nGeneralized additive models\nMixed models (linear and nonlinear)\nModels for correlated data (spatial, time series)\n\n\nSupport-vector machines\nNeural networks\nExtreme gradient boosting",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Statistical Learning</span>"
    ]
  },
  {
    "objectID": "learningtypes.html#unsupervised-learning",
    "href": "learningtypes.html#unsupervised-learning",
    "title": "5  Types of Statistical Learning",
    "section": "5.2 Unsupervised Learning",
    "text": "5.2 Unsupervised Learning\n\n\nDefinition: Unsupervised Learning\n\n\nIn unsupervised learning methods a target variable is not present.\n\n\nUnsupervised learning does not utilize a target variable; hence it cannot predict or classify observations. However, we are still interested in discovering structure, patterns, and relationships in the data.\nThe term unsupervised refers to the fact that we no longer know the ground truth because there is no target variable. Hence the concept of a teacher who knows the correct answers and supervises the learning progress of the student does not apply. In unsupervised learning there are no clear error metrics by which to judge the quality of an analysis, which explains the proliferation of unsupervised methods and the reliance on heuristics. For example, a 5-means cluster analysis will find five groups of observations in the data, whether this is the correct number or not, and it is up to us to interpret what differentiates the groups and to assign group labels. A hierarchical cluster analysis will organize the data hierarchically, whether that makes sense or not.\nOften, unsupervised learning is used in an exploratory fashion, improving our understanding of the joint distributional properties of the data and the relationships in the data. The findings then help lead us toward supervised approaches.\nA coarse categorization of unsupervised learning techniques also hints at their application:\n\nAssociation analysis: which values of the variables \\(x_{1},\\cdots,x_{p}\\) tend to occur together in the data? An application is market basket analysis, where the \\(X\\)s are items are in a shopping cart (or a basket in the market), and \\(x_{i} = 1\\) if the \\(i\\)th item is present in the basket and \\(x_{i} = 0\\) if the item is absent. If items frequently appear together, bread and butter, or beer and chips, for example, then maybe they should be located close together in the store. Association analysis is also useful to build recommender systems: shoppers who bought this item also bought the following items \nCluster analysis: can data be grouped based on \\(x_{1},\\cdots,x_{p}\\) into sets such that the observations within a set are more similar to each other than they are to observations in other sets? Applications of clustering include grouping customers into segments. Segmentation analysis is behind loyalty programs, lower APRs for customers with good credit rating, and churn models.\nDimension reduction: can we transform the inputs \\(x_{1},\\cdots,x_{p}\\) into a set \\(c_{1},\\cdots,c_{k}\\), where \\(k \\ll p\\) without losing relevant information? Applications of dimension reduction are in high-dimensional problems where the number of inputs is large relative to the number of observations. In problems with wide data, the number of inputs \\(p\\) can be much larger than \\(n\\), which eliminates many traditional methods of analysis from consideration.\n\nMethods of unsupervised learning often precede supervised learning; the output of an unsupervised learning method can serve as the input to a supervised method. An example is dimension reduction through principal component analysis (PCA, Chapter 23) prior to supervised regression. This technique is known as principal component regression (PCR, Section 8.3.2).\nSuppose you have \\(n\\) observations on a target variable \\(Y\\) and a large number of potential inputs \\(X_1,\\cdots,X_p\\) where \\(p\\) is large relative to \\(n\\). PCA computes linear combinations of the \\(p\\) inputs that account for decreasing amounts of variability among the \\(X\\)s. These linear combinations are called the principal components. For example, the first principal component explains 70% of the variability in the inputs, the second principal component explains 20% and the third principal component 5%. Rather than building a regression model with \\(p\\) predictors, we might use only the first three principal components as inputs in the regression model. The PCA is an unsupervised model because it does not use information about \\(Y\\) in forming the principal components. The PCR is a supervised method, we want to model the mean of \\(Y\\) as a function of the \\(X\\)s. If \\(p = 250\\), using the first three principal components replaces\n\\[Y = \\beta_{0} + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\cdots + \\beta_{250} x_{250} + \\epsilon\\]\nwith\n\\[Y = \\theta_0 + \\theta_1 z_1+ \\theta_2 z_2 + \\theta_3 z_3 + \\epsilon\\]\nwhere \\(z_1\\) denotes the first principal component, itself a linear combination of the 250 inputs\n\\[z_{1} = \\phi_1 x_1 + \\phi_2 x_2 + \\phi_3 x_3 + \\cdots + \\phi_{250} x_{250}\\] \\(\\text{E}[Y]\\) is still a function of all 250 \\(X\\)s, but indirectly so as the \\(Xs\\) are represented by linear combinations.\n\n\n\nFigure 5.1: An overview of statistical learning approaches",
    "crumbs": [
      "Part I. Foundation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Statistical Learning</span>"
    ]
  },
  {
    "objectID": "regintro.html",
    "href": "regintro.html",
    "title": "6  Introduction",
    "section": "",
    "text": "6.1 Adding Structure\nThe regression task can be described very generally as estimating the mean function of a target random variable as a function of input variables that are considered fixed (not random variables). Formally, suppose that we have \\(n\\) pairs of observations \\((y_i, \\textbf{x}_i)\\), \\(i=1,\\cdots, n\\). \\(y_i\\) is the observed value for the target variable of the \\(i\\)th observation and is considered a realization of the random variable \\(Y_i\\). \\(\\textbf{x}_i = [x_{i1}, \\cdots, x_{ip}]\\) is the vector of input variables for that observation. The regression task then is to find the function \\(f^*(\\textbf{x}_i)\\) for which \\[\n\\text{E}[Y_i | \\textbf{x}_i] = f^*(\\textbf{x}_i)\n\\]\nThe function on the right hand side is denoted with an asterisk because it is unknown. We also did not specify what \\(f^*\\) looks like or how it relates to model parameters such as the regression coefficients in a linear regression model. The problem is not well defined yet, there are infinitely many solutions. Any function that interpolates the \\(y_i\\) data points is a candidate for \\(f^*\\), but that is not a very interesting model.\nSince \\(Y_i\\) is a random variable, and \\(f^*(\\textbf{x}_i)\\) expresses how it mean varies as a function of the input variables, we are not interested in models that reproduce the noisy version of the data. We are interested in finding the smooth signal in the noise (Figure 6.1). Finding the appropriate degree of smoothness in the mean function is an important part of regression analysis by balancing the bias-variance tradeoff (Chapter 2).",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regintro.html#adding-structure",
    "href": "regintro.html#adding-structure",
    "title": "6  Introduction",
    "section": "",
    "text": "Note\n\n\n\nWe did not write this model as a regression model with additive errors, that is, \\(Y_i = f^*(\\textbf{x}_i) + \\epsilon_i\\). This is a particular class of regression models useful when \\(Y\\) is continuous. If you write the regression model in terms of an expected value, the model is more general and applies to categorical target variables as well.\n\n\n\n\n\n\n\n\n\nFigure 6.1: Finding the appropriate degree of smoothness in regression.\n\n\n\n\n\nConstraints on Model Structure\nBut we cannot just consider bias and variance among all possible functions \\(f^*\\), we need to add further structure to make solutions tractable. Adding structure means imposing constraints on \\(f^*\\) so that we can find a (hopefully unique) solution within the class of constraints. For example, we can structure the problem by insisting that only global functions with a linear predictor are considered.\nConsider the case of predicting the mean of the target variable at some location \\(\\textbf{x}_0\\) in the input space. \\(\\textbf{x}_0\\) can be a one of the observed data points or a new value. A global model is one that applies regardless of the value of \\(\\textbf{x}_0\\). A local model on the other hand applies a different model depending on the value of \\(\\textbf{x}_0\\).\nIn Section 1.2 the concept of the linear predictor was introduced:\n\\[\\eta = \\beta_{0} + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\]\nThe constraints—global model with linear predictor—are adding a lot of structure to the regression problem. We are now looking for mean functions of the following form\n\\[\n\\text{E}[Y_i | \\textbf{x}_i] = g(\\eta) = g(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip})\n\\]\nThe function \\(g()\\) is a transformation of the linear predictor, a map between the linear effects of the inputs and the mean of the target variable. Adding a further constraint, that \\(g()\\) is the identity function, leads to the linear regression model\n\\[\n\\text{E}[Y_i | \\textbf{x}_i] = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\nIn this special case we can write the model for \\(Y_i\\) with an additive error term\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i1}+\\cdots + \\beta_p x_{ip} + \\epsilon_i\n\\]\nwhere \\(\\text{E}[\\epsilon_i] = 0\\).\nFor many this is “the classical” or “the standard” or simply “the” regression model. It is just one form of structuring the regression problem, and a highly structured form at that. We are not done, however. One could insist on only a single predictor variable, leading to the simple linear regression (SLR) model\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i} + \\epsilon_i\n\\]\nor on input variables that are successive powers of a single input, the polynomial regression model\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + \\cdots + \\epsilon_i\n\\]\nFigure 6.2 categorizes the approaches to structuring the regression problem. The classical regression models (SLR/MLR) are just one of many ways to tackle estimating a mean function.\n\n\n\n\n\n\nFigure 6.2: Different approaches to structuring the regression problem.\n\n\n\n\n\nConstraints on Estimation\nImposing constraints on the solution to the regression problem can also be done through the estimation method itself. Take, for example, the linear regression model \\(y_i = \\textbf{x}_i^\\prime\\boldsymbol{\\beta}+ \\epsilon_i\\). The ordinary least squares (OLS) estimator satisfies \\[\n\\mathop{\\mathrm{arg\\,min}}_\\boldsymbol{\\beta}\\sum_{i=1}^n \\left(y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\beta}\\right)^2\n\\] The solution is constrained in the sense that it is the orthogonal projection of the data onto the space spanned by the columns of the \\(\\textbf{X}\\) matrix. While OLS estimators are easy to work with and have nice properties, for example they are unbiased if the model is correct, they suffer from large variability in high dimensional problems (large \\(p\\)) and when the \\(x\\)-s are highly interrelated. A solution to that problem lies in adding further constraints on the estimators.\nRidge regression minimizes instead of the OLS criterion the expression \\[\n\\sum_{i=1}^n \\left(y_i - \\textbf{x}_i^\\prime \\boldsymbol{\\beta}\\right)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2\n\\] The additional term (\\(\\lambda\\sum \\beta_j^2\\)) is a roughness (regularization) penalty added to the least-squares criterion. For a given value of the hyperparameter \\(\\lambda\\), large values of the \\(\\beta\\)-coefficients add a greater penalty to the error sum of squares than small values of the coefficients. This has the effect of suppressing the magnitude of the \\(\\beta\\)s, shrinking them toward zero. The Ridge regression estimator is an example of a class of estimators called shrinkage estimators for that reason.\nOne can also apply other penalty terms. Lasso regression (least absolute shrinkage and selection operator) uses absolute values in the regularization penalty \\[\n\\sum_{i=1}^n \\left(y_i - \\textbf{x}_i^\\prime \\boldsymbol{\\beta}\\right)^2 + \\lambda\\sum_{j=1}^p |\\beta_j|\n\\] and elastic net regularization combines the two types of penalties \\[\n\\sum_{i=1}^n \\left(y_i - \\textbf{x}_i^\\prime \\boldsymbol{\\beta}\\right)^2 + \\lambda \\left(\\frac{1-\\alpha}{2}\\sum_{j=1}^p\\beta_j^2 + \\alpha\\sum_{j=1}^p |\\beta_j|\\right)\n\\] In addition to the shrinkage penalty parameter \\(\\lambda\\), the elastic net has a mixing parameter \\(\\alpha\\) that determines the weighting between the Ridge-style and the Lasso-style penalty. \\(\\alpha=0\\) leads to Ridge regression, \\(\\alpha=1\\) leads to Lasso regression. Any value in between represents a mixture of the two.\nLasso is popular because the absolute-value regularization has an interesting side effect: coefficients are shrunk not just toward zero, they can be shrunk to exactly zero. This has the effect of eliminating the associated input variable from the model. In Ridge regression, on the other hand, the shrunk coefficients do not attain exactly zero. Lasso is thus used to regularize the model and to select input variables at the same time. This has also immediate practical advantages. If you start with 100 inputs and apply Lasso regression, you might end up with 10 non-zero coefficient, whereas in Ridge regression you will have 100 non-zero (albeit shrunk) coefficient. To use the fitted model for prediction you need information on only the 10 input variables with non-zero coefficients in the Lasso case. To use the Ridge regression model for prediction you need complete information on all 100 input variables.\nConstrained estimation can also be at work for local models on the right hand side of Figure 6.2. Kernel estimators apply kernel functions that weigh observations depending on their proximity to a target point of prediction. Suppose we have a single input variable \\(x\\). A kernel estimator minimizes \\[\n\\sum_{i=1}^n \\, K_\\lambda(x_0,x_i) (y_i - f(x_i))^2\n\\] \\(K_\\lambda(x_0,x_i)\\) is the kernel function that depends on the target point of prediction (\\(x_0\\)) and some hyperparameter \\(\\lambda\\). Choosing the kernel function and the bandwidth parameter \\(\\lambda\\) affects the smoothness of the predictions.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regintro.html#sec-local-global-models",
    "href": "regintro.html#sec-local-global-models",
    "title": "6  Introduction",
    "section": "6.2 Local and Global Models",
    "text": "6.2 Local and Global Models\nGlobal models structure the estimation problem by adding explicit parameters to the regression function. SLR and MLR are linear regression models where \\(f(\\textbf{x}) = \\eta = \\textbf{x}^\\prime \\boldsymbol{\\beta}\\). Generalized linear models introduce an invertible link function to map between linear predictor and the mean function, \\[f(\\textbf{x}) = g(\\eta) = g(\\textbf{x}^\\prime\\boldsymbol{\\beta})\\]\nIn nonlinear regression models the mean function depends non-linearly on at least one of the parameters, \\(f(\\textbf{x}) = f(x_1,\\cdots,x_k;\\beta_1,\\cdots,\\beta_p)\\). The number of input variables and the number of parameters can be different in nonlinear models. A model might have only one input but many nonlinear parameters.\n\n\nExample: First-order Compartmental Model\n\n\nThe data in the following graph show the concentration of a drug in the body of patients over time. The drug was administered at time 0 at one of two doses. The doses are shown with different colored symbols. After a period of absorption the body begins to eliminate the drug.\n\n\nLoading required package: DBI\n\n\n\n\n\n\n\n\n\nA common pharmacokinetic model for this situation is the first-order compartmental model \\[\n    C(t) = \\frac{D k_a k_e}{C(k_a-k_e)} \\left(\\exp(t\\,k_e) - \\exp(t\\,k_a)\\right)\n\\]\nThe quantities in the compartmental model represent\n\n\\(C\\): the concentration of the drug at time \\(t\\)\n\\(D\\): the administered dose of the drug\n\\(k_a\\): the rate of absorption by the body\n\\(k_e\\): the rate of elimination from the body\n\\(C\\): the clearance (volume of blood or plasma cleared of the drug per time unit)\n\nThis is a nonlinear regression model with two inputs (dose and time) and three parameters (\\(C, k_e, k_a\\)).\n\n\nRather than structuring the model with parameters, the local approaches on the right side of Figure 6.2 are structuring the method of statistical learning, introducing local variations of the model in a neighborhood of a target location \\(\\textbf{x}_0\\) in some form. This is done either explicitly through weight functions that emphasize observations close to \\(\\textbf{x}_0\\) more than remote observations or implicitly through reliance of spline basis expansions. Local models with the exception of \\(k\\)-nearest neighbor regression are referred to as scatterplot smoothers, they typically depend on a single input \\(x\\).\n\n\n\n\n\n\nFigure 6.3: Local linear trends approximate an overall trend\n\n\n\nThe intuition behind local models that rely on weighing is simple. If you are not sure about the global behavior of the mean function across all possible values of \\(x\\), you might be more comfortable specifying the behavior in a neighborhood of \\(x_0\\). A model that is too simple to describe the overall, global, trend might be good enough to describe a trend locally (Figure 6.3). Observations that are near each other in \\(x\\)-space are probably more alike than observations that are far apart, making simple local models that vary its parameters as \\(x\\) takes on different values a logical choice. Figure 6.4 summarizes the important ways in which models are localized.\n\n\n\n\n\n\nFigure 6.4: Techniques by which models can be localized\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can create a local version of any global model by applying a weighted analysis with a weight function that places more emphasis on data points near the prediction location \\(\\textbf{x}_0\\) of interest, and less emphasis on remote points. As you choose different values for \\(\\textbf{x}_0\\) the weights are recalculated and the weighted model is refit. Any class of global models can be localized this way.\n\n\nGeneralized additive models (GAM) are a fascinating class of regression models that combine aspects of global and local models. In a GAM, some model components can be based on purely parametric formulations while other components are scatterplot smoothers.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regintro.html#fitting-a-regression-model",
    "href": "regintro.html#fitting-a-regression-model",
    "title": "6  Introduction",
    "section": "6.3 Fitting a Regression Model",
    "text": "6.3 Fitting a Regression Model\nThe process of fitting a regression model to data can now be described in the following steps:\n\nDecide on how to structure the problem, that is, select the class of restrictions.\nEstimate the parameters by a direct method of estimation and the hyperparameters by a form of cross-validation.\nDiagnose the performance of the model. If not satisfactory, change the model or select a different class of restrictions and return to previous step.\n\nEvery decision in this flow has implications for the bias-variance tradeoff. A good data scientist learns what matters most and where to place the focus of the investigation. For example, in kernel methods it is typically the case that the bandwidth of the kernel function (the hyperparameter \\(\\lambda\\)) has more impact on the bias-variance tradeoff than the selection of the kernel function itself (Epanechnikov vs. Tricube kernel, for example).",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regintro.html#sec-knn-regression",
    "href": "regintro.html#sec-knn-regression",
    "title": "6  Introduction",
    "section": "6.4 \\(k\\)-nearest Neighbor Regression",
    "text": "6.4 \\(k\\)-nearest Neighbor Regression\nOne of the simplest regression techniques is \\(k\\)-nearest neighbor (\\(k\\)-NN) regression. It is a local method but can be made global by choosing \\(k = n\\). The idea is very simple: find the \\(k\\) observation nearest to \\(\\textbf{x}_0\\) according to some distant metric and compute some representative value from them (typically the arithmetic average, but can also use the median, harmonic mean, etc.). \\(k\\)-NN can also be used in classification problems: here we choose as the predicted category the class that is most frequent among the \\(k\\) neighbors.\n\n\nExample: Melanoma Incidences\n\n\nThe following data from the Connecticut Tumor Registry and represent age-adjusted numbers of melanoma incidences per 100,000 people for the 37 years from 1936 to 1972 (Houghton, Flannery, and Viola 1980). These data are used in the Getting Started example of the SAS PROC LOESS documentation.\n\nmelanoma &lt;- dbGetQuery(con, \"SELECT * FROM melanoma\")\n\nhead(melanoma)\n\n  year incidences\n1 1936        0.9\n2 1937        0.8\n3 1938        0.8\n4 1939        1.3\n5 1940        1.4\n6 1941        1.2\n\n\nSuppose we are interested in predicting the mean melanoma incidence at 1957.3 in a 3-NN analysis. The following graph shows the prediction location \\(x_0 = 1957.3\\), the three nearest neighbors, and the predicted incidence as their average (cross symbol).\n\n\n\n\n\n\n\n\n\nThe predictions for two time points and the global prediction \\(\\overline{y}\\) are shown next. Note that the global predicted value is the same as the \\(n\\)-nearest neighbor estimate.\n\n\n\n\n\n\n\n\n\n\n\nThe \\(k\\)-nearest neighbor analysis is a local regression method that shares many of the general features of local methods:\n\nFlexible, general, and intuitive\nCombines fitting and prediction into a single step. To compute a predicted value at \\(x_0\\), the model is fit to the data in the neighborhood of \\(x_0\\).\nInterpolative rather than extrapolative. You cannot extrapolate a local method reliably beyond the range of observed data. At the boundary of the space, the \\(k\\) nearest neighbors are the same data points near the edge of the observed range. The estimate does not change once the most extreme \\(k\\) data points are selected.\n\n\n\nExample: Melanoma Incidences (Cont’d)\n\n\nThe predicted 3-NN values for any time point beyond 1972 are the same, the average of the three observations with the largest \\(x\\)-values. The plot shows the data points that contribute to predictions for 1972 and 1975.\n\n\n\n\n\n\n\n\n\n\n\n\nHave issues at the boundary of the data range when observations contribute only from one direction into the estimation window.\nSuffer from the curse of dimensionality. (See next subsection)\nControl smoothness through hyperparameters, here, \\(k\\). To see the effect of \\(k\\) on the bias-variance tradeoff, we consider the mean squared error of the \\(k\\)-NN estimator. Suppose \\(Y = f(x)+\\epsilon\\), \\(\\epsilon \\sim (0,\\sigma^2)\\), and the \\(k\\)-NN estimator is written \\[\n\\widehat{f}(x_0) = \\frac{1}{k}\\sum_{i=1}^k f(x_{(i)})\n\\] The MSE of \\(\\widehat{f}(x_0)\\) as an estimator of \\(f(x)\\) is \\[\n\\text{E}\\left[(Y-\\widehat{f}(x_0))^2 \\right] = \\frac{\\sigma^2}{k} + \\left(f(x_0) - \\widehat{f}(x_0) \\right)^2 + \\sigma^2\n\\] The first term, \\(\\sigma^2/k\\) is the variance of a sample mean of \\(k\\) observations, the second term is the squared bias component, and the third term is the irreducible error. As \\(k\\) increases the \\(k\\)-NN estimator approaches the sample mean \\(\\overline{Y}\\) with variance \\(\\sigma^2\\n\\). While the variance term gets smaller, the squared bias term increases unless \\(f(x)\\) does not depend on \\(x\\), in which case the sample mean is the optimal estimator. As \\(k\\) decreases, squared bias component decreases but the variance component \\(\\sigma^2/k\\) is large. In the extreme case of \\(k=1\\) the mean squared error becomes \\[\n\\text{E}\\left[(Y-\\widehat{f}(x_0))^2 \\right] = 2 \\sigma^2 + (f(x_0) - f(x_{(1)})^2\n\\]\n\nIn summary: If \\(k\\) is large, the estimator has low variability and large bias; if \\(k\\) is small, the estimator has large variability and small bias. That is the classical bias-variance tradeoff.\n\n\nExample: Melanoma Incidences (Cont’d)\n\n\nThe next graph shows \\(k\\)-NN estimators for different values of \\(k\\) (\\(k =5,10, 20\\)). With increasing \\(k\\) the predictions show less variability and more bias, in particular near the edges.\n\n\n\n\n\n\n\n\n\n\n\nTo resolve the bias-variance tradeoff, the hyperparameter \\(k\\) can be chosen by cross-validation.\n\n\nExample: Melanoma Incidences (Cont’d)\n\n\nThe following statements perform leave-one-out cross-validation by setting the number of cross-validation folds equal to the number of observations (nfolds=nrow(melanoma)) using the knn.cv function in the Rfast library.\n\nkcv &lt;- Rfast::knn.cv(nfolds=nrow(melanoma),\n                     y=melanoma$incidences,\n                     x=as.matrix(melanoma$year),\n                     k=seq(1,20,1),\n                     type=\"R\")\n\nplot(kcv$crit, ylab=\"MSPE\",xlab=\"k\")\nabline(h=min(kcv$crit),col=\"red\",lty=\"dotted\")\nabline(v=which(kcv$crit==min(kcv$crit)),col=\"red\",lty=\"dotted\")\n\n\n\n\n\n\n\n\n\\(k=2\\) is the value for which the nearest-neighbor analysis achieves the lowest LOOCV error for this data set.\n\n\nThe \\(k\\)-NN estimator is a special case of a kernel estimator. Rather than a smooth bell-shaped kernel function that assigned real-valued weights to observations, its kernel is binary, an observation either contributes fully to the estimate or does not contribute at all–depending on whether it is in the neighborhood of \\(\\textbf{x}_0\\). The kernel function for the \\(k\\)-NN estimator can be written as \\[\nK(x_0,x_i) = I(||x_i-x_0|| \\le ||x_{(k)} - x_0 ||)\n\\] where \\(x_{(k)}\\) denotes the observation ranked \\(k\\)th in distance from \\(x_0\\), \\(||x_i-x_0||\\) denotes the distance of \\(x_i\\) from the prediction location \\(x_0\\), and \\(I()\\) is the indicator function (\\(I(a)\\) is 1 if \\(a\\) is true).\n\nCurse of Dimensionality\nThe curse of dimensionality describes a problem that is common with local methods that use data within a window for the analysis. As the number of dimensions \\(p\\) increases (the number of input variables), the number of samples (observations) needed to maintain the same sample intensity as in lower-dimensional problems increases exponentially. There are two ways to look at this curse.\n\nFixed-size neighborhood\nSuppose that \\(X_1, \\cdots, X_p\\) are uniformly distributed on \\([0,1]\\). Choose a local neighborhood that includes observations within 10% of the range. A model with a single input (\\(p=1\\)) selects \\(0.1 \\times 100\\% = 10\\%\\) of the data. A model with \\(p=2\\) selects \\(0.1^2 \\times 100\\% = 1\\%\\) of the data. At \\(p=10\\) we select only \\(0.00000001\\%\\) of the data. As \\(p\\) increases, the fraction of observations available to make a prediction decreases exponentially.\n\n\nFixed-fraction neighborhood\nNow suppose the same setup applies but we are selecting 10% of the observations:\n\n\\(p= 1\\) covers \\(0.1^1 = 0.1\\) of the range of \\(X_1\\)\n\\(p= 2\\) covers \\(0.1^{1/2} = 0.31\\) of the range of \\(X_1\\) and \\(X_2\\)\n\\(p=10\\) covers \\(0.1^{1/10} = 0.79\\) of the range of \\(X_1, \\cdots, X_{10}\\)\n\nAs \\(p\\) increases, you have to increase the “neighborhood” to capture a constant fraction of the observations. Eventually, there is nothing local about these neighborhoods and many points will be near the edge of the data space. If you place 50 observations in 20 dimensions no two observations are close to each other.\n\n\n\n\n\n\nFigure 6.5\n\n\n\nAs a consequence of the curse of dimensionality, local models are typically used in low-dimensional problems, \\(p=1\\) or \\(p=2\\). Global, parametric models do not suffer from this curse; you can add inputs (predictors) without increasing the sample size (within reason).\n\n\n\nFigure 6.2: Different approaches to structuring the regression problem.\nFigure 6.4: Techniques by which models can be localized\nFigure 6.5:",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regglobal.html",
    "href": "regglobal.html",
    "title": "7  The Classical Linear Model",
    "section": "",
    "text": "7.1 Simple and Multiple Linear Regression\nThe SLR and MLR models are examples of the classical linear model. In the SLR case there is a single input variable \\[\nY_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\] in the MLR case there are multiple input variables. These can be distinct variables, or transformations and/or combinations of variables. For example, \\[\nY_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i1}x_{i2} + \\epsilon_i\n\\] has three inputs formed from the main effects of \\(x_1\\) and \\(x_2\\) and their interaction \\(x_1x_2\\). More on the use and interpretation of interactions below.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "regglobal.html#simple-and-multiple-linear-regression",
    "href": "regglobal.html#simple-and-multiple-linear-regression",
    "title": "7  The Classical Linear Model",
    "section": "",
    "text": "Computing Estimates\n\n\nExample: Auto Data from ISLR\n\n\nThe Auto data is a data set used in James et al. (2021) (ISLR2). It comprises information on 397 automobiles, such as mileage (mpg), horsepower, number of cylinders, engine displacement (cu. inches), weight (lbs), etc. The following statements load the data set from DuckDB into a Pandas dataframe, drop records with missing values, and display the first observations.\n\nimport pandas as pd\nimport duckdb \n\ncon = duckdb.connect(database=\"ads.ddb\")\nauto = con.sql(\"SELECT * FROM auto\").df().dropna()\ncon.close()\n\nauto.head()\n\n    mpg  cylinders  displacement  ...  year  origin                       name\n0  18.0          8         307.0  ...    70       1  chevrolet chevelle malibu\n1  15.0          8         350.0  ...    70       1          buick skylark 320\n2  18.0          8         318.0  ...    70       1         plymouth satellite\n3  16.0          8         304.0  ...    70       1              amc rebel sst\n4  17.0          8         302.0  ...    70       1                ford torino\n\n[5 rows x 9 columns]\n\n\nSuppose we want to develop a model that predicts mpg from other variables. A multiple linear regression model with inputs cylinders, displacement, weight and horsepower is fit in Python with scikit-learn (sklearn) as follows.\n\nfrom sklearn import linear_model\n\nauto_x = auto[['cylinders', 'displacement', 'weight', 'horsepower']]\nauto_y = auto['mpg']\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Fit the model, the intercept is added by default \nregr.fit(auto_x, auto_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\ny_hat = regr.predict(auto_x)\nresiduals = auto_y - y_hat\n\n# The coefficients\nprint(\"Intercept : \\n\", regr.intercept_)\n\nIntercept : \n 45.756770522147704\n\nprint(regr.feature_names_in_)\n\n['cylinders' 'displacement' 'weight' 'horsepower']\n\nprint(\"Coefficients of inputs: \\n\", regr.coef_)\n\nCoefficients of inputs: \n [-3.93285431e-01  1.38893129e-04 -5.27717310e-03 -4.28124811e-02]\n\nprint(\"Coefficient of determination: %.2f\" % regr.score(auto_x,auto_y))\n\nCoefficient of determination: 0.71\n\n\nauto_x and auto_y are the \\(x\\)-matrix and the \\(y\\) vector for the regression model. Note that an intercept is added automatically by the model training routine, so it is not provided in auto_x. After a linear regression object is created, the fit method is called to compute the OLS estimates. The predict method computes \\(\\widehat{\\textbf{y}}\\) and the result is used to compute the vector of residuals, \\(\\widehat{\\boldsymbol{\\epsilon}}\\).\nThe remaining statements print the intercept, the regression coefficients for the input variables and the coefficient of determination, also known as the \\(R^2\\) statistic.\nThe regression model fit with the preceding statements is \\[\n\\text{mpg}_i = \\beta_0 + \\beta_1\\text{cylinders}_i+\\beta_2\\text{displacement}_i+\\beta_3\\text{weight}_i+\\beta_4\\text{horsepower}_i + \\epsilon_{i}\n\\] and it is assumed that \\(\\epsilon_i \\sim iid (0,\\sigma^2)\\). The OLS estimates are\n\n\\(\\widehat{\\beta}_0 = 45.756770\\)\n\\(\\widehat{\\beta}_1 = -0.393285\\)\n\\(\\widehat{\\beta}_2 = 0.0001388\\)\n\\(\\widehat{\\beta}_3 = -0.005277\\)\n\\(\\widehat{\\beta}_4 = -0.0428124\\)\n\nThe predicted miles per gallon of an automobile for which the data frame is representative, is\n\\[\\begin{align*}\n\\widehat{\\text{mpg}} = 45.75677 &-0.393285\\times\\text{cylinders} + 0.0001388\\times\\text{displacement} \\\\\n&-0.005277\\times \\text{weight} -0.0428124\\times\\text{horsepower}\n\\end{align*}\\]\n\n\nscikit-learn computes the OLS estimates but does not provide standard errors or an estimate of \\(\\sigma^2\\). These estimates of variability are necessary to compute \\(p\\)-values, confidence and prediction intervals, to test hypotheses, etc. The lack of uncertainty quantification is rooted in the algorithmic modeling approach that does not appeal to an underlying stochastic data-generating mechanism.\nThe estimate of \\(\\sigma^2\\) and the standard errors of the regression coefficients can be calculated the hard way, by performing the matrix math. First, we augment the matrix of input variables with a column of 1s to reflect the intercept. The estimate of \\(\\sigma^2\\) is based on the error (residual) sum of squares \\[\n\\widehat{\\sigma}^2 = \\frac{\\text{SSE}}{n-(p+1)} = \\frac{1}{n-(p+1)}\\sum_{i=1}^n\\,\\widehat{\\epsilon}_i^2\n\\]\n\nimport numpy as np\n\nn = len(auto_x)\np = len(auto_x.columns)\n\nX1 = np.empty(shape=(n, p+1), dtype=float)\nX1[:, 0] = 1\nX1[:, 1:(p+1)] = auto_x\n\n# recompute OLS estimate\nbeta_hat = np.linalg.inv(X1.T @ X1) @ X1.T @ auto_y\nprint(np.round(beta_hat,6))\n\n[ 4.5756771e+01 -3.9328500e-01  1.3900000e-04 -5.2770000e-03\n -4.2812000e-02]\n\n# Sum of squares error = residual sum of squares\nSSE = residuals.T @ residuals\nsigma2_hat = SSE / (n - (p+1))\nprint(f\"Estimate of sigma^2: {sigma2_hat}\")\n\nEstimate of sigma^2: 17.99336781437413\n\nvar_beta_hat = np.linalg.inv(X1.T @ X1) * sigma2_hat\nfor p_ in range(p+1):\n    standard_error = np.sqrt(var_beta_hat[p_, p_])\n    print(f\"beta_hat[{p_}]: {round(beta_hat[p_],6)}  std. error: {round(standard_error,6)}\")\n\nbeta_hat[0]: 45.756771  std. error: 1.520044\nbeta_hat[1]: -0.393285  std. error: 0.409552\nbeta_hat[2]: 0.000139  std. error: 0.00901\nbeta_hat[3]: -0.005277  std. error: 0.000717\nbeta_hat[4]: -0.042812  std. error: 0.01287\n\n\nUsing R, working with linear regression models is much simpler. Most R functions for statistical modeling support a formula expression to specify models directly based on information in data frames. You do not have to set up separate objects for \\(\\textbf{X}\\) and \\(\\textbf{y}\\). Also, R provides standard errors, \\(t\\)-statistics, \\(p\\)-values, and other estimates by default, and has default methods for handling missing values.\n\nlibrary(ISLR2)\n\nhead(Auto)\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n5  17         8          302        140   3449         10.5   70      1\n6  15         8          429        198   4341         10.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n5               ford torino\n6          ford galaxie 500\n\nlinreg &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower,\n             data=Auto)\nsummary(linreg)\n\n\nCall:\nlm(formula = mpg ~ cylinders + displacement + weight + horsepower, \n    data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5248  -2.7964  -0.3568   2.2577  16.3221 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.7567705  1.5200437  30.102  &lt; 2e-16 ***\ncylinders    -0.3932854  0.4095522  -0.960 0.337513    \ndisplacement  0.0001389  0.0090099   0.015 0.987709    \nweight       -0.0052772  0.0007166  -7.364 1.08e-12 ***\nhorsepower   -0.0428125  0.0128699  -3.327 0.000963 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.242 on 387 degrees of freedom\nMultiple R-squared:  0.7077,    Adjusted R-squared:  0.7046 \nF-statistic: 234.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16\n\n\nThe Estimate column of the lm summary reports the \\(\\widehat{\\beta}_k\\) estimates, the Std. Error column reports their standard errors. These values match the Python computations above.\nThe residual standard error of 4.241859 is \\(\\widehat{\\sigma}\\), the square root of the estimator \\(\\widehat{\\sigma}^2\\) derived above.\n\n\nCoefficient Interpretation\nHow do we interpret the regression coefficients of the fitted model\n\\[\\begin{align*}\n\\widehat{\\text{mpg}} = 45.75677 &-0.393285\\times\\text{cylinders} + 0.0001388\\times\\text{displacement} \\\\\n&-0.005277\\times \\text{weight} -0.0428124\\times\\text{horsepower}\n\\end{align*}\\]\nSince the model is linear, it is tempting to state that, for example, a change in 1 unit of displacement causes a change of 0.0001388 in miles per gallon. This interpretation is not correct, because\n\nWe cannot conclude causality between inputs and the target variable. The data are purely observational so we can at best state that changes in the input variables are associated with different predicted values for miles per gallon.\nWe cannot interpret one input variable in the absence of the others. The signs of the regression coefficients are somewhat counter-intuitive. Why would mileage go down for cars with more cylinders but go up with greater displacement. Does adding cylinders not imply a larger engine displacement? The point is that the inputs are related to each other, they do not vary freely from each other. When we interpret the magnitude of a regression coefficient in terms of the change in the target variable that corresponds to a unit change in the input variable, we are implicitly holding all other predictors fixed (ceteris paribus).\n\nThe correct interpretation of the displacement coefficient is thus when displacement increases by one cubic inch and all other attributes remain constant, the expected mileage increases by 0.0001388 miles per gallon.\n\n\n\n\n\n\nNote\n\n\n\nThe all-other-variables-held-fixed interpretation is also important when interpreting hypothesis test results. The \\(p\\)-values of variables cylinders and displacement are 0.33 and 0.98, respectively, suggesting that these variables do not make significant contributions toward explaining miles per gallon. These \\(p\\)-values are interpreted in the presence of the other variables in the model. The correct interpretation is that the number of cylinders is not a significant predictor of miles per gallon in a model that accounts for engine displacement, vehicle weight, and horsepower.\n\n\n\n\n\\(R^2\\), the Coefficient of Determination\nThe variability in the target \\(\\textbf{y}\\), not accounting for any information provided by the input variables can be estimated as \\[\n    s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\overline{y})^2\n\\] If the \\(y_i\\) had the same mean, this would be an unbiased estimator of \\(\\text{Var}[Y]\\). However, the regression model states very clearly that the mean of \\(Y\\) is a function of the \\(x\\)-inputs. This estimator is then a biased estimator of \\(\\text{Var}[Y] = \\sigma^2\\). The numerator of \\(s^2\\) is called the total sum of squares (SST). If SST captures variability of \\(Y\\) about a constant mean, how much of this is attributable to the input variables? To answer this we can look at the variability not attributable to the \\(x\\)s, the error sum of squares \\[\n\\text{SSE} = \\sum_{i=1}^n \\widehat{\\epsilon}_i = \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2\n\\] The ratio \\[\nR^2 = \\frac{\\text{SST}-\\text{SSE}}{\\text{SST}}=1-\\frac{\\text{SSE}}{\\text{SST}}\n\\] is known as the coefficient of determination or R-square. The name R-square comes from a simple relationship between \\(R^2\\) and the Pearson correlation coefficient in the SLR case. If \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), then \\(R^2\\) is the square of the correlation coefficient between \\(X\\) and \\(Y\\):\n\n\nExample: Auto Data (Cont’d)\n\n\n\nslr &lt;- lm(mpg ~ horsepower, data=Auto)\nsummary(slr)$r.squared\n\n[1] 0.6059483\n\ncor(Auto$mpg, Auto$horsepower)^2\n\n[1] 0.6059483\n\n\nThe squared correlation coefficient between mpg and horsepower is the same as the \\(R^2\\) statistic reported by lm.\nYou can compute the correlation coefficient between the two variables from simple linear regression output, but you need to take into account the sign of the regression coefficient. The correlation coefficient has the same sign as \\(\\beta_1\\).\n\ncor(Auto$mpg, Auto$horsepower)\n\n[1] -0.7784268\n\nas.numeric(sign(slr$coefficients[2])) * sqrt(summary(slr)$r.squared)\n\n[1] -0.7784268\n\n\n\n\n\\(R^2\\) ranges between 0 and 1; it achieves the lower bound \\(R^2=0\\) if SSE = SST, the input variables do not explain any variability in \\(Y\\). \\(R^2 = 1\\) results when SSE = 0, the model fits the data “perfectly”, it interpolates the \\(y_i\\).\nThe straightforward interpretation of \\(R^2\\) as the proportion of variability explained by the input variables unfortunately can lead one to chase models that have a high \\(R^2\\). This is a terrible practice for a number of reasons\n\nThe value of \\(R^2\\) does not tell us whether the model assumptions are met. You can explain a substantial amount of variability in the data with a seriously deficient model. For example, the four regressor model fit to the Auto data earlier has \\(R^2 = 0.71\\), explaining 71% of the variability in miles per gallon. A look at the residuals from that model shows that there is substantial trends in the residuals (Figure 7.1). Larger fitted values have larger variability and there is a definite trend in the residuals; the model is not (yet) correct.\n\n\n\n\n\n\n\n\n\nFigure 7.1\n\n\n\n\n\nHere is an example with simulated data where the mean function is not a straight line and the error variance depends on \\(x\\). A simple linear regression model is clearly not appropriate, but it explains more than 75% of the variability in \\(y\\).\n\nx &lt;- seq(0.15, 1, l = 100)\nset.seed(123456)\neps &lt;- rnorm(n = 100, sd = 0.25 * x^2)\ny &lt;- 1 - 2 * x * (1 + 0.25 * sin(4 * pi * x)) + eps\nslr &lt;- lm(y ~ x)\n\nplot(x,y,type=\"p\",col=\"red\")\nabline(slr$coefficients)\n\n\n\n\n\n\n\nsummary(slr)$r.squared\n\n[1] 0.7694696\n\n\n\n\\(R^2\\) is a function of SSE, the prediction error on the training data set. This can be made arbitrarily small by adding input variables. If \\(R^2_{\\text{cur}}\\) is the coefficient of determination in a linear regression model, and you add a new predictor \\(x_{p+1}\\), then \\[\nR^2_{\\text{new}} \\ge R^2_{\\text{cur}}\n\\] Predictors that make no relevant contribution will increase \\(R^2\\) because their addition reduces SSE.\nA model that interpolates the data (fits “perfectly”) is not a good model. It is certainly not perfect if the goal is to build a model that generalizes well to unseen observations. Different metrics are needed to develop models that generalize and do not overfit the data. Chasing \\(R^2\\) values invariably leads to overfitting and models that memorize too much of the training observations to perform well on new data.\n\n\n\nMeasuring Prediction Error\nSince we can make SSE on the training data arbitrarily small by adding more input variables, the MSPE on the training data is not a good metric if we want a model that performs well in predicting new observations. Instead of MSETr the test error MSETe should be used. As discussed in Section 2.5, the test error can be estimated by holding out some observations from training in a test data set or by cross-validation (Section 2.6).\nIn the classical linear model leave-one-out cross-validation is particularly appealing. In addition to not depending on any random selection of data points as test data sets or \\(k\\)-fold cross-validation do, LOOCV error can be calculated in the linear model without re-fitting the model \\(n\\) times. All the necessary pieces to compute the LOOCV error can be assembled on the same pass through the data that calculates the OLS estimates. The key is the Sherman, Morrison, Woodbury formula.\n\nSherman, Morrison, Woodbury Formula\nThis remarkable formula is at the heart of many regression-type diagnostics and cross-validation techniques. A version of this formula was first given by Gauss in 1821. Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.\nSuppose we are in a full-rank linear modeling context with design matrix \\(\\textbf{X}_{(n \\times p + 1)}\\), so that the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. In diagnosing the quality of a model, we are interested in measuring the prediction error for the \\(i\\)th observation as if the data point had not contributed to the analysis. This is an example of a leave-one-out estimate: remove an observation from the data, redo the analysis, and measure how well the quantity of interest can be computed for the withheld observation.\nIf you do this in turn for all \\(n\\) observations, you must fit the model \\(n + 1\\) times, an overall fit to the training data with \\(n\\) observations, and \\(n\\) additional fits with training data sets of size \\(n - 1\\), leaving out each observation in turn. The computationally expensive part of fitting the linear model is building the cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) and computing its inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\).\nThe Sherman-Morrison-Woodbury formula allows us to compute the inverse of the cross-product matrix based on \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) as if the \\(i\\)th observation had been removed.\nDenote as \\(\\textbf{X}_{-i}\\) the design matrix with the \\(i\\)th observation removed. Then\n\\[\\left( \\textbf{X}_{-i}^\\prime\\textbf{X}_{-i} \\right)^{- 1} = \\left( \\textbf{X}^\\prime\\textbf{X}- \\textbf{x}_{i}\\textbf{x}_{i}^{\\prime} \\right)^{-1}\\  = \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}{\\textbf{x}_{i}\\textbf{x}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - \\textbf{x}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{x}_{i}}\\]\nThe quantities on the right hand side are available in the standard regression calculations based on \\(n\\) data points. Because of this remarkable result, leave-one-out statistics can be calculated easily—without retraining any models—based on the fit to the full training data alone.\n\n\n\n\n\n\nNote\n\n\n\nNote that the quantity in the denominator of the right-hand side is the diagonal value of \\(\\textbf{I}- \\textbf{H}\\), where \\(\\textbf{H}\\) is the hat matrix. If \\(h_{ii}\\) denotes the diagonal values of \\(\\textbf{H}\\), we can write the update formula as\n\\[\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}{\\textbf{x}_{i}\\textbf{x}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - h_{ii}}\\]\nThe leverage values \\(h_{ii}\\) play an important role in the computation of residual, influence, and case-deletion diagnostics in linear models.\n\n\n\n\nPRESS Statistic\nIf we denote the predicted value of \\(y_i\\), obtained in a regression without the \\(i\\)th observation, as \\(\\widehat{y}_{-i}\\), then the leave-one-out residual \\[\ny_i - \\widehat{y}_{-i}\n\\] is the test error for the \\(i\\)th observation. Using the Sherman-Morrison-Woodbury result, it is a neat exercise to show that this is simply \\[\ny_i - \\widehat{y}_{-i} = \\frac{y_i - \\widehat{y}_i}{1-h_{ii}}\n\\] The leave-one-out error for the \\(i\\)th observation is obtained by dividing the \\(i\\)th residual by one minus the leverage value. When these deviations are squared and summed across the entire data set the PRESS statistic results \\[\n\\text{PRESS} = \\sum_{i=1}^n (y_i - \\widehat{y}_{-i})^2 = \\sum_{i=1}^n \\left(\\frac{y_i - \\widehat{y}_i}{1-h_{ii}}\\right)^2\n\\] The name is derived from prediction sum of squares. The average PRESS value estimates the mean square test error \\[\n\\text{MSE}_{Te} = \\frac{1}{n}\\text{PRESS}\n\\]\n\n\nExample: Auto Data (Cont’d)\n\n\nFor the four regressor model the PRESS statistic and the MSETe can be calculated by extracting the leverage values\n\nmlr &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower,\n             data=Auto)\n\nleverage &lt;- hatvalues(mlr)\nPRESS_res &lt;- mlr$residuals / (1-leverage)\nPRESS &lt;- sum(PRESS_res^2)\nMSE_Te  &lt;- PRESS/length(leverage)\ncat(\"PRESS statistic: \", PRESS, \"\\n\")\n\nPRESS statistic:  7138.559 \n\ncat(\"MSE Test based on LOOCV: \", MSE_Te,\"\\n\")\n\nMSE Test based on LOOCV:  18.21061 \n\n\nYou can validate this result the hard way by fitting \\(n\\) separate regression models, leaving one observation out each time and predicting that observation to obtain the PRESS residual.\n\nPRESS &lt;- 0\nfor (i in 1:nrow(Auto)) {\n    m &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower, \n            data=Auto[-i,])\n    yhat_minus_i &lt;- predict(m,newdata=Auto[i,])\n    PRESS &lt;- PRESS + (Auto[i,\"mpg\"] - yhat_minus_i)^2\n}\ncat(\"MSE Test based on LOOCV: \", PRESS/nrow(Auto))\n\nMSE Test based on LOOCV:  18.21061\n\n\n\n\n\n\n\nInteractions\nWhat is the difference between the following models\n\\[\\begin{align*}\nY &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\\\\nY &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\n\\end{align*}\\]\nBoth models depend on \\(x_1\\) and \\(x_2\\). In the first case the variables enter the model as main effects, that is, by themselves. Each input variable is allowed to make its contribution on the outcome given the presence of the other. The second model contains the additional interaction term \\(x_1 x_2\\). To be more precise, this is a two-way interaction term because it involves two input variables. A three-way interaction term would be \\(x_1 x_2 x_3\\).\nSuppose that \\(\\beta_3\\) is not zero, how should we interpret the presence of an interaction term in the model? We can no longer state that \\(\\beta_1\\) measures the effect on the target variable when \\(x_1\\) is changed by one unit. The effect of changing \\(x_1\\) by one unit in the second model is now a function of \\(x_2\\). To see this, consider the mean of \\(Y\\) at two points, \\(x_1\\) and \\(x_1+1\\).\n\\[\\begin{align*}\n    \\text{E}[Y | x_1] &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\\n    \\text{E}[Y | x_1 + 1] &= \\beta_0 + \\beta_1 (x_1+1) + \\beta_2 x_2 + \\beta_3 (x_1+1)x_2\n\\end{align*}\\]\nThe difference between the two is \\[\n\\text{E}[Y | x_1 + 1] - \\text{E}[Y | x_1] = \\beta_1 + \\beta_3 x_2\n\\] The effect of \\(x_1\\) is now a function of \\(x_2\\). That is the very meaning of an interaction. The effect of one variable (or factor) depends on another variable (or factor) and vice versa. In the example the effect of \\(x_1\\) is a linear regression in \\(x_2\\).\n\n\n\n\n\n\nTip\n\n\n\nWhen building models with interactions, it is customary to include lower-order effects in the model if higher-order effects are significant. For example, if \\(x_1 x_2\\) is in the model one includes the main effects \\(x_1\\) and \\(x_2\\) regardless of their significance. Similarly, if a three-way interaction is significant one includes the two-way interactions and the main effects in the model. The argument for doing so is that in order for two things to interact they must be present–otherwise, what interacts?\n\n\nThe presence/absence of an interaction between a categorical input variable and a numeric input variable can be seen by comparing the trends as in Figure 7.2. With a two-level categorical factor, representing for example, a treatment and a placebo, the absence of interactions manifests itself in parallel lines. The effect of the treatment is the distance between the two lines and is the same for all values of \\(x_1\\). Similarly, the effect of \\(x_1\\), the slope of the line, is the same for both groups. In the presence of an interaction the slopes are not the same and the distance between the lines (the effect of \\(x_2\\)) depends on the value for \\(x_1\\).\n\n\n\n\n\n\nFigure 7.2: Models with a categorical and a continuous input with and without interactions.\n\n\n\n\n\nExample: Auto Data (Cont’d)\n\n\nWe are now considering a series of model for the Auto data, based on the same four input variables used earlier.\nThe first four models add inputs and also two-way interactions of all inputs in the model. The formula expression y ~ (x1 + x2 + x3)^2 is a shorthand for including the main effects and two-way interactions of the three inputs. R displays the interaction terms as x1:x2, x1:x3, and x2:x3 in the output.\nModels 5 and 6 then add up to three-way and four-way interactions, respectively. For each model we calculate the number of non-zero coefficients (the rank of \\(\\textbf{X}\\)), SSE, \\(R^2\\) and the PRESS statistic.\n\ncalcPress &lt;- function(linModel) {\n    leverage &lt;- hatvalues(linModel)\n    r &lt;- linModel$residuals\n    Press_res &lt;- r/(1-leverage)\n    Press &lt;- sum(Press_res^2)\n    return(list(\"ncoef\"=linModel$rank,\n                \"R2\"   =summary(linModel)$r.squared, \n                \"SSE\"  =sum(r^2),\n                \"Press\"=Press))\n}\n\nl1 &lt;- lm(mpg ~ cylinders, data=Auto)\nl2 &lt;- lm(mpg ~ (cylinders + displacement)^2, data=Auto )\nl3 &lt;- lm(mpg ~ (cylinders + displacement + horsepower)^2, data=Auto )\nl4 &lt;- lm(mpg ~ (cylinders + displacement + horsepower + weight)^2, data=Auto )\nl5 &lt;- lm(mpg ~ (cylinders + displacement + horsepower + weight)^3, data=Auto )\nl6 &lt;- lm(mpg ~ (cylinders + displacement + horsepower + weight)^4, data=Auto )\n\ndf &lt;- rbind(as.data.frame(calcPress(l1)), \n            as.data.frame(calcPress(l2)),\n            as.data.frame(calcPress(l3)),\n            as.data.frame(calcPress(l4)),\n            as.data.frame(calcPress(l5)),\n            as.data.frame(calcPress(l6))\n      )\nknitr::kable(df,format=\"simple\")\n\n\n\n\nncoef\nR2\nSSE\nPress\n\n\n\n\n2\n0.6046890\n9415.910\n9505.534\n\n\n4\n0.6769104\n7695.670\n7846.982\n\n\n7\n0.7497691\n5960.247\n6197.767\n\n\n11\n0.7607536\n5698.608\n6089.687\n\n\n15\n0.7752334\n5353.714\n5818.953\n\n\n16\n0.7752893\n5352.382\n5861.117\n\n\n\n\n\nThe model complexity increases from the first to the sixth model; the models have more parameters and more intricate interaction terms. The SSE values decrease as terms are added to the model, and \\(R^2\\) increases accordingly. The PRESS statistic is always larger than the SSE, which makes sense because it is based on squaring the Press residuals \\((y_i - \\widehat{y}_i)/(1-h_{ii})\\) which are larger than the ordinary residuals \\(y_i - \\widehat{y}_i\\).\nFrom the fifth to the sixth model only one additional parameter is added to the model, the four-way interaction of all inputs. \\(R^2\\) barely increases but the PRESS statistic increases compared to the model with only three-way interaction. Interestingly, none of the effects in the four-way model are significant given the presence of other terms in the model.\n\nsummary(l6)\n\n\nCall:\nlm(formula = mpg ~ (cylinders + displacement + horsepower + weight)^4, \n    data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0497  -1.9574  -0.2334   1.8091  18.8569 \n\nCoefficients:\n                                           Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                               6.104e+01  7.744e+01   0.788    0.431\ncylinders                                -4.974e+00  1.465e+01  -0.339    0.734\ndisplacement                             -4.338e-01  5.655e-01  -0.767    0.444\nhorsepower                               -6.459e-01  7.861e-01  -0.822    0.412\nweight                                    2.735e-02  2.623e-02   1.042    0.298\ncylinders:displacement                    4.737e-02  7.861e-02   0.603    0.547\ncylinders:horsepower                      1.069e-01  1.324e-01   0.808    0.420\ncylinders:weight                         -2.757e-03  4.565e-03  -0.604    0.546\ndisplacement:horsepower                   8.090e-03  6.253e-03   1.294    0.197\ndisplacement:weight                      -7.045e-05  1.775e-04  -0.397    0.692\nhorsepower:weight                        -2.047e-04  2.689e-04  -0.761    0.447\ncylinders:displacement:horsepower        -1.041e-03  8.238e-04  -1.263    0.207\ncylinders:displacement:weight             8.719e-06  2.422e-05   0.360    0.719\ncylinders:horsepower:weight               1.351e-05  4.130e-05   0.327    0.744\ndisplacement:horsepower:weight           -4.645e-07  1.920e-06  -0.242    0.809\ncylinders:displacement:horsepower:weight  7.702e-08  2.517e-07   0.306    0.760\n\nResidual standard error: 3.773 on 376 degrees of freedom\nMultiple R-squared:  0.7753,    Adjusted R-squared:  0.7663 \nF-statistic: 86.48 on 15 and 376 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "regglobal.html#sec-ss-reduction-test",
    "href": "regglobal.html#sec-ss-reduction-test",
    "title": "7  The Classical Linear Model",
    "section": "7.2 Hypothesis Testing",
    "text": "7.2 Hypothesis Testing\nWhen testing hypothesis in statistical models it is useful to think of the hypothesis as imposing a constraint on the model. The test then boils down to comparing a constrained and an unconstrained model in such a way that we can make probability statements about the validity of the constraint. If it is highly unlikely that the constraint holds, we reject the hypothesis.\nThis principle applies to hypothesis testing in many model families, what differs is how the impact of the constraint on the model is measured. In least-squares estimation we look at how a sum of squares changes as the constraint is imposed. In models fit by maximum likelihood we measure how much the log likelihood changes when the constraint is imposed.\nSuppose we have a model with four predictors, \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\epsilon\n\\] and want to test the hypothesis that the absence of \\(x_3\\) and \\(x_4\\) does not make the model worse. The constraint we impose on the model is \\[\nH: \\beta_3 = \\beta_4 = 0\n\\] This is a hypothesis with two degrees of freedom, since two parameters of the model are constrained simultaneously.\n\n\n\n\n\n\nTip\n\n\n\nYou can usually figure out the degrees of freedom in a hypothesis by counting equal signs.\n\n\nThe unconstrained and constrained models are also called the full and the reduced models, respectively. In this case the full model is \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\epsilon\n\\] and the reduced model is \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + \\epsilon\n\\] :::{.definition} ::::{.definition-header} Definition: Nested Models :::: ::::{.definition-container} Two models are said to be nested when one model can be derived from the other by imposing constraints on the model parameters. The full and reduced models in the hypothesis testing context are nested models. On the contrary, if two models are not nested, the hypothesis testing framework described here does not apply. :::: :::\nIf based on the data the hypothesis cannot be rejected, we conclude that the full model is not significantly improved over the reduced model. The strength of evidence in favor of the hypothesis depends on how much variability is accounted for in the model when the constraint is relaxed, relative to the overall variability in the system. To measure that we need to introduce the idea of partitioning sums of squares.\n\nPartitioning Variability through Sums of Squares\nThe difference between the total sum of squares, \\(\\text{SST} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), which does not depend on the inputs, and the error sum of squares \\(\\text{SSE} = \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2\\), is the model sum of squares \\[\n\\text{SSM} = \\sum_{i=1}^n\\left(\\widehat{y}_i-\\overline{y}\\right)^2 = \\widehat{\\boldsymbol{\\beta}}\\textbf{X}^\\prime\\textbf{y}- n\\overline{y}^2\n\\]\nThe total and model sums of squares are also called the corrected total and model sums of squares because they adjust for an overall estimate of the mean of \\(Y\\) if there are no inputs. In other words, they account for the intercept \\(\\beta_0\\) in the model; \\(\\overline{y}\\) is the estimate of \\(\\beta_0\\) in an intercept-only model.\nAnother way of looking at SSM is as a measure of the combined contribution of \\(\\beta_1, \\ldots, \\beta_p\\) beyond the intercept. The notation \\[\n\\text{SSM} = SS(\\beta_1, \\cdots,\\beta_p | \\beta_0)\n\\] makes this explicit. We can now think of other sum of squares, for example, \\(SS(\\beta_3, \\beta_4 | \\beta_0, \\beta_1, \\beta_2)\\) is the sum of squares contribution when \\(\\beta_3\\) and \\(\\beta_4\\) are added to a model that contains \\(\\beta_0, \\beta_1\\), and \\(\\beta_2\\). Algebraically, \\[\nSS(\\beta_3, \\beta_4 | \\beta_0, \\beta_1, \\beta_2) = SS(\\beta_1, \\beta_2, \\beta_3, \\beta_4 | \\beta_0) - SS(\\beta_1, \\beta_2 | \\beta_0)\n\\] This will be one part of measuring the strength of hypothesis \\(H: \\beta_3 = \\beta_4 = 0\\), the change in the model sum of squares between the full model with four inputs and the reduced model with two inputs However, there has to be more to it. If the data are very noisy, this change will have to be large to convince us of evidence against the hypothesis. If the data have small error variability, a smaller change in model sums of squares will suffice. This leads to considering the following test statistic: \\[\nF_{obs} = \\frac{SS(\\beta_3, \\beta_4 | \\beta_0, \\beta_1, \\beta_2) / 2}{\\widehat{\\sigma}^2}\n\\] Notice that the sum of squares in the numerator is divided by the degrees of freedom of the hypothesis. To find an estimator for the variance in the denominator we rely on SSE in the larger of the two models, the unconstrained model, because it is more likely to be an unbiased estimator of \\(\\sigma^2\\).\n\n\nDefinition: Sum of Squares Reduction Test\n\n\nSuppose that \\(\\text{SSE}_f\\) and \\(\\text{SSE}_r\\) are the error sum of squares in a full and reduced model where the reduced model is defined by a constraint \\(H\\) with \\(q\\) degrees of freedom imposed on the full model. The statistic \\[\nF_{obs} = \\frac{(\\text{SSE}_r - \\text{SSE}_f)/q}{\\text{SSE}_f/\\text{dfE}_f}\n\\] follows an F distribution with \\(q\\) numerator and \\(\\text{dfE}_f\\) denominator degrees of freedom if the model errors follow a Gaussian distribution, \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\). \\(\\text{dfE}_f\\) are the degrees of freedom associated with SSE in the full model, \\(n-r(\\textbf{X})_f\\).\n\n\nThe sum of squares reduction test is very general and a very powerful tool to answer questions about the parameters in a linear model. However, in order to use any hypothesis testing framework that works with probability statements, a distributional assumption is required. We can always calculate \\(F_{obs}\\) between two models. Computing \\(p\\)-values, that is, the probability \\(\\Pr(F_{q,\\text{dfE}_f} &gt; F_{obs})\\), is only valid if the data are normally distributed.\n\n\n\n\n\n\nCaution\n\n\n\nIf you reject the hypothesis \\(H\\) based on a small \\(p\\)-value, you do not conclude that the full model is the correct model. You can say that there is significant evidence that the constraint can be relaxed. You might have compared a really bad model and a bad model.\n\n\n\n\nExample: Auto Data (Cont’d)\n\n\nTo test the hypothesis that the coefficient for weight and horsepower are simultaneously zero in a model that accounts for cylinders and displacement, we can use the sum of squares reduction test.\n\nlm_full &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)\nlm_red  &lt;- lm(mpg ~ cylinders + displacement                      , data=Auto)\n\nSSE_full &lt;- sum(lm_full$residuals^2)\nSSE_red  &lt;- sum(lm_red$residuals^2)\nq &lt;- lm_full$rank - lm_red$rank\nsigma2_hat &lt;- SSE_full / (lm_full$df.residual)\n\nFobs &lt;- ((SSE_red-SSE_full)/q)/sigma2_hat\npvalue &lt;- 1-pf(Fobs,q,lm_full$df.residual)\n\ncat(\"SSE_r: \", SSE_red, \"SSE_f: \", SSE_full, \"\\n\")\n\nSSE_r:  8342.566 SSE_f:  6963.433 \n\ncat(\"sigma2_hat: \", sigma2_hat, \"\\n\")\n\nsigma2_hat:  17.99337 \n\ncat(\"Fobs: \", Fobs, \"Pr(F &gt; Fobs): \", pvalue)\n\nFobs:  38.32337 Pr(F &gt; Fobs):  6.661338e-16\n\n\nRemoving weight and horsepower from the four-predictor model increases the error sum of squares from 6963.43334 to 8342.56637. The F statistic for this reduction test is \\(F_{obs} =\\) 38.3234 and the \\(p\\)-value is very small (6.6613381^{-16}). We reject the hypothesis, the two-predictor model is not a sufficient model to explain mileage compared to the four-predictor model.\nYou can get to this result more quickly by using the anova function in R:\n\nanova(lm_red, lm_full)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders + displacement\nModel 2: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    389 8342.6                                  \n2    387 6963.4  2    1379.1 38.323 6.529e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nSequential and Partial Sums of Squares\nThe sum of squares reduction test is helpful to understand the difference between two important special types of sum of squares, sequential and partial ones. How do we measure the contribution an individual predictor makes to the model \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon\n\\] Sequential sums of squares measures the contribution of an input relative to the inputs that precede it in the model. Partial sums of squares measure the contribution relative to all other inputs in the model. For example, if the input of interest is \\(x_3\\) in a four-regressor model, \\[\nSS(\\beta_3 | \\beta_0, \\beta_1, \\beta_2)\n\\] is the sequential sum of squares for \\(x_3\\) and \\[\nSS(\\beta_3 | \\beta_0, \\beta_1, \\beta_2, \\beta_4)\n\\] is the partial sum of squares. \\(\\beta_4\\) does not appear in the sequential sum of squares for \\(x_3\\) because it appears after \\(\\beta_3\\) in the model formula. In other words, the sequential sum of squares for \\(x_3\\) does not adjust for \\(x_4\\) at all, while the partial sum of squares does. Clearly, the two types of sum of squares are not identical, unless the inputs are orthogonal (independent).\nSequential sum of squares have an appealing additive property, the overall model sum of squares can be accumulated from a series of sequential terms,\n\\[\\begin{align*}\n    \\text{SSM} &= SS(\\beta_1 | \\beta_0)\\\\\n    &+ SS(\\beta_2 | \\beta_0, \\beta_1) \\\\\n    &+ SS(\\beta_3 | \\beta_0, \\beta_1, \\beta_2) \\\\\n    &+ \\cdots \\\\\n    &+ SS(\\beta_p | \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_{p-1})\n\\end{align*}\\]\nPartial sum of squares do not add up to anything meaningful, unless the inputs are orthogonal.\nThis raises an interesting question. If we divide a coefficient estimate by its standard error and compute a \\(p\\)-value from a \\(t\\)-distribution, what kind of hypothesis is being tested?\n\n\nExample: Auto Data (Cont’d)\n\n\nHere is the summary of the four-predictor model for mpg.\n\nsummary(lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto))\n\n\nCall:\nlm(formula = mpg ~ cylinders + displacement + weight + horsepower, \n    data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5248  -2.7964  -0.3568   2.2577  16.3221 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.7567705  1.5200437  30.102  &lt; 2e-16 ***\ncylinders    -0.3932854  0.4095522  -0.960 0.337513    \ndisplacement  0.0001389  0.0090099   0.015 0.987709    \nweight       -0.0052772  0.0007166  -7.364 1.08e-12 ***\nhorsepower   -0.0428125  0.0128699  -3.327 0.000963 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.242 on 387 degrees of freedom\nMultiple R-squared:  0.7077,    Adjusted R-squared:  0.7046 \nF-statistic: 234.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16\n\n\nHow do we interpret the \\(p-\\)-value of 0.337513 for the cylinders variable or the \\(p\\)-value of 0.987709 for the displacement variable? A two-sided \\(t\\)-test is equivalent to an F test with 1 numerator degrees of freedom where \\(F_{obs}\\) is the square of the \\(t_{obs}\\) statistic. Let’s first compute the sequential sum of squares tests and see if they match the \\(p\\)-values in the output.\n\nlm1 &lt;- lm(mpg ~ cylinders, data=Auto)\nlm2 &lt;- lm(mpg ~ cylinders + displacement, data=Auto)\nlm3 &lt;- lm(mpg ~ cylinders + displacement + weight, data=Auto)\nlm4 &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower, data=Auto)\n\nanova(lm1, lm2, lm3, lm4)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders\nModel 2: mpg ~ cylinders + displacement\nModel 3: mpg ~ cylinders + displacement + weight\nModel 4: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    390 9415.9                                  \n2    389 8342.6  1   1073.34 59.652 9.795e-14 ***\n3    388 7162.5  1   1180.02 65.581 7.342e-15 ***\n4    387 6963.4  1    199.12 11.066 0.0009633 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe \\(p\\)-values are different from those shown in the lm output, so Pr(&gt;|t|) = 0.987709 cannot have a sequential interpretation (adding displacement to a model that contains cylinders only). The \\(p\\)-values in the lm summary have a partial interpretation as seen by performing reduction tests between the following reduced models and the full model with four predictors:\n\nlm_nocyl &lt;- lm(mpg ~ displacement + weight + horsepower, data=Auto)\nlm_nodis &lt;- lm(mpg ~ cylinders + weight + horsepower, data=Auto)\nlm_nowgt &lt;- lm(mpg ~ cylinders + displacement + horsepower, data=Auto)\nlm_nohp  &lt;- lm(mpg ~ cylinders + displacement + weight, data=Auto)\n\nanova(lm_nocyl,lm4)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ displacement + weight + horsepower\nModel 2: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    388 6980.0                           \n2    387 6963.4  1    16.592 0.9221 0.3375\n\nanova(lm_nodis,lm4)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders + weight + horsepower\nModel 2: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1    388 6963.4                          \n2    387 6963.4  1  0.004276 2e-04 0.9877\n\nanova(lm_nowgt,lm4)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders + displacement + horsepower\nModel 2: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    388 7939.2                                  \n2    387 6963.4  1    975.72 54.227 1.085e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_nohp ,lm4)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders + displacement + weight\nModel 2: mpg ~ cylinders + displacement + weight + horsepower\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    388 7162.5                                  \n2    387 6963.4  1    199.12 11.066 0.0009633 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe \\(p\\)-values of the partial reduction F-tests are identical to those for the \\(t\\)-tests in the lm output. The \\(F_{obs}\\) statistics are the squared values of the \\(t_{obs}\\) statistics.\nOn the other hand, if you ask for the analysis of variance on the full model with the aov function, you get sequential tests of the inputs.\n\nsummary(aov(lm4))\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncylinders      1  14403   14403  800.47  &lt; 2e-16 ***\ndisplacement   1   1073    1073   59.65 9.79e-14 ***\nweight         1   1180    1180   65.58 7.34e-15 ***\nhorsepower     1    199     199   11.07 0.000963 ***\nResiduals    387   6963      18                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "regglobal.html#prediction",
    "href": "regglobal.html#prediction",
    "title": "7  The Classical Linear Model",
    "section": "7.3 Prediction",
    "text": "7.3 Prediction\nTo obtain the fitted values in the linear model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) we apply the OLS estimates and get \\[\n\\widehat{\\textbf{y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\n\\] To predict at a previously unobserved value of the inputs, a new observation \\(\\textbf{x}_0\\), we apply the regression equation and the OLS estimates to \\(\\textbf{x}_0\\): \\[\n\\widehat{\\textbf{y}}_0 = \\textbf{x}_0^\\prime \\widehat{\\boldsymbol{\\beta}}\n\\]\nThis seems simple, there must be a trick to it. The “trick” lies in the question: what is being predicted?\nThe target of the prediction (the what) can be \\(Y\\), the target variable, or \\(\\text{E}[Y]\\), the mean of the target variable. How are these different? \\(Y\\) is a random variable and \\(\\text{E}[Y]\\) is a constant; \\(\\text{Var}[Y] = \\sigma^2\\) and \\(\\text{Var}[\\text{E}[Y]] = 0\\). The uncertainty in predicting an individual observation and predicting the mean fucntion will differ. The former has to take into account the inherent variability \\(\\sigma^2\\) in the data.\nHow are the predicted values themselves different?\nFor illustration, cosnider an SLR model \\(Y = \\beta_0 + \\beta_1x + \\epsilon\\). To predict \\(Y\\) at \\(X=x_0\\) we choose the obvious expression \\[\n\\widehat{Y}_0 = \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_0 + \\widehat{\\epsilon}\n\\] substituting estimates for all unknowns on the right hand side. Since \\(\\epsilon\\) is a zero-mean random variable and cannot be observed directly, the best estimate is \\(\\widehat{\\epsilon} = 0\\) which leads to \\[\n\\widehat{Y}_0 = \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_0\n\\]\nThat is the same expression we use to predict \\(\\text{E}[Y] = \\beta_0 + \\beta_1 x_0\\), substituting the OLS estimates for unknowns on the right hand side: \\[\n\\widehat{\\text{E}}[Y_0] = \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_0\n\\] And therein lies the crux. The predicted values for an observation and for the mean of an observation are the same. But their variability is not the same. We need to be very clear about what it is we are shooting for. More frequently one is interested in predicting observations, not averages of observations. In a study of health outcomes over time you might be more interested in predicting how a patient does at time \\(t\\), rather than how the population of patients does on average at time \\(t\\). Yet when folks see the different levels of confidence we have in the two predictions, they wish they could make predictions for the average.\n\nPrediction Variance\nThe variance of \\(\\widehat{\\text{E}}[Y_0] = \\textbf{x}_0^\\prime\\widehat{\\boldsymbol{\\beta}}\\) is straightforward and depends only on the variance of \\(\\widehat{\\boldsymbol{\\beta}}\\): \\[\n\\text{Var}{\\widehat{\\text{E}}[Y_0]} = \\sigma^2 \\, \\textbf{x}_0^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_0\n\\] To account for the variability in the data, the proper variance to consider when predicting an individual observation is \\[\n\\text{Var}[\\widehat{Y}_0 - Y_0] = \\text{Var}[\\widehat{Y}_0 - \\epsilon] = \\text{Var}[\\textbf{x}_0^\\prime\\widehat{\\boldsymbol{\\beta}}-\\epsilon] = \\sigma^2\\left(1+\\textbf{x}_0^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_0\\right)\n\\]\nThe additional \\(1+\\) does not seem like a big deal but has substantial numeric consequences.\n\n\nConfidence and Prediction Intervals\nWhen the errors have a Gaussian distribution, the following random variables have \\(t\\) distributions with \\(n-r(\\textbf{X})\\) degrees of freedom:\n\\[\\begin{align*}\n    t &= \\frac{\\widehat{Y}_0 - \\text{E}[Y]_0}{\\sqrt{\\text{Var}[\\widehat{Y}_0]} \\\\\n    t &= \\frac{\\widehat{Y}_0 - Y_0}{\\sqrt{\\text{Var}[\\widehat{Y}_0]-Y_0} \\\\\n\\end{align*}\\]\nThe first is used to construct \\((1-\\alpha)\\)-level confidence intervals for the mean of the target \\[\n\\widehat{y}_0 \\pm t_{\\frac{\\alpha}{2},n-(p+1)} \\sqrt{\\sigma^2 \\textbf{x}_0^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_0}\n\\] The second is used to construct \\((1-\\alpha)\\)-level prediction intervals for an individual target \\[\n\\widehat{y}_0 \\pm t_{\\frac{\\alpha}{2},n-(p+1)} \\sqrt{\\sigma^2 \\left(1+ \\textbf{x}_0^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_0 \\right)}\n\\]\n\n\nExample: PISA OECD Study\n\n\nPISA (Program for International Student Assessment) is an OECD study in 65 countries to evaluate the performance of 15-year old students in math, science, and reading. Among the questions the study is trying to address is whether the educational level in a country is influenced by economic wealth, and if so, to what extent.\n\npisa &lt;- read.csv(file=\"data/pisa.csv\",stringsAsFactors=TRUE)\nhead(pisa)\n\n               Country MathMean MathShareLow MathShareTop ReadingMean\n1       Shanghai-China      613          3.8         55.4         570\n2            Singapore      573          8.3         40.0         542\n3 Hong Kong SAR, China      561          8.5         33.7         545\n4       Chinese Taipei      560         12.8         37.2         523\n5                Korea      554          9.1         30.9         536\n6     Macao SAR, China      538         10.8         24.3         509\n  ScienceMean     GDPp  logGDPp HighIncome\n1         580  6264.60  8.74267      FALSE\n2         551 54451.21 10.90506       TRUE\n3         555 36707.77 10.51074       TRUE\n4         523       NA       NA         NA\n5         538 24453.97 10.10455       TRUE\n6         521 77145.04 11.25344       TRUE\n\n\nThe following statements compute the simple linear regression of MathMean on the log GDP and 95% prediction and confidence intervals.\n\npisa_slr &lt;- lm(MathMean ~ logGDPp, data=pisa)\n\nxvals &lt;- data.frame(logGDPp=seq(from=6, to = 14, by=0.1))\np_pred &lt;- data.frame(predict(pisa_slr, newdata=xvals, interval=\"prediction\"))\np_conf &lt;- data.frame(predict(pisa_slr, newdata=xvals, interval=\"confidence\"))\n\nhead(p_pred)\n\n       fit      lwr      upr\n1 357.8802 250.6125 465.1479\n2 360.7589 254.0404 467.4775\n3 363.6376 257.4570 469.8183\n4 366.5163 260.8621 472.1706\n5 369.3951 264.2555 474.5346\n6 372.2738 267.6372 476.9104\n\nhead(p_conf)\n\n       fit      lwr      upr\n1 357.8802 307.8856 407.8749\n2 360.7589 311.9537 409.5642\n3 363.6376 316.0200 411.2553\n4 366.5163 320.0844 412.9483\n5 369.3951 324.1466 414.6435\n6 372.2738 328.2065 416.3410\n\n\nBoth types of intervals are most narrow near the center of the \\(x\\) data range and widen toward the edges. A prediction outside of the hull of the data will be much less precise than a prediction near the center of the data. The additional variance term that distinguishes the variance of \\(\\widehat{y}_0\\) \\(\\widehat{y}_0 - y_0\\) has a considerable effect; the prediction intervals are much wider than the confidence intervals (Figure 7.3).\n\n\n\n\n\n\n\n\nFigure 7.3: 95% prediction and confidence intervals.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "regglobal.html#diagnostics",
    "href": "regglobal.html#diagnostics",
    "title": "7  The Classical Linear Model",
    "section": "7.4 Diagnostics",
    "text": "7.4 Diagnostics\nModel diagnostics are useful to examine model assumptions and to decide whether the model is an adequate description of the data. Questions we would like to answer through model diagnostics include\n\nAre the assumptions of the model met?\n\nLinearity (zero mean errors, \\(\\text{E}[\\boldsymbol{\\epsilon}] = \\textbf{0}\\))\nEqual variance (homoscedasticity, \\(\\text{Var}[\\boldsymbol{\\epsilon}] = \\sigma^2\\textbf{I}\\))\nIndependence of the errors \\(\\text{Cov}[\\epsilon_i, \\epsilon_j] = 0\\)\nCan we assume that the errors are normally distributed?\n\nHow well does the model predict new observations?\nDo observations have undue influence on the results?\nDoes the relationship between the inputs negatively affect the analysis?\n\nThe three basic types of linear model diagnostics are\n\nResidual diagnostics to examine linearity, equal variance assumptions, and to detect outliers. Residual analysis relies on functions of \\(y_i - \\widehat{y}_i\\) to study the behavior of the unobservable \\(\\epsilon_i\\).\nCase-deletion diagnostics find data points that exert high influence on the analysis. These diagnostics ask how an aspect of the analysis (variability, predicted values, coefficient estimates, …) changes if an observation is removed from the analysis.\nCollinearity diagnostics examine the relationships among the inputs and whether they impact the analysis in a negative way. The situations at the end of the extremes include completely orthogonal inputs (\\(\\textbf{X}^\\prime\\textbf{X}\\) is a diagonal matrix) and inputs that are linear combinations of each other (\\(\\textbf{X}^\\prime\\textbf{X}\\) is non-singular and a unique OLS solution does not exist). Most applications fall in-between unless one or more inputs are factors.\n\n\nLeverage\nLinear model diagnostics depend on the leverage values \\(h_{ii}\\), the diagonal values of the Hat matrix \\(\\textbf{H}\\). That is not surprising because the fitted values are linear combinations of the entries in the Hat matrix \\[\n\\widehat{\\textbf{y}} = \\textbf{H}\\textbf{y}\n\\] The \\(i\\)th fitted value is a linear combination of the entries in the \\(i\\)th row of \\(\\textbf{H}\\) with the elements of \\(\\textbf{y}\\) \\[\n\\widehat{y}_i = \\textbf{x}_i^\\prime\\widehat{\\boldsymbol{\\beta}} = \\sum_{j=1}^n h_{ij}y_j\n\\]\nFrom the last expression it is easy to establish that \\[\n\\text{Var}[\\widehat{y}_i] = \\sigma^2 \\, \\textbf{x}_i^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_i = \\sigma^2 h_{ii}\n\\]\nWe can think of the leverages \\(h_{ii}\\) as standardized squared distance measures that tell us how far the \\(i\\)th data point is from the center of the \\(x\\)-data.\nIt is illustrative to look at the mean of the variances of the fitted values across the data set, \\[\n\\frac{1}{n}\\sum_{i=1}^n \\text{Var}[\\widehat{y}_i] = \\frac{1}{n} \\sum_{i=1}^n \\sigma^2h_{ii} = \\sigma^2\\left(\\frac{p+1}{n}\\right)\n\\] The last result follows because \\(\\textbf{H}\\) is a projection matrix and thus \\(tr(\\textbf{H})\\) equals its rank, \\(p+1\\).\nWhat does that result tell us about the precision of the estimated regression output? Suppose \\(p=4\\) and \\(n=10\\). The average variance is then \\(\\sigma^2/2\\). When \\(p=4\\) and \\(n=1000\\), the average variance is \\(\\sigma^2/200\\). As sample size increases, more precise estimates result as \\(p\\) remains fixed. As \\(p\\) increases for a given sample size, the average variance of the fitted values increases. In high-dimensional problems, where \\(p\\) is large, OLS estimates have high variability and are unstable. Regularized estimation methods such as Ridge or Lasso regression can perform better in those circumstances.\nSince \\(\\sum_{i=1}^n h_{ii} = p+1\\), the average leverage value in the data is \\((p+1)/n\\), and a good threshold for high leverage points is \\(h_{ii} &gt; 2(p+1)/n\\). This is not necessarily a problematic data point, it simply states that the point is an outlying point in the \\(x\\)-space. High leverage points have the potential to influence aspects of the analysis, to be highly influential data points. More on this below.\nIn summary, here are some important results involving the leverage values \\(h_{ii}\\):\n\n\\(h_{ii} = \\textbf{x}_i^\\prime (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_i\\)\n\\(\\frac{1}{n} \\le h_{ii} \\le 1\\). This holds only for the \\(n\\) training observations, the leverage \\(\\textbf{x}_0^\\prime(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{x}_0\\) of a new data point is not bounded in this way.\n\\(\\overline{h} = (p+1)/n\\); high leverage points are those for which \\(h_{ii} &gt; 2(p+1)/n\\).\n\\(\\text{Var}[\\widehat{y}_i] = \\sigma^2 h_{ii}\\)\n\\(\\text{Var}[y_i - \\widehat{y}_i] = \\sigma^2 (1-h_{ii})\\)\n\\(\\widehat{y}_i = \\sum_{j=1}^n h_{ij}y_j\\), the \\(i\\)th fitted value is a linear combination of the target values with the values in the \\(i\\)th row of \\(\\textbf{H}\\).\n\\(\\sum_{j=1}^n h_{ij} = 1\\), the sum of the leverage values in the \\(i\\)th row of \\(\\textbf{H}\\) is 1. Since the leverage values are bounded, they sum to 1 within a row, and the fitted values are linear combinations of \\(\\textbf{H}\\), this shows how a data point with \\(h_{ii} \\approx 1\\) has outsize influence. The fitted value is almost entirely determined by the input values of that observation.\n\n\n\nExample: Same Leverage–Different Influence\n\n\nThis simulation demonstrates the concept of high leverage points with and without high influence. Data are simulated under the model \\[\nY = \\beta_0 + \\beta_1 x + \\epsilon\n\\] with \\(\\epsilon \\sim G(0,0.25^2), \\beta_0 = 1, \\beta_1 = 0.5\\). The design points for the input variable are spread evenly from 1 to 2, and a high leverage point is added at \\(x=4\\). There are 22 observations, so the threshold for a high leverage point is \\(2(p+1)/n = 2*2/22 = 0.18\\).\nTwo data sets are simulated. One in which the target value at \\(x=4\\) concurs with the mean function, one in which the target value is unusually high.\n\nset.seed(187)\n\nx &lt;- seq(from=1, to=2, by=0.05)\nxlev &lt;- 4  # the high-leverage point\nx &lt;- c(x,xlev)\n\nmn &lt;- 1 + 0.5*x\ny1 &lt;- rnorm(n=length(x), mean=0, sd=0.25) + mn\n\ny2 &lt;- y1\ny2[length(y1)] &lt;- rnorm(1,0,0.25) + 1 + 1*xlev\n\nlm1 &lt;- lm(y1 ~ x)\nlm2 &lt;- lm(y2 ~ x)\n\nlm1$coefficients\n\n(Intercept)           x \n  0.9868996   0.5024693 \n\nlm2$coefficients\n\n(Intercept)           x \n 0.02317379  1.16105181 \n\n# the point at xlev has the same leverage in both regressions\nround(hatvalues(lm1),4)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n0.0932 0.0857 0.0789 0.0727 0.0671 0.0622 0.0579 0.0543 0.0512 0.0488 0.0471 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.0460 0.0455 0.0456 0.0464 0.0478 0.0499 0.0525 0.0558 0.0598 0.0644 0.7671 \n\nround(hatvalues(lm2),4)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n0.0932 0.0857 0.0789 0.0727 0.0671 0.0622 0.0579 0.0543 0.0512 0.0488 0.0471 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.0460 0.0455 0.0456 0.0464 0.0478 0.0499 0.0525 0.0558 0.0598 0.0644 0.7671 \n\n\nThe fitted linear regressions are quite different. The parameter estimates in the first model, where \\(y|x=4\\) is not unusual are close to the true values \\(\\beta_0 = 1, \\beta_1 = 0.5\\). In the second regression the parameter estimates are very different from the true values, the estimates are biased.\nThe leverage values in both models are identical, since they depend only on the \\(x\\)-data. The data point at \\(x=4\\) has high leverage, its value of 0.7671 exceeds the threshold considerably. No other data point has high leverage.\nFigure 7.4 displays the data and fitted regressions for the two data sets. Although \\(x=4\\) is a high leverage point, it has no undue influence on the estimated regression in the left panel. In the right panel, the high leverage point is a highly influential point due to its unusual \\(y\\)-value. The data point exerts its leverage by pulling the estimated regression towards it.\n\n\n\n\n\n\n\n\nFigure 7.4: A data point with high-leverage at \\(x=4\\) has little influence in one analysis and is highly influential in another analysis depending on its \\(y\\)-value. The dashed line is the true mean function.\n\n\n\n\n\n\n\n\n\nResidual Diagnostics\nBasic questions about the correctness of the model revolve around the properties of \\(\\boldsymbol{\\epsilon}\\). The usual assumption \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0}, \\sigma^2\\textbf{I})\\) states that the errors have zero mean, are uncorrelated, and have equal variance. Although \\(\\boldsymbol{\\epsilon}\\) is unobservable, we should be able to check those assumptions by looking at the OLS residuals of the fitted model, \\(\\widehat{\\boldsymbol{\\epsilon}} = \\textbf{y}- \\widehat{\\textbf{y}}\\). These are also called the raw residuals.\nUnfortunately, the properties of \\(\\widehat{\\boldsymbol{\\epsilon}}\\) match the properties of \\(\\boldsymbol{\\epsilon}\\) only partially. Because \\(\\widehat{\\boldsymbol{\\epsilon}}\\) is the result of fitting a model to data, the fitted residuals obey constraints that do not affect the model errors \\(\\boldsymbol{\\epsilon}\\). Because \\[\n\\textbf{X}^\\prime \\widehat{\\boldsymbol{\\epsilon}} = \\textbf{X}^\\prime (\\textbf{I}-\\textbf{H})\\textbf{y}= \\textbf{0}\n\\] the raw residuals sum to zero across each column of the \\(\\textbf{X}\\) matrix. In other words, there are only \\(n-r(\\textbf{X})\\) degrees of freedom in the raw residual vector. From a statistical perspective, the residuals have zero mean, \\(\\text{E}[\\widehat{\\boldsymbol{\\epsilon}}] = \\textbf{0}\\) and share this property with the model errors. The variance of \\(\\boldsymbol{\\epsilon}\\) and \\(\\widehat{\\boldsymbol{\\epsilon}}\\) is different, however:\n\\[\\begin{align*}\n    \\text{Var}[\\boldsymbol{\\epsilon}] &= \\sigma^2 \\textbf{I}\\\\\n    \\text{Var}[\\widehat{\\boldsymbol{\\epsilon}}] &= \\sigma^2(\\textbf{I}- \\textbf{H}) \\\\\n    \\text{Var}[\\widehat{\\epsilon}_i] &= \\sigma^2 (1-h_{ii})\n\\end{align*}\\]\nWhile the model errors are uncorrelated, the fitted residuals are correlated, \\(\\textbf{I}- \\textbf{H}\\) is not a diagonal matrix. The fitted residuals also do not have the same variance; the variance depends on the leverage of the \\(i\\)th data point.\nThese properties (or lack thereof) should give pause in using the raw residuals to diagnose the assumptions of equal variance or uncorrelated errors. Instead, residual diagnostics use transformations of the raw residuals.\n\nStudentized residuals\nThe unequal variance of the residuals can be handled by standardizing, dividing the residual by its standard deviation (the square root of its variance) \\[\n\\frac{\\widehat{\\epsilon}_i}{\\sigma\\sqrt{1-h_{ii}}}\n\\] \\(\\sigma\\) is unknown and the obvious solution is to substitute an estimator. Statisticians refer to this process, using an estimate to scale a random variable, as studentization. The studentized residual is thus \\[\nr_i = \\frac{\\widehat{\\epsilon}_i}{\\widehat{\\sigma}\\sqrt{1-h_{ii}}}\n\\] The usual estimator for \\(\\sigma\\) is the square root of the estimator of \\(\\sigma^2\\), \\[\n\\widehat{\\sigma} = \\sqrt{ \\frac{\\text{SSE}}{n-r(\\textbf{X})}}\n\\]\n\n\nR-student residuals\nA further adjustment can be made to the studentized residuals. Rather than use an estimate of the variance that is derived from all the data, an estimator can be used that does not depend on the \\(i\\)th observation. This technique is called external studentization in contrast to the internal studentization that gives rise to \\(r_i\\).\nFortunately such an external estimate of the variance \\(\\sigma^2\\) that does not rely on the \\(i\\)th observation can be computed based on the analysis of all \\(n\\) observations. Not surprisingly, as with PRESS residuals discussed earlier, the leverage plays a role again: \\[\n\\widehat{\\sigma}^2_{-i} = \\frac{(n-r(\\textbf{X}))\\widehat{\\sigma}^2 - \\frac{\\widehat{\\epsilon}^2_i}{1-h_{ii}}}{n-r(\\textbf{X})-1}\n\\] The externally studentized residual is called the R-student residual, \\[\nt_i = \\frac{\\widehat{\\epsilon}_i}{\\widehat{\\sigma}_{-i}\\sqrt{1-h_{ii}}}\n\\] Since \\(\\widehat{\\epsilon}_i\\) and \\(\\widehat{\\sigma}_{-i}\\) are independent, \\(t_i\\) behaves like a \\(t\\)-distributed random variable. The R-student residuals are good diagnostics to detect outliers and high-influence points (hips). Outliers are observations unusual in \\(y\\)-space. They are not necessarily hips, unless they are also high leverage points (see the previous example).\nThe \\(t_i\\) work well for outliers and hips because an outlier has large \\(\\widehat{\\epsilon}_i\\) and a hip has small \\(\\sqrt{1-h_{ii}}\\). Both effects increase the value of \\(t_i\\). This is also true for the (internally) studentized residual. In addition, outliers or hips will have a large \\[\n\\frac{\\widehat{\\epsilon}^2_i}{1-h_{ii}}\n\\] the adjustment term in the computation of \\(\\widehat{\\sigma}^2_{-i}\\). For those data points \\(\\widehat{\\sigma}^2_{-i} &lt; \\widehat{\\sigma}^2\\) and \\(t_i\\) will be more sensitive than \\(r_i\\).\nYou can obtain all three sets of residuals easily in R:\n\nThe residual vector returned on the lm return object contains the \\(\\widehat{\\epsilon}_i\\).\nThe rstandard() function returns the studentized residuals \\(r_i\\) (unfortunate function name)\nThe rstudent() function returns the R-student residuals \\(t_i\\)\n\nWhen testing model assumptions such as linearity, equal variance (homoscedasticity), and checking for outliers, the R-student residuals are the preferred quantities. The threshold \\(|r_i| &gt; 2\\) is often applied to indicate outlying observations.\n\n\nExample: Boston Housing Values\n\n\nTo demonstrate residual analysis in a linear model we use the Boston housing data that is part of the MASS library in R. The data set comprises 506 observations on the median value of owner-occupied houses (medv in $000s) in Boston suburbs and 13 variables describing the towns and properties.\nThe following statements fit a multiple linear regression model to predict median home value as a function of all but two inputs. The formula syntax medv ~ . requests to include all variables in the dataframe as inputs. medv ~ . -indus -age requests inclusion of all inputs except indus and age.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nfit &lt;- lm(medv ~ . - indus - age, data=Boston)\nsummary(fit)\n\n\nCall:\nlm(formula = medv ~ . - indus - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nAll 11 input variables are significant in this model; it explains 74% of the variability in median home values.\nA plot of the R-student residuals against the observation number helps to identify outlying observations. Observations with \\(t_i\\) values outside the [-2, +2] interval are outliers. The plot also shows whether the equal variance assumption is reasonable; if the assumption is met the residuals show as a band of equal width.\n\nRStudent &lt;- rstudent(fit)\npar(mar=c(5.1, 4.1, 2, 3))\nplot(RStudent, xlab=\"Obs no.\", las=1,bty=\"l\")\nabline(h= 2, lty=\"dashed\")\nabline(h=-2, lty=\"dashed\")\n\n\n\n\n\n\n\n\nA plot of the residuals against leverage shows observations that are unusual with respect to \\(y\\) (large absolute value of the residual), with respect to \\(x\\) (large leverage), or both. A high leverage point that is also an outlier is a highly influential data point.\n\nleverage &lt;- hatvalues(fit)\nlev_threshold &lt;- 2*fit$rank/length(fit$residuals)\npar(mar=c(5.1, 4.1, 2, 3))\nplot(leverage, RStudent, xlab=\"Leverage\")\nabline(h= 2, lty=\"dashed\")\nabline(h=-2, lty=\"dashed\")\nabline(v=lev_threshold, lty=\"dotted\")\n\n\n\n\n\n\n\n\nIn a simple linear regression model you can plot the residuals against the input \\(x\\). In a multiple linear regression model (or a SLR) you plot the residuals against the fitted values. The residuals should display no obvious trend against \\(\\widehat{y}\\).\nWhen plotting the residuals against the fitted value, add the predictions from a smoothing spline or other nonparametric model to identify trends in the residuals. Ideally, the smoothing spline should not show any gross departures from a flat line at zero.\nThe following plot raises concerns. We might not have the right set of inputs. Inputs might need to be transformed or additional/different terms are needed in the model, for example, interactions between the inputs.\n\nyhat &lt;- predict(fit)\nplot(yhat,RStudent)\nabline(h=2, lty=\"dashed\")\nabline(h=-1, lty=\"dashed\")\nlines(predict(loess(RStudent ~ yhat)),col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\nPartial regression plots\nIt is tempting to create plots comparing residuals against the values of the input variables. This is meaningful in a simple linear regression with one input and can help suggest transformations of \\(X\\) to achieve linearity and help diagnose heteroscedasticity.\nIn the following example, the variance of \\(Y\\) increases with the value of \\(X\\) and the relationship between \\(Y\\) and \\(X\\) is a second-degree polynomial.\n\nset.seed(543)\nx &lt;- seq(1,10,0.1) \nx &lt;- x + rnorm(length(x),0,1)\nmn &lt;- 2 - 0.5*x + 0.3*x^2\ny &lt;- rnorm(n=length(x), mean=mn, sd=sqrt(x))\nlfit &lt;- lm(y ~ x)\nri &lt;- rstudent(lfit)\n\nplot(x,ri,ylab=\"Rstudent\",las=1,bty=\"l\")\nabline(h=0,col=\"red\",lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe plot of the Rstudent residuals shows the increasing variability in \\(x\\) and a systematic quadratic trend in the residuals.\nIn a multiple linear regression (MLR) model it is tempting to create plots such as the previous one for all input variables. Unfortunately, such plots can be misleading because in an MLR model values of one input are changing with the values of other inputs. A way around this problem seems to be a plot of residuals versus the fitted values \\(\\widehat{y}_i\\) but that is not an optimal solution either; this plot does not tell us anything about the \\(X\\)s that could be transformed to improve the model. It mushes together the contributions of all inputs.\nSo, how can we diagnose whether \\(X_j\\) needs to be transformed to account for non-linearity and visualize the relationship in such a way that accounts for the other inputs in the model? The answer is the partial regression plot, also known as the added-variable plot.\nSuppose we partition the \\(\\textbf{X}\\) matrix of the MLR model as follows \\[\n\\textbf{X}= [\\textbf{X}_{-j}, \\textbf{x}_j]\n\\] so that \\(\\textbf{X}_{-j}\\) contains all inputs (including the intercept) except for the \\(j\\)th input. Now consider two new regression models:\n\nRegress \\(\\textbf{y}\\) on \\(\\textbf{X}_{-j}\\). Denote the residuals from this regression as \\(\\textbf{e}_{y,-j}\\)\nRegress \\(\\textbf{x}_j\\) on \\(\\textbf{X}_{-j}\\). Denote the residuals from this regression as \\(\\textbf{e}_{x,-j}\\)\n\nThe added variable plot for input \\(j\\) displays \\(\\textbf{e}_{y,-j}\\) on the vertical axis and \\(\\textbf{e}_{x,-j}\\) on the horizontal axis.\n\n\nExample: Auto (Cont’d)\n\n\nFor the four-regressor model in the Auto example, added-variable plots can be constructed with the avPlots() function in the car package.\n\nmlr &lt;- lm(mpg ~ cylinders + displacement + weight + horsepower,\n             data=Auto)\ncar::avPlots(mlr)\n\n\n\n\n\n\n\n\n\n\nTake the scatter of points in an added variable plot. This is a regression through the origin (a no-intercept) model of the form \\[\n\\textbf{e}_{y,-j} = \\beta_j \\, \\textbf{e}_{x,-j} + \\boldsymbol{\\epsilon}^*\n\\]\nUsing \\(\\beta_j\\) to denote the slope in this regression is no accident. The estimate of \\(\\beta_j\\) in this regression is the same as the estimate in the multiple linear regression model.\nThe added-variable plot is a visual representation of how \\(X_j\\) fares in the \\(p\\)-input model even if we are looking only at a two-dimensional plot. The partial regressions show the effect of \\(X_j\\) on \\(y\\) as if it was added last to the model. If the residual point scatter in the added-variable plot suggests nonlinearity, a transformation of \\(X_j\\) is in order.\nInputs that are highly significant in the multiple linear regression model will have a tight point cloud in the added-variable plot. Inputs that are correctly specified in the model will show non-systematic scatter of the points around the line. Note that we are not looking for horizontal point clouds in the added-variable plots, because the points are arranged around a non-zero line, its slope corresponds to the coefficient estimate.\n\n\n\nCase Deletion Diagnostics\nCase deletion diagnostics express how much an aspect of the model changes when an observation is removed from the analysis. The RStudent residual is a leave-one-out diagnostic in this spirit, as it uses an estimator of \\(\\sigma\\) that does not incorporate the \\(i\\)th data point.\nThe two most important case deletion diagnostics are Cook’s D and the DFFITS. The name DFFITS stands for difference in fit, standardized. The statistic measures the change in predicted value in units of standard errors when the \\(i\\)th observation is deleted. We are concerned when a DFFIT exceeds \\(2\\sqrt{(p+1)/n}\\).\nThe Cook’s D (“D” for distance) statistic measures the change in the parameter estimates \\(\\boldsymbol{\\beta}\\) when the \\(i\\)th observation is removed. If the purpose of modeling is to build a model that predicts well, focus more on DFFITS. If the purpose of modeling is to test hypotheses, then focus more on Cook’s D. We are concerned if the D statistic exceeds 1.\n\nCook’s D\nComputing Cook’s D for our model is easy:\n\nD &lt;- cooks.distance(fit)\nD[which.max(D)]\n\n      369 \n0.1612148 \n\nD[which(D &gt; 0.1)]\n\n      369       373 \n0.1612148 0.1085777 \n\n\nThere are no data points with a D &gt; 1. We conclude that there are no data points that unduly influence the regression coefficients.\nA plot of the D statistic against the observation number shows that there is a group of data points with much higher values of the D statistic. This group also coincides with larger residuals in the previous plots.\n\nplot(D, xlab=\"Obs no.\")\n\n\n\n\n\n\n\n\nHere are the observations with the larger D values\n\nD[which(D &gt; 0.035)]\n\n       365        366        368        369        370        371        372 \n0.07895966 0.07399805 0.04632638 0.16121476 0.06142387 0.04956350 0.04241984 \n       373        381        413        415 \n0.10857770 0.03622523 0.05795739 0.03974459 \n\n\nWhat do these observations have in common?\n\nsubset(Boston, D &gt; 0.035)\n\n        crim zn indus chas   nox    rm   age    dis rad tax ptratio  black\n365  3.47428  0  18.1    1 0.718 8.780  82.9 1.9047  24 666    20.2 354.55\n366  4.55587  0  18.1    0 0.718 3.561  87.9 1.6132  24 666    20.2 354.70\n368 13.52220  0  18.1    0 0.631 3.863 100.0 1.5106  24 666    20.2 131.42\n369  4.89822  0  18.1    0 0.631 4.970 100.0 1.3325  24 666    20.2 375.52\n370  5.66998  0  18.1    1 0.631 6.683  96.8 1.3567  24 666    20.2 375.33\n371  6.53876  0  18.1    1 0.631 7.016  97.5 1.2024  24 666    20.2 392.05\n372  9.23230  0  18.1    0 0.631 6.216 100.0 1.1691  24 666    20.2 366.15\n373  8.26725  0  18.1    1 0.668 5.875  89.6 1.1296  24 666    20.2 347.88\n381 88.97620  0  18.1    0 0.671 6.968  91.9 1.4165  24 666    20.2 396.90\n413 18.81100  0  18.1    0 0.597 4.628 100.0 1.5539  24 666    20.2  28.79\n415 45.74610  0  18.1    0 0.693 4.519 100.0 1.6582  24 666    20.2  88.27\n    lstat medv\n365  5.29 21.9\n366  7.12 27.5\n368 13.33 23.1\n369  3.26 50.0\n370  3.73 50.0\n371  2.96 50.0\n372  9.53 50.0\n373  8.88 50.0\n381 17.21 10.4\n413 34.37 17.9\n415 36.98  7.0\n\n\n\n\nDFFITs\nYou can calculate the DFFITs statistics with the dffits() function. The following plot shows those statistics along with the threshold.\n\nthreshold_dff &lt;- 2 * sqrt(12/506) \ndff &lt;- dffits(fit)\ndff[which(dff &gt; threshold_dff)]\n\n       65       142       149       162       163       164       167       187 \n0.3844495 0.4775638 0.3858408 0.4608252 0.4684540 0.4413915 0.4735623 0.4237227 \n      196       204       205       215       226       234       254       263 \n0.3407192 0.3117844 0.3319161 0.4907820 0.4158602 0.3670077 0.6332907 0.3271603 \n      268       366       368       369       370       371       372       373 \n0.3633354 0.9496438 0.7503581 1.4375975 0.8702277 0.7792767 0.7342909 1.1726437 \n      374       375       413       415 \n0.3126816 0.6200353 0.8436891 0.6941816 \n\n\n\nplot(dff,ylab=\"DFFITs\")\nabline(h=threshold_dff)\n\n\n\n\n\n\n\n\nThe same group of observations with high D values has also high DFFITs. Contrary to the D values, the observations exceed the threshold for the DFFITs. We conclude that the data points are not influential on the regression coefficient estimates, but they are influential on the predicted values. If the model is used to predict median home values, we should consider refining the model or excluding the outlying observations and refitting.\n\n\n\nCollinearity Diagnostics\nWhen inputs are related to each other it has two important consequences\n\nwe cannot interpret the regression coefficient for one input without considering the other inputs\nwith increasing dependence among the inputs, the least squares estimation procedure becomes increasingly numerically unstable.\n\nThe condition when inputs are linearly related is called collinearity and it negatively affects any calculations that involve the \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\) matrix (which is about all the calculations.)\nThe extreme case is when an input variable is linear combination of other inputs, for example \\[\nZ = aX_2 + bX_3\n\\] Adding \\(Z\\) to a model that contains inputs \\(X_2\\) and \\(X_3\\) leads to a singular (rank-deficient) \\(\\textbf{X}\\) matrix. The inverse cross-product matrix \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\) does not exist and the OLS estimator cannot be computed. Software uses instead a generalized inverse matrix \\((\\textbf{X}^\\prime\\textbf{X})^{-}\\) to find a solution, which happens to be not unique.\n\n\nExample: Boston Housing Values\n\n\nThe following code fits a linear model with four inputs. The variable newvar is the sum of the zn and nox variables. With zn and nox already in the model, newvar does not provide any additional information. The \\(\\textbf{X}\\) matrix is singular and an estimate for the coefficient of newvar cannot be found.\n\nB &lt;- Boston\nB$newvar &lt;- B$zn+B$nox\nsingular_model &lt;- lm(medv ~ crim + zn + nox + newvar, data=B)\nsummary(singular_model)\n\n\nCall:\nlm(formula = medv ~ crim + zn + nox + newvar, data = B)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.659  -4.620  -1.684   2.211  31.584 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.22033    2.20618  14.605  &lt; 2e-16 ***\ncrim         -0.27519    0.04516  -6.094 2.19e-09 ***\nzn            0.07749    0.01764   4.392 1.37e-05 ***\nnox         -17.25946    3.83527  -4.500 8.45e-06 ***\nnewvar             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.915 on 502 degrees of freedom\nMultiple R-squared:  0.2637,    Adjusted R-squared:  0.2593 \nF-statistic: 59.93 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nR indicates the singularity in \\(\\textbf{X}\\) with NA for the coefficient associated with the singularity. Note that this depends on the order in which the variables enter the model. When newvar is placed before zn and nox in the model, the nox coefficient is NA.\n\n\nCollinearity is the condition where inputs are highly correlated. They do not follow exact linear dependencies but are close to linearly dependent (a so-called near-linear dependency). The situation is more complex than involving just two input variables. For example, \\(X_1\\) might not be strongly correlated with \\(X_2\\) but can be strongly correlated with a linear combination of \\(X_2\\), \\(X_3\\), and \\(X_8\\). This condition is called multicollinearity.\nWhen multicollinearity is strong, the OLS estimates of the regression coefficients become unstable, small perturbations in the target values or inputs can lead to large changes in the coefficients. The \\(\\widehat{\\beta}_j\\) can be of the wrong size and/or sign.\nA nontechnical diagnostic for multicollinearity is to compute the matrix of pairwise correlations among the inputs. Large values of \\(\\text{Corr}[X_j,X_k]\\) is a sufficient condition for collinearity, but it is not a necessary condition. Even with weak pairwise correlations you can have strong linear dependencies among multiple inputs.\nNevertheless, the pairwise correlation plot is a good place to start.\n\nlibrary(\"corrplot\")\n\ncorrplot 0.94 loaded\n\nX &lt;- model.matrix(fit)\ncorrplot(cor(X[,2:12]), method = \"circle\", diag=FALSE, tl.col=\"black\")\n\n\n\n\n\n\n\n\nWe see some strong pairwise relationships between tax and rad, between dis and nox, and between lstat and rm. Do we need to worry?\nHere are some other, informal, ways to diagnose a multicollinearity problem:\n\nThe \\(R^2\\) statistic indicates the model explains substantial variability in \\(Y\\), but none or few of the inputs show statistically significant \\(p\\)-values. Because the \\(t\\)-tests are partial tests, the other input variables act as proxy for the variable being tested.\nDifferent variable selection methods lead to very different models.\nStandard errors of coefficients and/or fitted values are unusually large.\nSlight perturbations of the data, for example, by adding some small Gaussian random noise, change the results dramatically.\n\nA formal diagnosis relies on the computation of variance inflation factors (VIFs) or **condition indices*.\n\nVariance inflation factors\nEach predictor (input) variable in a linear model is associated with a variance inflation factor that quantifies the strength of linear dependencies between this input and all other inputs.\nThe \\(j\\)th VIF measures how many times more variable the variance of the standardized coefficients are due to the involvement of \\(X_j\\) in linear dependencies involving the other \\(X\\)s.\nYou can find the \\(\\text{VIF}_j\\) from the \\(R^2\\) statistic of a multiple linear regression of \\(X_j\\) on all the other input variables. \\[ \\text{VIF}_j = \\frac{1}{1-R^2_j}\\] For example, the VIF for rad can be obtained by regressing all other inputs onto rad\n\nvif_calc &lt;- lm(rad ~ crim + zn + chas + nox + rm + dis + tax + \n                   ptratio + black + lstat, data=Boston)\nR2_rad &lt;- summary(vif_calc)$r.squared\nVIF_rad &lt;- 1 / (1-R2_rad)\nVIF_rad\n\n[1] 6.861126\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhen calculating variance inflation factors this way make sure that the response variable does not appear on the right hand side of the model formula. The expression lm(rad ~ .) would include medv on the right hand side. Variance inflation factors capture relationships among the inputs and are not related to the response.\n\n\nNotice that a variance inflation factor does not depend on \\(Y\\). It is solely based on relationships among the inputs (predictors). Also, you can see from the model equation above that the VIF discovers more than a pairwise dependence on other variables. It models one input as a function of all other inputs.\nTo compute variance inflation factors in R directly, use the vif function in the car package (Companion to Applied Regression).\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(fit)\n\n    crim       zn     chas      nox       rm      dis      rad      tax \n1.789704 2.239229 1.059819 3.778011 1.834806 3.443420 6.861126 7.272386 \n ptratio    black    lstat \n1.757681 1.341559 2.581984 \n\n\nThe VIF for rad reported by vif() matches the previous calculation.\nAnother way to compute the variance inflation factors is to fit the linear regression with a scaled and centered \\(\\textbf{X}^*\\) matrix. The columns of \\(\\textbf{X}^*\\) are centered at their sample mean and are scaled by dividing by \\(\\sqrt{n-1}\\) times their standard error. As a result, the \\(\\textbf{X}^{*\\prime} \\textbf{X}^*\\) matrix is the matrix of the empirical pairwise correlations of the inputs. The regression coefficients of this model are called the standardized coefficients (\\(beta^*_j\\)) and the variance inflation factors are \\[\n\\text{VIF}_j = \\widehat{Var}[\\widehat{\\beta}^*_j] / \\widehat{\\sigma}^2\n\\]\n\n\nExample: Variance Inflation Factors from Scaled-Centered Regression; Boston Data (Cont’d)\n\n\nTo compute the VIFs in the model for the Boston data, we compute first the centered-and-scaled \\(\\textbf{X}\\) matrix. The scale() function in R centers and scales the data by default but uses the standard deviation as the scaling factor. We use a custom scaling so that the \\(\\textbf{X}^{*\\prime}\\textbf{X}^*\\) matrix equals the empirical correlation matrix.\nWe use model.matrix() to extract the \\(\\textbf{X}\\) matrix from the model object computed earlier. The intercept column is replaced with the target values so we can use this matrix as input to a call to lm.\n\nX &lt;- model.matrix(fit)\nn_1 &lt;- dim(X)[1] - 1\nscaled_X &lt;- scale(X,\n                  center=apply(X,2,mean),\n                  scale =apply(X,2,sd)*sqrt(n_1))\nscaled_X[,1] &lt;- Boston$medv\ncolnames(scaled_X)[1] &lt;- \"medv\"\nll &lt;- lm(medv ~ ., data=data.frame(scaled_X))\nsummary(ll)\n\n\nCall:\nlm(formula = medv ~ ., data = data.frame(scaled_X))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.5328     0.2106 107.018  &lt; 2e-16 ***\ncrim        -20.9558     6.3361  -3.307 0.001010 ** \nzn           24.0276     7.0873   3.390 0.000754 ***\nchas         15.5179     4.8758   3.183 0.001551 ** \nnox         -45.2476     9.2059  -4.915 1.21e-06 ***\nrm           60.0245     6.4155   9.356  &lt; 2e-16 ***\ndis         -70.6350     8.7888  -8.037 6.84e-15 ***\nrad          58.6248    12.4060   4.726 3.00e-06 ***\ntax         -44.6079    12.7724  -3.493 0.000521 ***\nptratio     -46.0495     6.2792  -7.334 9.24e-13 ***\nblack        19.0611     5.4858   3.475 0.000557 ***\nlstat       -83.8570     7.6104 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nThe variance inflation factors are obtained by dividing the square values in the Std. Error column with the estimator of the residual variance\n\ns &lt;- summary(ll)\nvif &lt;- s$coefficients[,2]^2 / s$sigma^2\nvif\n\n(Intercept)        crim          zn        chas         nox          rm \n0.001976285 1.789704160 2.239228671 1.059819222 3.778010991 1.834806373 \n        dis         rad         tax     ptratio       black       lstat \n3.443420336 6.861126315 7.272386358 1.757681497 1.341558750 2.581984268 \n\n\n\n\nThe smallest possible VIF value is 1.0, it indicates that the input is not linearly related to the other variables. The thresholds are as follows\n\n1 &lt; VIF &lt; 10: moderate collinearity\n10 &lt; VIF &lt; 30: moderate to severe collinearity\nVIF &gt; 30: severe collinearity problem\n\n\n\nCondition index and condition number\nA formal diagnostic for multicollinearity, based on the eigenvalue decomposition of the (scaled-and-centered) \\(\\textbf{X}^*\\) matrix, examines the spread of the eigenvalues of ^{*}$. If \\(\\textbf{X}^*\\) is a centered and scaled version of \\(\\textbf{X}\\) such that \\(\\textbf{X}^{*\\prime}\\textbf{X}\\) is the empirical correlation matrix, its eigen decomposition is \\[\n    \\textbf{X}^{*\\prime}\\textbf{X}= \\textbf{Q}\\boldsymbol{\\Lambda}\\textbf{Q}^\\prime\n\\] where \\(\\textbf{Q}\\) is a \\((p+p)\\) orthogonal matrix of eigenvectors and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix with the eigenvalues \\(\\lambda_j\\) on its diagonal. The number of eigenvalues close to zero indicates the number of near linear dependencies in \\(\\textbf{X}\\) (or \\(\\textbf{X}^*\\)). In this centered-and-scaled form the eigenvalues satisfy \\(\\sum_{j=1}^p \\lambda_j = p\\), so if some eigenvalues get small, others need to get bigger.\nThe condition index associated with the \\(j\\)th eigenvalue is \\[\n\\phi_j = \\frac{\\max(\\lambda_j)}{\\lambda_j}\n\\] and the condition number is \\(\\max(\\phi_j)\\).\n\nlibrary(Matrix)\n\nX &lt;- model.matrix(fit)[,2:12]\nX_star &lt;- scale(X,\n                center=apply(X,2,mean),\n                scale=apply(X,2,sd)*sqrt((n_1)))\nXpX_scaled &lt;- t(X_star) %*% X_star\neigen_decomp &lt;- eigen(XpX_scaled)\nevals &lt;- eigen_decomp$values\n\nevals\n\n [1] 4.88096727 1.27924337 1.23924750 0.83617610 0.83188592 0.65049762\n [7] 0.49759783 0.30025208 0.23419413 0.17181685 0.07812133\n\ncat(\"Sum of eigenvalues: \", sum(evals),\"\\n\")\n\nSum of eigenvalues:  11 \n\ncond_index &lt;- max(evals)/evals\ncat(\"Condition indices: \", cond_index,\"\\n\")\n\nCondition indices:  1 3.815511 3.938654 5.837248 5.867352 7.503436 9.809061 16.25623 20.84154 28.40797 62.47931 \n\ncond_number &lt;- max(cond_index)\ncat(\"Condition number: \", cond_number,\"\\n\")\n\nCondition number:  62.47931 \n\n\nCondition indices larger than 900 indicate that near linear dependencies exist.\n\n\n\n\n\n\nCaution\n\n\n\nIn contrast to variance inflation factors, where the \\(j\\)th factor is associated with the \\(j\\)th input, the eigenvalue \\(\\lambda_j\\) is not associated with a particular input. It is associated with a linear combination of all the \\(X\\)s.\n\n\nAn obvious remedy of the multicollinearity is to remove inputs that are associated with high variance inflation factors and to refit the model. If you cannot remove the variables from the model a different estimation method is called for. Regularization methods such as Ridge regression or Lasso regression handle high-dimensional problems and reduce the instability of the least-squares estimates by shrinking their values (suppressing the high variability of the coefficients). At the cost of introducing some bias, these estimators drastically reduce variability for an overall better mean square prediction error.\n\n\n\nFigure 7.1: \nFigure 7.2: Models with a categorical and a continuous input with and without interactions.\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "regfeature.html",
    "href": "regfeature.html",
    "title": "8  Feature Selection and Regularization",
    "section": "",
    "text": "8.1 Algorithmic Feature Selection\nSuppose you have \\(p\\) candidate input variables. How many possible linear models are there? One model without any inputs, one model with all \\(p\\) inputs, \\(p\\) models have a single input, and so on. The number of models having \\(k \\le p\\) inputs is \\[\n{p \\choose k} = \\frac{p!}{k!(p-k)!}\n\\] and the total number of models is \\[\n\\sum_{k=0}^p {p\\choose k} = {p\\choose0} + {p\\choose 1} + \\cdots + {p\\choose p-1} + {p\\choose p}\n\\] By the binomial theorem, \\((x+y)^n = \\sum_{k=0}^n {n\\choose k}x^{n-k}y^{k}\\). Setting \\(x=y=1\\), we find that the total number of models equals \\(2^p\\). This is a very large number even for moderate \\(p\\). With \\(p=10\\) there are “only” 1,024 models, with \\(p=20\\) this number increases to 1,048,576, with \\(p=30\\) there are 1,073,741,824 models–more than a billion.\nEvaluating all regression models becomes unfeasible quickly due to the large number of models. Instead, we use a two-step process:\nThe feature selection methods differ in how they construct the candidate sets \\(\\{M_k\\}\\) in step 1. For example, best subset selection uses efficient search algorithms to explore the space of possible models, forward selection considers \\(\\{M_k\\}\\) as a superset of \\(\\{M_{k-1}\\}\\), backward selection considers \\(\\{M_k\\}\\) as a subset of \\(\\{M_{k+1\\}\\).\nIn step 1 the “best” model is chosen among the \\(k\\)-size models using criteria such as SSE, \\(R^2\\), \\(p\\)-values, etc. In step 2 the models are compared based on an estimate of test error using cross-validation or, more commonly, an indirect estimate of test error.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature Selection and Regularization</span>"
    ]
  },
  {
    "objectID": "regfeature.html#sec-feature-select",
    "href": "regfeature.html#sec-feature-select",
    "title": "8  Feature Selection and Regularization",
    "section": "",
    "text": "Among the set \\(\\{M_k\\}\\) of \\(k\\)-size models, find the best candidate and call it \\(M_k^*\\).\nChoose the single best model among \\(M_0^*, M_1^*, \\cdots, M_p^*\\).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen choosing \\(p\\)-values to judge models against each other during variable selection, you are performing many tests and you are not testing hypotheses in the typical sense. Feature selection is not akin to formulating a research hypothesis, collecting data, and testing whether the data support the hypothesis. The use of \\(p\\)-values during variable selection is more akin to a rough check whether adding or removing a feature should be considered. Thus, larger thresholds such as \\(p=0.1\\) or \\(p=0.2\\) are used.\nEven if the process would be testing hypotheses in the usual sense, the large number of comparisons, each with a chance of a Type-I or Type-II error, creates a massive multiplicity (multiple testing) problem.\n\n\n\nIndirect Estimates of Test Error\nCross-validation approaches such as train:test split, leave-one-out cross-validation, or \\(k\\)-fold cross-validation produce a direct estimate of the mean-squared prediction error. Indirect estimates of the test error make adjustments to quantities derived from the training data and are easy to compute. These estimates are used to quickly quantify model performance without random elements and to compare non-nested models. The best \\(M_k^*\\) and best \\(M_j^*\\) models in feature selection are not necessarily nested in the sense that one model can be reduced from the other–they might have completely different inputs. Those models cannot be compared based on \\(p\\)-values or just SSE. Some adjustments is necessary to incorporate the model complexity and to avoid overfitting.\n\nMallows’ \\(C_p\\)\nThe \\(C_p\\) statistic of Mallows (1973) estimates the average sum of prediction errors \\[\n\\Gamma_p = \\frac{1}{\\sigma^2}\\text{E}\\left [\\sum_{i=1}^n \\left(\\widehat{Y}_i - \\text{E}[Y_i|\\textbf{x}_i]\\right)^2 \\right]\n\\] It is a prediction-oriented criteria that seeks to strike a balance between the bias of an underfit model and the variability of an overfit model. \\(\\Gamma_p\\) expands into \\[\n\\Gamma_p = \\frac{1}{\\sigma^2} \\left(\\sum_{i=1}^n\\text{Var}[\\widehat{Y}_i] + \\sum_{i=1}^n\\left[\\text{Bias}(\\widehat{Y}_i)\\right]^2 \\right)\n\\tag{8.1}\\]\nThe contribution of an overfit model is the first term in parentheses, \\(\\text{Var}[\\widehat{Y}_i]\\), the contribution of an underfit model is the squared bias term. It is insightful to take a look at the first piece, the sum of the variances of the predicted values. Suppose we have a model with \\(d\\) inputs. From \\(\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\) it follows that \\[\n\\text{Var}[\\widehat{\\textbf{Y}}] = \\sigma^2 \\textbf{X}(\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime = \\sigma^2 \\textbf{H}\n\\] The Hat matrix is a projection matrix of rank \\(d+1\\) and thus \\(\\sum_{i=1}^n \\text{Var}[\\widehat{Y}_i] = \\sigma^2(d+1)\\). The sum of the variances of the predicted values will go up when inputs are added to the model, whether the inputs are useful in explaining variability in \\(Y\\) or not. Adding junk variables to a model results in greater variability of the predicted values–there is “no free lunch”.\nThe bias term in Equation 8.1 can be estimated as \\[\n(\\widehat{\\sigma}^2_d - \\sigma^2)(n-d-1)\n\\] where \\(\\widehat{\\sigma}^2_d\\) is the estimate of \\(\\sigma^2\\) in a model with \\(d\\) inputs. Putting everything together we arrive at an estimator of \\(\\Gamma_p\\), known as Mallow’s \\(C_p\\) statistic \\[\nC_p = \\frac{1}{\\widehat{\\sigma}^2}\\left(\\widehat{\\sigma}^2(d+1) + (\\widehat{\\sigma}^2_d - \\widehat{\\sigma}^2)(n-d-1) \\right) = \\frac{\\text{SSE}}{\\widehat{\\sigma}^2} - n + 2(d+1)\n\\] where \\(d\\) is the number of inputs in the model, \\((d+1)\\) accounts for the intercept. In feature selection, \\(\\widehat{\\sigma}^2\\) is based on the full model with \\(p\\) inputs, since this model most likely yields the least biased estimator of the variance of the model errors. Among a set of competing models, select the one with the smallest \\(C_p\\) statistic. Among models with the same number of inputs, \\(d\\), selection based on \\(C_p\\) leads to choosing the model with the smallest SSE. The term \\(2(d+1)\\) can be viewed as a penalty term for model complexity. A larger model has to reduce SSE more substantially to overcome the additional parameters.\nAn alternative formulation for Mallow’s statistic is \\[\nC_p^\\prime = \\frac{1}{n}\\left(\\text{SSE} + 2(d+1)\\widehat{\\sigma}^2\\right)\n\\] \\(C_p\\) and \\(C_p^\\prime\\) are not identical but they lead to the selection of the same model if models are chosen according to smaller \\(C_p\\) or smaller \\(C_p^\\prime\\) values.\n\n\nAIC and BIC\nAkaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are based on likelihood theory and assume a distribution for the data, given the parameter estimates. In linear models, this distribution is typically Gaussian, and the criteria are computed as follows:\n\\[\n\\text{AIC} = \\frac{1}{n}\\left(\\text{SSE} + 2(d+1)\\widehat{\\sigma}^2\\right)\n\\] \\[\n\\text{BIC} = \\frac{1}{n}\\left(\\text{SSE} + \\log(n)(d+1)\\widehat{\\sigma}^2\\right)\n\\] In this formulation, choose the model with the smaller AIC and smaller BIC. AIC is identical to \\(C_p^\\prime\\), selection based on AIC and \\(C_p\\) lead to the same model.\nBIC applies a stronger complexity penalty when \\(\\log(n) &gt; 2\\), (\\(n &gt; 7\\)), and thus tends to select models smaller than \\(C_p\\) or AIC.\n\n\nAdjusted \\(R^2\\)\nThis statistic applies a correction to \\(R^2\\) that penalizes larger models. It is not an estimate of the test error, but is still useful to select models. For a model with \\(d\\) inputs, \\[\n\\text{Adjusted } R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\\left(\\frac{n-1}{n-d-1} \\right) = 1-(1-R^2)\\left(\\frac{n-1}{n-d-1} \\right)\n\\] When inputs are added to a model, SSE decreases and \\(R^2\\) increases. However, \\(\\text{SSE}/(n-d-1)\\) may increase or decrease. If unimportant variables are added, the reduction in SSE does not offset the loss of degree of freedom and Adjusted \\(R^2\\) will be smaller. When selecting models based on Adjusted \\(R^2\\), choose the model with the larger value.\n\n\n\n\nBest Subset Selection\nBest subsets regression uses an efficient branch-and-bound algorithm to explore the space of all possible models. A branch and bound algorithm does not visit all possible models. For example, the LEAPS algorithm of Furnival and Wilson (1974) uses a separate tree as the bounding function that eliminates models that need not be considered given the branches of models that have already been seen. This algorithm is implemented as method=\"exhaustive\" in the regsubsets function of the leaps package in R.\n\n\nExample: Credit Data from ISLR2\n\n\nWe are using here the Credit data from James et al. (2021, Sec 3.3, p.85). The data are simulated observations (400) on\n\nIncome: income in $1,000\nLimit: credit limit\nRating: credit rating\nCards: number of credit cards\nAge: age in years\nEducation: education in years\nOwn: two-level factor whether individual owns a home (“Yes”/“No”)\nStudent: two-level factor whether individual is a student (“Yes”/“No”)\nMarried: two-level factor whether individual is married (“Yes”/“No”)\nRegion: three-level factor of geographic location (“East”, “South”, “West”)\nBalance: average credit card balance in $\n\nThe target variable is Balance.\nThere are 10 input variables, 6 numeric variables, 3 two-level factors and one three-level factor (Region). When all variables are included, this leads to a model with \\(p = 6 + 3 + 2 = 11\\) predictors. The three-level Region variable expands to two columns in \\(\\textbf{X}\\), the first level serves as default as the reference level.\nThe following code performs best subset regression with the leaps algorithm. The nvmax parameter limits the maximum size of subsets to examine by the leaps-and-bound algorithm; the default is nvmax=8. Setting it to NULL forces the algorithm to consider all subsets up to size \\(p\\).\n\nlibrary(ISLR2)\nlibrary(leaps)\nregfit &lt;- regsubsets(Balance ~ ., data=Credit, method=\"exhaustive\", nvmax=NULL)\ns_all &lt;- summary(regfit)\ns_all\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = Credit, method = \"exhaustive\", \n    nvmax = NULL)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 11\nSelection Algorithm: exhaustive\n          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes\n1  ( 1 )  \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n2  ( 1 )  \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n3  ( 1 )  \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"       \n4  ( 1 )  \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n5  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n6  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"       \n7  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n8  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n9  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n10  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n11  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \"*\"    \"*\"        \"*\"       \n          RegionSouth RegionWest\n1  ( 1 )  \" \"         \" \"       \n2  ( 1 )  \" \"         \" \"       \n3  ( 1 )  \" \"         \" \"       \n4  ( 1 )  \" \"         \" \"       \n5  ( 1 )  \" \"         \" \"       \n6  ( 1 )  \" \"         \" \"       \n7  ( 1 )  \" \"         \" \"       \n8  ( 1 )  \" \"         \"*\"       \n9  ( 1 )  \" \"         \"*\"       \n10  ( 1 ) \"*\"         \"*\"       \n11  ( 1 ) \"*\"         \"*\"       \n\n\n\nWith \\(p=11\\) predictors, there are 11 sets \\(\\{M_1\\}, \\cdots, \\{M_{11}\\}\\). The best single-predictor model–the best model in the set \\(\\{M_1\\}\\)– is \\[\nM_1^*: \\text{Balance} = \\beta_0 + \\beta_1\\text{Rating} + \\epsilon\n\\] The best model in \\(\\{M_2\\}\\) is \\[\nM_2^*: \\text{Balance} = \\beta_0 + \\beta_1\\text{Income} + \\beta_2\\text{Rating} + \\epsilon\n\\] and so on. Notice that Rating is included in \\(M_1^*\\), \\(M_2^*\\), and \\(M_3^*\\) but is not present in \\(M_4^*\\).\nTo select the best model from the \\(k\\)-best models, we look at the model summary performance measures:\n\ns_all$cp\n\n [1] 1800.308406  685.196514   41.133867   11.148910    8.131573    5.574883\n [7]    6.462042    7.845931    9.192355   10.472883   12.000000\n\ns_all$bic\n\n [1]  -535.9468  -814.1798 -1173.3585 -1198.0527 -1197.0957 -1195.7321\n [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433\n\ns_all$adjr2\n\n [1] 0.7452098 0.8744888 0.9494991 0.9531099 0.9535789 0.9539961 0.9540098\n [8] 0.9539649 0.9539243 0.9538912 0.9538287\n\n\nSelecting models according to \\(C_p\\) the best 6-input model is chosen. Base on BIC and Adjusted \\(R^2\\) we would choose the best 4-input and best 7-input model, respectively (Figure 8.2).\n\nwhich.min(s_all$cp)\n\n[1] 6\n\nwhich.min(s_all$bic)\n\n[1] 4\n\nwhich.max(s_all$adjr2)\n\n[1] 7\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Results of best subsets regression.\n\n\n\n\n\n\n\n\n\nForward Selection\nForward selection greatly reduces the number of models being evaluated, since at each stage \\(k\\), the set \\(\\{M_{k+1}\\}\\) contains the \\(p-k\\) models with one additional predictor variable. The process starts with the null model, \\(M_0\\), containing only the intercept. All \\(p\\) predictors are then evaluated and the “best” is added to the model. Depending on the criteria, this is the predictor that reduces SSE or increases \\(R^2\\) the most, or has the smallest \\(p\\)-value. Suppose that \\(x_4\\) was added to the model in this round. We now have \\(M_1^*\\) and define as \\(\\{M_2\\}\\) the set of models that contain \\(x_4\\) and one additional predictor. At this stage we evaluate only \\(p-1\\) models, rather than \\({p \\choose 2}\\) models.\nIn summary, only one predictor is added during each stage of forward selection, input variables that have been added in a previous stage remain in the model, and the total number of models evaluated is \\[\n\\sum_{k=0}^p (p-k) = 1 + \\frac{p(p+1)}{2}\n\\] Recall that with \\(p=30\\), evaluating all models requires visiting 1,073,741,824 models. Forward selection evaluates only 466 of them.\nForward selection has clear advantages:\n\nthe number of models evaluated is small\nthe algorithm can be applied when \\(p &gt; n\\) since it does not need to fit a model with all predictors\n\nThere are also some clear disadvantages:\n\nit is not guaranteed that the algorithm visits the best model; in fact it is not even guaranteed that the algorithm finds the best \\(k\\)-size model if \\(k \\ge 1\\).\nvariables that are added early in the cycle can become unimportant with the addition of variables later in the cycle. A variable is not removed by the algorithm once it is added to the model.\n\nTo illustrate these points, consider that \\(x_4\\) is added to the model at stage \\(k=0\\). At \\(k=1\\) input variable \\(x_2\\) is chosen because it reduces SSE the most when one of the remaining predictors are added to a model that contains \\(x_4\\). The model \\(M_2^*\\) has inputs \\(\\{x_4, x_2\\}\\) according to forward selection. The best two-predictor model might be \\(\\{x_1,x_3\\}\\) if all possible models with \\(p=2\\) had been examined.\nAfter the best \\(k\\)-size models are found, the winning model is selected among those based on \\(C_p\\), BIC, Adjusted \\(R^2\\), or cross-validation. This is the second step of the general procedure for feature selection.\n\n\n\n\n\n\nNote\n\n\n\nA form of forward selection does not select among the \\(M_k^*\\) models in the second step. Instead, it specifies threshold values that a variable has to overcome to get added to the model, for example, a \\(p\\)-value &lt; 0.1. Forward selection then continues until no variable outside of the model can be added to the model. If this happens at stage \\(k+1\\), the process stops and \\(M_k^*\\) is chosen as the winning model.\n\n\n\n\nExample: Credit Data from ISLR2 (Cont’d)\n\n\nForward selection can be performed with method=\"forward\" in regsubsets:\n\nregfit &lt;- regsubsets(Balance ~ ., data=Credit, method=\"forward\", nvmax=NULL)\ns_forw &lt;- summary(regfit)\ns_forw\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = Credit, method = \"forward\", \n    nvmax = NULL)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 11\nSelection Algorithm: forward\n          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes\n1  ( 1 )  \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n2  ( 1 )  \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n3  ( 1 )  \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"       \n4  ( 1 )  \"*\"    \"*\"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"       \n5  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n6  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"       \n7  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n8  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n9  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n10  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n11  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \"*\"    \"*\"        \"*\"       \n          RegionSouth RegionWest\n1  ( 1 )  \" \"         \" \"       \n2  ( 1 )  \" \"         \" \"       \n3  ( 1 )  \" \"         \" \"       \n4  ( 1 )  \" \"         \" \"       \n5  ( 1 )  \" \"         \" \"       \n6  ( 1 )  \" \"         \" \"       \n7  ( 1 )  \" \"         \" \"       \n8  ( 1 )  \" \"         \"*\"       \n9  ( 1 )  \" \"         \"*\"       \n10  ( 1 ) \"*\"         \"*\"       \n11  ( 1 ) \"*\"         \"*\"       \n\n\nIn the first step of the algorithm, \\(k=0\\), the variable Rating is added. It adds the greatest improvement over the intercept-only model among the 11 predictor variables. From now on, every model will contain the Rating variable. Recall that in best subset selection this variable was not part of the best 4-predictor model.\nChoosing BIC as the criterion to select among \\(M_1^*\\)–\\(M_{11}^*\\), the 5-predictor model is chosen; it has the smallest BIC:\n\ns_forw$bic\n\n [1]  -535.9468  -814.1798 -1173.3585 -1186.2300 -1197.0957 -1195.7321\n [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433\n\nwhich.min(s_forw$bic)\n\n[1] 5\n\n\nThe best subset selection and forward selection algorithm lead to similar models\n\nModels selected by best subset and forward selection\n\n\nAlgorithm\n\n\n\n\n\nBIC\n\n\n\n\nBest Subset\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\nForward\nIncome\nLimit\nRating\nCards\nStudentYes\n-1197.1\n\n\n\n\n\n\n\nBackward Selection\nBackward selection is similar to forward selection in that at each stage only a limited number of candidate models are considered, namely those models that have one less predictor than the model in the previous stage. In contrast to forward selection, backward selection starts with the full model with \\(p\\) predictors and attempts to remove one variable at a time. The variable removed is the one that causes the smallest increase in SSE, smallest decrease in \\(R^2\\), or has the largest \\(p\\)-value.\nBackward selection has similar advantages and disadvantages compared to forward selection. It is computationally efficient because it visits only a subset of the possible models; \\(1 + p(p+1)/2\\) models like forward selection. It is also not guaranteed to visit the best \\(k\\)-size model or the best model overall.\nIf \\(p &gt; n\\), backward selection is not possible because the full model cannot be fit by least squares without regularization. On the other hand, starting with the full model provides the least biased estimate of the residual variance \\(\\sigma^2\\).\n\n\n\n\n\n\nNote\n\n\n\nAs with forward selection, a form of backward selection uses only \\(p\\)-values or threshold values on change in SSE (\\(R^2\\)) to stop the process of removing predictor variables if at any stage of the algorithm all variables exceed the threshold. That is, no variable can be removed without “significantly” deteriorating the model.\n\n\n\n\nExample: Credit Data from ISLR2 (Cont’d)\n\n\nBackward selection can be performed with method=\"backward\" in regsubsets. For this data set, the algorithm selects the same model as best subset selection.\n\nregfit &lt;- regsubsets(Balance ~ ., data=Credit, method=\"backward\", nvmax=NULL)\ns_backw &lt;- summary(regfit)\ns_backw\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = Credit, method = \"backward\", \n    nvmax = NULL)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 11\nSelection Algorithm: backward\n          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes\n1  ( 1 )  \" \"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n2  ( 1 )  \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n3  ( 1 )  \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"       \n4  ( 1 )  \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n5  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n6  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"       \n7  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n8  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n9  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n10  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n11  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \"*\"    \"*\"        \"*\"       \n          RegionSouth RegionWest\n1  ( 1 )  \" \"         \" \"       \n2  ( 1 )  \" \"         \" \"       \n3  ( 1 )  \" \"         \" \"       \n4  ( 1 )  \" \"         \" \"       \n5  ( 1 )  \" \"         \" \"       \n6  ( 1 )  \" \"         \" \"       \n7  ( 1 )  \" \"         \" \"       \n8  ( 1 )  \" \"         \"*\"       \n9  ( 1 )  \" \"         \"*\"       \n10  ( 1 ) \"*\"         \"*\"       \n11  ( 1 ) \"*\"         \"*\"       \n\n\nConsider the full model with all 11 predictors first. The variable that causes the smallest increase in SSE or decrease in \\(R^2\\) is Education and is removed. This variable is not considered in subsequent steps. The variable whose removal causes the smallest increase in SSE at the next step is RegionSouth and so on.\nBased on BIC, backward selection chooses the same model as best subset selection–for these data.\n\ns_backw$bic\n\n [1]  -530.7458  -801.5344 -1164.9522 -1198.0527 -1197.0957 -1195.7321\n [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433\n\nwhich.min(s_backw$bic)\n\n[1] 4\n\n\n\nModels selected by best subset, forward, and backward selection\n\n\nAlgorithm\n\n\n\n\n\nBIC\n\n\n\n\nBest Subset\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\nForward\nIncome\nLimit\nRating\nCards\nStudentYes\n-1197.1\n\n\nBackward\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\n\n\n\n\n\nStepwise Selection\nThis selection method combines elements of forward and backward selection. A problem of those algorithms is that once a variable has been added it cannot be removed (forward) or once a variable has been removed it cannot be added (backward) at a later step. A stepwise procedure that starts from the null model examines after the addition of a variable if any of the variables now in the model should be removed. Stepwise procedures, also called hybrid procedures, examine more models than forward or backward methods but do not exhaust the entire space of models.\nA variation is the sequential replacement algorithm of Miller (1984) implemented in the leaps package. Instead of removing a variable from a model, replacement attempts to replace any variable in the model with a variable not in the model. Variables are considered for replacement at every step, allowing variables that are being replaced at one stage to re-enter the model at a later stage.\n\n\nExample: Credit Data from ISLR2 (Cont’d)\n\n\nSequential replacement selection can be performed with method=\"seqrep\" in regsubsets.\n\nregfit &lt;- regsubsets(Balance ~ ., data=Credit, method=\"seqrep\", nvmax=NULL)\ns_seqrep &lt;- summary(regfit)\ns_seqrep\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = Credit, method = \"seqrep\", \n    nvmax = NULL)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 11\nSelection Algorithm: 'sequential replacement'\n          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes\n1  ( 1 )  \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n2  ( 1 )  \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"       \n3  ( 1 )  \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"       \n4  ( 1 )  \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"       \n5  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \" \"        \" \"       \n6  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \" \"    \" \"        \" \"       \n7  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n8  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"       \n9  ( 1 )  \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n10  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \"*\"       \n11  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \"*\"    \"*\"        \"*\"       \n          RegionSouth RegionWest\n1  ( 1 )  \" \"         \" \"       \n2  ( 1 )  \" \"         \" \"       \n3  ( 1 )  \" \"         \" \"       \n4  ( 1 )  \" \"         \" \"       \n5  ( 1 )  \" \"         \" \"       \n6  ( 1 )  \" \"         \" \"       \n7  ( 1 )  \" \"         \" \"       \n8  ( 1 )  \" \"         \"*\"       \n9  ( 1 )  \" \"         \"*\"       \n10  ( 1 ) \"*\"         \"*\"       \n11  ( 1 ) \"*\"         \"*\"       \n\n\nThe Rating variable is the strongest predictor in a single-regressor model but is replaced in the two-regressor model. It re-enters in \\(M_3^*\\) is replaced in \\(M_4^*\\) and re-enters in \\(M_5^*\\). Judged by BIC the best model among the 11 stage models is the \\(M_4^*\\):\n\ns_seqrep$bic\n\n [1]  -535.9468  -801.5344 -1173.3585 -1198.0527  -805.7491  -800.3585\n [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433\n\nwhich.min(s_seqrep$bic)\n\n[1] 4\n\n\n\nModels selected by best subset, forward, backward, and sequential replacement selection\n\n\nAlgorithm\n\n\n\n\n\nBIC\n\n\n\n\nBest Subset\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\nForward\nIncome\nLimit\nRating\nCards\nStudentYes\n-1197.1\n\n\nBackward\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\nSeq. Repl.\nIncome\nLimit\n\nCards\nStudentYes\n-1198.1\n\n\n\n\n\n\n\nFeature Selection with Cross-validation\nSo far we have based the selection of the best \\(k\\)-size model on indirect measures of test error, AIC, BIC, \\(C_p\\), or on Adjusted \\(R^2\\). Cross-validation is another option to choose among the \\(M_k^*\\) models. The caret::train function makes this easy. The following code performs backward selection with 10-fold cross-validation. Set the method parameter of the train() function to leapBackward, leapForward, or leapSeq to pick the corresponding selection method from leaps.\n\nlibrary(caret)\nset.seed(123)\ntrain.control &lt;- trainControl(method=\"cv\", number=10)\n# Train the model\nbkwd.model &lt;- train(Balance ~ . , data=Credit,\n                    method = \"leapBackward\", \n                    tuneGrid = data.frame(nvmax = 1:11),\n                    trControl = train.control)\nbkwd.model\n\nLinear Regression with Backwards Selection \n\n400 samples\n 10 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 359, 360, 360, 360, 361, 360, ... \nResampling results across tuning parameters:\n\n  nvmax  RMSE       Rsquared   MAE      \n   1     233.70389  0.7550260  179.03474\n   2     164.82730  0.8741834  124.94667\n   3     104.18429  0.9500264   83.87919\n   4      99.58394  0.9541938   79.31590\n   5      99.97431  0.9537228   79.76912\n   6      98.68603  0.9549101   79.01670\n   7      99.23361  0.9543259   79.35933\n   8      99.36847  0.9542728   79.41112\n   9      99.43304  0.9541716   79.36891\n  10      99.38019  0.9542248   79.39167\n  11      99.05830  0.9545716   79.31319\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was nvmax = 6.\n\n\nFor the ISLR2 Credit data 10-fold cross-validation for backward selection chooses \\(M_6^*\\) as the best model. The coefficients of this model are as follows:\n\ncoef(bkwd.model$finalModel,bkwd.model$bestTune$nvmax)\n\n (Intercept)       Income        Limit       Rating        Cards          Age \n-493.7341870   -7.7950824    0.1936914    1.0911874   18.2118976   -0.6240560 \n  StudentYes \n 425.6099369",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature Selection and Regularization</span>"
    ]
  },
  {
    "objectID": "regfeature.html#sec-regularization",
    "href": "regfeature.html#sec-regularization",
    "title": "8  Feature Selection and Regularization",
    "section": "8.2 Regularization",
    "text": "8.2 Regularization\nFeature selection attempts to select from \\(p\\) candidate features a set that models the signal in the data well and eliminates unimportant variables. Having too many predictor variables, especially ones that do not contribute substantially to the model, increases the variability of the least squares coefficient and leads to overfitting. Regularization approaches the problem from a different perspective: can we work with all \\(p\\) features and allay the negative effects on ordinary least squares estimation?\n\nShrinkage Estimation\nThe answer is “Yes” and it requires a slight modification to the estimation criterion. Instead of solving \\[\n\\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\beta}} \\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\n\\] we add a term that controls the variability of the coefficients:\n\\[\n\\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\beta}} \\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) + \\lambda f(\\boldsymbol{\\beta})\n\\] \\(\\lambda\\) is a hyper-parameter that controls the extent of the penalty and \\(f(\\boldsymbol{\\beta})\\) is a positive-valued function of the coefficients. If \\(\\lambda=0\\), the penalty term vanishes and ordinary least squares estimates result. Since \\(f(\\boldsymbol{\\beta})\\) is positive, a large value of \\(\\lambda\\) adds a heftier penalty to the residual sum of squares. This has the effect of reducing the size of the \\(\\widehat{\\beta}_j\\) in absolute value; hence the name shrinkage estimation.\nWhy does shrinkage estimation work? Suppose we want to estimate \\(\\theta\\) and have an unbiased estimator \\(h(\\textbf{Y})\\). The mean-squared error of this estimator is thus \\(\\text{MSE}[h(\\textbf{Y});\\theta] = \\text{Var}[h(\\textbf{Y})]\\). A simplistic shrinkage estimator could be \\(g(\\textbf{Y}) = c \\times h(\\textbf{Y})\\) where \\(0 \\le c \\le 1\\) is the shrinkage factor. When will \\(g(\\textbf{Y})\\) be superior to \\(h(\\textbf{Y})\\) in terms of mean-squared error? \\[\n\\frac{\\text{MSE}[g(\\textbf{Y});\\theta]}{\\text{MSE}[h(\\textbf{Y});\\theta]} = \\frac{c^2\\text{Var}[h(\\textbf{Y})]+(c-1)^2\\theta^2}{\\text{Var}[h(\\textbf{Y})]}=c^2+(c-1)^2\\frac{\\theta^2}{\\text{Var}[h(\\textbf{Y})]}\n\\]\nThe shrinkage estimator is preferred when this expression is less than 1. Since \\(0 \\le c \\le 1\\), \\(c^2 \\le 1\\), \\((c-1)^2 \\le 1\\) and it boils down to whether the reduction in variance (\\(c^2\\text{Var}[h(\\textbf{Y})]\\)) can overcome the increase in bias (\\((c-1)^2\\theta^2\\)). If \\(h(\\textbf{Y})\\) is highly variable relative to its mean, more shrinkage can be applied.\nLet’s return to the regularization setup. To make the procedure operational we need to choose \\(\\lambda\\) and \\(f(\\boldsymbol{\\beta})\\).\nThree penalty functions are common in statistical modeling and machine learning:\n\\[\nf(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\beta_j^2 = ||\\,[\\beta_1, \\cdots, \\beta_p]\\, ||_2^2\n\\tag{8.2}\\]\n\\[\nf(\\boldsymbol{\\beta}) = \\sum_{j=1}^p |\\beta_j|= ||\\,[\\beta_1,\\cdots,\\beta_p]\\, ||_1\n\\tag{8.3}\\]\n\\[\nf(\\boldsymbol{\\beta},\\alpha)  = \\frac{1-\\alpha}{2}\\sum_{j=1}^p\\beta_j^2 + \\alpha\\sum_{j=1}^p|\\beta_j|\n\\tag{8.4}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe intercept \\(\\beta_0\\) is not included in the penalty term. It models the mean of \\(Y\\) when all inputs are zero and does not need to be penalized.\n\n\nThe penalty function in Equation 8.2 is known as a \\(L_2\\) penalty (or \\(L_2\\) regularization), since it is based on the (squared) \\(L_2\\)-norm of the \\([\\beta_1, \\cdots, \\beta_p]\\). The \\(L_2\\)-norm of vector \\(\\textbf{z}\\) is \\[\n||\\textbf{z}||_2 = \\sqrt{\\sum_{j=1}^p z_j^2}\n\\] The \\(L_1\\)-norm of a vector, on the other hand, is \\[\n||\\textbf{z}||_1 = \\sum_{j=1}^p |z_j|\n\\]\nand this is the basis of the penalty function Equation 8.3. The function Equation 8.4 is a combination of \\(L_1\\) and \\(L_2\\) regularization: \\(\\alpha=0\\) results in the \\(L_2\\) penalty, \\(\\alpha=1\\) results in the \\(L_1\\) penalty and values \\(0 &lt; \\alpha &lt; 1\\) mix the two.\nRegularization using Equation 8.2 is known as ridge regression. The \\(L_1\\)-norm regularization in Equation 8.3 leads to lasso regression (also Lasso or LASSO) and the mixture is known as an elastic net regression.\n\n\n\n\n\n\nTip\n\n\n\nThere is a single regularization parameter \\(\\lambda\\) that applies to all coefficients. Because the size of \\(\\beta_j\\) depends on the scale of \\(x_j\\), it is highly recommended to standardize the columns of \\(\\textbf{X}\\) before applying any regularization. Software will often take care of standardization as part of model fitting. Check the documentation on whether that is the case and whether the results are reported for the standardized or for the original coefficients.\n\n\nThe value of \\(\\lambda\\) determines the extent of the shrinkage. For each value of \\(\\lambda\\) there is a set of coefficient estimates \\(\\widehat{\\boldsymbol{\\beta}}_\\lambda\\) that minimize the objective function \\[\n\\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\left(\\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) + \\lambda f(\\boldsymbol{\\beta})\n\\] The value of \\(\\lambda\\) thus needs to be set a priori, chosen by cross-validation or some other method.\n\n\nRidge Regression\nRidge regression applies the \\(L_2\\) regularization penalty \\[\n\\lambda \\sum_{j=1}^p \\beta_j^2\n\\] and shrinks the coefficient estimates toward 0 unless \\(\\lambda=0\\). A feature of ridge regression is that it shrinks toward zero in absolute value but the coefficients are not exactly zero. To make predictions in a ridge regression model requires information on all \\(p\\) attributes; they all make non-zero contributions toward predicted values.\n\n\nExample: Hitter Data (ISLR2)\n\n\nTo demonstrate regularization we use another data set from James et al. (2021). The Hitters data contains salaries and 19 other attributes about major league baseball players from the 1986 and 1987 seasons.\nRegression models with regularization can be fit with the glmnet function in the glmnet package. This function implements the elastic net regularization–by choosing the alpha= parameter you can choose between ridge, lasso, or elastic net regularization. glmnet does not support the formula syntax, instead you supply the \\(\\textbf{y}\\) vector and the \\(\\textbf{X}\\) matrix. The model.matrix() function in R extracts the model matrix based on the formula syntax.\n\nlibrary(ISLR2)\n\nHit &lt;- na.omit(Hitters)\nx &lt;- model.matrix(Salary ~ ., data=Hit)[,-1]\ny &lt;- Hit$Salary\n\nTo demonstrate the effects of shrinkage we examine the ridge regression estimates for several values of \\(\\lambda\\). By default, glmnet standardizes the \\(\\textbf{X}\\) matrix and reports the results on the original (non-standardized) scale. We explicitly standardize \\(\\textbf{X}\\) here to compare the effects of shrinkage based on standardized ridge regression coefficients.\nThe following code computes the ridge regression estimates for \\(\\lambda=[100, 10, 0.1, 0]\\). Setting alpha=0 results in the \\(L_2\\) regularization (ridge regression). Since we are passing a standardized \\(\\textbf{X}\\) matrix, we add standardize=FALSE.\n\nlibrary(glmnet)\nxstd &lt;- scale(x) \ngrid &lt;- c(100,10, 0.1, 0)\nridge_reg &lt;- glmnet(xstd,\n                    y,\n                    alpha      =0,\n                    lambda     =grid,\n                    standardize=FALSE)\nc &lt;- coef(ridge_reg)\nround(c,5)\n\n20 x 4 sparse Matrix of class \"dgCMatrix\"\n                   s0         s1         s2         s3\n(Intercept) 535.92588  535.92588  535.92588  535.92588\nAtBat       -16.83082 -177.44455 -286.63925 -286.92810\nHits         62.45187  193.42259  329.51110  329.93996\nHmRun        -6.23675   -4.39810   34.88397   34.89111\nRuns         29.79696   11.52654  -55.22778  -55.44889\nRBI          21.97064   10.40385  -24.45369  -24.43937\nWalks        47.54494   95.56907  133.47732  133.62728\nYears       -13.69724  -51.64044  -16.16510  -16.16546\nCAtBat       22.11360  -53.03444 -411.07544 -411.04487\nCHits        53.70545  107.85792  139.86360  139.79069\nCHmRun       44.61612   57.29178    0.51190    0.42362\nCRuns        54.37626  157.62866  451.98210  452.33319\nCRBI         56.03491  107.13165  237.22613  237.23363\nCWalks      -11.25886 -121.63931 -207.16539 -207.48727\nLeagueN      18.36541   29.51287   31.71804   31.73602\nDivisionW   -54.39637  -62.18622  -58.63766  -58.64605\nPutOuts      63.65551   76.76765   78.79568   78.79588\nAssists      11.14021   34.39178   54.09800   54.09470\nErrors      -17.34834  -25.43415  -22.67837  -22.70459\nNewLeagueN    0.01310  -12.69060  -12.91619  -12.94903\n\n\nThere are 19 predictors in addition to the intercept. The coefficient columns labeled s0, s1, s2, and s3 correspond to the four values of \\(\\lambda = [100, 10, 0.1, 0]\\). Note that the intercept is the same because the variables have been standardized and \\(\\beta_0\\) is not shrunk. For each of the predictors, the values are smaller (in absolute value) for the larger values of \\(\\lambda\\). For example, the coefficient estimate of AtBat increases from -16.8308 at \\(\\lambda=100\\) to -177.4445 at \\(\\lambda=10\\) and to -286.6393 at \\(\\lambda=0.1\\).\nFigure 8.3 shows the standardized ridge regression coefficients for the four values of \\(\\lambda\\). The larger variation of the coefficients for smaller values of \\(\\lambda\\) is evident.\n\n\n\n\n\n\n\n\nFigure 8.3: Standardized Ridge Regression Coefficients\n\n\n\n\n\n\n\n\nCross-validation for \\(\\lambda\\)\ncv.glmnet() performs \\(k\\)-fold cross-validation for glmnet() models. By default, \\(k=10\\) and the function goes through its own sequence of \\(\\lambda\\) values. You can provide a grid with the lambda parameter. The evaluation metric can be set with the type.measure= option, for example, \"mse\" for mean-squared error or \"auc\" for the area under the ROC curve.\n\nset.seed(6543)\ncv.out &lt;- cv.glmnet(x,y,alpha=0, nfolds=10, type.measure=\"mse\")\nplot(cv.out)\n\n\n\n\n\n\n\n\nThe numbers across the top of the plot indicate the number of predictors in the model. Ridge regression does not shrink coefficients to exactly zero, all 19 variables have non-zero coefficients for all values of \\(\\lambda\\).\nThe left vertical line is drawn at the \\(\\lambda\\) value that produces the minimum cross-validation error. The dashed vertical line on the right is the value of \\(\\lambda\\) (or log(\\(\\lambda\\)) to be more exact) such that the error is within 1 standard error of the minimum.\nYou can access key results from the cross-validation from the return object of cv.glmnet. The following statements show how to locate the best value for lambda and the index of that value in the cross-validation sequence. That index is then used to access the coefficients of the winning model and the minimum cross-validation measure.\n\nbestlam &lt;- cv.out$lambda.min\nbestlam\n\n[1] 25.52821\n\nlog(bestlam)\n\n[1] 3.239784\n\nbestIndex &lt;- cv.out$index[1]\nselcoef &lt;- cv.out$glmnet.fit$beta[,bestIndex]\nround(selcoef,4)\n\n     AtBat       Hits      HmRun       Runs        RBI      Walks      Years \n   -0.6816     2.7723    -1.3657     1.0148     0.7130     3.3786    -9.0668 \n    CAtBat      CHits     CHmRun      CRuns       CRBI     CWalks    LeagueN \n   -0.0012     0.1361     0.6980     0.2959     0.2571    -0.2790    53.2127 \n DivisionW    PutOuts    Assists     Errors NewLeagueN \n -122.8345     0.2639     0.1699    -3.6856   -18.1051 \n\ncat(\"10-fold CV error for Ridge regression, \", cv.out$cvm[bestIndex])\n\n10-fold CV error for Ridge regression,  115445.5\n\n\n\n\nRidge trace\nAnother method of selecting \\(\\lambda\\) is based on the ridge trace, a plot of the standardized ridge regression coefficient estimates as a function of \\(\\lambda\\). The point where the coefficients stop changing drastically as \\(\\lambda\\) increases is chosen. For the Credit data, the ridge trace stabilizes around \\(\\lambda\\)=20–25 (Figure 8.4).\n\n\n\n\n\n\n\n\nFigure 8.4: Rige trace for credit data\n\n\n\n\n\n\n\nHigh-dimensional ridge regression\nAn important use case for regularized regression is in high-dimensional problems where \\(p\\) is very large. If \\(p &gt; n\\), the ordinary least squares solution does not exist because \\(\\textbf{X}^\\prime\\textbf{X}\\) is not of full rank (it is a \\((p \\times p)\\) matrix of rank \\(n &lt; p\\) in that case). Similarly, the cross-product matrix \\(\\textbf{X}^{*\\prime} \\textbf{X}^*\\) formed from the standardized \\(\\textbf{X}\\) matrix is not of full rank. However, the ridged matrix \\[\n\\textbf{X}^{*\\prime}\\textbf{X}^* + \\lambda\\textbf{I}\n\\] is of full rank. The ridge regression estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_R = \\left( \\textbf{X}^{*\\prime}\\textbf{X}^* + \\lambda\\textbf{I}\\right)^{-1} \\textbf{X}^{*\\prime}\\textbf{Y}\n\\] can be computed.\nThe following R statements simulate a data set with \\(n=5\\), \\(p=10\\).\n\nset.seed(1234)\nvec &lt;- runif(50)\nx &lt;- matrix(vec, nrow=5, ncol=10)\ny &lt;- rnorm(dim(x)[1]) + rowSums(x)\n\nThese matrix manipulations verify that \\(\\textbf{X}^{*\\prime}\\textbf{X}^*\\) is singular but the ridged cross-product matrix can be inverted.\n\nxstd &lt;- scale(x)\nXpX &lt;- t(xstd) %*% xstd\nsolve(XpX)\n\nError in solve.default(XpX): system is computationally singular: reciprocal condition number = 3.41198e-18\n\nsolve(XpX + 10*diag(dim(x)[2]))\n\n               [,1]         [,2]         [,3]         [,4]         [,5]\n [1,]  0.0821411908  0.003628743  0.004526331  0.017361000  0.009667878\n [2,]  0.0036287426  0.080470255 -0.005436822 -0.006134563  0.005396869\n [3,]  0.0045263311 -0.005436822  0.083568814 -0.001248939  0.003501061\n [4,]  0.0173610002 -0.006134563 -0.001248939  0.080073460 -0.013669623\n [5,]  0.0096678784  0.005396869  0.003501061 -0.013669623  0.080268426\n [6,]  0.0098875667  0.002636003 -0.003964861 -0.003660393  0.005067454\n [7,]  0.0089612445 -0.007389909 -0.011840167 -0.005062966  0.005296744\n [8,] -0.0008862907 -0.018453251 -0.007896700 -0.001721646  0.006448275\n [9,]  0.0041343402  0.003959455 -0.016026019 -0.000876888 -0.003877599\n[10,] -0.0032869355  0.005193974  0.009113818 -0.002328403 -0.012638898\n              [,6]         [,7]          [,8]          [,9]        [,10]\n [1,]  0.009887567  0.008961245 -0.0008862907  0.0041343402 -0.003286935\n [2,]  0.002636003 -0.007389909 -0.0184532509  0.0039594553  0.005193974\n [3,] -0.003964861 -0.011840167 -0.0078967001 -0.0160260191  0.009113818\n [4,] -0.003660393 -0.005062966 -0.0017216462 -0.0008768880 -0.002328403\n [5,]  0.005067454  0.005296744  0.0064482749 -0.0038775986 -0.012638898\n [6,]  0.080214593 -0.011206391  0.0070933814 -0.0014132303  0.013646288\n [7,] -0.011206391  0.086123672 -0.0059057078 -0.0075954321  0.012779318\n [8,]  0.007093381 -0.005905708  0.0799896388 -0.0002186338  0.003733199\n [9,] -0.001413230 -0.007595432 -0.0002186338  0.0775284851  0.003203787\n[10,]  0.013646288  0.012779318  0.0037331994  0.0032037868  0.083988113\n\n\nA linear regression of \\(\\textbf{Y}\\) on \\(\\textbf{X}\\) produces a saturated model (a perfect fit). Only four of the predictors are used in the model, since least squares runs out of degrees of freedom.\n\nlinreg &lt;- lm(y ~ x)\nsummary(linreg)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\nALL 5 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (6 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -1.409        NaN     NaN      NaN\nx1             3.677        NaN     NaN      NaN\nx2            -1.336        NaN     NaN      NaN\nx3             5.556        NaN     NaN      NaN\nx4             2.660        NaN     NaN      NaN\nx5                NA         NA      NA       NA\nx6                NA         NA      NA       NA\nx7                NA         NA      NA       NA\nx8                NA         NA      NA       NA\nx9                NA         NA      NA       NA\nx10               NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 4 and 0 DF,  p-value: NA\n\n\nThe ridge regression estimates can be computed, however:\n\nridge_reg &lt;- glmnet(x,y,alpha=0,lambda=c(100,10,0.1,0.01))\nc &lt;- coef(ridge_reg)\nround(c,5)\n\n11 x 4 sparse Matrix of class \"dgCMatrix\"\n                  s0       s1       s2       s3\n(Intercept)  4.06577  3.66008  1.40561  1.17276\nV1          -0.00702 -0.03370  0.28411  0.35818\nV2           0.00999  0.05776 -0.17253 -0.18980\nV3           0.03661  0.28534  1.58285  1.75290\nV4          -0.00096 -0.02597 -0.53019 -0.59407\nV5          -0.02444 -0.16639 -0.06003  0.10446\nV6           0.00657  0.03162 -0.35915 -0.40210\nV7           0.05149  0.36197  0.96726  1.01550\nV8           0.01292  0.09384  0.35413  0.34608\nV9           0.06133  0.50795  3.62746  3.79823\nV10         -0.02833 -0.19215 -0.26185 -0.21501\n\n\nNotice that all 10 predictors make non-zero contributions.\nThe ridge regression does not produce a perfect fit, although the predicted values are close to y if \\(\\lambda\\) is small.\n\ny - predict(ridge_reg,newx=x)\n\n              s0         s1          s2           s3\n[1,]  0.08844951 -0.0283334 -0.02216955 -0.002276488\n[2,]  0.54213531  0.5889800  0.07612739  0.008183588\n[3,] -1.29871702 -1.1272270 -0.09536399 -0.010107069\n[4,]  1.44471028  1.1194956  0.05774551  0.005868841\n[5,] -0.77657808 -0.5529153 -0.01633935 -0.001668871\n\n\n\n\n\nLasso Regression\nThe lasso acronym stands for least absolute shrinkage and selection operator and hints at a key difference from Ridge regression: in addition to shrinking the estimates, the lasso can also be used to select features. The reason is that the lasso \\(L_1\\) regularization can shrink estimates to exactly zero, whereas ridge regression shrinks toward zero.\nThis feature of \\(L_1\\) regularization is considered important by many data scientists and lasso is often preferred over ridge regression for this reason. Neither approach dominates the other in terms of mean-squared error, however. In situations where some inputs dominate and many are irrelevant, the lasso tends to outperform ridge regression in MSE. When standardized coefficients are of similar size across the inputs, ridge regression tends to be superior.\nThe ability to combine regularization with feature selection is sufficient to prefer lasso over ridge regression for many. In order to apply a model to predict new observations, information on all input variables is necessary. A ridge regression with \\(p=50\\) requires data on 50 features. If lasso shrinks half of them to zero, only 25 attributes need to be measured to make a prediction.\n\n\nExample: Hitter Data (ISLR2) (Cont’d)\n\n\nThe following statements fit a lasso regression to the Hitter data. The only change from previous code is the specification alpha=1 to trigger the \\(L_1\\) regularization penalty.\n\nx &lt;- model.matrix(Salary ~ ., data=Hit)[,-1]\ny &lt;- Hit$Salary\n\nlasso_reg &lt;- glmnet(x,y,alpha=1,lambda=grid)\nc &lt;- coef(lasso_reg)\nlasso_reg$lambda\n\n[1] 100.0  10.0   0.1   0.0\n\nround(c,5)\n\n20 x 4 sparse Matrix of class \"dgCMatrix\"\n                   s0         s1         s2         s3\n(Intercept) 220.10409   -1.49700  160.56341  162.43338\nAtBat         .          .         -1.94742   -1.95829\nHits          1.13626    2.01155    7.29164    7.35600\nHmRun         .          .          3.76633    3.98321\nRuns          .          .         -2.11611   -2.20911\nRBI           .          .         -0.86604   -0.94010\nWalks         1.18265    2.24853    6.10253    6.17861\nYears         .          .         -3.27870   -3.51815\nCAtBat        .          .         -0.17544   -0.17490\nCHits         .          .          0.19355    0.19447\nCHmRun        .          0.04705   -0.00305   -0.01667\nCRuns         0.11012    0.21931    1.37504    1.37697\nCRBI          0.31456    0.40399    0.73697    0.74204\nCWalks        .          .         -0.78353   -0.79268\nLeagueN       .         18.93190   61.82690   63.24387\nDivisionW     .       -115.19852 -116.61419 -117.12570\nPutOuts       0.00330    0.23596    0.28159    0.28164\nAssists       .          .          0.36796    0.37108\nErrors        .         -0.78124   -3.35896   -3.41847\nNewLeagueN    .          .        -23.96279  -25.77874\n\n\nFor \\(\\lambda=100\\) and \\(\\lambda=10\\), several coefficients are shrunk to zero, leaving 5 and 9 non-zero coefficients, respectively (not counting the intercept). The smaller values for \\(\\lambda\\) shrink coefficients but not all the way to zero.\nThe following code chooses \\(\\lambda\\) by cross-validation\n\nset.seed(987)\ncv.out &lt;- cv.glmnet(x, y, alpha=1)\nbestlam &lt;- cv.out$lambda.min\nbestlam\n\n[1] 2.674375\n\nlog(bestlam)\n\n[1] 0.9837159\n\nplot(cv.out)\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\n\nThe optimal value for \\(\\lambda\\) per 10-fold cross-validation is 2.674. Figure 8.5 displays the results of cross-validation graphically. At the optimal value of \\(\\lambda\\), the lasso model has 13 non-zero coefficients, six of the variables have been deselected from the model. The following output shows the final model.\n\nbestIndex &lt;- cv.out$index[1]\nround(cv.out$glmnet.fit$beta[,bestIndex],5)\n\n     AtBat       Hits      HmRun       Runs        RBI      Walks      Years \n  -1.54734    5.66090    0.00000    0.00000    0.00000    4.72969   -9.59584 \n    CAtBat      CHits     CHmRun      CRuns       CRBI     CWalks    LeagueN \n   0.00000    0.00000    0.51082    0.65949    0.39275   -0.52916   32.06508 \n DivisionW    PutOuts    Assists     Errors NewLeagueN \n-119.29902    0.27240    0.17320   -2.05851    0.00000 \n\ncat(\"10-fold CV error for lasso regression, \", cv.out$cvm[bestIndex])\n\n10-fold CV error for lasso regression,  114101\n\n\nThe CV error is lower for the lasso model than for the cross-validated ridge regression.\n\n\n\nHigh-dimensional lasso regression\nLike ridge regression, lasso regression can be used when \\(p &gt; n\\). Unlike ridge regression, the lasso will give you an idea about the important variables since it sets coefficients for redundant variables to zero.\nTo demonstrate, consider this small simulation study. Data are generated with \\(n=30\\) and \\(p=60\\) but only the first 5 predictors are significant and have the same coefficient 3.0.\n\nlibrary(Matrix)\nset.seed(12345)\nn &lt;- 30\np &lt;- 60\np1 &lt;- 5\nbeta &lt;- c(rep(3,p1),rep(0,p-p1))\nxmat &lt;- scale(matrix(rnorm(n*p),n,p))\neps &lt;- rnorm(n,mean=0,sd = 0.1)\nfx &lt;- xmat %*% beta\nyvec &lt;- fx + eps\n\nNow let’s choose \\(\\lambda\\) by 10-fold cross-validation\n\nset.seed(527)\ncv.out &lt;- cv.glmnet(xmat,yvec,alpha=1)\nplot(cv.out)\n\n\n\n\n\n\n\n\nLet’s see what the coefficients look like for the best model:\n\nlasso.fit &lt;- glmnet(xmat,yvec,alpha=1,lambda=cv.out$lambda.min)\nc &lt;- coef(lasso.fit)\nwhich(c != 0)\n\n[1]  1  2  3  4  5  6 58\n\nround(c[which(c != 0)],3)\n\n[1]  0.015  2.857  2.929  2.882  2.851  2.962 -0.014\n\n\nThe lasso regression recovered the true model pretty well. Recall that the true model had \\(\\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 3\\) and all other coefficients were zero.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature Selection and Regularization</span>"
    ]
  },
  {
    "objectID": "regfeature.html#sec-feature-dimred",
    "href": "regfeature.html#sec-feature-dimred",
    "title": "8  Feature Selection and Regularization",
    "section": "8.3 Dimension Reduction",
    "text": "8.3 Dimension Reduction\nWe can think of regression as a dimension reduction technique. The target vector \\(\\textbf{Y}\\) is an \\((n \\times 1)\\) vector in \\(n\\)-dimensional space. \\(\\textbf{X}\\boldsymbol{\\beta}\\) is a \\((p+1 \\times 1)\\) vector in \\((p+1)\\)-dimensional space. We are finding the least squares solution by projecting \\(\\textbf{Y}\\) onto the column space of \\(\\textbf{X}\\)–in other words, we are finding the closest representation of \\(\\textbf{Y}\\) in a \\((p+1)\\)-dimensional space. The techniques discussed so far in this chapter to deal with the problem of \\(p\\) being large are\n\nSet some \\(\\beta_j\\) to zero \\(\\rightarrow\\) feature selection\nImpose constraints on the \\(\\beta_j \\rightarrow\\) regularization\n\nA third technique to reduce the dimensionality of the problem is to apply a two-step procedure. In the first step we create \\(M\\) linear combinations of the \\(p\\) inputs, call them \\(Z_1, \\cdots, Z_M\\). We choose \\(M \\ll p\\) and in the second step use \\(Z_1\\) through \\(Z_M\\) as the input variables in a regression model.\n\nPrincipal Components\nIt is important that the \\(Z_M\\) are linear combinations of all predictors \\[\n        Z_{im} = \\sum_{j=1}^p \\phi_{jm}X_{ij}\n\\tag{8.5}\\]\nThe coefficients \\(\\phi_{jm}\\) are called the loadings or rotations and the scores \\(Z_{im}\\) are constructed as the principal components of the \\(\\textbf{X}\\) matrix. We will discuss principal component analysis (PCA) and the construction of the \\(Z_{im}\\) in detail in Chapter 23.\n(PCA) finds linear combinations of \\(p\\) inputs that explain decreasing amounts of variability among the \\(x\\)’s. Not any linear combination will do, the principal components are orthogonal to each other and project in the directions in which the inputs are most variable. That means they decompose the variability in the inputs into non-overlapping chunks. The first principal component explains the most variability, the second principal component explains the second-most variability, and so forth.\nConsider the following data set with \\(n=10\\) and \\(p=4\\).\n\n\n\nTable 8.1: Example data for principal component analysis. Sample mean and standard deviation of the columns shown in the last two rows.\n\n\n\n\n\nObs\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\n\n\n\n1\n0.344\n0.364\n0.806\n0.160\n\n\n2\n0.363\n0.354\n0.696\n0.249\n\n\n3\n0.196\n0.189\n0.437\n0.248\n\n\n4\n0.200\n0.212\n0.590\n0.160\n\n\n5\n0.227\n0.229\n0.437\n0.187\n\n\n6\n0.204\n0.233\n0.518\n0.090\n\n\n7\n0.197\n0.209\n0.499\n0.169\n\n\n8\n0.165\n0.162\n0.536\n0.267\n\n\n9\n0.138\n0.116\n0.434\n0.362\n\n\n10\n0.151\n0.151\n0.483\n0.223\n\n\n\\(\\overline{x}_j\\)\n0.218\n0.222\n0.544\n0.211\n\n\n\\(s_j\\)\n0.076\n0.081\n0.122\n0.075\n\n\n\n\n\n\nWhen the \\(x\\)s are centered with their means and scaled by their standard deviations, and the principal components are computed, the matrix of loadings is \\[\n\\boldsymbol{\\Phi} = \\left [ \\begin{array}{r r r r }\n-0.555 & 0.235 &  0.460 & -0.652\\\\\n-0.574 &0.052  &0.336 & 0.745\\\\\n-0.530 &0.208 &-0.821 &-0.053\\\\\n0.286 &0.948  &0.047  & 0.132\\\\\n     \\end{array} \\right]\n\\]\nThis matrix can now be used, along with the data in Table 8.1 to compute the scores \\(Z_{im}\\). For example,\n\\[\\begin{align*}\n    Z_{11} &= -0.555 \\frac{0.344-0.218}{0.076} -0.574\\frac{0.364-0.222}{0.081} - 0.530\\frac{0.806-0.544}{0.122} + 0.286\\frac{0.16-0.211}{0.075} = -3.248 \\\\\n    Z_{32} &= 0.235 \\frac{0.196-0.218}{0.076} +0.052\\frac{0.189-0.222}{0.081} + 0.208\\frac{0.437-0.544}{0.122} +0.948\\frac{0.248-0.211}{0.075} = 0.194\\\\\n\\end{align*}\\]\nTable 8.2 displays the four inputs and the four scores \\(Z_1, \\cdots Z_4\\) from the PCA.\n\n\n\nTable 8.2: Data (\\(X_1,\\cdots,X_4\\)) and principal component scores (\\(Z_1,\\cdots,Z_4\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(Z_1\\)\n\\(Z_2\\)\n\\(Z_3\\)\n\\(Z_4\\)\n\n\n\n\n1\n0.344\n0.364\n0.806\n0.160\n-3.248\n0.282\n-0.440\n0.029\n\n\n2\n0.363\n0.354\n0.696\n0.249\n-2.510\n1.257\n0.425\n-0.025\n\n\n3\n0.196\n0.189\n0.437\n0.248\n0.993\n0.194\n0.465\n0.001\n\n\n4\n0.200\n0.212\n0.590\n0.160\n-0.191\n-0.629\n-0.498\n-0.041\n\n\n5\n0.227\n0.229\n0.437\n0.187\n0.255\n-0.459\n0.780\n-0.001\n\n\n6\n0.204\n0.233\n0.518\n0.090\n-0.329\n-1.614\n0.055\n0.023\n\n\n7\n0.197\n0.209\n0.499\n0.169\n0.286\n-0.691\n0.087\n0.012\n\n\n8\n0.165\n0.162\n0.536\n0.267\n1.058\n0.479\n-0.485\n0.009\n\n\n9\n0.138\n0.116\n0.434\n0.362\n2.380\n1.391\n-0.098\n0.023\n\n\n10\n0.151\n0.151\n0.483\n0.223\n1.306\n-0.210\n-0.291\n-0.028\n\n\n\n\n\n\nFor each observation there is a corresponding component score and there are as many components as there are input variables. Although there is a 1:1 correspondence at the row level, there is no such correspondence at the column level. Instead, each PCA score \\(Z_j\\) is a linear combination of all \\(p\\) input variables. Even if we were to proceed with only \\(Z_1\\) in a linear model \\[\nY_i = \\theta_0 + \\theta_1 Z_{i1} + \\epsilon_i\n\\] the model contains information from from \\(X_1\\) through \\(X_4\\) because \\[\nZ_{i1} = \\sum_{j=1}^p \\phi_{j1}X_{ij}\n\\]\nSo what have we gained? The \\(Z_j\\) have very special properties, not shared by the \\(X_j\\):\n\nThey have zero mean :\\(\\sum_{i=1}^n Z_{ij} = 0\\) (if data was centered)\nThey are uncorrelated: \\(\\text{Corr}[Z_j, Z_k] = 0, \\forall j \\ne k\\)\n\\(\\text{Var}\\left[\\sum_{j=1}^p Z_j\\right] = \\sum_{j=1}^p \\text{Var}[Z_j] = p\\) (if data was scaled)\nThe components are ordered in terms of their variance: \\(\\text{Var}[Z_1] &gt; \\text{Var}[Z_2] &gt; \\cdots &gt; \\text{Var}[Z_p]\\)\n\n\n\n\nTable 8.3: Statistics computed for \\(X_j\\) and \\(Z_j\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistic\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(Z_1\\)\n\\(Z_2\\)\n\\(Z_3\\)\n\\(Z_4\\)\n\n\n\n\nSample Mean\n0.218\n0.222\n0.544\n0.211\n0\n0\n0\n0\n\n\nSample Sd\n0.076\n0.081\n0.122\n0.075\n1.719\n0.918\n0.445\n0.024\n\n\nSample Var\n\n\n\n\n2.957\n0.844\n0.199\n0.0006\n\n\n% Variance\n\n\n\n\n73.9 %\n21%\n5%\n\\(&lt;\\) 1%\n\n\n\n\n\n\nThe sum of the sample variances of the \\(Z_j\\) in Table 8.3 is (within rounding error) \\[\n2.957 + 0.844 + 0.199 + 0.0006 = 4\n\\] The first principal component, \\(Z_1\\), explains \\(2.957/4 \\times 100\\% = 73.9\\%\\) of the variability in the input variables.\nAs you can see from Equation 8.5, PCA is an unsupervised learning method, it does not involve a target variable. The result of PCA, however, can be used in a supervised learning method, such as a regression model. A regression model that uses principal components as the input is called a principal component regression (PCR).\n\n\nPrincipal Component Regression (PCR)\nBased on the variance decomposition of the principal components (see the last row of Table 8.3), we can select a subset \\(Z_1, \\cdots, Z_M\\) from the scores \\(Z_1, \\cdots, Z_p\\). The number of principal components included into the model depends on how much variability in the \\(X\\)s we want to account for. If all \\(p\\) principal components are included in the model we have not really reduced the dimensionality. In the example above, the first two principal components account for 73.9% + 21 % = 94.9% of the variability; there is not much gained in choosing \\(M &gt; 2\\).\nOnce the \\(M\\) principal components have been selected, the linear model becomes\n\\[\n\\textbf{Y}_{n \\times 1} = \\theta_0 + \\textbf{Z}_{n \\times M}\\boldsymbol{\\theta}+ \\boldsymbol{\\epsilon}\n\\] The dimension of the problem has been reduced from \\(p+1\\) to \\(M+1\\).\nYou can show that this is equivalent to a linear model with coefficients \\[\n    \\beta_j = \\sum_{m=1}^M\\theta_m \\phi_{jm}\n\\] Principal component regression can be viewed as a method of constraining the coefficients, forcing the \\(\\beta_j\\) to take on this particular form.\nThe number of components \\(M\\) in PCR can be chosen heuristically or through cross-validation as in the following example.\n\n\nExample: Hitter Data (ISLR2) (Cont’d)\n\n\nTo apply PCR to the Hitters data we use the pcr function in the pls library. The validation= option determines whether \\(k\\)-fold cross-validation or leave-one-out cross-validation is performed. Here, we choose LOOCV. By default, pcr centers the data but does not scale it. scale=TRUE makes sure that the data are also scaled. We recommend that analyses based on principal components are always centered and scaled.\n\nlibrary(pls)\npcr.fit &lt;- pcr(Salary ~ ., data=Hit, \n               scale=TRUE, \n               validation=\"LOO\")\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 263 leave-one-out segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452      352    351.4    351.3      349    345.2    342.5\nadjCV          452      352    351.4    351.2      349    345.2    342.5\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       343.5    345.3      347     348.9     350.0     351.6     355.6\nadjCV    343.4    345.3      347     348.9     349.9     351.6     355.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        347.7     348.6     339.6     340.9     339.7     343.6\nadjCV     347.6     348.6     339.5     340.8     339.6     343.5\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\nThere are 19 input variables and hence there are 19 principal components. The output displays two tables with 19 columns each. The first reports the cross-validated root mean square error of prediction and a bias-adjusted version. The smallest error is achieved with six components in the model. The second table displays the cumulative proportion of variability explained. Using just the first principal component explains 38.31 % of the variability in the \\(X\\)s. That model has an \\(R^2\\) of 0.4063. Adding the second principal component adds 21.84 % of variability in the \\(X\\)s.\nThe model with 6 components, chosen by cross-validation, explains 88.63% of the variability in \\(X\\) and 46.48% of the variability in the Salary target. The sharp drop-off in mean-square error after the first component enters the model is seen in Figure 8.6. This is a pretty typical picture, because the components are ordered in terms of the proportion of variability explained. Selecting the hyper-parameter based on the “kink” or “elbow” in cross-validation plots is sometimes referred to as the “elbow method”.\n\nvalidationplot(pcr.fit,val.type=\"MSEP\",legendpos=\"topright\")\n\n\n\n\n\n\n\nFigure 8.6: Mean square prediction error as a function of number of principal components in PCR.\n\n\n\n\n\nThe chosen model can be fit with the following statements. Note that there is no change to the percentages of variability explained. The components 7–19 are simply not used in the model.\n\npcr.final &lt;- pcr(Salary ~ ., data=Hit, \n               scale=TRUE, \n               validation=\"none\",\n               ncomp=6)\nsummary(pcr.final)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 6\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63\nSalary    40.63    41.58    42.17    43.22    44.90    46.48\n\n\nHere are the loadings for the six components in the final model.\n\nround(loadings(pcr.final)[,1:6],4)\n\n            Comp 1  Comp 2  Comp 3  Comp 4  Comp 5  Comp 6\nAtBat       0.1983  0.3838 -0.0886  0.0320 -0.0281 -0.0706\nHits        0.1959  0.3773 -0.0740  0.0180  0.0047 -0.0822\nHmRun       0.2044  0.2371  0.2162 -0.2358 -0.0777 -0.1496\nRuns        0.1983  0.3777  0.0172 -0.0499  0.0385 -0.1367\nRBI         0.2352  0.3145  0.0731 -0.1390 -0.0243 -0.1117\nWalks       0.2089  0.2296 -0.0456 -0.1306  0.0325 -0.0195\nYears       0.2826 -0.2624 -0.0346  0.0953  0.0104  0.0332\nCAtBat      0.3305 -0.1929 -0.0836  0.0911 -0.0117  0.0244\nCHits       0.3307 -0.1829 -0.0863  0.0838 -0.0085  0.0294\nCHmRun      0.3190 -0.1263  0.0863 -0.0743 -0.0327 -0.0408\nCRuns       0.3382 -0.1723 -0.0530  0.0692  0.0176  0.0069\nCRBI        0.3403 -0.1681 -0.0150  0.0067 -0.0280  0.0115\nCWalks      0.3168 -0.1923 -0.0421  0.0304  0.0340  0.0340\nLeagueN    -0.0545 -0.0952 -0.5477 -0.3960 -0.0120 -0.1368\nDivisionW  -0.0257 -0.0367  0.0162  0.0427 -0.9857 -0.0909\nPutOuts     0.0777  0.1557 -0.0513 -0.2876 -0.1059  0.9241\nAssists    -0.0008  0.1687 -0.3979  0.5241  0.0111  0.0352\nErrors     -0.0079  0.2008 -0.3829  0.4219 -0.0553  0.1482\nNewLeagueN -0.0419 -0.0776 -0.5446 -0.4177 -0.0145 -0.1570\n\n\nThe loadings give us the weights of the input variables for each component. They show that each principal component is a linear combination of all the inputs. Examining the magnitude of the loading values gives an idea which inputs influence the \\(j\\)th component most. For example, the first component has large values for inputs related to hits, at bats, and runs.\nThe scores represent the \\(\\textbf{X}\\) matrix of a linear regression of \\(\\textbf{Y}\\) on the principal components.\n\nscores(pcr.final)[1:10,]\n\n                        Comp 1     Comp 2     Comp 3      Comp 4     Comp 5\n-Alan Ashby       -0.009630358 -1.8669625 -1.2627377 -0.93370088 -1.1075240\n-Alvin Davis       0.410650757  2.4247988  0.9074630 -0.26370961 -1.2296868\n-Andre Dawson      3.460224766 -0.8243753 -0.5544124 -1.61364990  0.8558560\n-Andres Galarraga -2.553449083  0.2305443 -0.5186536 -2.17210952  0.8187399\n-Alfredo Griffin   1.025746581  1.5705427 -1.3288484  3.48735458 -0.9815556\n-Al Newman        -3.973081710 -1.5044104  0.1551832  0.36913641  1.2070332\n-Argenis Salazar  -3.445150319 -0.5988471  0.6252834  1.99597066 -0.8054899\n-Andres Thomas    -3.425848614 -0.1133262 -1.9959449  0.76635168 -1.0141581\n-Andre Thornton    3.892286472 -1.9441629  1.8170103 -0.02666265  1.1349640\n-Alan Trammell     3.168770232  2.3878127 -0.7929565  2.56411717  0.9455254\n                       Comp 6\n-Alan Ashby        1.20966568\n-Alvin Davis       1.82314071\n-Andre Dawson     -1.02675473\n-Andres Galarraga  1.48885745\n-Alfredo Griffin   0.51269788\n-Al Newman         0.03344990\n-Argenis Salazar   0.20557943\n-Andres Thomas    -0.27227807\n-Andre Thornton   -0.81948303\n-Alan Trammell    -0.06113485\n\n\nYou can validate the score calculation by combining the \\(\\textbf{X}\\) matrix of the model with the loadings. For the selected PCR model with 6 components\n\nxm &lt;- scale(model.matrix(Salary ~ .,data=Hit)[,-1]) %*% loadings(pcr.final)[,1:6]\n\nThe \\(\\textbf{X}\\) matrix is centered and scaled to match the computations of the pcr() function. The scores and the matrix calculated from the loadings should be identical\n\nround(sum(scores(pcr.final)[,1:6] - xm),5) \n\n[1] 0\n\n\nIf the first 6 components are used in a regression with target Salary, the \\(R^2\\) of that regression should equal 0.4648, corresponding to 46.48% variance explained by 6 components in the PCR output.\n\nsummary(lm(Hit$Salary ~ scores(pcr.final)[,1:6]))$r.squared\n\n[1] 0.4648\n\n\n\n\n\n\n\nFigure 8.4: Rige trace for credit data\n\n\n\nFurnival, George M., and Robert W. Wilson. 1974. “Regression by Leaps and Bounds.” Technometrics 16 (4): 499–511.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nMallows, C. L. 1973. “Some Comments on \\(C_p\\).” Technometrics 15 (4): 661–75.\n\n\nMiller, Alan J. 1984. “Selection of Subsets of Regression Variables.” Journal of the Royal Statistical Society, Series A. 147 (3): 389–425.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature Selection and Regularization</span>"
    ]
  },
  {
    "objectID": "regnlr.html",
    "href": "regnlr.html",
    "title": "9  Nonlinear Regression",
    "section": "",
    "text": "9.1 Introduction\nA model is nonlinear if the derivative of the mean function with respect to any parameter is a function of one or more parameter. Curvilinear models such as the polynomial \\[\n\\text{E}[Y] = \\beta_0 + \\beta_1x + \\beta_2x^2\n\\] have a curved appearance when \\(\\text{E}[Y]\\) is plotted against \\(x\\) but they are not nonlinear models by our definition. Many models we work with in data science and statistics are highly nonlinear. A logistic regression model is nonlinear in the parameters. Neural networks deliberately introduce nonlinearity through activation functions. Support vector machines use nonlinear kernel transformation to translate the classification problem into a space where linear decision boundaries can be applied. The list goes on and on.\nThe models considered in this section are nonlinear regression models in the narrow sense, the target variable is continuous and the model errors are additive. The models depend on input variables but there is not a 1–1 correspondence between inputs and parameters. A nonlinear regression model might have one input variable and four parameters. The general form of the nonlinear regression model is\n\\[\\begin{align*}\n    \\textbf{Y}&= f(\\textbf{x},\\boldsymbol{\\theta}) + \\boldsymbol{\\epsilon}\\\\\n    \\boldsymbol{\\epsilon}&\\sim (\\textbf{0}, \\sigma^2\\textbf{I})\n\\end{align*}\\]\nExcept for the nonlinearity of the mean function, the assumptions from the classical linear model carry over: uncorrelated errors are additive with zero mean and constant variance.\nNonlinear regression models have advantages and disadvantages compared to their linear counterparts. Among the advantages are:\nAmong the disadvantages are:\nOverall, the advantages outweigh the added complications of nonlinear model fitting. Schabenberger and Pierce (2001) give numerous examples why nonlinear regression models are preferable over linear ones.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nonlinear Regression</span>"
    ]
  },
  {
    "objectID": "regnlr.html#introduction",
    "href": "regnlr.html#introduction",
    "title": "9  Nonlinear Regression",
    "section": "",
    "text": "the models are typically intrinsically interpretable, meaning that they can be interpreted based on the structure of the model alone. The parameters are meaningful quantities in terms of the subject matter domain.\nthe models are based on physical, biological, economic, etc. theory.\nthe models have fewer parameters compared to linear models.\n\n\n\nthe need for iterative fitting algorithms, there are no closed-form solutions for the parameter estimates\nthe need for starting values and potential sensitivity to the choice of starting values\ninference is often approximate\n\n\n\n\nExamples: 4:0 for Nonlinear Models\n\n\nParsimony. To achieve curvature with inflection in a polynomial model requires at least a model of third degree (\\(x^3\\)). Including the lower order terms, this model has 4 parameters. The nonlinear model \\(\\text{E}[Y] = 1-\\exp\\{-\\beta x^\\alpha\\}\\) achieves inflection with only two parameters.\nLimiting Behavior. Many biological or physical processes have bounds and asymptotes. It is difficult to incorporate that behavior into linear models and simple to build nonlinear models that are concave, convex, sigmoidal, sinusoidal, and so on.\nFirst Principles. Elementary assumptions about processes naturally lead to nonlinear models. For example, if \\(y\\) is the size of an organism at time \\(t\\), \\(\\alpha\\) is the maximum size, and the rate of growth \\(\\partial y/\\partial t\\) is proportional to the remaining size \\(\\alpha-y\\), then integrating the differential equation \\(\\partial y/\\partial t = \\beta(\\alpha - y)\\) leads to the nonlinear growth model \\[\ny(t) = \\alpha + (\\gamma - \\alpha)\\exp\\{-\\beta t\\}\n\\] where \\(\\gamma\\) represents the initial size (at time \\(t=0\\)).\nInterpretation. The parameters of nonlinear regression models are typically meaningful quantities that capture important aspects of the phenomenon under study. For the growth model in the previous equation \\(\\alpha\\) is the asymptotically achieved final size, \\(\\gamma\\) is the initial size at time \\(t=0\\), and \\(\\beta\\) relates to the rate of change that determines how quickly the organism grows from \\(\\gamma\\) to \\(\\alpha\\).\nThe mineralization potential of soil can be modeled as \\(\\text{E}[Y] = \\theta_0 (1-\\exp\\{-\\theta_1 t\\})\\) where \\(\\theta_0\\) is the maximum amount and \\(0 \\le 1-\\exp\\{-\\theta_1 t\\} \\le 1\\) is the proportional mineralization over time (for \\(\\theta_1 &gt; 0\\)) (Figure 9.1).\n\n\n\n\n\n\n\n\nFigure 9.1: The term \\(1-\\exp\\{-\\theta t\\}\\) for \\(\\theta = [0.5, 1, 2]\\).",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nonlinear Regression</span>"
    ]
  },
  {
    "objectID": "regnlr.html#estimation",
    "href": "regnlr.html#estimation",
    "title": "9  Nonlinear Regression",
    "section": "9.2 Estimation",
    "text": "9.2 Estimation\nParameter estimation in nonlinear regression models is typically done by nonlinear least squares (Section 4.2.3), finding a numeric solution that minimizes the residual sum of squares criterion Equation 4.1. Many algorithms can be used for that purpose, Gauss-Newton, Newton-Raphson, Levenberg-Marquardt, or steepest descent algorithms are common.\nThe Gauss-Newton or Newton-Raphson algorithms are usually implemented with modifications. Gauss-Newton, for example, does not guarantee that the residual sum of squares decreases from one iteration to the next. It is thus combined with a step size (or learning rate) that moves the parameter estimates only a partial step into the direction of the update, making sure that \\(\\text{SSE}^{(t+1)} &lt; \\text{SSE}^{(t)}\\). The step size can be determined by a line search algorithm or be fixed a priori.\nAs discussed in Section 4.2.3, finding the nonlinear LS solution can be expressed in terms of a series of linear models where the \\(\\textbf{X}\\) matrix is made up of the derivatives of the model, evaluated at the current iterate. Nonlinear least squares suffers from poorly conditioned cross-product matrices because the columns of the matrix of derivative tend to be highly correlated. The derivatives often involve similar terms, creating near-linear dependency and multicollinearity. To stabilize parameter estimation, regularization techniques can be used. The Levenberg-Marquardt algorithm, for example, applies ridge regression to reduce the collinearity in the cross-product matrix formed from the derivatives.\nThe iterations stop when some convergence criteria is met. The hope is that the algorithm converges to a global minimum in the residual sum of square surface. Monitoring the change in the SSE between iterations is thus common, as is monitoring the change in the parameter estimates. These criteria measure different aspects: a small change in the parameter estimates implies that a small increment of the estimates an be tolerated; a small change in SSE implies that the objective function is flat in the neighborhood of the current solution. Convergence criteria should always be relative criteria, not measuring the absolute change in a criterion between iterations. Finally, convergence of the algorithm according to some measure of change between iterations does not necessarily mean that a global minimum has been reached. Iterations might stop because of lack of progress. For example, when the residual sum of square surface is flat in the neighborhood of the current parameter estimates, a SSE-based criterion might halt the iterations when the iterations should continue to crawl out of the flat spot.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nonlinear Regression</span>"
    ]
  },
  {
    "objectID": "regnlr.html#sec-nlr-starting-values",
    "href": "regnlr.html#sec-nlr-starting-values",
    "title": "9  Nonlinear Regression",
    "section": "9.3 Starting Values",
    "text": "9.3 Starting Values\nA central issue in fitting nonlinear regression models is the choice of starting values. The algorithm kicks off with initial estimates \\(\\boldsymbol{\\theta}^{(0)}\\) and updates them (iterates) until the minimum of SSE is found. Starting values that are close to the solution greatly facilitate the estimation. Starting values far from the solution require more iterations and increase the odds to find a local minimum.\nA number of techniques can be used to find starting values for the parameters and these are often used in combination. We illustrate with an example from modeling plant yield with the Mitscherlich equation and simulated data.\n\n\nExample: Mitscherlich Equation for Plant Yield\n\n\nThe Mitscherlich equation is used in agricultural applications to model the yield of a crop as a function of some input \\(x\\), for example, a fertilizer. The systematic part of the equation is\n\\[\n\\lambda + (\\xi-\\lambda) \\exp\\left\\{ -\\kappa x\\right\\}\n\\]\nThe parameters have straightforward interpretations:\n\n\\(\\xi\\): the crop yield at \\(x=0\\)\n\\(\\lambda\\): the upper yield asymptote as \\(x\\) increases\n\\(\\kappa\\): is related to the rate of change, how quickly the yield increases from \\(\\xi\\) to \\(\\lambda\\)\n\nThe following code simulates 21 observations from a Mitscherlich model with \\(\\lambda = 40\\), \\(\\xi = 40\\), and \\(\\kappa = 0.01\\).\n\nset.seed(675)\nx &lt;- seq(from=0, to=400, by=20)\nlambda &lt;- 80\nxi &lt;- 40  \nkappa &lt;- 0.01 \nM &lt;- lambda + (xi-lambda) * exp(-kappa * x)\nYield &lt;- M + rnorm(length(M),mean=0,sd=3)\nmitsch &lt;- data.frame(Yield,M,x)\nplot(x,M, type=\"l\",las=1,bty=\"l\",ylab=\"Yield\",ylim=c(40,80))\npoints(x,Yield)\n\n\n\n\n\n\n\nFigure 9.2: Simulated yield data and Mitscherlich model\n\n\n\n\n\n\n\n\nGraphical Inspection\nA simple method for finding starting values is to glean values from a scatterplot of the data. For example, a four-parameter logistic response model \\[\n\\text{E}[Y] = \\delta + \\frac{\\alpha}{1+\\exp\\{\\beta-\\gamma x\\}}\n\\] has lower and upper asymptotes \\(\\delta\\) and \\(\\alpha + \\delta\\), respectively. The inflection point is located at \\(x= \\beta/\\gamma\\) and the slope at the inflection point is \\(\\alpha\\gamma/2\\). Starting values for all four parameters can be found by guesstimating the asymptotes and slope on a scatterplot and solving.\n\n\nExample: Mitscherlich Equation (Cont’d)\n\n\nThe Mitscherlich equation has a lower and upper bound of yield. The lower bound \\(\\xi\\) occurs at \\(x=0\\). Based on the data points in Figure 9.2, starting values of \\(\\xi^{(0)} = 40\\) and \\(\\lambda^{(0)} = 80\\) are reasonable.\n\n\n\n\nEliminating Linear Parameters\nSome models contain linear and nonlinear components. A simple transformation or holding a parameter fixed can then lead to an interim linear model for which values can be obtained by linear regression. The model \\[\n\\text{E}[Y] = \\theta_0 + \\theta_1 x + \\theta_2z^{\\theta_3}\n\\] is nonlinear in \\(\\theta_3\\) but once \\(\\theta_3\\) is fixed, this is a three-parameter linear model. Starting values for \\([\\theta_0, \\theta_1, \\theta_2]\\) can be found by linear least squares after setting \\(\\theta_3\\) to some reasonable value. \\(z^{\\theta_3}\\) is then just another input variable in the linear model.\n\n\nExample: Mitscherlich Equation (Cont’d)\n\n\nObtaining good starting values for \\(\\lambda\\) and \\(\\xi\\) can be done by visual inspection of the data. Finding a good starting value for \\(\\kappa\\) is trickier. We can lean on the math to find a value using linear regression.\nThe formula for the Mitscherlich equation is \\[\n\\text{E}[Y] = \\lambda + (\\xi-\\lambda) \\exp\\left\\{ -\\kappa x\\right\\}\n\\]\nMoving terms around and taking logarithms gives \\[\n\\log(\\lambda - \\text{E}[Y]) = \\log(\\lambda-\\xi) -\\kappa x\n\\]\nWe can approximate \\(\\kappa\\) as the negative slope of a linear regression with target \\(\\log(\\lambda^{(0)}-Y)\\) where \\(\\lambda^{(0)}\\) is the initial guess for \\(\\lambda\\). We can then also get a starting value for \\(\\xi^{(0)}\\) by transforming the intercept of the regression.\n\nlm.fit &lt;- lm(log(80-Yield) ~ x, data=mitsch)\nlm.fit\n\n\nCall:\nlm(formula = log(80 - Yield) ~ x, data = mitsch)\n\nCoefficients:\n(Intercept)            x  \n      3.754       -0.010  \n\n\nFrom this linear regression we get starting values\n\n\\(\\kappa^{(0)}\\) = 0.01\n\\(\\xi^{(0)}\\) = 80 - exp(3.7544) = 37.2898\n\n\n\n\n\nGrid Search\nA grid search evaluates the residual sum of squares at a set of starting values. Each parameter takes on a set of starting values and the grid is the cartesian product of the parameter sets. For example, if the set of starting values for \\(\\theta_1\\) is \\(S_1 = \\{1, 1.5, 2\\}\\), the set for \\(\\theta_2\\) is \\(S_2 = \\{0.1, 0.2\\}\\) and the set for \\(\\theta_3\\) is \\(S_3 = \\{10, 100\\}\\), their cartesian product–the grid of the starting value sets–has cardinality \\(|S_1| \\times |S_2| \\times |S_3| = 3 \\times 2 \\times 2 = 12\\).\nThe grid search evaluates the residual sum of squares at each point on the grid and starts the iterations from the set of starting values that produce the smallest SSE.\n\n\n\n\n\n\nNote\n\n\n\nThe grid search does not fit the model to each set of parameters on the grid and chooses the one that leads to the lowest converged SSE. The model is fit to only one vector of starting values, the one that produces the smallest SSE when the model is evaluated at the starting values.\n\n\nThe number of parameter combinations for which the initial SSE needs to be computed can grow very quickly when the number of parameters in the grid is large or when the grid is very granular. This can take considerable conmputing resources, although the grid evaluation can be easily executed in parallel.\n\n\nExample: Mitscherlich Equation (Cont’d)\n\n\nYou can fit nonlinear regression models by least squares in R with the nls function in the built-in stats package. Unfortunately, nls accepts only one vector of starting values and does not perform a grid search. The nls2 function in the package by the same name allows a grid of starting values. You combine the functions as follows:\n\nSet up a grid of starting values and call nls2 with algorithm=\"grid-search\". This will return as the coefficients of a nonlinear regression object the best set of starting values.\nCall nls with the best set returned from nls2 as the starting values or call nls2 again, passing the best set from the grid search as the starting values.\n\nThe following code creates a \\(5 \\times 6 \\times 4\\) grid of 120 sets of starting values for the three parameters of the Mitscherlich equation. The equation is defined through a model formula that is passed to nls2 as the first argument.\n\nlibrary(nls2)\n\ngr &lt;- expand.grid(lambda=seq(10,100,20), \n                  xi    =seq(0,50,10),\n                  kappa =seq(0.001,0.01,0.0025))\n\nmit_eq &lt;- Yield ~ lambda + (xi-lambda)*exp(-kappa*x)\nbest_start &lt;- nls2(formula  =mit_eq, \n                   data     =mitsch,\n                   start    =gr,\n                   algorithm=\"grid-search\")\n\nbest_start\n\nNonlinear regression model\n  model: Yield ~ lambda + (xi - lambda) * exp(-kappa * x)\n   data: mitsch\nlambda     xi  kappa \n 9e+01  4e+01  6e-03 \n residual sum-of-squares: 334.6\n\nNumber of iterations to convergence: 120 \nAchieved convergence tolerance: NA\n\n\nAmong the sets of starting values examined, \\(\\lambda_0\\) =90, \\(\\xi_0\\) = 40, \\(\\kappa_0\\) =0.006 produce the smallest SSE= 334.645\nNote that the output from nls2 suggests that an actual optimization with 120 iterations was performed and that the algorithm converged. 120 is the size of the grid of starting values passed to nls2. The reported error sum of squares is obtained by evaluating the Mitscherlich equation at the starting values, it is not the SSE at a converged solution. The following code verifies this:\n\nlambda_0 &lt;- coef(best_start)[1]\nxi_0 &lt;- coef(best_start)[2]\nkappa_0 &lt;- coef(best_start)[3]\nfit_0 &lt;- lambda_0 + (xi_0-lambda_0)*exp(-kappa_0*mitsch$x)\n\nsum((mitsch$Yield - fit_0)^2)\n\n[1] 334.6452\n\n\nWe now need to perform the actual nonlinear regression estimation using the best starting values determined by nls2 as starting values for nls().\n\nmitsch_fit &lt;- nls(mit_eq, start=coef(best_start), data=mitsch)\nmitsch_fit\n\nNonlinear regression model\n  model: Yield ~ lambda + (xi - lambda) * exp(-kappa * x)\n   data: mitsch\n  lambda       xi    kappa \n80.02190 39.93216  0.00906 \n residual sum-of-squares: 102.5\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 5.945e-06\n\n\nThe algorithm coverges very quickly, after only three iterations. The converged parameter estimates are \\(\\widehat{\\lambda} =\\) 80.0219, \\(\\widehat{\\xi} =\\) 39.9322, \\(\\widehat{\\kappa} =\\) 0.0091.\nAlternatively, you can call the nls2() function again and pass the previous return object, using the default fitting algorithm:\n\nnls2(mit_eq, best_start, data=mitsch)\n\nNonlinear regression model\n  model: Yield ~ lambda + (xi - lambda) * exp(-kappa * x)\n   data: mitsch\n  lambda       xi    kappa \n80.02190 39.93216  0.00906 \n residual sum-of-squares: 102.5\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 5.945e-06\n\n\nYou can also let nls2() generate a grid for you by providing a two-row data frame that defines the bounding box for the starting values and control the density of the grid with the maxiter parameter of nls.control().\n\nparm_box &lt;- data.frame(lambda=c(10,100),\n                       xi    =c(0,50),\n                       kappa =c(0.001,0.01))\n\nmit_eq &lt;- Yield ~ lambda + (xi-lambda)*exp(-kappa*x)\nbest_start &lt;- nls2(formula  =mit_eq, \n                   data     =mitsch,\n                   control  =nls.control(maxiter=100),\n                   start    =parm_box,\n                   algorithm=\"grid-search\")\n\nbest_start\n\nNonlinear regression model\n  model: Yield ~ lambda + (xi - lambda) * exp(-kappa * x)\n   data: mitsch\nlambda     xi  kappa \n 77.50  37.50   0.01 \n residual sum-of-squares: 160.4\n\nNumber of iterations to convergence: 125 \nAchieved convergence tolerance: NA\n\n\n\nnls2(mit_eq, best_start, data=mitsch)\n\nNonlinear regression model\n  model: Yield ~ lambda + (xi - lambda) * exp(-kappa * x)\n   data: mitsch\n  lambda       xi    kappa \n80.02192 39.93217  0.00906 \n residual sum-of-squares: 102.5\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 3.783e-06\n\n\nWith the best starting values from this evaluation, the final fit converges to almost the same parameter estimates as with the best starting values from the earlier grid.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nonlinear Regression</span>"
    ]
  },
  {
    "objectID": "regnlr.html#reparameterization",
    "href": "regnlr.html#reparameterization",
    "title": "9  Nonlinear Regression",
    "section": "9.4 Reparameterization",
    "text": "9.4 Reparameterization\nReparameterization of a model is the re-expression of a model in terms of a different set of parameters. The overall fit of the model remains the same, but the meaning of the parameters changes. Reparameterization is an important and underappreciated aspect of nonlinear modeling. Expressing the model in terms of different parameters has many uses:\n\nfacilitating finding of starting values\nreducing the multicollinearity between the columns of the derivative matrix\nimposing constraints on the parameter estimates\n\n\nImposing a Positivity Constraint\n\n\nExample: Theophylline Concentration\n\n\nBoeckmann, Sheiner, and Beal (1992) report serum concentration profiles for the anti-asthma drug theophylline, administered orally to 12 subjects. The serum concentrations were measured at ten time points over a 25-hour period. The subjects fall into two groups, depending on one of two doses of theophylline administered. The data have a longitudinal structure and have been analyzed using nonlinear mixed models by Davidian and Giltinan (1995), Pinheiro and Bates (1995), and others.\nWe ignore the longitudinal aspect of the data here and fit a nonlinear regression model to data from all subjects–we will return to these data with a longitudinal analysis in Chapter 30.\n\nlibrary(\"duckdb\")\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\ntheoph &lt;- dbGetQuery(con, \"SELECT * FROM theoph_all\")\n\ndbDisconnect(con)\nhead(theoph,20)\n\n   Subject   Wt Dose  Time  conc\n1        1 79.6 4.02  0.00  0.74\n2        1 79.6 4.02  0.25  2.84\n3        1 79.6 4.02  0.57  6.57\n4        1 79.6 4.02  1.12 10.50\n5        1 79.6 4.02  2.02  9.66\n6        1 79.6 4.02  3.82  8.58\n7        1 79.6 4.02  5.10  8.36\n8        1 79.6 4.02  7.03  7.47\n9        1 79.6 4.02  9.05  6.89\n10       1 79.6 4.02 12.12  5.94\n11       1 79.6 4.02 24.37  3.28\n12       2 72.4 4.40  0.00  0.00\n13       2 72.4 4.40  0.27  1.72\n14       2 72.4 4.40  0.52  7.91\n15       2 72.4 4.40  1.00  8.31\n16       2 72.4 4.40  1.92  8.33\n17       2 72.4 4.40  3.50  6.85\n18       2 72.4 4.40  5.02  6.08\n19       2 72.4 4.40  7.03  5.40\n20       2 72.4 4.40  9.00  4.55\n\n\nLike Davidian and Giltinan (1995), we eliminate the data points at time \\(t=0\\) from the analysis of a one-compartmental model with first-order absorption and elimination. This model expresses \\(C(t)\\), the serum concentration at time \\(t\\) following administration of dose \\(D\\), as \\[\nC(t) = \\frac{D k_a}{V(k_a - Cl/V)}\\left\\{\\exp\\left(-\\frac{Cl}{V}t\\right) - \\exp(-k_a t) \\right\\}\n\\tag{9.1}\\]\nThe parameters to be estimated in Equation 9.1 are \\(\\{k_a,V,Cl\\}\\), the rate of absorption \\(k_a\\), the volume of distribution \\(V\\), and the clearance \\(Cl\\).\nWe first fit the model in this parameterization\n\ncomp_model &lt;- conc ~ (Dose * ka)/(V*(ka-Cl/V)) * (exp(-Cl/V*Time) - exp(-ka*Time))\nfit1 &lt;- nls(formula=comp_model,\n            start  =list(Cl=0.01, ka=1, V=0.2), \n            data   =theoph,\n            subset =(theoph$Time &gt; 0))\nsummary(fit1)\n\n\nFormula: conc ~ (Dose * ka)/(V * (ka - Cl/V)) * (exp(-Cl/V * Time) - exp(-ka * \n    Time))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nCl 0.038842   0.003031  12.816  &lt; 2e-16 ***\nka 1.490674   0.183765   8.112 5.56e-13 ***\nV  0.484798   0.024701  19.626  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.53 on 117 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 1.2e-06\n\n\nThe fit converges in 6 iterations to estimates \\(\\widehat{Cl}\\) = 0.0388, \\(\\widehat{k}_a\\) = 1.4907, and \\(\\widehat{V}\\) = 0.4848.\nThankfully, the estimates for all parameters are positive; a negative absorption, negative clearance, or negative volume would be difficult to explain. The positivity constraint can be built into the model through reparameterization. Instead of estimating \\(k_a\\), \\(Cl\\), and \\(V\\), the model can be written in terms of \\(\\beta_1 = \\log Cl\\), \\(\\beta_2 = \\log k_a\\), and \\(\\beta_3 = \\log V\\).\n\ncomp_model2 &lt;- conc ~ (Dose * exp(beta2)) / (exp(beta3)*(exp(beta2)-exp(beta1)/exp(beta3))) *\n                       (exp(-exp(beta1)/exp(beta3)*Time) - exp(-exp(beta2)*Time))\n\nfit2 &lt;- nls(formula=comp_model2, \n            start  =list(beta1=-3, beta2=0.2, beta3=-0.8), \n            data   =theoph,\n            subset =(theoph$Time &gt; 0))\nsummary(fit2)\n\n\nFormula: conc ~ (Dose * exp(beta2))/(exp(beta3) * (exp(beta2) - exp(beta1)/exp(beta3))) * \n    (exp(-exp(beta1)/exp(beta3) * Time) - exp(-exp(beta2) * Time))\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nbeta1 -3.24826    0.07803 -41.630  &lt; 2e-16 ***\nbeta2  0.39923    0.12328   3.238  0.00156 ** \nbeta3 -0.72402    0.05095 -14.210  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.53 on 117 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 2.822e-06\n\n\nThe \\(\\beta\\) estimates now can range on the real line, \\(-\\infty &lt; \\beta_j &lt; \\infty\\), the exponentiated values will be positive. The reparameterized model converges to the same solution, expressed in terms of different parameters. The equivalence can be verified by exponentiating the estimates of fit2:\n\nexp(coef(fit2))\n\n     beta1      beta2      beta3 \n0.03884159 1.49067652 0.48479808 \n\ncoef(fit1)\n\n        Cl         ka          V \n0.03884161 1.49067360 0.48479779 \n\n\n\n\nFor parameters that represent proportions a logit transform is useful. If \\(0 \\le \\alpha \\le 1\\), we can instead fit the unconstrained parameter \\(\\theta = \\log (\\alpha/(1-\\alpha)\\). This should look familiar, it is the transformation that maps the mean in logistic regression to the linear predictor.\n\n\nExpected Value Parameterization\nThis technique is helpful to find starting values for parameters and has a positive side-effect in reducing a curvature component of the nonlinear model. A model has two curvatures, called the intrinsic and the parameter-effects curvatures. Intrinsic curvature measures how much the model bends if the parameters are changed when the inputs are held fixed. (This is not the same as measuring how much the mean function changes with a change in the inputs.)\nParameterization does not affect the intrinsic curvature, but it affects the second component, the parameter-effects curvature. This component measures the quality of the linear approximation in a neighborhood of the parameter values.\nModels with large curvature have undesirable statistical properties, they converge more slowly, the parameter estimates are biased with unreliable standard errors and the asymptotic inference assuming that the estimates are Gaussian distributed is questionable.\nThe expected value parameterization introduced by Ratkowsky (1983, 1990) reduces the parameter-effects curvature which he argues is the greater of the two curvature components.\nSuppose the model has a single input variable, \\(x\\). By visual inspection based on the scatterplot of \\(Y\\) versus \\(x\\) you can obtain a guesstimate of \\(\\text{E}[Y|x^*]\\) for a particular value \\(x^*\\) of the input. Call this estimate \\(\\mu^*\\) and set it equal to \\(f(x^*,\\boldsymbol{\\theta})\\). You can now replace one of the parameters with \\(\\mu^*\\) for which you already have a starting value. An example from Schabenberger and Pierce (2001, 209) will make this approach more tangible.\n\n\nExample: Michaelis-Menten Model\n\n\nThe Michaelis-Menten model is popular in biochemical applications to describe chemical reactions in enzyme systems: \\[\n\\text{E}[Y|x] = \\frac{Vx}{x+K}\n\\tag{9.2}\\] \\(Y\\) is the velocity of the chemical reaction, \\(x\\) is the substrate concentration. The parameters \\(V\\) and \\(K\\) measure the theoretical maximum velocity (\\(V\\)) and the substrate concentration at with velocity \\(V/2\\) is attained (\\(K\\)).\nChoose any concentration \\(x^*\\) and estimate the velocity \\(\\mu^*\\) from a scatterplot of the data. Under the Michaelis-Menten model we now have \\[\n\\mu^* = \\frac{Vx^*}{x^*+K}\n\\] Solve for \\(V\\), the parameter most difficult to specify: \\[\nV = \\mu^* \\, \\frac{x^*+K}{x^*}\n\\] You can now substitute this expression for \\(V\\) in Equation 9.2, leading to the reparameterized model \\[\n\\text{E}[Y|x] = \\mu^* \\frac{x+xK/x^*}{x^*+xK/x^*}\n\\] Instead of the parameters \\(\\boldsymbol{\\theta}= [V,K]\\) you are now estimating \\(\\boldsymbol{\\theta}^* = [\\mu^*,K]\\). This model has the same intrinsic curvature as Equation 9.2 but has lower parameter-effects curvature and the starting value for \\(\\mu^*\\) is easily found by graphical inspection.\nChoosing a second pair of values \\((x^{**},\\mu^{**})\\), you can also replace \\(K\\) with an expected-value parameter.\n\n\n\n\nDefining Relationships\nThe expected-value reparameterization does not lead to a more readable form of the model, we choose a value \\(x^*\\) and replace one of the parameters with \\(\\mu^*\\). Schabenberger and Pierce (2001) describe the reverse process, choosing a value \\(\\mu^*\\) and replacing a model parameter with the input value \\(x^*\\) that corresponds to it. This is termed reparameterization through defining relationships and is useful when the goal is to estimate \\(x^*\\), an inverse prediction problem.\nThis occurs frequently in dose-response studies where one is interested in the amount of input one has to provide to achieve a particular output. For example, the \\(LD_{50}\\) represents the lethal dosage at which 50% of the subjects die, the \\(GR_{25}\\) represents the input that increases or decreases growth by 25%. Turning these quantities into model parameters has the added benefit that we can obtain estimates and standard errors, compute confidence intervals and test hypotheses.\nAs with expected-value parameters, the defining relationship is \\[\n\\text{E}[Y|x^*] = f(x^*,\\boldsymbol{\\theta})\n\\] but now we are solving for \\(x^*\\) in the quest to replace a parameter and not for \\(\\text{E}[Y|x^*]\\).\n\n\nExample: Log-logistic Growth Model\n\n\nSchabenberger and Pierce (2001, 234–36) apply these ideas to a log-logistic model for growth: \\[\n\\text{E}[Y|x] = \\delta + \\frac{\\alpha-\\delta}{1+\\psi\\exp\\{\\beta\\log(x)\\}}\n\\] The asymptotes of the growth model are \\(\\alpha\\) and \\(\\delta\\) and growth transitions in sigmoidal shape between those. Define \\(\\lambda_K\\) as the the value at which \\(K\\%\\) of the difference between lower and upper asymptote is reached \\[\n\\text{E}[Y|\\lambda_K] = \\delta + \\frac{100-K}{100}(\\alpha-\\delta)\n\\] Now consider the defining relationship \\[\n\\delta + \\frac{100-K}{100}(\\alpha-\\delta) = \\delta + \\frac{\\alpha-\\delta}{1+\\psi\\exp\\{\\beta\\log(\\lambda_K)\\}}\n\\] Schabenberger et al. (1999) solve for \\(\\psi\\), leading to the reparameterized log-logistic model \\[\n\\text{E}[Y|x] = \\delta + \\frac{\\alpha-\\delta}{1+\\frac{K}{100-K}\\exp\\{\\beta\\log(x/\\lambda_K)\\}}\n\\] Figure 9.3 shows simulated data from a log-logistic model.\n\n\n\n\n\n\n\n\nFigure 9.3: Simulated data under a log-logistic model\n\n\n\n\n\nWe can fit the model in the \\(\\lambda_K\\) parameterization as follows, for example, with \\(K=50\\):\n\nK &lt;- 50\nloglog &lt;- y ~ delta + (alpha-delta)/(1+(K/(100-K)*exp(beta*log(x/lambda))))\nfit &lt;- nls(formula=loglog, data=df,\n           start=list(alpha=100, delta=3, lambda=0.2, beta=1)\n           )\nsummary(fit)\n\n\nFormula: y ~ delta + (alpha - delta)/(1 + (K/(100 - K) * exp(beta * log(x/lambda))))\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nalpha  100.64422    6.48491  15.520  &lt; 2e-16 ***\ndelta   13.52256    4.50904   2.999  0.00404 ** \nlambda   0.19033    0.01441  13.205  &lt; 2e-16 ***\nbeta     2.78986    0.61379   4.545 2.98e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.26 on 56 degrees of freedom\n\nNumber of iterations to convergence: 12 \nAchieved convergence tolerance: 5.33e-06\n\n\nFor \\(K=50\\), \\(\\lambda\\) is the point on the \\(x\\)-axis where the difference between upper and lower asymptote is reduced by half, \\(\\widehat{\\lambda}=\\) 0.1903. We can validate that easily:\n\npredict(fit,newdata=data.frame(x=coef(fit)[3]))\n\n[1] 57.08339\n\nas.numeric(coef(fit)[1] - 0.5*(coef(fit)[1] - coef(fit)[2]))\n\n[1] 57.08339\n\n\nFigure 9.4 displays observed and fitted values along with the \\(\\widehat{\\lambda}_{50}\\) value.\n\n\n\n\n\n\n\n\nFigure 9.4: Observed and fitted values with \\(\\lambda_{50}\\)\n\n\n\n\n\nThe model is easily changed to estimate a different point of reduction, for example \\(\\lambda_{75}\\):\n\nK &lt;- 75\nloglog &lt;- y ~ delta + (alpha-delta)/(1+(K/(100-K)*exp(beta*log(x/lambda))))\nfit2 &lt;- nls(formula=loglog, data=df,\n           start=list(alpha=100, delta=3, lambda=0.2, beta=1)\n           )\nsummary(fit2)\n\n\nFormula: y ~ delta + (alpha - delta)/(1 + (K/(100 - K) * exp(beta * log(x/lambda))))\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nalpha  100.64457    6.48500  15.520  &lt; 2e-16 ***\ndelta   13.52231    4.50915   2.999  0.00404 ** \nlambda   0.28218    0.03042   9.275 6.50e-13 ***\nbeta     2.78981    0.61378   4.545 2.98e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.26 on 56 degrees of freedom\n\nNumber of iterations to convergence: 11 \nAchieved convergence tolerance: 7.44e-06\n\n\nNotice that the parameter estimates of \\(\\alpha\\), \\(\\delta\\), and \\(\\beta\\) do not change, and the overall fit is the same, as can be seen from the residual standard error. The estimate of the point where the difference between the asymptotes is reduced by 75% is \\(\\widehat{\\lambda}_{75}=\\) 0.2822.\n\npredict(fit2,newdata=data.frame(x=coef(fit2)[3]))\n\n[1] 35.30288\n\nas.numeric(coef(fit2)[1] - 0.75*(coef(fit2)[1] - coef(fit2)[2]))\n\n[1] 35.30288\n\n\n\n\n\n\n\n\nBoeckmann, A. J., L. B. Sheiner, and S. L. Beal. 1992. NONMEM Users Guide: Part v, Introductory Guide. NONMEM Project Group, University of California, San Francisco.\n\n\nDavidian, M., and D. M. Giltinan. 1995. Nonlinear Models for Repeated Measurement Data. Chapman & Hall, London.\n\n\nPinheiro, J. C., and D. M. Bates. 1995. “Approximations to the Log-Likelihood Function in the Nonlinear Mixed-Effects Model.” Journal of Computational and Graphical Statistics 4: 12–35.\n\n\nRatkowsky, D. A. 1983. Nonlinear Regression Modeling. Marcel Dekker, New York.\n\n\n———. 1990. Handbook of Nonlinear Regression Models. Marcel Dekker, New York.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary Statistical Models for the Plant and Soil Sciences. CRC Press, Boca Raton.\n\n\nSchabenberger, O., B. E. Tharp, Kells J. J., and D. Penner. 1999. “Statistical Tests for Hormesis and Effective Dosages in Herbicide Dose Response.” Agronomy Journal 91: 713–21.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nonlinear Regression</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html",
    "href": "regdiscrete.html",
    "title": "10  Discrete Target Variables",
    "section": "",
    "text": "10.1 Introduction\nThe classical linear model (Chapter 7) is appropriate when the mean function is linear in the parameters and the errors are uncorrelated with constant variance. The error conditions are met in a random sample from a Gaussian distribution. It does not take much to shake the foundation of the classical model. For example, when the target variable is the time until a manufactured item fails, the exponential or Weibull distribution is more appropriate than the Gaussian and the assumption of constant variance no longer holds. A characteristic of these distributions is a dependence of the variance on the mean. A random variable \\(Y\\) has an exponential distribution if its density function is given by \\[\nf(y) = \\left \\{ \\begin{array}{ll}\\lambda e^{-\\lambda y} & y \\ge 0 \\\\ 0 & y &lt; 0 \\end{array} \\right .\n\\] The mean and variance are \\(\\text{E}[Y] = 1/\\lambda\\) and \\(\\text{Var}[Y] = 1/\\lambda^2 = 1/\\text{E}[Y]^2\\). If the mean changes, for example as a function of inputs, the variance will change as well.\nAlso, the support of the exponential random variable is different from the Gaussian. The former takes on non-negative values (the time lapsed between telephone calls cannot be negative) whereas the Gaussian takes on values from \\(-\\infty\\) to \\(+\\infty\\). A linear model for the mean function is probably a bad idea because there is no guarantee that predicted values \\(\\widehat{y} = \\textbf{x}^\\prime \\widehat{\\boldsymbol{\\beta}}\\) are non-negative. To ensure that predicted values comply with the support of the random variable, a transformation such as \\[\n\\widehat{y} = \\exp\\{ \\textbf{x}^\\prime \\widehat{\\boldsymbol{\\beta}} \\}\n\\] might be useful. The linear predictor \\(\\widehat{\\eta} = \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}}\\) can now take on values between \\(-\\infty\\) and \\(+\\infty\\) and the predicted values obey \\(\\widehat{y} \\ge 0\\). But the exponential transformation made the model for the mean nonlinear in the parameters. By considering a different distribution for the target variable, the classical linear model framework breaks down quickly–even for a continuous random variable like the time between some events.\nWhen the target variable is discrete–rather than continuous–the linear model breaks down in the same way, the consequences for sticking with the classical linear model framework are more severe, however.\nRecall from Section 1.2.0.4 that target variables are categorized as continuous or discrete depending on whether the number of possible values is countable. Discrete variables, with countable support, are further categorized into count variables and categorical variables. The value of a count variable is an integer that indicates how many times something occurred. The value of a categorical variable is a label that describes a state. For example, the number of vehicle accidents per hour is a count variable, the make of the vehicle involved in the accidents is a categorical variable.\nIn this chapter we consider regression models for discrete random variables: binary variables that take on two values, ordinal and nominal variables that take on more than two values and true count variables. The discussion will be somewhat informal. The models will be re-introduced and discussed more formally in a later chapter under the umbrella of generalized linear models (GLMs, Chapter 27).\nHow does the failure time example in this introduction relate to modeling discrete targets? If the time between events has an exponential distribution, the number of events that occur within a unit of time–a count variable–has a Poisson distribution. Both distributions are members of a special family of probability distributions, known as the exponential family. GLMs are statistical models for target variables that have a distribution in the exponential family—more details in Section 27.2.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#introduction",
    "href": "regdiscrete.html#introduction",
    "title": "10  Discrete Target Variables",
    "section": "",
    "text": "Caution\n\n\n\nDo not confuse the exponential distribution with the exponential family of distributions.\nThe term exponential in the family of distributions comes from a particular way of writing the mass or density function, using exponentiation, see Section 27.2. Many distributions can be written that way, including the exponential density shown earlier. The exponential distribution is one member of the exponential family, other members include the Binary, Binomial, Negative Binomial, Poisson, Gaussian, Beta, and Gamma distributions.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#modeling-binary-data",
    "href": "regdiscrete.html#modeling-binary-data",
    "title": "10  Discrete Target Variables",
    "section": "10.2 Modeling Binary Data",
    "text": "10.2 Modeling Binary Data\nA categorical variable is called binary if it takes on two possible values. The two states (or classes) are sometimes referred to as “success” and “failure”, we prefer to use the generic terms “event” and “non-event”. When modeling the recurrence of cancer, it is best not to refer to the event being modeled as a “success”. The states are coded numerically as \\((1,0)\\) or \\((-1,1)\\). The former is common in regression modeling and the state coded as 1 is the event of interest. Coding the states as \\(-1\\) and \\(1\\) can be useful in classification models.\nA binary random variable takes on the values 1 and 0 with probabilities \\(\\pi\\) and \\(1-\\pi\\), respectively. The probability mass function can be written conveniently as \\[\n\\Pr(Y=y) = \\pi^y \\, (1-\\pi)^{(1-y)}\n\\]\nIt is easy to show that \\(\\text{E}[Y] = \\pi\\) and \\(\\text{Var}[Y] = \\pi(1-\\pi)\\). The model for the mean function of a binary target variable must be suitable for a probability. If the model has a linear predictor component \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\), some transformation is needed to map between \\[-\\infty &lt; \\eta &lt; \\infty\\] and \\[0 \\le \\pi \\le 1\\]\nThis transformation, when applied to the mean, is called the link function. The inverse transformation, the inverse link function, is applied to the linear predictor\n\\[\\begin{align*}\n    g(\\pi) &= \\eta \\\\\n    \\pi &= g^{-1}(\\eta)    \n\\end{align*}\\]\n\nLogistic Regression\nWhere do we find functions that can be used as link/inverse link functions? One place to look are the cumulative distribution functions of random variables that are defined on \\(-\\infty &lt; x &lt; \\infty\\). For example, take the c.d.f. of the logistic distribution \\[F(x) = \\Pr(X \\le x) = \\frac{1}{1-\\exp\\{-(x-\\mu)/\\phi\\}} \\quad -\\infty &lt; x &lt; \\infty\\]\n\\(\\mu\\) and \\(\\phi\\) are the mean and scale parameter of the logistic distribution. The standard logistic has \\(\\mu=0\\) and \\(\\sigma=1\\) and c.d.f. \\[\nF(x) = \\frac{1}{1+e^{-x}}\n\\] The inverse operation, to compute the value of \\(x\\) associated with a cumulative probability \\(p\\), is the quantile function. For the standard logistic distribution the quantile function is \\[\nQ(p) = \\log \\left(\\frac{p}{1-p} \\right)\n\\] This quantile function is known as the logit.\n\n\nDefinition: Logistic Regression\n\n\nA logistic regression model is a generalized linear model for Binary data with linear predictor \\(\\eta = \\textbf{x}^\\prime \\boldsymbol{\\beta}\\) and logit link function \\[\n\\log\\left(\\frac{\\text{E}[Y|\\textbf{x}]}{1-\\text{E}[Y|\\textbf{x}]} \\right) = \\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\n\\] \\[\n\\text{E}[Y|\\textbf{x}] = \\frac{1}{1+\\exp\\{-\\eta\\}}=\\frac{1}{1+\\exp\\{-\\textbf{x}^\\prime\\boldsymbol{\\beta}\\}}\n\\]\n\n\nThe logit link function is a convenient choice for modeling binary data, it is symmetric about zero, has a simple expression and its inverse can be computed easily. With a logit link the coefficients have a simple and appealing interpretation in terms of log odds ratios (see below). Many other functions could serve as link functions. For example, if \\(\\phi(x)\\) is the c.d.f. of a standard Gaussian random variable, the quantile function \\(\\phi^{-1}(p)\\) can be used as a link function. This configuration is called **probit* regression. Logistic and Gaussian c.d.f. and quantile function have similar shapes (Figure 10.1), computations of the Gaussian probabilities and quantiles is numerically much more involved, requiring approximations.\n\n\n\n\n\n\n\n\nFigure 10.1: Logistic (dashed) and standard normal (solid) distribution functions.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term logistic regression is often used for any general linear model for binary data, regardless of which link function is used. Other link functions than the logit and probit you might encounter are the log-log and complementary log-log links. These are similar in shape to the logit link but are not symmetric about zero.\n\n\n\n\nExample: Credit Default–ISLR\n\n\nThe Default data is part of the ISLR2 library (James et al. 2021), a simulated data set with ten thousand observations. The target variable is default, whether a customer defaulted on their credit card debt. Input variables include a factor that indicates student status, account balance and income information.\n\nlibrary(ISLR2)\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n\nThe default target variable is a factor. It can be passed directly to the glm function, which models the first level of the factor as the non-event and all other levels are considered events. It is thus important to know how the factor levels are ordered:\n\nstr(Default$default)\n\n Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nIf you wish to work with the underlying values of the factor, use the unclass function:\n\nunique(unclass(Default$default))\n\n[1] 1 2\n\n\nNext, we split the data into a train and test data set using a 90:10 split.\n\nset.seed(765)\nn &lt;- nrow(Default)\ntestset &lt;- sort(sample(n,n*0.1))\ntest &lt;- Default[testset,]\ntrain &lt;- Default[-testset,]\nnrow(train)\n\n[1] 9000\n\nnrow(test)\n\n[1] 1000\n\n\nWorking with binary data is easier in some ways compared to continuous response data and trickier in other ways. The following plot shows the responses (coded Default=1, non-Default=0) against the income input.\n\nplot(train$income,\n     ifelse(train$default==\"No\",0,1), \n     ylim=c(0,1),\n     ylab=\"Credit default: Yes = 1\",\n     xlab=\"Income\",\n     col=c(\"red\",\"blue\")[unclass(train$student)],yaxt=\"n\")\naxis(2, at = seq(0, 1, by = 1), las=2)\nlegend(\"left\",legend=c(\"Non-student\",\"Student\"),fill=c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n\nWhat do we learn from this plot? Students have lower incomes that non-students but are students more likely to default on credit card debt? It is difficult to discern too much from a plot that shows only two response values.\n\nx &lt;- table(Student=Default$student,Default=Default$default)\nx\n\n       Default\nStudent   No  Yes\n    No  6850  206\n    Yes 2817  127\n\n\nCross-tabulating student status and default status shows that the proportion of defaults is higher among students (0.0431) than among non-students (0.0292).\nThe logistic regression of default on all input variables is performed with the glm function in R.\n\nlog_reg &lt;- glm(default ~ ., data=train, family=binomial)\n\nsummary(log_reg)\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.106e+01  5.252e-01 -21.060   &lt;2e-16 ***\nstudentYes  -6.098e-01  2.530e-01  -2.410    0.016 *  \nbalance      5.811e-03  2.491e-04  23.323   &lt;2e-16 ***\nincome       5.038e-06  8.731e-06   0.577    0.564    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2617.1  on 8999  degrees of freedom\nResidual deviance: 1398.0  on 8996  degrees of freedom\nAIC: 1406\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe Binary distribution is a special case of the Binomial distribution: the sum of \\(n\\) independent Bernoulli(\\(\\pi\\)) experiments is a Binomial(\\(n,\\pi\\)) random variable. family=binomial covers both Binary and Binomial cases. The default link function for the binomial family in R is the logit. A probit regression would use family=binomial(link=\"probit\").\nHow do we use the coefficients in the output to make predictions? Consider a student with $20,000 income and a credit card balance of $1,500. How likely are they to default?\n\nnew_x &lt;- data.frame(student=\"Yes\",balance=1500,income=20000)\n\npred_linp &lt;- predict(log_reg,newdata=new_x,type=\"link\")\npred_mean &lt;- predict(log_reg,newdata=new_x,type=\"response\")\n\ncat(\"Predicted linear predictor :\", pred_linp)\n\nPredicted linear predictor : -2.852752\n\ncat(\"Predicted probability to default: \", pred_mean)\n\nPredicted probability to default:  0.05453925\n\n\nThe predict function for a generalized linear model takes a type= argument that determines the kind of prediction. The default is type=\"link\" for a prediction on the scale of the linear predictor: \\(\\widehat{\\eta} = \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}}\\). To obtain a prediction on the inverse link scale, the scale of the mean response, use type=\"response\". The following calculations verify:\n\nlinp = coef(log_reg)[1] + coef(log_reg)[2] + 1500*coef(log_reg)[3] + \n       20000*coef(log_reg)[4]\nprob = 1 / (1+exp(-linp))\n\ncat(\"Predicted linear predictor :\", linp)\n\nPredicted linear predictor : -2.852752\n\ncat(\"Predicted probability to default: \", prob)\n\nPredicted probability to default:  0.05453925\n\n\nThe predicted proportion of students with an account balance of $1,500 and an income of $20,000 is {r}round(prob,5).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt is easy to forget for which scale predictions are calculated–different software implementations have different defaults. In the previous example it was obvious that the negative value -2.8528 is not estimating a probability. How about the following prediction?\n\npredict(log_reg,newdata=data.frame(student=\"No\",balance=2000,income=20000))\n\n        1 \n0.6623438 \n\n\n\n\n\n\nInterpretation\nThe sign of the coefficient tells us whether the predicted probability increases. It is important to check which of the two binary states is being modeled. If the target variable is a factor, glm in R models the second level of the factor–default=\"Yes\" in our example. The probability of credit card default thus increases with account balance and income and is lower if student = \"Yes compared to student = \"No\".\nIn the classical linear model the coefficients measure the change in the mean response if the associated input variable increases in value by one unit and all other inputs are held constant. In the logistic regression model, because of the involvement of the link function, such an interpretation applies to the link scale. In other words, the coefficient \\(\\widehat{\\beta}_2 =\\) 0.0058 for balance measures the change in logits when the account balance increases by one dollar. Because the logistic function is nonlinear the effect of an input on the predicted mean, the probability to default, is also nonlinear. The effect of \\(\\beta_j\\) on the probability is greater when the linear predictor is near zero where the logistic function has the greatest slope.\nThe odds \\(O(\\eta)\\) are defined as the ratio of event and non-events for the value of the linear predictor: \\[\nO(\\eta) = \\frac{\\pi(\\eta)}{1-\\pi(\\eta)} = e^\\eta\n\\] The odds ratio compares the odds for two different linear predictors \\[\nOR(\\eta_1,\\eta_2) = \\frac{O(\\eta_1)}{O(\\eta_2)}=\\exp\\{\\eta_1 - \\eta_2\\}\n\\] \\(OR(\\eta_1,\\eta_2)\\) measures how much the odds have changed from \\(\\eta_1\\) to \\(\\eta_2\\). These comparisons are particularly meaningful if the linear predictors differ in an effect we wish to test. For example, if \\(\\eta_1\\) is the linear predictor for a student and \\(\\eta_2\\) is the linear predictor for a student (with the same income and account balance), then exponentiating $_2 = $ 0.0058 gives the odds ratio for student status.\nWe have finally established that the logistic regression coefficients are interpreted as changes in the log odds ratios–this holds only for the logit link.\n\n\nFrom Regression to Classification\nClassification methods assign observations to categories. Any statistical method that yields probabilities can be used as a classifier. The difference between the two is in the statements\n\nThe predicted probability of a student with $x account balance and $xx income to default is …\n\n\nA student with $x account balance and $xx income to default is classified as a (non-)defaulter\n\nAll we need to turn the predictions from a logistic regression model into a classification model is a rule that assigns predictions to events or non-events based on the predicted probabilities. The cutoff \\(c = 0.5\\) is common and is known as the Bayes classifier. If a predicted probability is greater than 0.5 it is classified as an event, otherwise it is classified as a non-event. Classification models are explored in much more detail in the next part of the material.\nThe results of a binary classification are displayed in a 2 x 2 table, called the confusion matrix. It compares the observed and predicted categories. The cells of the matrix contain the number of data points that fall into the cross-classification when the decision rule is applied to \\(n\\) observations. If one of the categories is labeled positive and the other is labeled negative, the cells give the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n\nConfusion matrix for a classification problem with two states.\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nNo (Negative)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\nGiven the counts in the four cells, we can calculate a number of statistics (Table 10.1). A more in-depth look at confusion statistics follows in Section 12.4.\n\n\n\nTable 10.1: Some statistics calculated from a 2 x 2 confusion matrix.\n\n\n\n\n\n\n\n\n\n\nStatistic\nCalculation\nNotes\n\n\n\n\nSensitivity\nTP / (TP + FN)\nThis is the true positive rate; also called Recall\n\n\nSpecificity\nTN / (FP + TN)\nThis is the true negative rate\n\n\nAccuracy\n(TP + TN) / (TP + TN + FP + FN)\nOverall proportion of correct classifications\n\n\nMisclassification rate\n(FP + FN) / (TP + TN + FP + FN)\nOverall proportion of incorrect classifications, 1 – Accuracy\n\n\nPrecision\nTP / (TP + FP)\nRatio of true positives to anything predicted as positive\n\n\n\n\n\n\nThe model accuracy is measured by the ratio of observations that were correctly classified, the sum of the diagonal cells divided by the total number of observations. The misclassification rate, another common performance measure is the complement of the accuracy.\nThe sensitivity is the ratio of true positives to what should have been predicted as positive. The specificity is the ratio of true negatives to what should have been predicted as negative. Note that these are are not complements of each other; they are calculated with different denominators.\n\n\nExample: Credit Default–ISLR (Cont’d)\n\n\nWith the logistic regression model fitted to the training data we can compute the confusion matrix for the test data. If the application of the logistic regression is classification, metrics based on confusion statistics for test data are a good method for judging the performance of the model.\nThe following statements apply the Bayes classification rule to the test data and call the confusionMatrix function in the caret package. The option positive=\"Yes\" is used to identify the level of the factor considers the “positive” level for the calculation of the statistics. This will not affect the overall confusion matrix but will affect the interpretation of sensitivity, specificity and others. By default, the function uses the first level of a factor as the “positive” result.\n\npredicted_prob_test &lt;- predict(log_reg, newdata=test, type=\"response\")\nclassify_test &lt;- as.factor(ifelse(predicted_prob_test &gt; 0.5,\"Yes\",\"No\"))\n\ncaret::confusionMatrix(classify_test,test$default, positive=\"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  958  26\n       Yes   7   9\n                                         \n               Accuracy : 0.967          \n                 95% CI : (0.954, 0.9772)\n    No Information Rate : 0.965          \n    P-Value [Acc &gt; NIR] : 0.407906       \n                                         \n                  Kappa : 0.3384         \n                                         \n Mcnemar's Test P-Value : 0.001728       \n                                         \n            Sensitivity : 0.2571         \n            Specificity : 0.9927         \n         Pos Pred Value : 0.5625         \n         Neg Pred Value : 0.9736         \n             Prevalence : 0.0350         \n         Detection Rate : 0.0090         \n   Detection Prevalence : 0.0160         \n      Balanced Accuracy : 0.6249         \n                                         \n       'Positive' Class : Yes            \n                                         \n\n\n\n\nThe classification has an accuracy of 96.7%, which seems impressive. But consider that the proportion of observations in the larger observed class is (958 + 7)/1,000 = 0.965. If you were to take a naïve approach and predict all observations as “No” without looking at the data, that decision rule would have an accuracy of 96.5%. In light of this, using tree inputs has not much approved the quality of the model.\nThe model is very sensitive, but not specific. It is much less likely to predict a “Yes” when the true state is “No”, than it is to predict a “No” when the true state is “Yes”. Whether we can accept a model with a sensitivity of only 25.71% is questionable, despite its high accuracy. An evaluation of this model should consider whether the two errors, false positive and false negative predictions, are of equal importance and consequence.\nWe will revisit decision rules for these data in Chapter 13.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#modeling-counts",
    "href": "regdiscrete.html#modeling-counts",
    "title": "10  Discrete Target Variables",
    "section": "10.3 Modeling Counts",
    "text": "10.3 Modeling Counts\nCount targets, where the possible values of the response variables are countable, \\(0, 1, 2, \\cdots\\), arise in many situations. Two situations, which reflect different distributional properties, are\n\nCounts out of a total. We are counting the number of event occurrences out of a total number of experiments. The support for these counts has an upper limit and the counts can be expressed as proportions. For example, the number of defective manufactured items out of 100 items has support\\(\\{0,1,2,\\cdots,100\\}\\).\nCounts per unit. We are counting how many times an event of interest occurs per day, per square foot, or per some other units. The counts cannot be converted to proportions and they depend on the unit of measurement. By increasing the units by factor 2 one expects twice as many events. Examples are the number of disease incidences per 10,000 population, the number of customer calls per day received by a call center, the number of chocolate chips on a chocolate chip cookie, and so on.\n\nCounts out of a total have a direct connection to binary data. If \\(Y_1, Y_2, \\cdots, Y_n\\) are independent Bernoulli(\\(\\pi\\)) random variables, then their sum is a Binomial(\\(n,\\pi\\)) random variable. The approach to model such counts is also called Binomial regression.\nThe typical assumption for counts per unit is that they follow a Poisson distribution.\nAn introduction to binomial and Poisson regression follows here. Other distributions such as the Negative Binomial and concepts such as overdispersion and zero-inflated count processes are covered in more detail in the chapter on Generalized Linear Models (Chapter 27).\n\nBinomial Regression\nA Binomial(\\(n,\\pi\\)) random variable is the sum of \\(n\\) independent Bernoulli(\\(\\pi\\)) random variables. The mean and variance of a \\(Y\\sim \\text{Binomial}(n,\\pi\\)) random variable follow from this definition \\[\n\\text{E}[Y] = n\\pi \\quad \\text{Var}[Y] = n\\pi(1-\\pi)\n\\]\nA regression for binomial data is thus a special case of a logistic regression with grouped data.\n\n\nExample: Insecticide Dose Response\n\n\nThe data for this problem are from $6.7.1 of (Schabenberger and Pierce 2001). For each of seven concentrations of an insecticide, 20 larvae were exposed and the number of larvae killed was recorded.\n\nconc &lt;- c(0.375,0.75,1.5,3.0,6.0,12.0,24.0)\nkilled &lt;- c(0,1,8,11,16,18,20)\ntotal &lt;- c(20,20,20,20,20,20,20)\ninsect &lt;- data.frame(conc,killed,total)\nknitr::kable(insect,format=\"html\")\n\n\n\n\n\nconc\nkilled\ntotal\n\n\n\n\n0.375\n0\n20\n\n\n0.750\n1\n20\n\n\n1.500\n8\n20\n\n\n3.000\n11\n20\n\n\n6.000\n16\n20\n\n\n12.000\n18\n20\n\n\n24.000\n20\n20\n\n\n\n\n\n\n\n\nNone of the twenty larvae exposed to a concentration of 0.375 succumbed to the insecticide. At a concentration of 3.0, 11 of the twenty larvae exposed to that concentration died, and so on.\nEach larva’s response to the insecticide is a Bernoulli (binary) random variable. Since the larvae respond independently, the number of larvae killed out of 20 is a Binomial(20,\\(\\pi\\)) random variable.\nA goal of the data analysis is to derive the \\(LD_{50}\\) value, the insecticide concentration that leads to 50% mortality.\nBinomial regression models the event probability \\(\\pi\\) as a function of predictor (input) variables.\nFor each concentration we can calculate the proportion \\(p = \\text{killed}/\\text{total}\\) and its logit \\(\\log(p/(1-p))\\) and plot the logits against the concentration. This is not possible for binary data where outcomes are coded either \\(Y=1\\) or \\(Y=0\\) and a logit cannot be calculated.\nThat gives us an idea of the relationship between mortality and concentration on the logit scale.\nNo larvae were killed at the first concentration and all larvae were killed at the last concentration. This leads to an undefined logit. To approximately visualize the observed logits we add/subtract a small amount to the counts for those concentrations. The original data will be used in modeling.\n\nlibrary(dplyr)\nk2 &lt;- c(0.2,1,8,11,16,18,19.8)\ninsect2 &lt;- insect %&gt;% mutate(prop = k2/total,\n                  observed_logit = log(prop/(1-prop)),\n                  logconc = log10(conc))\n\npar(mfrow=c(1,2))\nplot(insect2$conc,insect2$observed_logit, \n     xlab=\"Concentration\", \n     ylab=\"Logit of sample proportion\", type=\"b\")\nplot(insect2$logconc,insect2$observed_logit, \n     xlab=\"log10(Concentration)\", \n     ylab=\"Logit of sample proportion\", type=\"b\")\n\n\n\n\n\n\n\n\nThe relationship between the logits of the sample proportion and insecticide concentration is not linear. Taking the log of the concentration removes the curvature. The graph in the right-hand panel suggests the following binomial regression model\n\n\\(Y | x \\sim \\text{Binomial}(n,\\pi(x))\\)\n\\(\\eta = \\beta_0 + \\beta_1 \\text{log10}(x)\\)\nlink function: logit\n\n\n\n\n\nExample: Insecticide Dose Response (Cont’d)\n\n\nTo signal to the glm() function that the response is a binomial proportion we specify the response as a two-column matrix where the first column contains the number of events (larvae killed) for the binomials and the second column contains the non-events (larvae survived).\n\nbin_reg &lt;- glm(cbind(killed,total-killed) ~ logconc, \n               family=\"binomial\",\n               data=insect2)\nsummary(bin_reg)\n\n\nCall:\nglm(formula = cbind(killed, total - killed) ~ logconc, family = \"binomial\", \n    data = insect2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.7305     0.3741  -4.626 3.73e-06 ***\nlogconc       4.1651     0.6520   6.388 1.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 98.2178  on 6  degrees of freedom\nResidual deviance:  4.6206  on 5  degrees of freedom\nAIC: 23.019\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe regression coefficient for logconc is highly significant. Figure 10.2 shows the predicted logistic probability function of the model and the \\(LD_{50}\\) value.\n\n\n\n\n\n\n\n\nFigure 10.2: Predicted mortality function for insecticide and \\(LD_{50}\\) value.\n\n\n\n\n\nThe \\(LD_{50}\\) is a value on the \\(x\\)-axis. This is an inverse prediction problem: to find the value on the \\(x\\)-axis that is associated with a predicted mortality of 0.5. Any value on the x-axis corresponding to a particular mortality rate \\(\\alpha\\) can be found by solving the following equation for \\(x_\\alpha\\)\n\\[\n\\text{logit}(\\alpha) = \\beta_0 + \\beta_1\\text{log}_{10}(x_\\alpha)\n\\]\nThe solution is \\[\nx_\\alpha = 10^{(\\text{logit}(\\alpha) - \\beta_0)/\\beta_1}\n\\]\nWith \\(\\alpha = 0.5\\) and applying the estimated regression coefficients you get \\[\nx_{0.5} = 10^{(\\text{logit}(0.5) + 1.7305)/4.1651} = 10^{0.4155} = 2.603\n\\]\nThe insecticide dosage that is fatal for 50% of the larvae, the \\(LD_{50}\\) value, is 2.603 (the log10 concentration is 0.4155).\n\n\n\n\nPoisson Regression\nThe Poisson distribution is arguably the go-to distribution when the target variable is a count per unit. It is a simple yet powerful probability model. A Poisson random variable has support \\(\\{0,1,2,\\cdots\\}\\) and probability mass function (p.m.f.) \\[\n\\text{Pr}(Y=y) = \\frac{\\lambda^y}{y!} e^{-y}\n\\] The parameter \\(\\lambda\\) is the average count per unit and is also the variance of \\(Y\\), \\(\\text{E}[Y] = \\text{Var}[Y] = \\lambda\\).\nThe following plots show the p.m.f. for Poisson distributions with \\(\\lambda = 2\\) and \\(\\lambda = 5\\).\n\npar(mfrow=c(1,2))\nbarplot(dpois(seq(0,10,1),2), names.arg=seq(0,10,1), main = \"Poisson(2)\")\nbarplot(dpois(seq(0,14,1),5), names.arg=seq(0,14,1), main = \"Poisson(5)\")\n\n\n\n\n\n\n\n\nThe natural link function for the Poisson distribution is the log link. A Poisson regression model has the following configuration:\n\n\\(Y | \\textbf{x}\\sim \\text{Poisson}(\\lambda(\\textbf{x}))\\)\n\\(\\eta = \\textbf{x}^\\prime \\boldsymbol{\\beta}\\)\n\\(\\log(\\lambda(\\textbf{x})) = \\eta\\)\n\n\n\nExample: Poisson Regression for Bike Sharing Data\n\n\nThe data for this example comes with the ISLR2 library and contains 8,645 records of the number of bike rentals per hour in Washington, DC along with time/date and weather information.\n\nlibrary(ISLR2)\nattach(Bikeshare)\nstr(Bikeshare)\n\n'data.frame':   8645 obs. of  15 variables:\n $ season    : num  1 1 1 1 1 1 1 1 1 1 ...\n $ mnth      : Factor w/ 12 levels \"Jan\",\"Feb\",\"March\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ day       : num  1 1 1 1 1 1 1 1 1 1 ...\n $ hr        : Factor w/ 24 levels \"0\",\"1\",\"2\",\"3\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ holiday   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ weekday   : num  6 6 6 6 6 6 6 6 6 6 ...\n $ workingday: num  0 0 0 0 0 0 0 0 0 0 ...\n $ weathersit: Factor w/ 4 levels \"clear\",\"cloudy/misty\",..: 1 1 1 1 1 2 1 1 1 1 ...\n $ temp      : num  0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.2 0.24 0.32 ...\n $ atemp     : num  0.288 0.273 0.273 0.288 0.288 ...\n $ hum       : num  0.81 0.8 0.8 0.75 0.75 0.75 0.8 0.86 0.75 0.76 ...\n $ windspeed : num  0 0 0 0 0 0.0896 0 0 0 0 ...\n $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...\n $ registered: num  13 32 27 10 1 1 0 2 7 6 ...\n $ bikers    : num  16 40 32 13 1 1 2 3 8 14 ...\n\n\nThe following statements fit the same Poisson regression model as in (James et al. 2021, 187)\n\nmod.pois &lt;- glm(bikers ~ mnth + hr + workingday + temp + weathersit,\n                data = Bikeshare, \n                family = \"poisson\")\nsummary(mod.pois)\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = \"poisson\", data = Bikeshare)\n\nCoefficients:\n                           Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)                2.693688   0.009720  277.124  &lt; 2e-16 ***\nmnthFeb                    0.226046   0.006951   32.521  &lt; 2e-16 ***\nmnthMarch                  0.376437   0.006691   56.263  &lt; 2e-16 ***\nmnthApril                  0.691693   0.006987   98.996  &lt; 2e-16 ***\nmnthMay                    0.910641   0.007436  122.469  &lt; 2e-16 ***\nmnthJune                   0.893405   0.008242  108.402  &lt; 2e-16 ***\nmnthJuly                   0.773787   0.008806   87.874  &lt; 2e-16 ***\nmnthAug                    0.821341   0.008332   98.573  &lt; 2e-16 ***\nmnthSept                   0.903663   0.007621  118.578  &lt; 2e-16 ***\nmnthOct                    0.937743   0.006744  139.054  &lt; 2e-16 ***\nmnthNov                    0.820433   0.006494  126.334  &lt; 2e-16 ***\nmnthDec                    0.686850   0.006317  108.724  &lt; 2e-16 ***\nhr1                       -0.471593   0.012999  -36.278  &lt; 2e-16 ***\nhr2                       -0.808761   0.014646  -55.220  &lt; 2e-16 ***\nhr3                       -1.443918   0.018843  -76.631  &lt; 2e-16 ***\nhr4                       -2.076098   0.024796  -83.728  &lt; 2e-16 ***\nhr5                       -1.060271   0.016075  -65.957  &lt; 2e-16 ***\nhr6                        0.324498   0.010610   30.585  &lt; 2e-16 ***\nhr7                        1.329567   0.009056  146.822  &lt; 2e-16 ***\nhr8                        1.831313   0.008653  211.630  &lt; 2e-16 ***\nhr9                        1.336155   0.009016  148.191  &lt; 2e-16 ***\nhr10                       1.091238   0.009261  117.831  &lt; 2e-16 ***\nhr11                       1.248507   0.009093  137.304  &lt; 2e-16 ***\nhr12                       1.434028   0.008936  160.486  &lt; 2e-16 ***\nhr13                       1.427951   0.008951  159.529  &lt; 2e-16 ***\nhr14                       1.379296   0.008999  153.266  &lt; 2e-16 ***\nhr15                       1.408149   0.008977  156.862  &lt; 2e-16 ***\nhr16                       1.628688   0.008805  184.979  &lt; 2e-16 ***\nhr17                       2.049021   0.008565  239.221  &lt; 2e-16 ***\nhr18                       1.966668   0.008586  229.065  &lt; 2e-16 ***\nhr19                       1.668409   0.008743  190.830  &lt; 2e-16 ***\nhr20                       1.370588   0.008973  152.737  &lt; 2e-16 ***\nhr21                       1.118568   0.009215  121.383  &lt; 2e-16 ***\nhr22                       0.871879   0.009536   91.429  &lt; 2e-16 ***\nhr23                       0.481387   0.010207   47.164  &lt; 2e-16 ***\nworkingday                 0.014665   0.001955    7.502 6.27e-14 ***\ntemp                       0.785292   0.011475   68.434  &lt; 2e-16 ***\nweathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***\nweathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***\nweathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n\nInput variables mnth, hr, and weathersit are factors, their first levels serve as the reference levels in the model (mnth=\"Jan\", hr=0, weathersit=\"clear\"). The coefficient estimates inform us that bikeshare usage is high during commute hours (hr 8, 17, 18) and highest in spring and fall. Clear weather has higher usage than any of the other recorded weather situations.\n\n\nAs the mean of the Poisson distribution increases, the distribution becomes more symmetric and approaches that of a Gaussian(\\(\\lambda,\\lambda\\)) distribution. The average daily bike sharing count in the bike share data set is 143.7944 so it might be tempting to analyze the bikers variable with a classical linear model.\n\n\nExample: Classical Linear Model for Bike Sharing Data\n\n\n\nmod.lm &lt;- lm(bikers ~ mnth + hr + workingday + temp + weathersit,\n                data = Bikeshare)\nsummary(mod.lm)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***\nmnthFeb                      6.845      4.287   1.597 0.110398    \nmnthMarch                   16.551      4.301   3.848 0.000120 ***\nmnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***\nmnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***\nmnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***\nmnthJuly                    45.324      7.081   6.401 1.63e-10 ***\nmnthAug                     53.243      6.640   8.019 1.21e-15 ***\nmnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***\nmnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***\nmnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***\nmnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***\nhr1                        -14.579      5.699  -2.558 0.010536 *  \nhr2                        -21.579      5.733  -3.764 0.000168 ***\nhr3                        -31.141      5.778  -5.389 7.26e-08 ***\nhr4                        -36.908      5.802  -6.361 2.11e-10 ***\nhr5                        -24.135      5.737  -4.207 2.61e-05 ***\nhr6                         20.600      5.704   3.612 0.000306 ***\nhr7                        120.093      5.693  21.095  &lt; 2e-16 ***\nhr8                        223.662      5.690  39.310  &lt; 2e-16 ***\nhr9                        120.582      5.693  21.182  &lt; 2e-16 ***\nhr10                        83.801      5.705  14.689  &lt; 2e-16 ***\nhr11                       105.423      5.722  18.424  &lt; 2e-16 ***\nhr12                       137.284      5.740  23.916  &lt; 2e-16 ***\nhr13                       136.036      5.760  23.617  &lt; 2e-16 ***\nhr14                       126.636      5.776  21.923  &lt; 2e-16 ***\nhr15                       132.087      5.780  22.852  &lt; 2e-16 ***\nhr16                       178.521      5.772  30.927  &lt; 2e-16 ***\nhr17                       296.267      5.749  51.537  &lt; 2e-16 ***\nhr18                       269.441      5.736  46.976  &lt; 2e-16 ***\nhr19                       186.256      5.714  32.596  &lt; 2e-16 ***\nhr20                       125.549      5.704  22.012  &lt; 2e-16 ***\nhr21                        87.554      5.693  15.378  &lt; 2e-16 ***\nhr22                        59.123      5.689  10.392  &lt; 2e-16 ***\nhr23                        26.838      5.688   4.719 2.41e-06 ***\nworkingday                   1.270      1.784   0.711 0.476810    \ntemp                       157.209     10.261  15.321  &lt; 2e-16 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear model explains 67.45% of the variability in the number of bike shares and has many highly significant predictors. Are the predictions comparable to those from the Poisson model? A look at Figure 10.3 shows that they are not at all the same. Predictions are higher under the Poisson model for small and large values. The predictions of the linear model extend into the negative values.\n\n\n\n\n\n\n\n\nFigure 10.3: Preditions from Poisson and linear regression for bike share data.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#sec-lrt",
    "href": "regdiscrete.html#sec-lrt",
    "title": "10  Discrete Target Variables",
    "section": "10.4 Log-likelihood, Deviance, and the Likelihood-ratio Test",
    "text": "10.4 Log-likelihood, Deviance, and the Likelihood-ratio Test\nInterestingly, all coefficients are highly statistically significant in this analysis. As we will discover in greater detail in the chapter on generalized linear models, the analysis shows a red flag: the ratio of the residual deviance and the residual degrees of freedom should generally be about 1.\nThe deviance of a model is twice the difference between the log likelihood evaluated at the parameter estimates and the largest possible log likelihood that is achievable in a saturated (yet overfit) model. The Null deviance reported by glm is the deviance for a model without input variables (intercept-only model). The Residual deviance is the deviance for the model with the specified input variables.\n\nmod.null &lt;- glm(bikers ~ 1, data=Bikeshare, family=poisson)\nsummary(mod.null)\n\n\nCall:\nglm(formula = bikers ~ 1, family = poisson, data = Bikeshare)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 4.9683848  0.0008969    5539   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance: 1052921  on 8644  degrees of freedom\nAIC: 1105961\n\nNumber of Fisher Scoring iterations: 5\n\n\nIn the null model the Null and Residual deviances are identical and match the null deviance reported in the previous model with input variables.\nThe amount by which the deviance decreases through the addition of the input variables is the test statistic for the likelihood ratio test (LRT) that all coefficients except the intercept are simultaneously zero.\n\n\nExample: Likelihood Ratio Test for Bike Sharing Data\n\n\nThe logLik function extracts the log likelihood for model classes that export that information.\n\nLRT &lt;- 2*(logLik(mod.pois) - logLik(mod.null))\nas.numeric(LRT)\n\n[1] 824880.2\n\nmod.pois$null.deviance - mod.pois$deviance\n\n[1] 824880.2\n\n\n\n\nUnder certain conditions, this likelihood-ratio test statistic has a \\(\\chi^2\\) distribution with degrees of freedom equal to the rank of \\(\\textbf{X}\\). The LRT is the equivalent test in likelihood-based estimation to the sum-of-squares reduction test in least-squares based estimation (Section 7.2)\n\n\nDefinition: Likelihood Ratio Test\n\n\nSuppose that \\(\\ell_f\\) and \\(\\ell_r\\) are the log likelihoods in a full and reduced model where the reduced model is nested within the full model through a constraint (a hypothesis) \\(H\\) with \\(q\\) degrees of freedom. The likelihood ratio test statistic \\[\n\\Lambda = 2(\\ell_f - \\ell_r)\n\\] has an asymptotic \\(\\chi^2_q\\) distribution under the hypothesis.\n\n\nThe LRT for the constraint \\(H: \\beta_1 = \\beta_2 = \\cdots = \\beta_{39} = 0\\) has a near-zero \\(p\\)-value, indicating not all of the coefficients are simultaneously zero.\n\npchisq(LRT,df=39,lower.tail=FALSE)\n\n'log Lik.' 0 (df=40)\n\n\nThe anova function provides a convenient way of performing the LRT for nested models:\n\nanova(mod.null,mod.pois,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: bikers ~ 1\nModel 2: bikers ~ mnth + hr + workingday + temp + weathersit\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      8644    1052921                          \n2      8605     228041 39   824880 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnder certain conditions the residual deviance has a \\(\\chi^2\\) distribution with \\(n-r(\\textbf{X})\\) degrees of freedom.\nSince the mean of a \\(\\chi^2_\\nu\\) random variable is equal to the degrees of freedom, \\(\\nu\\), this suggests a simple calculation with the residual deviance: the ratio of the deviance and its degrees of freedom should be about 1. We can see that this ratio is far from 1 in the Poisson regression:\n\nmod.pois$deviance / mod.pois$df.residual\n\n[1] 26.50099\n\n\nThis value indicates that there remains considerably more variability than expected. The model is somehow not correct. This condition is called overdispersion and can have many causes:\n\nimportant input variables are missing from the model\nthe data are auto-correlated, for example in time series or longitudinal data\nthe data do not follow a Poisson distribution but a model with greater dispersion, e.g., the Negative Binomial distribution\nthe data were generated by more than one process and we observe a mixture.\n\nOverdispersion is addressed by turning the appropriate knob. If overdispersion is not addressed, the precision of the fitted model is overstated: standard error estimates are too small, confidence and prediction intervals are too narrow, \\(p\\)-values are too small. More on this topic in Section 27.7.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#working-with-offsets",
    "href": "regdiscrete.html#working-with-offsets",
    "title": "10  Discrete Target Variables",
    "section": "10.5 Working with Offsets",
    "text": "10.5 Working with Offsets\nAn offset is an input variable that has a known coefficient of 1. Consider the model \\[\ng(\\mu) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + x^o\n\\] \\(x^o\\) acts as an offset to the linear predictor. It is not an intercept, which takes on the same value for all observations. The offset can change from one observation to the next. But it is also not an input variable whose coefficient needs to be estimated. Somehow we know that the offset has a coefficient \\(\\beta = 1\\).\nOffsets are important when working with count variables that relate to different units. If \\(Y_1\\) is a count per day and \\(Y_2\\) is a count of the same random process per week, the two do not have the same mean, \\(\\text{E}[Y_2] = 7\\text{E}[Y_1]\\). When incidences (crime, cancer, …) are counted in areas of different size or population (by city, by zip code, by county, by age group,…) we are more interested in modeling the event rate (cancer rate in Seattle per 100,000) than the raw counts (cancer cases in Seattle).\nIf count variables relate to different units we need to adjust the analysis, otherwise changes in the mean due to size differences are attributed to the wrong effects. For Poisson regression the problem can be formalized as follows:\n\\[\nY_i \\sim \\text{Poisson}( x_i^0 \\lambda)\n\\] and \\(\\lambda = \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}\\}\\). In other words, we have to adjust the counts to a common unit to account for the differing sizes on the mean of the data. In the case of a log link, the model for the mean of \\(Y_i\\) becomes \\[\n\\text{E}[Y_i] = x_i^0\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}\\} = \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\log(x_i^0)\\}\n\\] The log of the size of the units is an offset variable on the linear predictor. The interpretation of \\(\\lambda\\) is now in terms of a rate of events rather than a raw count.\n\n\nExample: Non-melanoma Skin Cancer\n\n\nThe following data appear in (Kleinbaum et al. 2013) and describe the incidence of non-melanoma skin cancer among women by age group in two cities.\n\nnonmel &lt;- read.table(header = TRUE,\n                     text = \"\n   cases city u1 u2 u3 u4 u5 u6 u7      n\n1      1    0  1  0  0  0  0  0  0 172675\n2     16    0  0  1  0  0  0  0  0 123065\n3     30    0  0  0  1  0  0  0  0  96216\n4     71    0  0  0  0  1  0  0  0  92051\n5    102    0  0  0  0  0  1  0  0  72159\n6    130    0  0  0  0  0  0  1  0  54722\n7    133    0  0  0  0  0  0  0  1  32185\n8     40    0  0  0  0  0  0  0  0   8328\n9      4    1  1  0  0  0  0  0  0 181343\n10    38    1  0  1  0  0  0  0  0 146207\n11   119    1  0  0  1  0  0  0  0 121374\n12   221    1  0  0  0  1  0  0  0 111353\n13   259    1  0  0  0  0  1  0  0  83004\n14   310    1  0  0  0  0  0  1  0  55932\n15   226    1  0  0  0  0  0  0  1  29007\n16    65    1  0  0  0  0  0  0  0   7583\n\")\n\nnonmel &lt;- within(nonmel, {\n    agegroup &lt;- rep(c(\"15_24\",\"25_34\",\"35_44\",\"45_54\",\"55_64\",\"65_74\",\"75_84\",\"85+\"), 2)\n    agegroup &lt;- factor(agegroup)\n\n    city &lt;- factor(city, 0:1, c(\"Minneapolis\", \"Dallas\"))\n})\n\nnonmel &lt;- nonmel[c(\"cases\",\"n\",\"city\",\"agegroup\")]\n\n\nknitr::kable(nonmel,format=\"html\")\n\n\n\n\n\ncases\nn\ncity\nagegroup\n\n\n\n\n1\n172675\nMinneapolis\n15_24\n\n\n16\n123065\nMinneapolis\n25_34\n\n\n30\n96216\nMinneapolis\n35_44\n\n\n71\n92051\nMinneapolis\n45_54\n\n\n102\n72159\nMinneapolis\n55_64\n\n\n130\n54722\nMinneapolis\n65_74\n\n\n133\n32185\nMinneapolis\n75_84\n\n\n40\n8328\nMinneapolis\n85+\n\n\n4\n181343\nDallas\n15_24\n\n\n38\n146207\nDallas\n25_34\n\n\n119\n121374\nDallas\n35_44\n\n\n221\n111353\nDallas\n45_54\n\n\n259\n83004\nDallas\n55_64\n\n\n310\n55932\nDallas\n65_74\n\n\n226\n29007\nDallas\n75_84\n\n\n65\n7583\nDallas\n85+\n\n\n\n\n\n\n\n\nThe cases variable is the raw count for the number of cases. Is the rate of non-melanoma skin cancer in 25–34 year old individuals in Dallas higher than in Minneapolis? To answer this question we need to adjust the raw counts for the population size. If the log of the population size n is used as the offset in a Poisson regression, then we are modeling the cancer rate per person in the two cities. Other offsets are possible, for example, to model the rate per 10,000 population. What matters is that the raw counts are adjusted to a common unit.\nSince city and agegroup are factors in the analysis, we choose reference levels for both. The results are expressed relative to the oldest age group in Minneapolis.\n\nnonmel$city &lt;- relevel(nonmel$city, ref=\"Minneapolis\")\nnonmel$agegroup &lt;- relevel(nonmel$agegroup, ref=\"85+\")\n\nThe offset=log(n) option in the glm call defines the offset for the model.\n\nmod_poi &lt;- glm(cases ~ city + agegroup, \n               offset=log(n), \n               family=\"poisson\",\n               data=nonmel)\nsummary(mod_poi)\n\n\nCall:\nglm(formula = cases ~ city + agegroup, family = \"poisson\", data = nonmel, \n    offset = log(n))\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -5.4834     0.1037 -52.890  &lt; 2e-16 ***\ncityDallas      0.8039     0.0522  15.399  &lt; 2e-16 ***\nagegroup15_24  -6.1742     0.4577 -13.488  &lt; 2e-16 ***\nagegroup25_34  -3.5440     0.1675 -21.160  &lt; 2e-16 ***\nagegroup35_44  -2.3268     0.1275 -18.254  &lt; 2e-16 ***\nagegroup45_54  -1.5790     0.1138 -13.871  &lt; 2e-16 ***\nagegroup55_64  -1.0869     0.1109  -9.800  &lt; 2e-16 ***\nagegroup65_74  -0.5288     0.1086  -4.868 1.13e-06 ***\nagegroup75_84  -0.1157     0.1109  -1.042    0.297    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2789.6810  on 15  degrees of freedom\nResidual deviance:    8.2585  on  7  degrees of freedom\nAIC: 120.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nWith the chosen reference levels, the intercept is the linear predictor for residents of Minneapolis in the 85+ age group. Exponentiating the intercept is the predicted cancer rate in that group.\n\ncat(\"The predicted cancer rate among residents of \\nMinneapolis in the 85+ age group is \",\nexp(mod_poi$coefficients[1]),\"\\n\")\n\nThe predicted cancer rate among residents of \nMinneapolis in the 85+ age group is  0.004155093 \n\ncat(\"The predicted number of non-melanoma skin cancer cases \\namong 1,000 residents of Minneapolis 85 years is \",\n    1000*exp(mod_poi$coefficients[1]), \"\\n\")\n\nThe predicted number of non-melanoma skin cancer cases \namong 1,000 residents of Minneapolis 85 years is  4.155093 \n\n\nThe number of cases for a particular population size can also be obtained by supplying the value for the offet variable:\n\nxvals &lt;- data.frame(city=\"Minneapolis\", agegroup=\"85+\", n=1000)\npredict(mod_poi,newdata=xvals,type=\"response\")\n\n       1 \n4.155093 \n\n\nSince we are modeling a standardized rate, comparisons across cities and age groups are now possible. For example, the following computes the excess number of non-melanoma skin cancer cases per 100,000 40-year old individuals versus 20-year old individuals for the two cities:\n\nxvals &lt;- data.frame(city=\"Minneapolis\", agegroup=\"15_24\", n=100000)\nxvals[nrow(xvals)+1,] &lt;- list(city=\"Minneapolis\", agegroup=\"35_44\", n=100000)\nxvals[nrow(xvals)+1,] &lt;- list(city=\"Dallas\", agegrouop=\"15_24\", n=100000)\nxvals[nrow(xvals)+1,] &lt;- list(city=\"Dallas\", agegroup=\"35_44\", n=100000)\n\np &lt;- predict(mod_poi,newdata=xvals,type=\"response\")\n\ncat(\"Excess cancers per 100,000 in Minneapolis \", p[2]-p[1], \"\\n\")\n\nExcess cancers per 100,000 in Minneapolis  39.69063 \n\ncat(\"Excess cancers per 100,000 in Dallas      \", p[4]-p[3], \"\\n\")\n\nExcess cancers per 100,000 in Dallas       88.67815 \n\n\nThere are 2.2342 more cancer cases per 100,000 in Dallas compared to Minneapolis. Since the model does not include an interaction between city and age group, this multiplier applies to all age groups. You can find it more easily by exponentiating the coefficient for cityDallas:\n\nas.numeric(exp(mod_poi$coefficients[2]))\n\n[1] 2.234234",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "regdiscrete.html#sec-zero-inflation",
    "href": "regdiscrete.html#sec-zero-inflation",
    "title": "10  Discrete Target Variables",
    "section": "10.6 Zero-inflated Models",
    "text": "10.6 Zero-inflated Models\nWe briefly mentioned overdispersion in this chapter, the condition by which the data are more dispersed than is permissible under an assumed probability model. Gaussian data can never be overdispersed, because the mean and variance of the Gaussian are not related: any mean can be combined with any variance. That is quite unusual among probability distributions. In most cases, there is some relationship between the mean and the variance. For example, the Poisson(\\(\\lambda\\)) distribution has \\[\n\\text{E}[Y] = \\text{Var}[Y] = \\lambda\n\\]\nRegression explicitly models the mean function; when mean and variance are functionally related, the model implicitly captures the variance. Suppose you perform linear Poisson regression with \\[\n\\log(\\text{E}[Y]) = \\beta_0 + \\beta_1 x\n\\] and you find that the variability grows faster in \\(x\\) than the mean. Either the mean function is not correctly specified, or \\(Y|x\\) does not follow a Poisson distribution, or both.\nEspecially with count data, a possible reason for overdispersion in the data is zero-inflated responses. Under this condition we observe more zero counts than expected under the reference distribution, usually a Poisson or Negative Binomial distribution. For example, if you ask visitors to a state park how many fish they caught that day, you will probably receive more zero responses than expected under a Poisson model because multiple processes are at work, generating zeros. Some visitors do not fish at all and naturally won’t catch any fish. Some visitors fish but are unlucky anglers that day. Zero count values appear in your data from multiple sources, inflating the overall count.\n\n\nExample: Simulated Zero Inflation\n\n\nThe following code simulates a zero-inflated Poisson process. 1,000 observations are drawn from a Poisson(5) process. A second process, p2, is deterministic and generates only zeros. With probability 0.2 an observation is drawn from p2, with probabilityt 0.8 an observation is drawn from the Poisson(5) process p1. The result is a zero-inflated Poisson process.\n\nset.seed(876)\nu &lt;- runif(1000)\n\np1 &lt;- rpois(1000,5)\np2 &lt;- 0\nzip &lt;- ifelse(u &lt; 0.2,p2,p1)\n\n# Effect of zero inflation\npar(mfrow=c(1,2))\nhist(zip)\nhist(p1)\n\n\n\n\n\n\n\nFigure 10.4: Zero-inflated Poisson(5) process (left) and Poisson(5) process (right).\n\n\n\n\n\nThe excess zeros in the zero-inflated process leads to greater variability in the data compared to the non-inflated process.\n\nsd(zip)\n\n[1] 2.83468\n\nsd(p1)\n\n[1] 2.214244\n\n\nIf unaccounted for, zero inflation manifests itself as overdispersion in count data.\n\nsummary(glm(p2 ~ 1, family=\"poisson\"))$deviance\n\n[1] 4.122307e-10\n\nsummary(glm(zip ~ 1, family=\"poisson\"))$deviance\n\n[1] 2589.894\n\n\n\n\nA zero-inflated model is a special case of a finite mixture model (FMM). In a \\(k\\)-component finite mixture the probability distribution of the target variable is expressed as a weighted combination of \\(k\\) distributions \\[\np(y) = \\sum_{j=1}^k \\pi_j p_j(y)\n\\]\nThe \\(\\pi_j\\) are called the mixing probabilities of the FMM. In a homogeneous mixture model the component distributions \\(p_j(y)\\) are from the same family, for example, they are all Gaussian distributions but with different parameters: \\(p_j(y) = G(\\mu_j,\\sigma^2_j)\\). A heterogeneous mixture model features component distributions from different families.\nThe zero-inflated Poisson model is a heterogeneous, two-component mixture in which one component distribution is concentrated at 0 and the other distribution is Poisson(\\(\\lambda\\))\n\\[\np_1(y) = \\left \\{ \\begin{array}{ll} 1 & y = 0 \\\\ 0 & \\text{otherwise}\\end{array}\\right .\n\\] \\[\np_2(y) = \\frac{\\lambda^y}{y!} e{-\\lambda}\n\\] The overall model can be written as \\[\n\\Pr(Y=y) = \\pi p_1 + (1-\\pi)p_2(y)\n\\]\nOpportunities to introduce input variables are the mixing probability \\(\\pi\\) and the mean of the Poisson model. The mixing probability can be modeled as a lothe gistic-type model. The model for the mean of \\(p_2(y)\\) is a Poisson regression with log link. The two models can depend on the same inputs or different inputs. For example, the likelihood of excess zeros might depend on age of an individual but the mean of the Poisson variable is not a function of age.\nUsing \\(\\textbf{z}\\) for the vector of inputs in the model for the mixing probability and \\(\\textbf{x}\\) for the vector of inputs in the Poisson model, the zero-inflated Poisson model in terms of estimable parameters is\n\\[\\begin{align*}\n    \\text{logit}(\\pi) &= \\textbf{z}^\\prime \\boldsymbol{\\alpha}\\\\\n    \\log(\\lambda) &= \\textbf{x}^\\prime \\boldsymbol{\\beta}\n\\end{align*}\\]\n\n\nExample: Catching Fish\n\n\nThe data for this example comes from the documentation of the FMM procedure in SAS/STAT software. The data represent gender, age, and the number of fish reportedly caught by visitors of a park.\n\nsuppressWarnings(library(\"duckdb\"))\n\nLoading required package: DBI\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\ncatch &lt;- dbGetQuery(con, \"SELECT * FROM catch\")\n\ndbDisconnect(con)\nhead(catch)\n\n  gender age count\n1      F  54    18\n2      M  37     0\n3      F  48    12\n4      M  27     0\n5      M  55     0\n6      M  32     0\n\n\nA Poisson generalized linear model with a common intercept and different age-slopes for the genders exhibits considerable overdispersion.\n\npoi_reg &lt;- glm(count ~ gender:age, data=catch,family=\"poisson\")\nsummary(poi_reg)\n\n\nCall:\nglm(formula = count ~ gender:age, family = \"poisson\", data = catch)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.98109    0.54391  -7.319 2.49e-13 ***\ngenderF:age  0.12778    0.01149  11.125  &lt; 2e-16 ***\ngenderM:age  0.10442    0.01224   8.531  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 315.58  on 51  degrees of freedom\nResidual deviance: 104.84  on 49  degrees of freedom\nAIC: 188.71\n\nNumber of Fisher Scoring iterations: 5\n\n\nA zero-inflated Poisson model can be fit in R with the zeroinfl function in the pscl package. The function specifies separate linear predictors for the count model and the zero-inflated part, separated by a vertical slash. count ~ gender:age | 1 implies the same mean model for the count part as in the Poisson model above and an intercept-only model for the zero-inflation part. The dist= option specifies the distribution of the count variable, the link= option specifies the link function for the zero-inflation.\n\nlibrary(pscl)\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nzip &lt;- zeroinfl(count ~ gender:age | 1, \n                data=catch,\n                dist=\"poisson\", #link for the GLM part assumed to be log()\n                link=\"logit\" #this is the link for the zero-inflated part!\n                )\nsummary(zip)\n\n\nCall:\nzeroinfl(formula = count ~ gender:age | 1, data = catch, dist = \"poisson\", \n    link = \"logit\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3540 -0.6922 -0.3865  0.6969  2.4075 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.52151    0.64473  -5.462 4.71e-08 ***\ngenderF:age  0.12157    0.01344   9.046  &lt; 2e-16 ***\ngenderM:age  0.10562    0.01393   7.580 3.45e-14 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.8342     0.4768  -1.749   0.0802 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -72.81 on 4 Df\n\n\nThe estimate of the intercept in the zero-inflation model is \\(\\widehat{\\alpha}_0 =\\) -0.8342. Applying the inverse link yields the estimated mixing probability:\n\ncoef &lt;- zip$optim$par\ncoef\n\n(Intercept) genderF:age genderM:age (Intercept) \n -3.5215051   0.1215663   0.1056187  -0.8341843 \n\n1/(1+exp(-coef[4])) #mixing prob. for the zero-inflated part\n\n(Intercept) \n  0.3027611 \n\n\n30.2% of the observations come from the zero-inflated process. The complement, 69.8% of the observations come from a Poisson process with estimated mean\n\\[\n\\exp\\{\\widehat{\\beta}_0 + \\widehat{\\beta}_{1f} \\,\\text{age} \\} = \\exp\\{-3.52149 + 0.12157\\times \\text{age}\\}\n\\]\nfor females and \\[\n\\exp\\{\\widehat{\\beta}_0 + \\widehat{\\beta}_{1m} \\, \\text{age} \\} = \\exp\\{-3.52149 + 0.10562\\times \\text{age}\\}\n\\] for males.\nHow do you calculate the predicted values in the zero-inflated Poisson model? The predictions of the two processes and the mixing probabilities need to be combined. For the first two observations,\n\nmixprob &lt;- 1/(1+exp(-coef[4]))\n\ncat(\"Predicted value for obs 1: \",\n    mixprob * 0 + (1-mixprob) * exp(coef[1] + catch[1,\"age\"]*coef[2]),\"\\n\")\n\nPredicted value for obs 1:  14.62083 \n\ncat(\"Predicted value for obs 2: \",\n    mixprob * 0 + (1-mixprob) * exp(coef[1] + catch[2,\"age\"]*coef[3]))\n\nPredicted value for obs 2:  1.026095\n\n\n\npredict(zip,newdata=catch[1:2,])\n\n        1         2 \n14.620832  1.026095 \n\n\nA likelihood-ratio test can be used to test whether adding the zero-inflation to the Poisson model improved the overall model. The two models are nested, the ZIP model reduces to a Poisson model as \\(\\pi \\rightarrow 0\\), or equivalently, \\(\\alpha \\rightarrow -\\infty\\).\n\nlrt &lt;- 2*(logLik(zip) - logLik(poi_reg))\npvalue &lt;- pchisq(lrt,df=1,lower.tail=FALSE)\ncat(\"p-value of LRT :\", pvalue)\n\np-value of LRT : 1.120097e-09\n\n\nThe ZIP model provides a significantly better fit than the Poisson model.\nThe next model adds the gender variable to the zero-inflated model. There are now two mxing probabilities, one for males and one for females.\n\nzip_g &lt;- zeroinfl(count ~ gender:age | gender, \n                data=catch,\n                dist=\"poisson\", \n                link=\"logit\"  \n                )\nsummary(zip_g)\n\n\nCall:\nzeroinfl(formula = count ~ gender:age | gender, data = catch, dist = \"poisson\", \n    link = \"logit\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.7975 -0.6309 -0.3893  0.5731  2.3716 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.40113    0.63517  -5.355 8.57e-08 ***\ngenderF:age  0.11893    0.01327   8.965  &lt; 2e-16 ***\ngenderM:age  0.10448    0.01372   7.617 2.60e-14 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.5405     0.7141  -2.157   0.0310 *\ngenderM       1.5856     0.8858   1.790   0.0734 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 11 \nLog-likelihood: -71.1 on 5 Df\n\n\n\nlrt &lt;- 2*(logLik(zip_g) - logLik(zip))\npvalue &lt;- pchisq(lrt,df=1,lower.tail=FALSE)\ncat(\"p-value of LRT :\", pvalue)\n\np-value of LRT : 0.06473306\n\n\nThe likelihood ratio test of a gender-specific mixing probability is not significant at the 5% level (\\(p =\\) 0.0647, very similar to the Wald-type \\(p\\)-value of 0.0734 reported in the output.\n\n\n\n\n\nFigure 10.2: Predicted mortality function for insecticide and \\(LD_{50}\\) value.\nFigure 10.4: Zero-inflated Poisson(5) process (left) and Poisson(5) process (right).\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nKleinbaum, David G., Lawrence L. Kupper, A. Nizam, and Eli S. Rosenberg. 2013. Applied Regression Analysis and Other Multivariable Methods, 5 Ed. Cengage Learning.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary Statistical Models for the Plant and Soil Sciences. CRC Press, Boca Raton.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Discrete Target Variables</span>"
    ]
  },
  {
    "objectID": "reglocal.html",
    "href": "reglocal.html",
    "title": "11  Local Models",
    "section": "",
    "text": "11.1 Kernel Regression\nA special case of a kernel regression estimator was introduced in Section 6.4, the \\(k\\)-nearest neighbor regression estimator. The idea is simple: to predict the mean of \\(Y\\) at \\(x_0\\), find the \\(k\\) observations closest to \\(x_0\\) and take their average. The procedure can be formalized as follows:\n\\[\nK(x_0,x_i) = I(||x_i-x_0|| \\le ||x_{(k)} - x_0 ||)\n\\] where \\(x_{(k)}\\) denotes the observation ranked \\(k\\)th in distance from \\(x_0\\), \\(||x_i-x_0||\\) denotes the distance of \\(x_i\\) from the prediction location \\(x_0\\), and \\(I()\\) is the indicator function (\\(I(a)\\) is 1 if \\(a\\) is true).\nWhat have we accomplished with this setup? A global model, \\(Y(x) = \\beta + \\epsilon^*\\), which is probably not correct over the range of \\(X\\) has turned into a local model for \\(Y(x_0)\\) by introducing a weight function that gives more weight to observations close to \\(x_0\\) than to observations far away. It is much more reasonable to assume an intercept-only model holds at \\(x_0\\) than across the entire range of \\(X\\). As the prediction problem moves to another location, say \\(x^*_0\\), the weighted regression problem is solved again, this time based on the weight function \\[\nK(x^*_0,x_i) = I(||x_i-x^*_0|| \\le ||x_{(k)} - x^*_0 ||)\n\\]\nRather than one global estimate \\(\\widehat{\\boldsymbol{\\beta}}\\) we get as many estimates as there are points we wish to predict at: \\(\\widehat{\\beta}(x_0), \\widehat{\\beta}(x^*_0), \\widehat{\\beta}(x^{**}_0), \\cdots\\).\nThe following simulated data is taken from the course on Predictive Modeling by García-Portugués (2024). The true mean function is \\[\nf(x) = x^2 \\, \\cos(x)\n\\] and the observed data (Figure 11.1) follow the model \\[\nY(x) = f(x) + \\epsilon\\quad \\epsilon \\sim \\textit{iid } G(0,4)\n\\]\nset.seed(12345)\nn &lt;- 100\neps &lt;- rnorm(n, sd = 2)\nm &lt;- function(x) x^2 * cos(x)\nX &lt;- rnorm(n, sd = 2)\nY &lt;- m(X) + eps\nFigure 11.1: Simulated data according to García-Portugués (2024).",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Models</span>"
    ]
  },
  {
    "objectID": "reglocal.html#kernel-regression",
    "href": "reglocal.html#kernel-regression",
    "title": "11  Local Models",
    "section": "",
    "text": "The general model is \\(Y(x) = f(x) + \\epsilon\\) with \\(\\text{E}[\\epsilon] = 0\\)\nTo predict \\(f(x_0)\\) use the local estimator \\(\\widehat{f}(x_0) = \\frac{1}{k}\\sum_{i=1}^k y(x_{(i)})\\) where \\(x_{(i)}\\) is the \\(i\\)th closest observation to \\(x_0\\) and \\(y(x_{(i)})\\) is the observed value of the target variable at \\(x_{(i)}\\).\nThis is equivalent to considering at \\(x_0\\) the regression model \\[\nY(x_0) = \\beta + \\epsilon^*\n\\] in just a neighborhood of \\(x_0\\). This is an intercept-only model and the OLS estimator is \\(\\widehat{\\beta} = \\overline{y}\\). To make it so the model uses only the \\(k\\) observations closest to \\(x_0\\) we fit it as a weighted regression. The weight of an observation is 1 if it is within the \\(k\\)-neighborhood and 0 if it is outside. Formally, the weight function at \\(x_0\\) is\n\n\n\nWhen the weight function is applied, the estimator of \\(\\beta\\) becomes \\[\n\\widehat{\\beta}(x_0) = \\frac{\\sum_{i=1}^n K(x_0,x_i) \\, y(x_i)}{\\sum_{i=1}^n K(x_0,x_i)} = \\frac{1}{k}\\sum_{i=1}^k y(x_{(i)})\n\\]\n\n\n\n\n\n\n\n\nExample: 3-nearest Neighbor Estimation\n\n\nThe following code computes the 3-NN regression estimator at a grid of \\(x_0\\) values, ranging from -4 to 4.\n\nx_0 &lt;- seq(-4,4,1)\nk3_reg &lt;- Rfast::knn(as.matrix(x_0),\n                     as.matrix(Y),\n                     as.matrix(X),k=3,type=\"R\") \n\ncbind(x_0,k3_reg)\n\n      x_0           \n [1,]  -4  -9.681715\n [2,]  -3 -10.319916\n [3,]  -2  -2.134633\n [4,]  -1   1.066642\n [5,]   0  -0.755860\n [6,]   1   3.510851\n [7,]   2   1.982169\n [8,]   3  -9.101672\n [9,]   4  -8.201705\n\n\nThe same results can be obtained using an intercept-only model with weights calculated according to \\(K(x_0,x_i)\\). The proxy::dist() function computes the distance between the \\(x_0\\) locations and the \\(x\\) values. The loop constructs the weight vector for each prediction point, assigning 1 to the points within the 3-neighbor distance and 0 to the points outside. The only change in the call to lm() from one prediction point to the next is the vector of weights.\n\nd &lt;- proxy::dist(x_0,X,method=\"euclidean\")\nfor (i in 1:length(x_0)) {\n    d_lim &lt;- head(sort(d[i,]),3)\n    w &lt;- rep(0,length(X))\n    w[which((d[i,] &gt;= d_lim[1] & (d[i,] &lt;= d_lim[3])))] &lt;- 1\n    cat(\"x_0: \", x_0[i], \"beta_hat: \",lm(Y ~ 1, weights=w)$coefficients,\"\\n\")\n}\n\nx_0:  -4 beta_hat:  -9.681715 \nx_0:  -3 beta_hat:  -10.31992 \nx_0:  -2 beta_hat:  -2.134633 \nx_0:  -1 beta_hat:  1.066642 \nx_0:  0 beta_hat:  -0.75586 \nx_0:  1 beta_hat:  3.510851 \nx_0:  2 beta_hat:  1.982169 \nx_0:  3 beta_hat:  -9.101672 \nx_0:  4 beta_hat:  -8.201705 \n\n\n\n\n\nNadaraya-Watson Estimator\nThe previous estimator is a special case of the Nadaraya-Watson kernel estimator of \\(f(x_0)\\), \\[\n\\widehat{f}(x_0) = \\frac{\\sum_{i=1}^n K_\\lambda(x_0,x_i)\\,y_i}{\\sum_{i=1}^nK_\\lambda(x_0,x_i)}=\\sum_{i=1}^n w_i(x_0)\\,y_i\n\\]\nThe expression on the right shows that this is a weighted estimator where the weights are given by \\[\nw_i(x_0) = \\frac{K_\\lambda(x_0,x_i)}{\\sum_{i=1}^n K_\\lambda(x_0,x_i)}\n\\] The kernel function \\(K_\\lambda(x_0,x_i)\\) depends on the prediction location \\(x_0\\), the distance \\(|x_i-x_0|\\) between \\(x_i\\) and \\(x_0\\), and a parameter \\(\\lambda\\) that controls the shape of the kernel. \\(\\lambda\\) is called the bandwidth of the kernel because it controls how quickly the kernel weights drop with increasing distance from \\(x_0\\); or in other words, how wide the kernel function is.\nPopular kernel functions are\n\nEpanechnikov kernel \\[\nK_\\lambda(x_0,x) = \\left \\{ \\begin{array}{ll} \\frac{3}{4}\\left(1-t^2\\right) & \\text{if } |t| \\le 1 \\\\\n0 & \\text{otherwise} \\end{array}\\right .\n\\]\nTricube kernel \\[\nK_\\lambda(x_0,x) = \\left \\{ \\begin{array}{ll} \\frac{3}{4}\\left(1-|t|^3\\right)^3 & \\text{if } |t| \\le 1 \\\\\n0 & \\text{otherwise} \\end{array}\\right .\n\\]\nGaussian kernel \\[\nK_\\lambda(x_0,x) = \\phi(t)\n\\]\n\nIn these expressions \\(t = |x - x_0|/\\lambda\\) and \\(\\phi(t)\\) is the standard Gaussian density function.\nFigure 11.2 shows the kernel functions for \\(\\lambda=0.5\\) and \\(\\lambda=1\\), respectively. The functions reach their maximum at \\(|x-x_0| = 0\\) and decrease symmetrically in both directions. The Epanechnikov has a kink at \\(|t| = \\lambda\\) while the tricube kernel transitions smoothly through that point. Tricube kernels are sometimes preferred over the Epanechnikov kernel because the latter is not differentiable at \\(|t|=1\\). The Gaussian kernel is very smooth and for a given value of \\(\\lambda\\) assigns greater weights for points remote from \\(x_0\\).\n\n\n\n\n\n\n\n\nFigure 11.2: Epanechnikov, tricube and Gaussian kernel functions popular in kernel regression and density estimation. Bandwidth set to 0.5 and 1 for each kernel.\n\n\n\n\n\nIn practice, the choice of the kernel function is less important than the choice of the bandwidth parameter \\(\\lambda\\).\n\n\nExample: Kernel Regression with Simulated Data\n\n\nThe ksmooth() function in the stats package (built-in) can be used to compute the kernel regression smoother. x.points= specifies the points at which to evaluate the smoothed fit. kernel=\"normal\" chooses the Gaussian kernel\n\nkr_05 &lt;- ksmooth(X,Y,kernel=\"normal\",bandwidth=0.5, x.points=xGrid)\nkr_09 &lt;- ksmooth(X,Y,kernel=\"normal\",bandwidth=0.9, x.points=xGrid)\n\n\n\n\n\n\n\n\n\nFigure 11.3: Kernel regression estimates for Gaussian kernel with \\(\\lambda=0.5\\) and \\(\\lambda=0.9\\).\n\n\n\n\n\n\n\nSeveral noteworthy items in Figure 11.3:\n\nThe kernel regression estimate for \\(\\lambda=0.9\\) is more smooth than the estimate for \\(\\lambda=0.5\\). A larger bandwidth widens the kernel (weight) function. As \\(\\lambda\\) increases the fit approaches a flat line at \\(\\widehat{\\beta} = \\overline{y}\\), a global model.\nIn contrast to the \\(k\\)-NN estimator, the kernel regression estimator is smooth. \\(k\\)-NN estimates change abruptly when a point leaves the \\(k\\)-neighborhood and another point enters. The \\(k\\)-NN weight function \\[\nK(x_0,x_i) = I(||x_i-x_0|| \\le ||x_{(k)} - x_0 ||)\n\\] is not smooth. The weight associated with a data point \\(x_i\\) in kernel regression changes gradually with its distance from \\(x_0\\).\nFitting the model at \\(x_0\\) and predicting at \\(x_0\\) is the same operation; unlike in global models where you fit the model first, obtain the parameters, and then use them to predict at any point you choose.\nThere must be a variance-bias tradeoff between the bias of a model with large bandwidth and the variability of a model with small bandwidth. The hyperparameter \\(\\lambda\\) can be chosen by some form of cross-validation.\nThe kernel regression estimates generally capture the mean function well, but seem to struggle near the edges of the \\(x\\)-range. Both underestimate the trend at the edge, the estimate with \\(\\lambda=0.5\\) turns away more sharply than the estimate with \\(\\lambda = 0.9\\). The problem near the edge is that the estimator can use information only from one side of the edge. Fitting the model at \\(x_0 = \\max(x_i)\\) only observations with \\(x &lt; x_0\\) are assigned non-zero weight. Unless the mean function is flat beyond the range of \\(x\\), the kernel estimator will be biased near the edges of the range.\n\n\n\nCross-validation for \\(\\lambda\\)\nThe best value for \\(\\lambda\\) that resolves the bias-variance tradeoff for a particular set of data can be determined by a form of cross-validation.\n\n\nExample: Kernel Regression with Simulated Data\n\n\nHere we implement a function that performs leave-one-out CV and computes the average squared prediction residual (PRESS). The function is then evaluated on a grid of candidate bandwidth values.\n\nksm.cv &lt;- function(X,Y,bw=0.5) {\n    MSTe &lt;- rep(0,length(bw))\n    for (j in seq_along(bw)) {\n        PRESS &lt;- 0\n        ngood &lt;- 0\n        for (i in seq_along(X)) {\n            nw &lt;- ksmooth(x        =X[-i],\n                          y        =Y[-i],\n                          kernel   =\"normal\",\n                          bandwidth=bw[j],\n                          x.points =X[i])\n            if (!is.na(nw$y)){\n                PRESS &lt;- PRESS + (Y[i] - nw$y)^2\n                ngood &lt;- ngood + 1\n            }\n        }\n        MSTe[j] = PRESS/ngood\n    }\n    return (as.data.frame(cbind(bw,MSTe,ngood)))\n}\n\nbw_grid &lt;- seq(0.5,1.5,0.05)\ncvres &lt;- ksm.cv(X,Y,bw_grid)\n\ncat(\"Smallest LOOCV MSE \", \n    cvres$MSTe[which.min(cvres$MSTe)],\n    \" at bandwidth \",\n    cvres$bw[which.min(cvres$MSTe)])\n\nSmallest LOOCV MSE  7.09627  at bandwidth  1.1\n\n\nFigure 11.4 displays the LOOCV mean-squared error for bandwidth values from 0.5 to 1.5 in steps of 0.05. The smallest error is achieved at \\(\\lambda = 1.1\\). Figure 11.5 displays the kernel regression estimate for that bandwidth. The local model fits the data well but continues to show bias at the boundary of the \\(x\\)-range.\n\n\n\n\n\n\n\n\nFigure 11.4: Leave-one-out cross-validation MSE as a function of bandwidth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: Kernel regression estimates at the cross-validated value of \\(lambda=1.1\\) and true mean function.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you graph the results of cross-validation and the minimum criteria is achieved for the smallest (or largest) value of the hyperparameter being validated, you need to extend the grid of values below (or above) the values considered. Otherwise you might erroneously conclude that the “best” value is the value on the boundary of your candidate values. This is not a problem in Figure 11.4 where the minimum MSE does not fall on the boundary.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Models</span>"
    ]
  },
  {
    "objectID": "reglocal.html#local-regression",
    "href": "reglocal.html#local-regression",
    "title": "11  Local Models",
    "section": "11.2 Local Regression",
    "text": "11.2 Local Regression\nThe kernel regression estimate at \\(x_0\\) is equivalent to fitting a weighted intercept-only model \\[\nY(x) = \\beta_0 + \\epsilon\n\\] where the weights are calculated according to a kernel function with some bandwidth \\(\\lambda\\): \\[\nw_i(x_0) = \\frac{K_\\lambda(x_0,x_i)}{\\sum_{i=1}^n K_\\lambda(x_0,x_i)}\n\\]\nLocal regression generalizes this idea to weighted polynomial regression models. A local regression model of degree 1 (linear) is a weighted regression with mean function \\[\nY(x) = \\beta_0 + \\beta_1 x + \\epsilon\n\\] A local regression model of degree 2 (quadratic) is a weighted regression with mean function \\[\nY(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\n\\] and so on. The kernel regression estimate of the previous section is a special case, a local regression model of degree 0.\nThe parameters in the local regression model change with prediction location \\(x_0\\) because the kernel weight function changes with \\(x_0\\). Formally, the estimates at \\(x_0\\) satisfy a weighted least squares criterion. For a model of degree 1 the criterion is\n\\[\n            \\mathop{\\mathrm{arg\\,min}}_{\\beta_0(x_0), \\beta_1(x_0)} = \\sum_{i=1}^n K_\\lambda(x_0,x_i) \\left(y_i - \\beta_0(x_0) - \\beta_1(x_0) x_i \\right )^2\n\\]\n\nChoosing the Degree\nThe degree of a local regression polynomial is typically small, \\(p=1\\) or \\(p=2\\). Higher-order polynomials are against the spirit of local methods that any function can be approximated in a small neighborhood by a low-order polynomials.\nThe boundary bias we noticed in the kernel regression (\\(p=0\\)) estimates is greatly reduced when \\(p &gt; 0\\). The local model now depends on \\(x\\) and does not approximate the mean with a flat line near the boundary.\nFor the same bandwidth, a quadratic model will be more flexible (wiggly) than a linear model. With curved trends, quadratic models can fit better than linear models in the interior of the \\(x\\) range. This is a classical bias-variance tradeoff.\n\n\nExample: Linear and Quadratic Local Polynomials with Simulated Data\n\n\nThe KernSmooth::locpoly function in R computes local polynomial regression estimates with Gaussian kernels. For comparison purposes, the following fit models of degree 0, 1, and 2 for the same bandwidth.\n\nlibrary(KernSmooth)\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\nlp_05_0 &lt;- locpoly(X,Y,\n                   degree   =0,\n                   kernel   =\"normal\",\n                   bandwidth=0.5,\n                   gridsize =250,\n                   range.x  =c(-5,6))\n\nlp_05_1 &lt;- locpoly(X,Y,\n                   degree   =1,\n                   kernel   =\"normal\",\n                   bandwidth=0.5,\n                   gridsize =250,\n                   range.x  =c(-5,6))\n\nlp_05_2 &lt;- locpoly(X,Y,\n                   degree   =2,\n                   kernel   =\"normal\",\n                   bandwidth=0.5,\n                   gridsize =250,\n                   range.x  =c(-5,6))\n\nThe local quadratic model is the most flexible at the same bandwidth (Figure 11.6). All three estimates are similar in the interior of the \\(x\\) range. The boundary bias of the linear and quadratic model is much reduced compared to the kernel regression (degree 0) estimate.\n\n\n\n\n\n\n\n\nFigure 11.6: Local polynomials of degree 0, 1, and 2 with Gaussian kernels and \\(\\lambda = 0.5\\).\n\n\n\n\n\n\n\n\n\nLoess Regression\nLoess (or LOESS) is a special case of local polynomial regression described by Cleveland (1979). The abbreviation stands for Local Estimated Scatterplot Smoothing. In terms of concepts introduced so far, Cleveland recommended a local linear model (degree 1) with a tricube kernel weight function and leave-one-out cross-validation to determine the bandwidth.\nThe tricube kernel was preferred because it reduces to exactly 0 and thus excludes observations outside of the tricube window from the calculations. The resulting computational savings were important at the time LOESS was proposed.\nThe loess function in R implements LOESS and specifies the bandwidth through a span parameter that determines the neighborhood of \\(x_0\\) as a proportion of the data points. The tricube weight function is then applied to the points in the spanned neighborhood. By default loess fits quadratic models with span=0.75. This value is fairly large for applications where you want to model \\(Y\\) as a function of \\(X\\), but works well in situations where you want to detect the presence of larger trends, as in the analysis of residuals.\nFitting of LOESS models and cross-validating the span parameter can be done conveniently in R with the train function in the caret package.\n\n\nExample: LOESS Model of Degree 1 with Simulated Data\n\n\n\ndf &lt;- data.frame(cbind(Y,X))\nmodel &lt;- caret::train(Y ~ X, \n               data    =df,\n               method  =\"gamLoess\", \n               tuneGrid=expand.grid(span = 0.2, degree=1), \n               trContro=trainControl(method = \"none\"))\n\n\n\n\n\n\n\n\n\nFigure 11.7: LOESS smooth with span 0.2, degree 1.\n\n\n\n\n\n\n\nTo perform cross-validation for the span parameter, you can supply instructions to the train function. The tuneGrid parameter provides a data frame with values for the hyperparameters. In the code below, the span is varied from 0.15 to 0.75 in 40 steps and the degree of the local polynomial is held fixed at 1. The trControl parameter defines the behavior of the train function. Here, 10-fold cross-validatoin of the hyperparameters is requested.\n\nset.seed(3678)\n\nctrl &lt;- trainControl(method = \"cv\", number = 10)\ngrid &lt;- expand.grid(span = seq(0.15, 0.75, len = 40), degree = 1)\n\nmodel &lt;- train(Y ~ X, \n               data     =df, \n               method   =\"gamLoess\", \n               tuneGrid =grid, \n               trControl=ctrl)\n\nprint(model)\n\nGeneralized Additive Model using LOESS \n\n100 samples\n  1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 89, 89, 90, 92, 91, 90, ... \nResampling results across tuning parameters:\n\n  span       RMSE      Rsquared   MAE     \n  0.1500000  2.901927  0.7325910  2.145192\n  0.1653846  2.919378  0.7336561  2.140403\n  0.1807692  2.615121  0.7373488  2.036346\n  0.1961538  2.624746  0.7364527  2.030095\n  0.2115385  2.656667  0.7304098  2.042989\n  0.2269231  2.668584  0.7263728  2.058201\n  0.2423077  2.679732  0.7206956  2.064964\n  0.2576923  2.694870  0.7175513  2.077081\n  0.2730769  2.757301  0.7032400  2.107686\n  0.2884615  2.787906  0.6907119  2.117425\n  0.3038462  2.811633  0.6848968  2.142258\n  0.3192308  2.831422  0.6761193  2.170968\n  0.3346154  2.885184  0.6611100  2.218675\n  0.3500000  2.967564  0.6400800  2.275226\n  0.3653846  3.004447  0.6280659  2.305204\n  0.3807692  3.066032  0.6113772  2.340453\n  0.3961538  3.097985  0.6022572  2.362501\n  0.4115385  3.163806  0.5835644  2.402466\n  0.4269231  3.203701  0.5713253  2.422956\n  0.4423077  3.257056  0.5567435  2.457546\n  0.4576923  3.292731  0.5471882  2.476899\n  0.4730769  3.338219  0.5342448  2.504264\n  0.4884615  3.351780  0.5291961  2.515220\n  0.5038462  3.391540  0.5169620  2.539841\n  0.5192308  3.443358  0.5013535  2.572879\n  0.5346154  3.478873  0.4891299  2.597151\n  0.5500000  3.501811  0.4799188  2.620423\n  0.5653846  3.536125  0.4668376  2.645889\n  0.5807692  3.592910  0.4487972  2.679671\n  0.5961538  3.645415  0.4337016  2.712018\n  0.6115385  3.605952  0.4463734  2.686381\n  0.6269231  3.574397  0.4561588  2.666609\n  0.6423077  3.623664  0.4421608  2.695329\n  0.6576923  3.605723  0.4552149  2.666462\n  0.6730769  3.632124  0.4461638  2.684623\n  0.6884615  3.650800  0.4401234  2.699304\n  0.7038462  3.667380  0.4341022  2.713919\n  0.7192308  3.690093  0.4270565  2.732718\n  0.7346154  3.710603  0.4196154  2.752871\n  0.7500000  3.743538  0.4038491  2.785017\n\nTuning parameter 'degree' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were span = 0.1807692 and degree = 1.\n\nplot(model)\n\n\n\n\n\n\n\nmin(model$results$RMSE)\n\n[1] 2.615121\n\nmodel$results$span[which.min(model$results$RMSE)]\n\n[1] 0.1807692\n\n\nThe smallest root mean square error (RMSE) of 2.6151 is achieved with a span of 0.1808.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Models</span>"
    ]
  },
  {
    "objectID": "reglocal.html#sec-basis-functions",
    "href": "reglocal.html#sec-basis-functions",
    "title": "11  Local Models",
    "section": "11.3 Basis Functions",
    "text": "11.3 Basis Functions\n\nSegments, Knots, and Constraints\nLocal models using weight functions provide a lot of flexibility to model smooth functions based on a simple idea: if a constant, linear, or quadratic model approximates the local behavior at \\(x_0\\) well then we can estimate the parameters of that local model through a weighted analysis that downweighs observations based on their distance from \\(x_0\\). Data points that are far from \\(x_0\\) will not have much impact on determining the local behavior if we choose the bandwidth of the kernel weight function properly.\nGreater flexibility in the fitted model could also be achieved by adding higher-order polynomial terms and fitting a global model. Unfortunately, this does not work well. Adding a higher-order term such as a cubic (\\(x^3\\)), quartic (\\(x^4\\)), or quintic (\\(x^5\\)) term to pick up curvature in one region can introduce strange behavior in another region. Figure 11.8 shows the fit of global polynomial models with up to third, fourth, and fifth degree terms for the simulated data. The quartic model \\[\nY = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4 x^4 + \\epsilon\n\\] does a pretty good job in capturing the curvature in the center of the \\(x\\) range. It could benefit from more flexibility in the upper region of \\(x\\), though. Adding a quintic term \\(\\beta_5 x^5\\) to the model improves the fit for larger values of \\(x\\) but leads to poor behavior for small values of \\(x\\).\n\n\n\n\n\n\nFigure 11.8: Polynomial models up to the 5th degree.\n\n\n\nCan we introduce flexibility into the model by creating local behavior and avoid the uncontrolled behavior of higher-degree polynomials? The answer is “yes”, and splines is the way we achieve that.\nA spline model is a piecewise polynomial model. Rather than fitting a local model at every prediction point, a spline model connects polynomial models at specific points along the \\(x\\)-axis, called knots. As with global models, fitting a spline model and making predictions are separate steps. The model is local in the sense that we have different polynomials in different sections of the \\(x\\) range. Figure 11.9 displays the concept for piecewise polynomials of degree 1–that is, piecewise linear models. The extreme points of the \\(x\\) range serve as the boundary knots, four interior knots are evenly spaced along the \\(x\\) axis. This results in five linear segments. If we denote the interior knots as \\(c_1, \\cdots, c_4\\), this piecewise linear model can be written as follows\n\\[\nY = \\left \\{ \\begin{array}{ll} \\beta_{01} + \\beta_{11} x & x \\le c_1 \\\\\n\\beta_{02} + \\beta_{12}x & c_1 &lt; x \\le c_2 \\\\\n\\beta_{03} + \\beta_{13}x & c_2 &lt; x \\le c_3 \\\\\n\\beta_{04} + \\beta_{14}x & c_3 &lt; x \\le c_4 \\\\\n\\beta_{05} + \\beta_{15}x & x &gt; c_4 \\\\\n\\end{array}\n\\right .\n\\]\nEach segment has a separate intercept and slope estimate. As the number of knots increases, the model becomes more localized and has more parameters.\n\n\n\n\n\n\nFigure 11.9: Piecewise linear model with four interior knots.\n\n\n\nThe name spline originates in construction where thin strips of wood were used to create curved shapes, for example, in shipbuilding. The strips of wood, called splines, are fixed at two points with devices called “dogs” or “ducks” and can be flexed between them (Figure 11.10).\n\n\n\n\n\n\nFigure 11.10: A spline in construction. Source\n\n\n\n\n\n\n\n\n\nSpline Table\n\n\n\n\n\nAnd now for something completely different! Inspired by the idea of a spline as a strip of wood bent around anchors I built a table from Pauwlonia wood harvested and dried on my property in Blacksburg, VA. The splines are 3/8 inches thick strips of wood, the “ducks” are small pieces of walnut. The legs are also made from the same tree. The board milled from the center of the trunk was used for the curved piece in the middle of the table; its natural channel is filled with colored epoxy.\n\n\n\n\n\n\n\nPaulownia spline table.\n\n\n\n\n\nThe segments in Figure 11.9 are not curved, that can be fixed by increasing the degree of the polynomial in each segment. But we recognize another problem with the piecewise linear model, the fitted lines do not connect at the segments. In order for the model to be continuous across the interior knots, the parameter estimates cannot be chosen freely–they have to obey certain constraints. For example, in order for the first two segments in Figure 11.9 to connect, we need to have \\[\n\\beta_{01} + \\beta_{11} c_1 = \\beta_{02} + \\beta_{12} c_1\n\\]\nWe would not be satisfied with the segments just touching at the knots, we also would want the spline to transition smoothly through the knots. So we can impose additional constraints on the polynomials. To ensure that the segments have the same slope at the knot we constrain their first derivative \\[\n\\beta_{11} = \\beta_{12}\n\\]\nWith each constraint the number of free parameters in the model is reduced. We started with ten parameters in the model with four (interior) knots and disconnected lines (Figure 11.9). Imposing continuity at each knot imposes one constraint at each knot, leaving us with 6 free parameters (we are using 6 degrees of freedom for the model). Imposing continuity in the first derivative at each knot removes another 4 parameters, leaving us with 2 degrees of freedom. Without increasing the degree of the polynomial we will not be able to add further constraints. In fact, the continuous piecewise linear model with continuity in the first derivatives is just a global simple linear regression model.\nTo leave degrees of freedom to pick up local trends in the mean function we use cubic splines that are continuous at the knots and have the same first and second derivatives to be equal at the knots. A spline that satisfies those conditions is known as a cubic spline (Figure 11.11) or a degree-3 spline.\nA spline of degree \\(d\\) in general is a piecewise polynomial of degree \\(d\\) with continuity constraints in the derivatives up to degree \\(d-1\\) at each knot. There is typically no need to engage splines of higher orders than cubic splines. They still have a discontinuity at the knots since they are continuous only up to second derivatives but it is said that the discontinuity is not visible to the human eye (Hastie, Tibshirani, and Friedman 2001, 134).\n\n\n\n\n\n\nFigure 11.11: Cubic spline with four interior knots.\n\n\n\nWe also want to handle the boundary properly and are imposing another constraint: the model should have degree 1 beyond the range of \\(x\\) in the training data. This is known as a natural cubic spline (Figure 11.12). The natural cubic spline achieves linearity at the boundary by imposing additional constraints, two at each boundary knot. The four degrees of freedom made available–compared to the cubic spline–can be used to place an additional interior knot.\n\n\n\n\n\n\nFigure 11.12: Natural cubic spline with four interior knots.\n\n\n\nMore degrees of freedom in the spline translates into more free parameters and more flexibility in the fit. For a cubic spline with \\(K\\) knots, there are \\(K+1\\) degree-3 polynomial segments that obey \\(K\\times 3\\) constraints at the knots. The model thus uses \\(4(K+1) - 3K = K+4\\) degrees of freedom. The flexibility of a spline model increases with the number of knots.\n\n\nBasis Function Representation\nFitting a spline model is a constrained optimization problem: minimize the residual sum of squares of the piecewise polynomial subject to the continuity constraints imposed on the parameters. There is a more direct way in which the model is written in a way that incorporates the constraints and a standard least-squares routine for linear models can be used.\nLet’s think of our model in a slightly different formulation, \\[\nY = \\beta_0 + \\beta_1 h_1(x) + \\beta_2 h_2(x) + \\cdots + \\beta_p h_p(x) + \\epsilon\n\\] The functions \\(h_1(x), \\cdots, h_p(x)\\) are known transformations of \\(x\\). For example, a third-degree polynomial has \\(h_1(x)=x\\), \\(h_2(x)=x^2\\), and \\(h_3(x)=x^3\\).\nThis is called the basis function representation of the linear model. The cubic spline model can be written as a third-degree polynomial augmented by truncated power basis functions. A truncated power basis function takes the general form\n\\[\nt^n_+ = \\left \\{ \\begin{array}{cc} t^n & t &gt; 0 \\\\ 0 & t \\le 0\\end{array}\\right .\n\\]\nThe cubic spline model with \\(K\\) (interior) knots, written in terms of truncated power basis functions becomes \\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_{3+1}(x-c_1)^3_+ + \\cdots + \\beta_{3+K}(x-c_K)^3_+ + \\epsilon\n\\tag{11.1}\\]\nFor each knot, an input variable is added, its values are the 3-rd order truncated power basis functions of the distance from the \\(k\\)th knot. The model has \\(K+4\\) parameters, as expected for a cubic spline.\n\n\nExample: Cubic Spline with 3 Knots\n\n\nFor the simulated data we can fit a cubic spline with 3 knots by constructing a model matrix for Equation 11.1 and passing it to the lm.fit function. First we need to decide on the placement of the three knots. One option is to place knots evenly throughout the range of \\(x\\). Or, we can place knots at certain percentiles of the distribution of the \\(x\\) data. We choose the 1st, 2nd (median), and 3rd quartile here.\n\ns &lt;- summary(X)\nc_1 &lt;- s[2] # 1st. quartile\nc_2 &lt;- s[3] # median\nc_3 &lt;- s[5] # 3rd quartile\n\nc_1\n\n  1st Qu. \n-1.025803 \n\nc_2\n\n    Median \n0.05192918 \n\nc_3\n\n3rd Qu. \n1.39678 \n\ntpf &lt;- function(x,knot,degree) {\n    t &lt;- ifelse(x &gt; knot, (x-knot)^degree, 0)\n}\n\nXmatrix &lt;- cbind(rep(1,length(X)), X, X^2, X^3, \n                 tpf(X,c_1,3), tpf(X,c_2,3), tpf(X,c_3,3))\ncolnames(Xmatrix) &lt;- c(\"Inctpt\",\"x\",\"x^2\",\"x^3\",\"tpf_c1\",\"tpf_c2\",\"tpf_c3\")\nround(Xmatrix[1:10,],4)\n\n      Inctpt       x    x^2      x^3  tpf_c1  tpf_c2 tpf_c3\n [1,]      1  0.4479 0.2006   0.0898  3.2003  0.0621 0.0000\n [2,]      1 -2.3124 5.3474 -12.3656  0.0000  0.0000 0.0000\n [3,]      1  0.8448 0.7137   0.6030  6.5459  0.4985 0.0000\n [4,]      1 -2.6495 7.0199 -18.5993  0.0000  0.0000 0.0000\n [5,]      1  0.2822 0.0796   0.0225  2.2377  0.0122 0.0000\n [6,]      1 -1.0721 1.1494  -1.2323  0.0000  0.0000 0.0000\n [7,]      1 -0.6232 0.3884  -0.2421  0.0653  0.0000 0.0000\n [8,]      1  3.1122 9.6859  30.1447 70.8563 28.6608 5.0481\n [9,]      1 -0.8961 0.8029  -0.7195  0.0022  0.0000 0.0000\n[10,]      1  0.6422 0.4125   0.2649  4.6412  0.2057 0.0000\n\n\nObservations to the left of \\(c_1\\) = -1.0258 do not receive a contribution from the augmented power function terms, for example the second observation. Observation #8, which falls to the right of \\(c_3\\) = 1.3968 receives a contribution from all three truncated power function terms.\n\ncub_spline &lt;- lm.fit(x=Xmatrix,y=Y)\ncub_spline$coefficients\n\n    Inctpt          x        x^2        x^3     tpf_c1     tpf_c2     tpf_c3 \n -4.324405 -11.263153  -6.970494  -0.951829   3.572409  -5.185659   4.441695 \n\n\n\n\n\n\n\n\n\n\nFigure 11.13: Cubic spline fit by linear least squares based on truncated power basis functions.\n\n\n\n\n\nFor a model with only seven parameters the cubic spline with knots at the 1st, 2nd, and 3rd quartile provides an excellent fit (Figure 11.13)\n\n\n\n\nRegression Splines\nThe model in the previous example is known as a regression spline, characterized by placing a fixed number of knots. A common approach is to specify \\(K\\), the number of knots and placing knots at percentiles of the \\(x\\) values, as in the previous example. Another option is to place more knots in regions where the mean function is expected to vary more.\nThe truncated power basis function is one approach to generating the \\(\\textbf{X}\\) matrix for a least-squares regression that produces a particular spline. It is not the numerically most advantageous approach. There are many equivalent basis functions to represent polynomials. A popular basis are the B-spline basis functions.\nThe R functions bs() and ns() in the splines library compute cubic spline matrices and natural cubic spline matrices based on B-splines.\n\nlibrary(splines)\nxx &lt;- bs(X,df=6)\nattr(xx,\"knots\")\n\n[1] -1.02580300  0.05192918  1.39678037\n\n\nThe number of knots generated by bs() equals the chosen degrees of freedom minus the degree of the spline, which is 3 by default. The knots are placed at percentiles of \\(x\\), a third-order spline with 6 degrees of freedom results in 3 knots at the 1st, 2nd, and 3rd quartiles; the same values used earlier in the truncated power function example.\n\nround(xx[1:10,],5)\n\n            1       2       3       4       5       6\n [1,] 0.00000 0.04647 0.76826 0.18360 0.00167 0.00000\n [2,] 0.40941 0.43423 0.09265 0.00000 0.00000 0.00000\n [3,] 0.00000 0.00914 0.66063 0.31682 0.01340 0.00000\n [4,] 0.47892 0.34084 0.05217 0.00000 0.00000 0.00000\n [5,] 0.00000 0.07531 0.79106 0.13331 0.00033 0.00000\n [6,] 0.07129 0.51921 0.40950 0.00000 0.00000 0.00000\n [7,] 0.01545 0.38341 0.59720 0.00394 0.00000 0.00000\n [8,] 0.00000 0.00000 0.08153 0.39302 0.44131 0.08414\n [9,] 0.04277 0.47603 0.48107 0.00013 0.00000 0.00000\n[10,] 0.00000 0.02336 0.72313 0.24798 0.00553 0.00000\n\n\nAlthough the xx matrix produced by bs() is not identical to the Xmatrix based on the truncated power basis functions, the models are equivalent and the predicted values are identical. The conditioning of the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix based on the truncated power basis is poorer than the conditioning of the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix computed from the B-spline basis. The pairwise correlations are generally higher in the former.\n\nround(cor(Xmatrix[,2:7]),5)\n\n             x     x^2     x^3  tpf_c1  tpf_c2  tpf_c3\nx      1.00000 0.20112 0.82204 0.71892 0.63024 0.50541\nx^2    0.20112 1.00000 0.42635 0.76803 0.77741 0.73487\nx^3    0.82204 0.42635 1.00000 0.89977 0.88997 0.84555\ntpf_c1 0.71892 0.76803 0.89977 1.00000 0.98705 0.92475\ntpf_c2 0.63024 0.77741 0.88997 0.98705 1.00000 0.97202\ntpf_c3 0.50541 0.73487 0.84555 0.92475 0.97202 1.00000\n\nround(cor(xx),5)\n\n         1        2        3        4        5        6\n1  1.00000  0.46034 -0.59238 -0.49291 -0.29365 -0.13162\n2  0.46034  1.00000 -0.08910 -0.75758 -0.48386 -0.21712\n3 -0.59238 -0.08910  1.00000 -0.00750 -0.45067 -0.30818\n4 -0.49291 -0.75758 -0.00750  1.00000  0.59160 -0.05045\n5 -0.29365 -0.48386 -0.45067  0.59160  1.00000  0.28366\n6 -0.13162 -0.21712 -0.30818 -0.05045  0.28366  1.00000\n\n\nThe function ns() produces the natural cubic splines from the B-spline basis.\n\nnn &lt;- ns(X,df=6)\nround(nn[1:10,],5)\n\n            1       2       3        4       5        6\n [1,] 0.01539 0.53224 0.44483  0.00664 0.00211 -0.00122\n [2,] 0.24048 0.00000 0.00000 -0.20417 0.48178 -0.27761\n [3,] 0.00000 0.24623 0.69323  0.05335 0.01697 -0.00978\n [4,] 0.13541 0.00000 0.00000 -0.20501 0.48376 -0.27875\n [5,] 0.04383 0.63577 0.31891  0.00131 0.00042 -0.00024\n [6,] 0.68611 0.15236 0.00000 -0.05195 0.12258 -0.07063\n [7,] 0.54971 0.41216 0.00251 -0.01149 0.02711 -0.01562\n [8,] 0.00000 0.00000 0.13784  0.50699 0.26615  0.08901\n [9,] 0.66500 0.23641 0.00000 -0.03179 0.07502 -0.04323\n[10,] 0.00205 0.38855 0.58442  0.02201 0.00700 -0.00404\n\n\nThe models can be fit without first generating the spline matrix:\n\nns_spline &lt;- lm(Y ~ ns(X,df=6))\nbs_spline &lt;- lm(Y ~ bs(X,df=6))\n\nFigure 11.14 displays the fitted values for the 6-df cubic and natural cubic splines.\n\n\n\n\n\n\n\n\nFigure 11.14: Cubic and natural cubic splines with 6 degrees of freedom.\n\n\n\n\n\nAn objective method for choosing the number of knots in regression splines is through cross-validation.\n\n\nExample: Choosing Degrees of Freedom by Cross-validation\n\n\nThe following code use the caret::train function in R to choose the degrees of freedom of the cubic spline based on 10-fold cross-validation. The degrees of freedom evaluated range from 2 to 20.\n\ndf &lt;- data.frame(Y=Y,X=X)\n\nset.seed(3678)\n\nctrl &lt;- trainControl(method = \"cv\", number = 10)\ngrid &lt;- expand.grid(df = seq(2,20,by=1))\n\nmodel &lt;- train(Y ~ X, \n               data      = df, \n               method    = \"gamSpline\", \n               tuneGrid  = grid, \n               trControl = ctrl)\n\nmodel\n\nGeneralized Additive Model using Splines \n\n100 samples\n  1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 89, 89, 90, 92, 91, 90, ... \nResampling results across tuning parameters:\n\n  df  RMSE      Rsquared   MAE     \n   2  4.044198  0.3076557  3.020568\n   3  3.659884  0.4245450  2.749611\n   4  3.304364  0.5299490  2.565251\n   5  2.989824  0.6241050  2.373329\n   6  2.749716  0.6861406  2.208537\n   7  2.589525  0.7207527  2.094446\n   8  2.489518  0.7400459  2.028971\n   9  2.427930  0.7516088  1.999040\n  10  2.389342  0.7591292  1.983502\n  11  2.364524  0.7643125  1.973074\n  12  2.348485  0.7679405  1.965031\n  13  2.338639  0.7703677  1.959352\n  14  2.333664  0.7717649  1.955700\n  15  2.332962  0.7722239  1.955451\n  16  2.336070  0.7718901  1.957769\n  17  2.343017  0.7708601  1.961188\n  18  2.353694  0.7692620  1.965484\n  19  2.368154  0.7672486  1.973497\n  20  2.386424  0.7648974  1.982863\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was df = 15.\n\nplot(model)\n\n\n\n\n\n\n\n\nThe square root of the mean square error (RMSE) achieves a minimum for df=15 although any value between 12 and 17 is reasonable.\nNext, the final model is trained using the degrees of freedom chosen by cross-validation.\n\nmodel &lt;- train(Y ~ X, \n               data      = df, \n               method    = \"gamSpline\", \n               tuneGrid  = expand.grid(df=model$bestTune$df), \n               trControl = trainControl(method = \"none\"))\n\n\n\n\n\n\nCross-validated cubic spline fit.\n\n\n\n\n\n\n\n\nSmoothing Splines\nRegression splines require specification of the number of knots \\(K\\) and their placement \\(c_1, \\cdots, c_K\\). Equivalently, the degrees of freedom of the spline can be specified instead of \\(K\\). Choosing \\(K\\) too small results in bias and choosing \\(K\\) too large leads to overfit models with high variance.\nSmoothing splines approach the issue of knot placement from a different angle. Why not place knots at all unique values of \\(X\\)? This creates a high-dimensional problem that can be adjusted through regularization. This resolves the questions of knot number and knot placement and shifts attention from cross-validating \\(K\\) to choosing the regularization parameter.\nThe regularization penalty for smoothing splines is \\[\n   \\lambda \\int f^{\\prime\\prime}(v)\\,dv  \n\\] where \\(f^{\\prime\\prime}\\) denotes the second derivative of the mean function and \\(\\lambda\\) is the hyperparameter. The rationale for this penalty term is as follows: the second derivative \\(f^{\\prime\\prime}(x)\\) measures the curvature of \\(f\\) at \\(x\\). High absolute values indicate that \\(f\\) changes quickly in the neighborhood of \\(x\\), the sign of a wiggly function. The integral of the second derivative can be interpreted as the degree of wiggliness across the range of \\(x\\).\nFor a given value of \\(\\lambda\\) more wiggly functions result in a higher penalty. For \\(\\lambda = 0\\) no penalty is added. The basic model for a smoothing spline is a cubic spline with knots at all unique \\(x\\)-values. The regularization penalty shrinks the coefficient estimates of the spline and reduces the variability of the fitted function this way.\nAn interesting aspect are the effective or equivalent degrees of freedom. The smoothing spline has \\(n\\) parameters but these are shrunk if the penalty term kicks in. The equivalent degrees of freedom are a measure of the flexibility of the fitted function. Mathematically, the fitted values of a smoothing spline is a linear function of the observed data. For each value \\(\\lambda\\), the solution can be written as\n\\[\n\\widehat{y}_\\lambda = \\textbf{S}_\\lambda \\textbf{y}\n\\] where \\(\\textbf{S}_\\lambda\\) is the smoother matrix, akin to the Hat matrix in global regression models. Recall that the Hat matrix is an orthogonal projection matrix and its trace equals the number of parameters (the degrees of freedom) of the model. Similarly, the equivalent degrees of freedom of a smoother are obtained as the trace of the smoother matrix, \\[\ndf_\\lambda = tr(\\textbf{S}_\\lambda) = \\sum_{i=1}^n S[ii]_\\lambda\n\\]\n\n\nExample: Apple Share Prices\n\n\nThe data for this example are the weekly closing share prices of Apple (ticker symbol AAPL) between January 1996 and January 2024. Figure 11.15 shows a time series of these prices along with the dates major Apple products were introduced.\n\nlibrary(splines)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(\"duckdb\")\n\nLoading required package: DBI\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nweekly &lt;- dbGetQuery(con, \"SELECT * FROM AAPLWeekly\")\nweekly$date &lt;- make_date(weekly$year,weekly$month,weekly$day)\n\ndbDisconnect(con)\n\n\n\n\n\n\n\n\n\nFigure 11.15: Weekly AAPL close share prices from January 1996 until January 2024 and dates major products were introduced.\n\n\n\n\n\nThe smooth.spline function in R computes smoothing splines and selects the smoothing parameter (the penalty hyperparameter) by leave-one-out cross-validation. It should be noted that smooth.spline by default does not place knots at all unique \\(x\\) values, but places evenly-spaced knots, their number is a function of the unique number of \\(x\\)-values. You can change this behavior by setting all.knots=TRUE.\nIn the AAPL example, there are 1,461 unique dates, and smooth.spline would choose 154 knots by default.\n\n# general method for computing the number of unique values in a numeric vector\nnx &lt;- length(weekly$date) - \n      sum(duplicated(round((weekly$date - mean(weekly$date))/1e-6)))\nnx\n\n[1] 1461\n\n.nknots.smspl(nx)\n\n[1] 154\n\n\n\nsmspl_a &lt;- smooth.spline(y=weekly$Close,\n                       x=weekly$date,\n                       all.knots=TRUE,\n                       cv=TRUE)\nsmspl_a\n\nCall:\nsmooth.spline(x = weekly$date, y = weekly$Close, cv = TRUE, all.knots = TRUE)\n\nSmoothing Parameter  spar= 0.3941696  lambda= 1.661548e-10 (11 iterations)\nEquivalent Degrees of Freedom (Df): 607.3517\nPenalized Criterion (RSS): 1235.319\nPRESS(l.o.o. CV): 2.544907\n\n\nThe cross-validation leads to a PRESS criterion of 2.5449 and equivalent degrees of freedom of 607.352. The regularization penalized the spline from 1461 interior knots to the equivalent of a spline with about 610 knots. The smoothing parameter spar has a substantial value of 0.3942.\nSetting all.knots=FALSE leads to the following analysis\n\nsmspl_s &lt;- smooth.spline(y=weekly$Close,\n                         x=weekly$date,\n                         all.knots=FALSE,\n                         cv=TRUE)\nsmspl_s\n\nCall:\nsmooth.spline(x = weekly$date, y = weekly$Close, cv = TRUE, all.knots = FALSE)\n\nSmoothing Parameter  spar= 0.007723702  lambda= 2.115138e-09 (15 iterations)\nEquivalent Degrees of Freedom (Df): 152.8225\nPenalized Criterion (RSS): 8154.007\nPRESS(l.o.o. CV): 6.96885\n\n\nThe smoothing parameter is much smaller, spar=0.00772 and the equivalent degrees of freedom are not much different from the number of knots used (154). This suggests that this smoothing spline has not been penalized sufficiently, the appropriate number of equivalent degrees of freedom is higher–this was confirmed in the first call to smooth.spline with all.knots=TRUE. Also note that the PRESS statistic for this run is almost 3 times higher than in the first analysis.\nFigure 11.16 compares the fitted values for the smoothing splines. The smoothing spline from the second analysis does not capture the changes in share price in recent years but does a good job in the earlier years. The degree of smoothness in the weekly share prices changes over time which presents a challenge for any method that attempts to distill the degree of smoothness in a single parameter.\n\n\n\n\n\n\n\n\nFigure 11.16: Smoothing splines for AAPL close share prices.\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Simulated data according to García-Portugués (2024).\nFigure 11.2: Epanechnikov, tricube and Gaussian kernel functions popular in kernel regression and density estimation. Bandwidth set to 0.5 and 1 for each kernel.\nFigure 11.8: Polynomial models up to the 5th degree.\nFigure 11.9: Piecewise linear model with four interior knots.\nPaulownia spline table.\nFigure 11.11: Cubic spline with four interior knots.\nFigure 11.12: Natural cubic spline with four interior knots.\nFigure 11.14: Cubic and natural cubic splines with 6 degrees of freedom.\nFigure 11.15: Weekly AAPL close share prices from January 1996 until January 2024 and dates major products were introduced.\nFigure 11.16: Smoothing splines for AAPL close share prices.\n\n\n\nCleveland, William S. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36. https://doi.org/10.1080/01621459.1979.10481038.\n\n\nGarcía-Portugués, E. 2024. Notes for Predictive Modeling. https://bookdown.org/egarpor/PM-UC3M/.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.",
    "crumbs": [
      "Part II. Supervised Learning I: Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Local Models</span>"
    ]
  },
  {
    "objectID": "classintro.html",
    "href": "classintro.html",
    "title": "12  Introduction",
    "section": "",
    "text": "12.1 Categorical Targets\nIn a classification problem the target variable is qualitative (categorical). The \\(k\\) categories of the target variable are also called the classes, hence the name classification. Recall from Section 1.2.0.4 that categorical variables belong to the discrete variables that have a countable number of values. With categorical variables, in contrast to counts, the values consist of labels, even if numbers are used for labeling.\nThe observed values of the target variable are simply labels that enumerate the possible classes. The set of labels can be as small as two, we call those binary target variables or can have thousands of elements, as can be the case when classifying objects on images or analyzing words based on a dictionary. Even if the categories are labeled numerically, we think of the numbers as identifiers for the categories, not in a numerical sense. A two-category target for customers with and without a subscription could be labeled as “subscriber”/“non-subscriber”, or “1”/“0”, or “Green”/“Blue”, and so on.\nWhen using numeric class labels, the temptation to treat them as values of a continuous variable grows with the number of classes. A frequent example are ratings on a preference scale (Figure 12.1). If the possible ratings are 1-star, 2-stars, 3-stars, 4-stars, and 5-stars, how can we possibly achieve 0.5 or 3.5 stars?\nThese calculations assume that the star-scale is continuous, which implies that differences between values are defined. This is not the case. A 4-star rating is higher than a 2-star rating, but it is not double that. The distance between a 4-star and a 3-star rating is not the same as that between a 3-star and a 2-star rating. The correct way to interpret the rating scale is as an ordinal lesser-greater sense. If a website claims that a product has a rating of 4.7, they are treating the rating as continuous. The correct way to state the central tendency of the ratings is to provide estimates of the multinomial category probabilities (Figure 12.2).\nClassification problems are everywhere. Here are some examples:\nThese examples show that predicting an observation and assigning an observation are ultimately the same in classification models. The predicted value in a classification model is the category we predict an observation to fall in. Depending on the type of model, this classification of an observation might first go through a prediction of another kind: the prediction of the probability of category membership. This is precisely the link between regression and classification models—more on that below.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#categorical-targets",
    "href": "classintro.html#categorical-targets",
    "title": "12  Introduction",
    "section": "",
    "text": "Multinomial Variables\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\n\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)\n\n\n\n\n\n\n\n\n\nFigure 12.1: What is wrong with this picture?\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: There. This is better.\n\n\n\n\n\nMedical diagnosis: given a patient’s symptoms and history assign a medical condition\nFinancial services: determine whether a payment transaction is fraudulent\nCustomer intelligence: assign a new customer to a customer profile (segmentation)\nComputer vision: detect defective items on an assembly line\nComputer vision: identify objects on an image\nText classification: detect spam email\nDigital marketing: predict which advertisement a user will click on\nSearch engine: Given a user’s query and search history, predict which link they will follow",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#from-label-to-encoding",
    "href": "classintro.html#from-label-to-encoding",
    "title": "12  Introduction",
    "section": "12.2 From Label to Encoding",
    "text": "12.2 From Label to Encoding\nSeparate from the labeling of the categories is the encoding of the categories. To process the data computationally, we need to replace at some point the descriptive class labels with numerical values. In logistic regression, for example, the two possible states of the target variable are coded as \\(1\\)s and \\(0\\)s where a \\(1\\) represents the event of interest. Another common coding scheme for the two states is to use \\(1\\) and \\(-1\\). This is common in boosting methods such as adaptive boosting (Section 20.1) and leads to a simple classification rule based on the sign of the predictor.\nIn situations where \\(k &gt; 2\\), categories are typically one-hot encoded. This is a fancy way of saying that one \\(Y\\) column with \\(k\\) unique values is expanded into \\(k\\) columns \\(Y_1,\\cdots,Y_k\\), each consists of zeros and ones. \\(X_j\\) contains ones in the rows where \\(Y\\) takes on the \\(j\\)th label, zero otherwise. Table 12.1 shows two categorical variables A and B with 2 and 4 levels, respectively, and their one-hot encoded expansions. A uses numerical values to identify the categories, B uses characters.\n\n\n\nTable 12.1: One-hot encoding of \\(A\\) and \\(B\\).\n\n\n\n\n\nA\n\\(A_1\\)\n\\(A_2\\)\nB\n\\(B_1\\)\n\\(B_2\\)\n\\(B_3\\)\n\\(B_4\\)\n\n\n\n\n1\n1\n0\nff\n1\n0\n0\n0\n\n\n1\n1\n0\ngg\n0\n1\n0\n0\n\n\n1\n1\n0\nhh\n0\n0\n1\n0\n\n\n2\n0\n1\nff\n1\n0\n0\n0\n\n\n2\n0\n1\ngg\n0\n1\n0\n0\n\n\n2\n0\n1\nhh\n0\n0\n1\n0\n\n\n1\n1\n0\nii\n0\n0\n0\n1\n\n\n\n\n\n\nThere is redundant information in Table 12.1. Once you know how many categories a variable has, you can fully determine the data from \\(k-1\\) one-hot encoded columns. If \\(Y_1,\\cdots,Y_{k-1}\\) are all zero, then \\(Y_k = 1\\); if any of the \\(Y_1,\\cdots,Y_{k-1}\\) is \\(1\\), then we know that \\(Y_k = 0\\). In statistical models it is common to use only \\(k-1\\) of the encoded columns, and to treat one of the columns as a baseline. In machine learning applications using neural networks for classification it is common to use all \\(k\\) columns.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#sec-class-rule",
    "href": "classintro.html#sec-class-rule",
    "title": "12  Introduction",
    "section": "12.3 Classification Rule",
    "text": "12.3 Classification Rule\nSuppose the \\(k\\) possible labels of a categorical target are \\(C_1, \\cdots, C_k\\) and the probability that \\(Y_i\\) takes on the label \\(C_j\\) is \\(\\Pr(Y_i=j|\\textbf{x}_i)\\). A classifier is a rule that assigns a predicted label based on estimates \\[\n\\widehat{\\Pr}(Y_i=j|\\textbf{x}_i)\n\\]\nWe use the shorthand \\(Y_i\\) to refer to the observed label and \\(\\widehat{Y}_i\\) to refer to the classified value (the predicted label). So which label should we assign to \\(\\widehat{Y}_i\\). With \\(k\\) classes, there are \\(k\\) probabilities, \\(\\widehat{\\Pr}(Y=1|\\textbf{x}), \\cdots, \\widehat{\\Pr}(Y=k|\\textbf{x})\\). The most common rule is to assign the category with the highest probability. This rule is known as the Bayes classifier: \\[\n\\widehat{Y}_i = \\mathop{\\mathrm{arg\\,max}}_j \\; \\widehat{\\Pr}(Y_i = j | \\textbf{x}_i)\n\\]\nThe Bayes classifier is optimal in the sense that no other classification rule achieves a lower misclassification rate (MCR): \\[\nMCR = \\frac{1}{n}\\sum_{i=1}^n I(\\widehat{y}_i \\neq y_i)\n\\tag{12.1}\\]\nIn Equation 12.1, \\(I()\\) is the identity function, returning \\(1\\) if the condition in parentheses is true, \\(0\\) otherwise. The misclassification rate is the proportion of observations for which the predicted category is different from the observed category.\nIn a binary problem the Bayes rule reduces to choosing the category with the largest predicted probability. Or, equivalently, predict the category coded as \\(1\\) if \\[\n\\widehat{\\Pr}(Y=1|\\textbf{x}) &gt; 0.5\n\\]",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#sec-class-confusionmatrix",
    "href": "classintro.html#sec-class-confusionmatrix",
    "title": "12  Introduction",
    "section": "12.4 Confusion Matrix",
    "text": "12.4 Confusion Matrix\nThe performance of a classification model is not measured in terms of the mean-squared error. The difference between observed and predicted category is not a meaningful measure of discrepancy. Instead, the performance of classification model is measured by comparing the (relative) frequencies of observed and predicted categories. In the binary case, the performance measures are derived from the confusion matrix of the model.\n\n\nDefinition: Confusion Matrix\n\n\nThe \\(k \\times k\\) confusion matrix in a classification problem is the cross-classification between the observed and the predicted categories. In a problem with two categories, the confusion matrix is a \\(2 \\times 2\\) matrix.\nThe cells of the matrix contain the number of data points that fall into the cross-classification when the classification rule is applied to \\(n\\) observations. If one of the categories is labeled positive and the other is labeled negative, the cells give the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n\n\nThe following table shows the layout of a typical confusion matrix for a 2-category classification problem. A false positive prediction, for example, is to predict a positive (“Yes”) result when the true state (the observed state) was negative. A false negative result is when the decision rule assigns a “No” to an observation with positive state.\n\nConfusion matrix for a classification problem with two classes.\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nNo (Negative)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\nBased on the four cells in the confusion matrix we can calculate several statistics (Table 12.2).\n\n\n\nTable 12.2: Statistics calculated from a 2 x 2 confusion matrix.\n\n\n\n\n\n\n\n\n\n\nStatistic\nCalculation\nNotes\n\n\n\n\nFalse Positive Rate (FPR)\nFP / (FP + TN)\nThe rate of the true negative cases that were predicted to be positive\n\n\nFalse Negative Rate (FNR)\nFN / (TP + FN)\nThe rate of the true positive cases that were predicted to be negative\n\n\nSensitivity\nTP / (TP + FN) = 1 – FNR\nThis is the true positive rate; also called Recall\n\n\nSpecificity\nTN / (FP + TN) = 1— FPR\nThis is the true negative rate\n\n\n\n\n\n\n\nAccuracy\n(TP + TN) / (TP + TN + FP + FN)\nOverall proportion of correct classifications\n\n\nMisclassification rate\n(FP + FN) / (TP + TN + FP + FN)\nOverall proportion of incorrect classifications, 1 – Accuracy\n\n\nPrecision\nTP / (TP + FP)\nRatio of true positives to anything predicted as positive; the accuracy of positive predictions\n\n\n\\(F_1\\)-score\n\\(\\frac{2\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\nHarmonic mean of precision and recall\n\n\nDetection Rate\nTP / (TP + TN + FP + FN)\n\n\n\nNo Information Rate\n\\(\\frac{\\max(TP+FN,FP+TN)}{TP+TN+FP+FN}\\)\nThe proportion of observations in the larger observed class\n\n\n\n\n\n\nThe model accuracy is measured by the ratio of observations that were correctly classified, the sum of the diagonal cells divided by the total number of observations. The misclassification rate is the complement of the accuracy.\nThe sensitivity is the ratio of true positives to what should have been predicted as positive. In machine learning, this measure is called the recall of the model.\nThe specificity is the ratio of true negatives to what should have been predicted as negative. Sensitivity and specificity are not complements of each other; they are calculated with different denominators.\nThe no information rate is an interesting statistic, it represents how well a model would perform on the data if it were to assign all predictions to the more frequent class. That is the performance of a model without input variables.\nJust like regression models for continuous data are often (mis-)judged based on \\(R^2\\), classification models are often (mis-)judged based on their accuracy. That is not without problems. The issue is that the two possible errors, false positives and false negatives, might not be of equal consequence.\nSuppose a model is developed to diagnoses a serious medical condition. A false positive error is to tell a patient that they have the disease, when in fact they have not. A false negative error is the failure to diagnose the disease when the patient is ill. How to weigh the errors depends on their consequences and their prevalence.\nFraudulent credit card transactions are rare, only a fraction of a percent of all transactions are not legitimate. A false positive in a credit card fraud detection algorithm means that a legitimate transaction is flagged as fraudulent with potentially embarrassing consequences for the card holder—a declined transaction when nothing was wrong. A false negative means not detecting a fraudulent transaction—which is rare to begin with. Banks try their hardest to reduce the false positive rate of their fraud detection models, a 2% change in that rate affects many more customers than a 2% change in the false negative rate. If it turns out later that a fraudulent transaction occurred, the bank will still make its customers whole. When a bank reaches out to you to confirm a transaction they are trying to manage potential false positives.\nIt might not matter how accurate a model is unless it achieves a certain sensitivity—the ability to correctly identify positives.\nConsider the data in the Table 12.3, representing 1,000 observations and predictions.\n\n\n\nTable 12.3: Example of a confusion matrix for 1,000 observations.\n\n\n\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\n9\n7\n\n\nNo (Negative)\n26\n958\n\n\n\n\n\n\nThe classification has an accuracy of 96.7%, which seems impressive. Its false positive and false negative rates are very different, however: FPR = 0.0073, FNR = 0.7429. The model is much less likely to predict a “Yes” when the true state is “No”, than it is to predict a “No” when the true state is “Yes”. Whether we can accept a model with such low sensitivity (100 – 74.29) = 25.71% is questionable, despite the high accuracy. An evaluation of this model should consider whether the two errors, false positive and false negative predictions, are of equal importance and consequence.\nIt is also noteworthy that the accuracy of 96.7% is not as impressive if you check the no-information rate of the data. The proportion of observations in the larger observed class is (958 + 7)/1,000 = 0.965. The accuracy of the decision rule is only slightly larger. In other words, if you were to take a naïve (dumb) approach and predict all observations as “No” without looking at the data, that naïve decision rule would have an accuracy of 96.5%. The use of input variables to improve the model has not helped much compared to a dumb model. The accuracy improved by only .2 percentage points.\nTable 12.2 also lists the \\(F_1\\)-score, a combination of precision and recall (sensitivity) that is popular in machine learning. The \\(F_1\\)-score is the harmonic mean of the two basic measures: \\[\n\\begin{align*}\nF_1 &= \\frac{2}{\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Recall}}} \\\\\n    &= \\frac{2\\,\\text{Precision}\\times\\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\\n    &= \\frac{\\frac{TP}{(TP+FP)(TP+FN)}}{\\frac{1}{TP+FP} + \\frac{1}{TP+FN}}\n\\end{align*}\n\\tag{12.2}\\]\n\\(F_1\\) is bounded \\(0 \\le F_1 \\le 1\\); a model with \\(F_1 = 1\\) has perfect precision and recall.\nThe \\(F_1\\)-score is sometimes preferred over accuracy when the data are unbalanced—that is, negative and positive outcomes occur with very different proportions. However, the \\(F_1\\) can be misleading for unbalanced cases since it does not take into account the number of true negatives (there is no TN in Equation 12.2). The \\(F_1\\) score is also not symmetric; if you exchange the roles of positives and negatives, you can get a different \\(F_1\\), the accuracy will remain the same. The \\(F_1\\) score can be a good performance measure for binary classification when you want a model that has both high sensitivity (high recall) and high precision. Such a model combines a low rate of false positives with a low rate of false negatives. If the costs of errors are the same, balancing recall and precision makes sense.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#receiver-operator-characteristic-roc-and-area-under-the-curve-auc",
    "href": "classintro.html#receiver-operator-characteristic-roc-and-area-under-the-curve-auc",
    "title": "12  Introduction",
    "section": "12.5 Receiver Operator Characteristic (ROC) and Area under the Curve (AUC)",
    "text": "12.5 Receiver Operator Characteristic (ROC) and Area under the Curve (AUC)\nSo why do we not build models that have a high accuracy, small false positive rate, and small false negative rate? That seems desirable and we can affect those parameters through the decision rule. To reduce the likelihood of a false positive we could require more evidence before predicting a positive result. Similarly, to reduce the likelihood of false negatives, we could make it more difficult to classify an observation as negative. If \\(c\\) is the threshold in the decision rule of a binary classification, we could treat it as a parameter and evaluate the performance of the model for different rules\n\\[\n\\widehat{\\Pr}(Y=1|\\textbf{x}) &gt; c\n\\] You can easily see that changing \\(c\\) affects both false positives and false negatives, as well as the accuracy. For \\(c=0.5\\) you arrive at the accuracy-optimal Bayes classifier.\nTo see how the choice of cutoff affects the performance of a binary classifier, several graphical devices are in use. The receiver operator characteristic (ROC) is a plot of the true positive rate (TPR, sensitivity, recall) against the false positive rate for different values of \\(c\\). It shows the sensitivity of the classifier as a function of the false positive rate (Figure 12.3).\n\n\n\n\n\n\nFigure 12.3: Schema of ROC curves.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe name receiver operator characteristic stems from the method’s origin, it was developed in World War II for operators of military radar receivers to detect enemy objects in battlefields, specifically after the attack on Pearl Harbor in 1941 to measure the operator’s ability to correctly detect Japanese aircraft in radar signals.\n\n\nA perfect model is one with perfect sensitivity and perfect specificity, it makes no incorrect predictions. It is represented by the point at the (0,1) coordinate in Figure 12.3. The diagonal line represents a random classifier. Suppose you are flipping a coin to predict positive outcomes. The long-run (expected) behavior of this decision rule is captured by the diagonal line. If the coin is fair, you get a point on the line at coordinate (0.5, 0.5). Loaded coins where one side is more likely than the other generate the other points on the 45-degree line.\nAn actual model will be somewhere between the random classifier and the perfect model. The closer the operating curve is to the left and upper margins of the plot, the more performant the model. Actual ROC curves appear as step functions, after calculating FPR and TPR for a set of threshold values \\(c_1, c_2, \\cdots\\).\nThe ROC curve shows how sensitivity and specificity change with the threshold value \\(c\\) and help solve the tradeoff between sensitivity and specificity of the model. The ROC is often summarized by computing AUC, the area under the curve. AUC is a single summary statistic that ranges from 0 to 1, with the uninformative random classifier at AUC = 0.5. The idealistic optimal ROC line that passes through (0, 1) has an AUC of 1. While AUC is a popular summary of a binary classifier, it is not without problems:\n\nReducing the ROC curve to a single number hides the tradeoffs associated with choosing thresholds; that is the point of the ROC in the first place.\nThe integration under the ROC curve incorporates extreme ranges of FPR one would usually not consider in real applications. If a false positive rate of more than 80% is unacceptable, why consider such decision rules in evaluating the performance of a model? Typically we are interested in regions of the ROC rather than the entire curve. In screening tests interest is in the ranges of low FPR, for example. To overcome this issue, partial AUC statistics have been proposed. These restrict FPR, TPR, or both to certain regions of interest.\n\n\n\n\n\n\n\nCaution\n\n\n\nMany performance metrics have 0 as the point of uninformedness, for example, correlation coefficients and \\(R^2\\). For AUC this point is 0.5, the area under the diagonal line in Figure 12.3. If someone presents an AUC of 0.36 for their classification model, remember that it performs worse than a random coin flip.\nI attended a seminar where a statistician presented a new model for classifying brain conditions. Their model slightly beat out different approaches, all of them had AUCs less than 0.5! It is embarrassing if after months or years of expenditures and time spent developing methodology, collecting data, and computing a coin flip would have been more accurate.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#precision-recall-curve",
    "href": "classintro.html#precision-recall-curve",
    "title": "12  Introduction",
    "section": "12.6 Precision-Recall Curve",
    "text": "12.6 Precision-Recall Curve\nAnother graphical display that summarizes the performance of a binary classifier across multiple thresholds is the precision-recall curve (Figure 12.4). Like the ROC, it compares actual classifiers against a no-skill baseline (the horizontal line drawn at the proportion of positives) and an ideal classifier.\n\n\n\n\n\n\nFigure 12.4: Schema of precision-recall curves.\n\n\n\nThe precision-recall curve can also be summarized by computing the area under its curve (AUC-PR). A large area under the precision-recall curve indicates high precision and high recall of the model across the thresholds. Precision-recall curves are preferred over ROC curves when the data are very imbalanced with respect to positive and negative outcomes.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "classintro.html#types-of-classification-models",
    "href": "classintro.html#types-of-classification-models",
    "title": "12  Introduction",
    "section": "12.7 Types of Classification Models",
    "text": "12.7 Types of Classification Models\nThe remaining chapters of this part are dedicated to specific types of classification models: regression-based models, pure classifiers approaches assuming the \\(X\\)s are random, and support vector machines. Figure 12.5 organizes these approaches. However, there are many other methods that can be used in both a regression and a classification context. For example, decision trees, boosting machines, artificial neural networks and more. These techniques are covered in other parts of the material.\n\n\n\n\n\n\nFigure 12.5: Classification mind map.\n\n\n\n\n\n\nFigure 12.5: Classification mind map.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "class_reg.html",
    "href": "class_reg.html",
    "title": "13  Regression Approach to Classification",
    "section": "",
    "text": "13.1 Binary Data\nWith binary data we can apply logistic regression to predict events (positives) and non-events (negatives). Recall from Chapter 10 that a logistic regression model (with logit link) is a generalized linear model where \\[\n\\begin{align*}\n  Y &\\sim \\text{Bernoulli}(\\pi) \\\\\n  \\pi &= \\frac{1}{1+\\exp\\{-\\eta\\}} \\\\\n  \\eta &= \\textbf{x}^\\prime \\boldsymbol{\\beta}\n\\end{align*}\n\\]\nOnce \\(\\boldsymbol{\\beta}\\) is estimated by maximum likelihood, predicted values on the link scale and on the mean scale are obtained as \\[\n\\begin{align*}\n\\widehat{\\eta} &= \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}} \\\\\n\\widehat{\\pi} &= \\frac{1}{1+\\exp\\{-\\widehat{\\eta}\\}}\n\\end{align*}\n\\]\nand an observation is classified as an event if \\(\\widehat{\\pi} &gt; c\\). If the threshold \\(c=0.5\\), the Bayes classifier results.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression Approach to Classification</span>"
    ]
  },
  {
    "objectID": "class_reg.html#sec-class-reg-bin",
    "href": "class_reg.html#sec-class-reg-bin",
    "title": "13  Regression Approach to Classification",
    "section": "",
    "text": "Example: Credit Default–ISLR\n\n\nWe continue the analysis of the credit default data from Chapter 10, but with the goal to classify the observations in the test data set as defaulting/not defaulting on their credit and analyzing the performance of the model based on confusion matrix, ROC and Precision-recall curves.\nRecall that the Default data is part of the ISLR2 library (James et al. 2021), a simulated data set with ten thousand observations. The target variable is default, whether a customer defaulted on their credit card debt. Input variables include a factor that indicates student status, account balance and income information.\nAs previously, we randomly split the data into 9,000 training observations and 1,000 test observations\n\nlibrary(ISLR2)\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\nset.seed(765)\nn &lt;- nrow(Default)\ntestset &lt;- sort(sample(n,n*0.1))\ntest &lt;- Default[testset,]\ntrain &lt;- Default[-testset,]\nnrow(train)\n\n[1] 9000\n\nnrow(test)\n\n[1] 1000\n\n\nNext we fit a logistic regression model to the training data.\n\nlog_reg &lt;- glm(default ~ ., data=train, family=binomial)\nsummary(log_reg)\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.106e+01  5.252e-01 -21.060   &lt;2e-16 ***\nstudentYes  -6.098e-01  2.530e-01  -2.410    0.016 *  \nbalance      5.811e-03  2.491e-04  23.323   &lt;2e-16 ***\nincome       5.038e-06  8.731e-06   0.577    0.564    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2617.1  on 8999  degrees of freedom\nResidual deviance: 1398.0  on 8996  degrees of freedom\nAIC: 1406\n\nNumber of Fisher Scoring iterations: 8\n\n\nStudent status and account balance are significant predictors of credit default, the income seems less important given the other predictors (\\(p\\)-value of 0.5639).\nThe confusion matrix can be computed with the confusionMatrix function in the caret package. The option positive=\"Yes\" identifies the level of the factor considered the “positive” level for the calculation of the statistics. This will not affect the overall confusion matrix but will affect the interpretation of sensitivity, specificity and other statistics. By default, the function uses the first level of a factor as the “positive” result which would be “No” in our case.\nBefore calling confusionMatrix we first calculate the predicted probabilities from the logistic regression model, then calculate the Bayes classifier (\\(c=0.5\\)). mode=\"everything\" requests statistics based on sensitivity and specificity as well as statistics based on precision and recall.\n\nlibrary(caret)\n\npredicted_prob_test &lt;- predict(log_reg, newdata=test, type=\"response\")\nclassify_test &lt;- as.factor(ifelse(predicted_prob_test &gt; 0.5,\"Yes\",\"No\"))\n\nconfusionMatrix(classify_test,test$default, \n                positive=\"Yes\",\n                mode=\"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  958  26\n       Yes   7   9\n                                         \n               Accuracy : 0.967          \n                 95% CI : (0.954, 0.9772)\n    No Information Rate : 0.965          \n    P-Value [Acc &gt; NIR] : 0.407906       \n                                         \n                  Kappa : 0.3384         \n                                         \n Mcnemar's Test P-Value : 0.001728       \n                                         \n            Sensitivity : 0.2571         \n            Specificity : 0.9927         \n         Pos Pred Value : 0.5625         \n         Neg Pred Value : 0.9736         \n              Precision : 0.5625         \n                 Recall : 0.2571         \n                     F1 : 0.3529         \n             Prevalence : 0.0350         \n         Detection Rate : 0.0090         \n   Detection Prevalence : 0.0160         \n      Balanced Accuracy : 0.6249         \n                                         \n       'Positive' Class : Yes            \n                                         \n\n\nThe accuracy of the model appears high with 96.7%, but the no-information rate of 0.965 shows that the inclusion of the three input variables did not improve the model much. If you were to simply classify all observations as “No”, this naïve approach would result in an accuracy of 96.5%, simply because defaults are very rare.\nThe sensitivity of the model is dismal with 0.2571, the false negative rate is high (1-0.2571 = 0.7429). If someone defaults, the model has only a 25.71% chance to detect that. Not surprisingly, the specificity is very high, 958/(958+7) = 0.9927. This is again driven by the high number of non-defaulters and a low false positive rate (FPR = 7 / (958 + 7) = 0.0073).\nWould changing the threshold improve the model by increasing its sensitivity? If we declare a default if the predicted probability exceeds 0.25, we get more positive predictions. How does this affect accuracy and other measures?\n\nclassify_test &lt;- as.factor(ifelse(predicted_prob_test &gt; 0.25,\"Yes\",\"No\"))\n\nconfusionMatrix(classify_test,\n                test$default, \n                positive=\"Yes\",\n                mode=\"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  935  15\n       Yes  30  20\n                                         \n               Accuracy : 0.955          \n                 95% CI : (0.9402, 0.967)\n    No Information Rate : 0.965          \n    P-Value [Acc &gt; NIR] : 0.96033        \n                                         \n                  Kappa : 0.4479         \n                                         \n Mcnemar's Test P-Value : 0.03689        \n                                         \n            Sensitivity : 0.5714         \n            Specificity : 0.9689         \n         Pos Pred Value : 0.4000         \n         Neg Pred Value : 0.9842         \n              Precision : 0.4000         \n                 Recall : 0.5714         \n                     F1 : 0.4706         \n             Prevalence : 0.0350         \n         Detection Rate : 0.0200         \n   Detection Prevalence : 0.0500         \n      Balanced Accuracy : 0.7702         \n                                         \n       'Positive' Class : Yes            \n                                         \n\n\nThe sensitivity of the model increases, as expected, while the specificity does not take much of a hit. Interestingly, the accuracy of this decision rule has now sunk below the no-information rate. Precision has gone down but the \\(F_1\\) score has gone up. This decision rule balances better between precision and recall than the Bayes classifier.\nThe next code blocks uses the ROCR package to compute the ROC curve (Figure 13.1), the AUC, the Precision-recall curve (Figure 13.2) and the AUC-PR. The first step in using ROCR is to call the prediction function to create a prediction object. The performance function of the package is then used to compute statistics and visualizations based on that object.\n\nlibrary(ROCR)\n\npred_prob &lt;- predict(log_reg, newdata=test, type=\"response\")\n\npred_obj &lt;- prediction(pred_prob,test$default,\n                       label.ordering=c(\"No\",\"Yes\"))\n\nperf &lt;- performance(pred_obj,\"sens\",\"fpr\")\nplot(perf,colorize=TRUE)\n\nauc &lt;- performance(pred_obj,\"auc\")\nauc@y.values[[1]]   \n\n[1] 0.9468246\n\n\n\n\n\n\n\n\nFigure 13.1: ROC curve for credit default logistic regression.\n\n\n\n\n\nThe ROC curve looks quite good for this data–model combination. The cutoffs \\(c\\) corresponding to the steps in the plot are shown with different colors. As the cutoff drops below 0.35, the sensitivity of the decision rule increases sharply. The area under the curve of 0.9468 is impressive. However, we know that the data are highly unbalanced, so let’s take a look at the precision-recall plot (Figure 13.2)\n\nperf &lt;- performance(pred_obj,\"prec\",\"rec\")\nplot(perf,colorize=TRUE)\n\naucpr &lt;- performance(pred_obj,\"aucpr\")\naucpr@y.values[[1]] \n\n[1] 0.4987905\n\n\n\n\n\n\n\n\nFigure 13.2: Precision-recall curve for credit default logistic regression.\n\n\n\n\n\nAnother powerful feature of ROCR is the calculation of any measure as a function of the threshold value. The next code block computes and displays accuracy and \\(F_1\\) measure as a function of \\(c\\). Both are maximized for values in the neighborhood of \\(c=0.4\\)\n\nf1 &lt;- performance(pred_obj,\"f\")\nacc &lt;- performance(pred_obj,\"acc\")\n\npar(mfrow=c(1,2))\nplot(f1@x.values[[1]],f1@y.values[[1]],cex=0.4,\n     xlab=\"Threshold c\",\n     ylab=\"F1\",\n     las=1,bty=\"l\")\nmaxval &lt;- which.max(f1@y.values[[1]])\nabline(v=f1@x.values[[1]][maxval],col=\"red\")\n\nplot(acc@x.values[[1]],acc@y.values[[1]],cex=0.4,\n     xlab=\"Threshold c\",\n     ylab=\"Accuracy\",\n     las=1,bty=\"l\")\nmaxval &lt;- which.max(acc@y.values[[1]])\nabline(v=acc@x.values[[1]][maxval],col=\"red\")",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression Approach to Classification</span>"
    ]
  },
  {
    "objectID": "class_reg.html#multinomial-data",
    "href": "class_reg.html#multinomial-data",
    "title": "13  Regression Approach to Classification",
    "section": "13.2 Multinomial Data",
    "text": "13.2 Multinomial Data\nSuppose you draw \\(n\\) observations from a population where labels \\(C_1, \\cdots, C_k\\) occur with probabilities \\(\\pi_1, \\cdots, \\pi_k\\). The probability to observe \\(C_1\\) exactly \\(y_1\\) times, \\(C_2\\) exactly \\(y_2\\) times, and so forth, is \\[\n\\begin{align*}\n\\Pr(\\textbf{Y}= [y_1,\\cdots,y_k]) &= \\frac{n!}{y_1!y_2!\\cdots y_k!} \\pi_1 \\times \\cdots \\pi_k \\\\\n       &= \\frac{n!}{\\prod_{j=1}^ky_j} \\prod_{j=1}^k \\pi_j\n\\end{align*}\n\\] This is the probability mass function of the multinomial distribution. For \\(k=2\\) this reduces to the binomial distribution, \\[\n{n \\choose y} \\pi^y (1-\\pi)^{n-y}\n\\] The mean of the multinomial distribution is \\(n\\) times the vector of the category probabilities \\(\\boldsymbol{\\pi} = [\\pi_1, \\cdots,\\pi_k]^\\prime\\).\nA classification model for multinomial data can thus be based on a multinomial regression model that predicts the category probabilities, and then applies a classification rule in a second step to determine the predicted category. How these regression models are constructed differs depending on whether the categories of the multinomial are unordered (nominal) or ordered.\n\nModeling Nominal Data\nThe models for unordered multinomial data are a direct extension of the logistic regression type models for binary data. The predicted probabilities in the logistic model are \\[\n\\begin{align*}\n\\Pr(Y=1 | \\textbf{x}) &= \\frac{1}{1+\\exp\\{-\\textbf{x}^\\prime\\boldsymbol{\\beta}\\}} \\\\\n\\Pr(Y=0 | \\textbf{x}) &= \\frac{\\exp\\{-\\textbf{x}^\\prime\\boldsymbol{\\beta}\\}}{1+\\exp\\{-\\textbf{x}^\\prime\\boldsymbol{\\beta}\\}}\n\\end{align*}\n\\]\n\nSoftmax function and reference category\nThe generalization to \\(k\\) categories is \\[\n\\Pr(Y=j | \\textbf{x}) = \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}}{\\sum_{l=1}^k \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}}\n\\tag{13.1}\\]\nEach of the \\(k\\) categories has its own parameter vector \\(\\boldsymbol{\\beta}_j\\). But wait, why does the logistic regression model with \\(k=2\\) have only one (\\(k-1\\)) parameter vector? Since the category probabilities sum to 1, one of the probabilities is redundant, it can be calculated from the other probabilities. In multinomial logistic regression this constraint is built into the calculations by setting \\(\\boldsymbol{\\beta}_j = \\textbf{0}\\) for one of the categories. This is called the reference category. Suppose we choose the first category as reference. Then Equation 13.1 becomes\n\\[\n\\begin{align*}\n\\Pr(Y=1 | \\textbf{x}) &= \\frac{1}{1 + \\sum_{l=2}^k \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}} \\\\\n\\Pr(Y=j &gt; 1 | \\textbf{x}) &= \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}}{1 + \\sum_{l=2}^k \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}}\n\\end{align*}\n\\] If the last category is chosen as the reference, Equation 13.1 becomes \\[\n\\begin{align*}\n\\Pr(Y=j &lt; k | \\textbf{x}) &= \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}}{1 + \\sum_{l=1}^{k-1} \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}}\\\\\n\\Pr(Y=k | \\textbf{x}) &= \\frac{1}{1 + \\sum_{l=1}^{k-1} \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWhen using software to model multinomial data, make sure to check how the code handles the reference category. There is no consistency across software packages, the choice is arbitrary. By default, SAS uses the last category as the reference, the nnet:mulitnom function in R uses the first category. The interpretation of the regression coefficients depends on the choice of the reference category. Fortunately, the predicted category probabilities do not depend on the choice of the reference category.\nIn addition, check on how the levels of the target variable are ordered.\n\n\nIn training neural networks, Equation 13.1 is called the softmax activation function (see Chapter 31). Activation functions have two important roles in neural networks: to introduce nonlinearity and to map between input and output of a network layer. The softmax activation function is used in networks that are built to classify data into \\(k\\) categories. Since neural networks are typically overparameterized, that is, they have more parameters than observations, the softmax transformation is applied there without constraining one of the parameter vectors to zero. This parallel development will lead us down the path in Section 33.2.3.3 to express multinomial regression for classification as a special case of a neural network (without hidden layers) and a softmax output activation.\n\n\n\n\n\n\nSoftmax in neural nets\n\n\n\n\n\nUsing the softmax criterion in multinomial logistic regression is the equivalence of the inverse logit link function in logistic regression. It maps from the linear predictor space to the mean of the target. The result is a probability. In neural networks, there are no distributional assumptions, so the softmax transformation should be seen as mapping \\(\\textbf{x}^\\prime\\boldsymbol{\\beta}_1, \\cdots, \\textbf{x}^\\prime\\boldsymbol{\\beta}_k\\) to buckets of the (0,1) interval such that \\[\n\\sum_{j=1}^k \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}}{\\sum_{l=1}^k \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}} = 1\n\\] It is a stretch to think of the terms as probabilities in that context.\n\n\n\n\n\nMultinomial Regression in R\nMultiple packages can fit multinomial regression models in R.\n\nThe nnet::multinom function uses a neural network to estimate the parameters. It uses the first category as the reference.\nThe mlogit::mlogit function fits multinomial models by maximum likelihood and has the ability to include random effects (a multinomial mixed model). It uses a discrete choice formulation which is popular in econometrics.\n\nWe use the nnet::multinom function here and demonstrate model fitting, prediction, and classification for a simple model using the Iris data.\nBefore fitting a multinomial regression model, we split the data into training and test data sets with 1/3 of the observation for testing the model. Stratified sampling via caret::CreateDataPartition is used to make sure that all species are represented in the training and test data sets with appropriate proportions.\n\nset.seed(654)\ntrainset &lt;- caret::createDataPartition(iris$Species, p=2/3, list=FALSE,times=1)\niris_train &lt;- iris[trainset,]\niris_test &lt;- iris[-trainset,]\n\ntable(iris_train$Species)\n\n\n    setosa versicolor  virginica \n        34         34         34 \n\ntable(iris_test$Species)\n\n\n    setosa versicolor  virginica \n        16         16         16 \n\n\nBefore we start modeling, let’s confirm the ordering of the Species factor.\n\nstr(iris_train$Species)\n\n Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThere are three species, the first level of the factor is setosa. This will be the reference level in the multinomial regression—that is, the coefficients for the I. setosa category will be set to zero. To choose a different level as the reference level, you can rearrange the factor with relevel().\nThe following code fits a multinomial regression model with a single input variable (Petal.Length) and target Species. The model tries to predict the iris species from just the measured length of the flower petals.\n\nlibrary(nnet)\nmulti &lt;- multinom(Species ~ Petal.Length, data=iris_train)\n\n# weights:  9 (4 variable)\ninitial  value 112.058453 \niter  10 value 13.522921\niter  20 value 12.661974\niter  30 value 12.554082\niter  40 value 12.551329\niter  50 value 12.550885\niter  60 value 12.550774\niter  70 value 12.550401\niter  80 value 12.550267\niter  90 value 12.549892\nfinal  value 12.549891 \nconverged\n\ns &lt;- summary(multi)\ns\n\nCall:\nmultinom(formula = Species ~ Petal.Length, data = iris_train)\n\nCoefficients:\n           (Intercept) Petal.Length\nversicolor   -25.90912     10.07796\nvirginica    -68.32912     18.87635\n\nStd. Errors:\n           (Intercept) Petal.Length\nversicolor    52.89464     21.27824\nvirginica     54.35859     21.43631\n\nResidual Deviance: 25.09978 \nAIC: 33.09978 \n\n\nThe fit converges after several iterations on the training data. The coefficient estimates for the multinomial regression model are\n\n\\(\\boldsymbol{\\beta}_{\\text{setosa}} = [0, 0]\\),\n\\(\\boldsymbol{\\beta}_{\\text{versicolor}} =\\) [ -25.9091, 10.078], and\n\\(\\boldsymbol{\\beta}_{\\text{virgnica}} =\\) [ -68.3291, 18.8764].\n\nSuppose we wish to predict the predicted probabilities for the 35th observation in the training set, a flower with petal length of 4.7.\n\niris_train[35,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n51            7         3.2          4.7         1.4 versicolor\n\n\nFirst we need to compute the terms \\(\\exp\\{\\beta_0 + \\beta_1 \\text{Petal.Length}\\}\\) for all three species. The sum of those is the term in the denominator of Equation 13.1. The following code computes the linear predictors, the denominator and the category probabilities for that observation\n\n# The input vector for the prediction\nx_data &lt;- c(1,iris_train[35,\"Petal.Length\"])\n\n# The coefficient vectors for the three species \nb_setosa &lt;- rep(0,2)\nb_versicolor &lt;- s$coefficients[1,]\nb_virginica &lt;- s$coefficients[2,]\n\n# the linear predictors for the three species\neta_setosa &lt;- x_data%*%b_setosa\neta_versicolor &lt;- x_data%*%b_versicolor\neta_virginica &lt;- x_data%*%b_virginica\n\n# The denominator for the softmax criterion\ndenom &lt;- sum(exp(eta_setosa) + \n             exp(eta_versicolor) +\n             exp(eta_virginica))\n\n# The category probabilities\npr_setosa &lt;- exp(eta_setosa)/denom\npr_versicolor &lt;- exp(eta_versicolor)/denom\npr_virginica &lt;- exp(eta_virginica)/denom\n\ncat(\"Pr(setosa | 4.7) = \"    , round(pr_setosa,4),\"\\n\")\n\nPr(setosa | 4.7) =  0 \n\ncat(\"Pr(versicolor | 4.7) = \", round(pr_versicolor,4),\"\\n\")\n\nPr(versicolor | 4.7) =  0.7441 \n\ncat(\"Pr(virginica | 4.7) = \" , round(pr_virginica,4),\"\\n\")\n\nPr(virginica | 4.7) =  0.2559 \n\n\nThe calculation is confirmed by checking the fitted value for this observation:\n\nround(s$fitted.values[35,],4)\n\n    setosa versicolor  virginica \n    0.0000     0.7441     0.2559 \n\n\nPredicted values for any value of the input(s) can also be constructed with the predict function:\n\nx_pred &lt;- data.frame(\"Petal.Length\"=4.7)\nround(predict(multi,newdata=x_pred,type=\"probs\"),4)\n\n    setosa versicolor  virginica \n    0.0000     0.7441     0.2559 \n\n\nIf you request a prediction of type \"class\", the function performs classification based on the largest predicted probability.\n\npredict(multi,newdata=x_pred,type=\"class\")\n\n[1] versicolor\nLevels: setosa versicolor virginica\n\n\nIn this case, the predicted and observed category agree.\nHow well does this single-regressor model classify iris species? To answer the question we turn to the confusion matrix for the test data:\n\npred_class &lt;- predict(multi,newdata=iris_test,type=\"class\")\n\ncm &lt;- caret::confusionMatrix(pred_class, iris_test$Species, mode=\"everything\")\ncm\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         14         0\n  virginica       0          2        16\n\nOverall Statistics\n                                          \n               Accuracy : 0.9583          \n                 95% CI : (0.8575, 0.9949)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9375          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8750           1.0000\nSpecificity                 1.0000            1.0000           0.9375\nPos Pred Value              1.0000            1.0000           0.8889\nNeg Pred Value              1.0000            0.9412           1.0000\nPrecision                   1.0000            1.0000           0.8889\nRecall                      1.0000            0.8750           1.0000\nF1                          1.0000            0.9333           0.9412\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.2917           0.3333\nDetection Prevalence        0.3333            0.2917           0.3750\nBalanced Accuracy           1.0000            0.9375           0.9688\n\n\nThe model classifies extremely well based on just one input, Petal.Length. 2 I. versicolor are misclassified as I. virginica, the model has an accuracy of 0.9583.\nIn the binary classification, confusionMatrix returns a single column of confusion statistics. When \\(k &gt; 2\\), a separate column is returned for each of the factor levels, comparing that level to all other levels combined. This is called the one-versus-all approach. For example, classifying I. setosa against the other two species, the model has perfect sensitivity, specificity, and recall. Classifying I. versicolor against the other species, the model has a sensitivity of 0.875.\n\n\n\nModeling Ordinal Data\nOrdered multinomial data has category labels that imply an ordering in a greater-lesser sense. While numeric distances between the categories are not defined, we at least know that one category is more or less than another category. Examples of ordinal data are ratings (5-star scale), assessments of severity (minor, moderate, extreme), indications of sentiment (strongly disagree, disagree, agree, strongly agree), and so forth.\n\nCumulative link models\nA statistical model for ordinal data must preserve the ordering of the data. One method of accomplishing that is to base the model on cumulative probabilities rather than category probabilities. If \\(\\pi_j = \\Pr(Y = j)\\) is the probability that \\(Y\\) takes on the label associated with the \\(j\\)th category, then \\(\\gamma_j = \\Pr(Y \\leq j)\\) is called the cumulative probability of the \\(j\\)th category.\nTo classify an observation based on the cumulative probabilities, we calculate the category probabilities \\[\n\\begin{align*}\n    \\pi_1 &= \\gamma_1 \\\\\n    \\pi_j &= \\gamma_j - \\gamma_{j-1} \\quad \\text{for } 1 &lt; j &lt; k\\\\\n    \\pi_k &= 1 - \\gamma_{k-1}\n\\end{align*}\n\\]\nand then assign the class with the largest category probability.\nThe proportional odds model (POM) is a representative of this type of model. It is also known as a cumulative link model because the link function is applied to the cumulative probabilities. In case of a logit link, the POM is \\[\n\\text{logit}(\\gamma_j) = \\log\\{\\frac{\\gamma_j}{1-\\gamma_j}\\} = \\eta_j = \\textbf{x}^\\prime\\boldsymbol{\\beta}_j\n\\]\nHowever, in contrast to the multinomial regression model for nominal data, the linear predictors in the proportional odds model are more constrained: only the intercepts vary between the categories: \\[\n\\eta_j = \\beta_{0j} + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n\\] The slopes are the same for all categories.\n\n\n\n\n\n\nCaution\n\n\n\nCumulative link models can be motivated in different ways. When formulated based on a latent variable approach —where some unobserved random variable carves out segments of its support—you end up with a linear predictor of the form \\[\n\\eta_j = \\beta_{0j} - \\beta_1 x_1 - \\cdots - \\beta_p x_p\n\\] As always, check the documentation! The MASS::polr function in R uses this formulation. SAS uses a linear predictor with plus signs.\n\n\nIn the logistic or multinomial regression model we could reduce the number of parameters because of the built-in constraint that the categories must sum to 1. A related constraint applies to cumulative link models. The cumulative probability in the last category is known to be 1, \\(\\gamma_k = \\Pr(Y \\leq k) = 1\\). Thus we do not need to estimate a separate intercept for the last category. A proportional odds model with \\(p=4\\) inputs and \\(k=3\\) target categories has \\(p + k-1 = 6\\) parameters.\n\n\nExample: Ordinal Ratings in Completely Randomized Design\n\n\nFor this exercise we use the data in Table 6.13 of Schabenberger and Pierce (2001, p .350). Four treatments (A, B, C, D) were assigned in a completely randomized design with four replications. The state of the replicates of the experimental units was rated as Poor, Average, or Good on four occasions (Table 13.1). For example, on the first measurement occasion all replicates of treatment A were rated in the Poor category. At the second occasion two replicates of treatment A were in Poor condition, two replicates were in Average condition.\n\n\n\nTable 13.1: Observed frequencies for CRD measured on 4 occasions.\n\n\n\n\n\nRating\nA\nB\nC\nD\n\n\n\n\nPoor\n4,2,4,4\n4,3,4,4\n0,0,0,0\n1,0,0,0\n\n\nAverage\n0,2,0,0\n0,1,0,0\n1,0,4,4\n2,2,4,4\n\n\nGood\n0,0,0,0\n0,0,0,0\n3,4,0,0\n1,2,0,0\n\n\n\n\n\n\nThe following code creates the data in data frame format with three columns, factor rating for the target variable, factor tx for the treatment, and a date variable for the measurement occasion.\n\nfreqs &lt;- c(4,2,4,4,4,3,4,4,1,2,1,1,4,4,2,2,4,4,3,4,1,2)\ncats &lt;- c(1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,3,3,3,3)\ntx &lt;- c(1,1,1,1,2,2,2,2,4,1,2,3,3,3,4,4,4,4,3,3,4,4)\ndate &lt;- c(1,2,3,4,1,2,3,4,1,2,2,1,3,4,1,2,3,4,1,2,1,2)\ndf &lt;- data.frame(freqs,resp=cats,Treatment=tx,date)\n\nordinal &lt;- data.frame()\nfor (i in 1:nrow(df)) {\n    for (j in 1:df$freqs[i]) {\n        if (df$resp[i] == 1) {\n            resp_ch = \"Poor\"\n        } else if (df$resp[i] == 2) {\n            resp_ch = \"Average\"\n        } else {\n            resp_ch = \"Good\"\n        }\n        ordinal &lt;- rbind(ordinal,cbind(rating=resp_ch,\n                                   date=df$date[i],\n                                   tx=df$Treatment[i]))\n    }\n}\nordinal$date &lt;- as.numeric(ordinal$date)\nordinal$rating &lt;- factor(ordinal$rating)\nordinal$tx &lt;- factor(ordinal$tx)\n\nWhen working with ordinal target variables, we need to make sure that the factor levels are ordered correctly.\n\nordinal$rating[25:35]\n\n [1] Poor    Poor    Poor    Poor    Poor    Poor    Average Average Average\n[10] Average Average\nLevels: Average Good Poor\n\n\nThe response levels are ordered alphabetically, which is not the order in which the data should be processed. Use the factor() function to tell R how the levels should be arranged.\n\nordinal$rating &lt;- factor(ordinal$rating, \n                         levels = c(\"Poor\", \"Average\", \"Good\"))\nordinal$rating[25:35]\n\n [1] Poor    Poor    Poor    Poor    Poor    Poor    Average Average Average\n[10] Average Average\nLevels: Poor Average Good\n\n\nAnother issue to watch with factors is the factor-level coding. An input factor such as tx with 4 levels will not contribute 4 parameters to a model that also contains an intercept because the \\(\\textbf{X}\\) matrix would be singular. One of the levels is usually dropped as the reference level.\nR chooses the first level. SAS, for example, chooses the last level. You can use the relevel() function to tell R which level to choose as the reference. For example, the next statement makes the level with value 4 the reference level for the tx factor.\n\nordinal$tx &lt;- relevel(ordinal$tx, ref=4)\nordinal$tx[1:20]\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\nLevels: 4 1 2 3\n\n\nThe POM can now be fit with the MASS::polr function. The method= parameter chooses the link function for the cumulative probabilities. The value of the parameter is “logistic” rather than “logit” because polr uses the latent variable formulation for the POM. Assuming that the latent variable follows a logistic distribution leads to a cumulative link model with logit link function. Because of the latent variable genesis of the model, polr constructs a linear predictor of the form \\(\\eta_j = \\beta_{0j} - \\beta_1x_1 - \\cdots - \\beta_p x_p\\).\n\nlibrary(MASS)\npom &lt;- polr(rating ~ tx + date, data=ordinal, \n            method=\"logistic\")\npom_sum &lt;- summary(pom)\npom_coef &lt;- pom_sum$coefficients\npom_sum\n\nCall:\npolr(formula = rating ~ tx + date, data = ordinal, method = \"logistic\")\n\nCoefficients:\n       Value Std. Error t value\ntx1  -5.7070     1.4154  -4.032\ntx2  -6.5191     1.5922  -4.094\ntx3   1.4468     0.8633   1.676\ndate -0.8773     0.3539  -2.479\n\nIntercepts:\n             Value   Std. Error t value\nPoor|Average -5.6152  1.5569    -3.6066\nAverage|Good -0.4868  1.0022    -0.4857\n\nResidual Deviance: 57.12394 \nAIC: 69.12394 \n\n\nThe output of the summary has two sections with parameter estimates. Coefficients lists the slopes of the proportional odds model. These apply the same to all categories. Intercepts displays the intercepts \\(\\beta_{0j}\\) for the cumulative categories. The intercept labeled Poor|Average applies to \\(\\eta_1\\), the intercept labeled Average|Good applies to \\(\\eta_2\\).\nThe coefficients labeled tx1 through tx3 do not measure the effect of the treatments. They measure the difference between the treatment and the reference level. With a t value of more than 4 in absolute value, there is strong evidence that the ratings between A and D and between B and D are significantly different. Also, it appears that the rating distribution, once adjusted for treatments, changes over time; the t value for the date is larger than 2 in absolute value. You can supplement these calculations with \\(p\\)-values to make these statements more statistically precise.\n\nTo calculate the probability to get at most an average rating for treatment 2 at date 3 (observation # 22), the linear predictor and the (cumulative) probability are\n\nlinp_ave_cumul &lt;- pom_coef[6,1] - pom_coef[2,1] - 3*pom_coef[4,1]\nprob_ave_cumul&lt;- 1/(1+exp(-linp_ave_cumul))\nc(linp_ave_cumul,prob_ave_cumul)\n\n[1] 8.6643181 0.9998274\n\n\nSimilarly, for the probability of (at most) a poor rating at date 3 for treatment 2\n\nlinp_poor &lt;- pom_coef[5,1] - pom_coef[2,1] - 3*pom_coef[4,1]\nprob_poor &lt;- 1/(1+exp(-linp_poor))\nc(linp_poor,prob_poor)\n\n[1] 3.5358517 0.9716908\n\n\nThe category probabilities to get an average or good rating are obtained by subtraction:\n\nprob_ave = prob_ave_cumul - prob_poor\nprob_good = 1 - prob_ave_cumul\nc(prob_poor,prob_ave, prob_good)\n\n[1] 0.9716908233 0.0281365693 0.0001726074\n\n\nThis are the category probabilities in contrast to the cumulative probabilities. You can see the category probabilities for all levels of the response variables as\n\nround(pom_sum$fitted.values[22,],4)\n\n   Poor Average    Good \n 0.9717  0.0281  0.0002 \n\n\nObservation 22 would be classified as Poor since this has the largest predicted category probability.\n\nYou can compute predictions directly and more easily with the predict function. The type=\"probs\" option produces the predicted category probabilities, type=\"class\" produces the classification.\n\nnewx &lt;- data.frame(\"tx\"=as.factor(2),date=3)\npredict(pom,newdata=newx,type=\"probs\")\n\n        Poor      Average         Good \n0.9716908233 0.0281365693 0.0001726074 \n\npredict(pom,newdata=newx,type=\"class\")\n\n[1] Poor\nLevels: Poor Average Good\n\n\n\nThe confusion matrix of this model is a 3 x 3 table with correct classifications on the diagonal.\n\npredicted_class &lt;- predict(pom, type=\"class\")\ncaret::confusionMatrix(predicted_class,ordinal$rating)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Poor Average Good\n   Poor      29       3    0\n   Average    1      20    3\n   Good       0       1    7\n\nOverall Statistics\n                                          \n               Accuracy : 0.875           \n                 95% CI : (0.7685, 0.9445)\n    No Information Rate : 0.4688          \n    P-Value [Acc &gt; NIR] : 1.195e-11       \n                                          \n                  Kappa : 0.7935          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Poor Class: Average Class: Good\nSensitivity               0.9667         0.8333      0.7000\nSpecificity               0.9118         0.9000      0.9815\nPos Pred Value            0.9062         0.8333      0.8750\nNeg Pred Value            0.9688         0.9000      0.9464\nPrevalence                0.4688         0.3750      0.1562\nDetection Rate            0.4531         0.3125      0.1094\nDetection Prevalence      0.5000         0.3750      0.1250\nBalanced Accuracy         0.9392         0.8667      0.8407\n\n\nThe accuracy is calculated similar to the 2 x 2 case: the ratio of the sum of the diagonal cells versus the total number: (29+20+7)/64 = 0.875.\nThe proportional odds model has a misclassification rate of \\(MCR = 1-0.875 = 0.125\\).\nThe Kappa statistic is a measure of the strength of the agreement of the predicted and the observed values. The more counts are concentrated on the diagonal, the stronger the agreement and the larger the Kappa statistic. Values of Kappa between 0.4 and 0.6 indicate moderate agreement, 0.6-0.8 substantial agreement, &gt; 0.8 very strong agreement.\nAs in the multinomial case, the confusion statistics are calculated for each category using an one-versus-all approach. For example, the sensitivity of 0.9667 for Poor is calculated by contrasting Poor against the two other classes combined: 29 / (29 + 1) = 0.9667.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary Statistical Models for the Plant and Soil Sciences. CRC Press, Boca Raton.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression Approach to Classification</span>"
    ]
  },
  {
    "objectID": "class_random.html",
    "href": "class_random.html",
    "title": "14  Classification with Random Inputs",
    "section": "",
    "text": "14.1 Introduction\nIn Chapter 13 the approach to classification passed through a regression model. Characteristic of these models is that the mean of a random variable is modeled conditionally on the inputs. In other words, we assume that the \\(X\\)s are fixed. If they are random variables, then we condition the inference on the observed values. This is expressed simply in the conditional probabilities such as \\[\n\\Pr(Y = j | \\textbf{x})\n\\]\nHow would things change if we treat the \\(X\\)s as random variables?",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification with Random Inputs</span>"
    ]
  },
  {
    "objectID": "class_random.html#introduction",
    "href": "class_random.html#introduction",
    "title": "14  Classification with Random Inputs",
    "section": "",
    "text": "Prior and Posteriors\nRecall the multinomial regression model in Section 13.2.1.2, modeling Iris species as a function of petal length. If we think of petal length as a random variable, we can study its distribution for each of the three species (Figure 14.1).\n\n\n\n\n\n\n\n\nFigure 14.1: Estimated densities of petal length by Iris species.\n\n\n\n\n\nThere is an overall probability that a randomly chosen Iris belongs to one of the species. Call this overall probability \\(\\pi_j = \\Pr(Y=j)\\). In a Bayesian context \\(\\pi_j\\) is the prior probability to observe species \\(j\\). For each species \\(j\\), we now have a continuous distribution of the petal lengths:\n\n\\(f_1(x)\\) is the distribution of petal lengths for I. setosa\n\\(f_2(x)\\) is the distribution of petal lengths for I. versicolor\n\\(f_3(x)\\) is the distribution of petal lengths for I. virginica\n\nEstimates of these densities are shown in Figure 14.1. These are conditional probabilities, \\(\\Pr(X | Y=j)\\). Given those estimated densities and the prior probabilities of occurrence of the categories (species), can we classify an observation based on its input values? In other words, can we compute \\(\\Pr(Y=j | X=x)\\) based on \\(\\pi_j = \\Pr(Y=j)\\) and \\(\\Pr(X | Y=j)\\)?\nYes, we can, by applying Bayes’ Theorem: \\[\n\\Pr(Y = j | X=x) = \\frac{\\pi_j f_j(x)}{\\sum_{l=1}^k \\pi_l f_l(x)}\n\\tag{14.1}\\]\n\n\n\n\n\n\nBayes Rule\n\n\n\n\n\nThe generic formulation of Bayes’ rule, for events \\(A\\) and \\(B\\) is \\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)} = \\frac{\\Pr(A)\\Pr(B|A)}{\\Pr(B)}\n\\] The rule allows us to reverse the conditioning from \\(\\Pr(B|A)\\) to \\(\\Pr(A|B)\\).\n\n\n\n\\(\\pi_j\\) is the prior probability that \\(Y\\) belongs to the \\(j\\)th class. In the absence of any additional information, if asked which Iris species you thinks a flower belongs to, you would go with the most frequent species. But now that we have seen data and can compute the distribution of petal lengths by species, we can use that information for a more precise calculation of the likelihood that \\(Y\\) belongs to category \\(j\\); \\(\\Pr(Y=j|X=x)\\) is the posterior probability.\nApplying this rationale to the Iris data, suppose we have not taken any petal length measurement. The species have the same estimate of the prior probability, \\(\\widehat{\\pi}_j = 1/3\\), because we have the same number of observations for each. If someone now tells you that they measured a petal length of 1.8, then given the densities in Figure 14.1, we should assign a high posterior probability that this is indeed a member of I. setosa. Similarly, for a petal length of 4.2, I. setosa is highly unlikely, I. versicolor is quite likely, but I. viriginica cannot be ruled out. If we have to settle on one category, we would side with the species that has the largest posterior probability.\nThe difference between the regression-based approach to classification and the methods in this chapter can be expressed as follows. In regression, we directly estimate \\(\\Pr(Y=j | \\textbf{x})\\). An alternative approach is to assemble this probability based on the prior probabilities \\(\\pi_j\\) and the distribution of the inputs in the \\(k\\) categories.\n\n\nBayes Decision Boundary\nLet’s consider the simple case where the \\(X_j\\) follow a \\(G(\\mu_j,\\sigma^2)\\) distribution. That means, they are Gaussian distributed and differ only in their means, not their variance. Plugging into Equation 14.1 with the normal density function for the \\(f_j(x)\\) yields \\[\n    \\Pr(Y = j | X=x) = \\frac{\\pi_j \\frac{1}{2\\pi\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu_j)^2\\}}\n{f(x)}\n\\] In finding the category index \\(j\\) that maximizes the right hand side, the denominator can be ignored–it is the same for all categories. Maximizing the numerator is equivalent to maximizing its logarithm. This leads to the following classification rule: for a given value of \\(x\\), choose the category for which \\[\n\\delta_j(x) = \\text{ln}(\\pi_j) + x\\frac{\\mu_j}{\\sigma^2} - \\frac{\\mu_j^2}{2\\sigma^2}\n\\] is largest. The Bayes decision boundary between categories \\(j\\) and \\(k\\) is the value of \\(x\\) for which \\(\\delta_j(x) = \\delta_k(x)\\): \\[\n    x = \\frac{\\sigma^2}{\\mu_j - \\mu_k} \\left(\\log(\\pi_k) - \\log(\\pi_j) \\right )+ \\frac{\\mu_j + \\mu_k}{2}\n\\]\nIf \\(\\pi_j = \\pi_k\\) this is simply \\(x = (\\mu_j + \\mu_k)/2\\), the average of the two means.\nFigure 14.2 shows two Gaussian densities, \\(G(-1,1)\\) and \\(G(1.5,1)\\) and their 30:70 mixture. We can calculate the Bayes decision boundary with \\(\\mu_1 = -1\\), \\(\\sigma^2 = 1\\), \\(\\pi_1 = 0.3\\), \\(\\mu_2 = 1.5\\), and \\(\\pi_2 = 0.7\\) as \\[\nx = \\frac{1}{-1 - 1.5}(\\log(0.7) - \\log(0.3)) + \\frac{-1+1.5}{2}=-0.08892\n\\]\n\n\n\n\n\n\n\n\nFigure 14.2: 30:70 mixture of two Gaussian densities.\n\n\n\n\n\nIf you randomly draw an observation and its value is greater than \\(x=-0.08892\\) we conclude it comes from the \\(G(1.5,1)\\) distribution (category 2), if it is smaller we conclude it comes from the \\(G(-1,1)\\) distribution (category 1). If the value is exactly \\(-0.08892\\) then both distributions have equal posterior probabilities.\n\n\nSimulation of Accuracy as Function of Decision Boundary\nWe can validate with a simulation that any other classifier that assigns observations to categories 1 and 2 based on a value other than \\(x=-0.08892\\) will have a larger misclassification rate (smaller accuracy).\nWe simulate 20,000 draws from Gaussian distributions with mean \\(-1\\) and \\(1.5\\) and variance \\(1\\). We also draw 20,000 times from a uniform(0,1) distribution which helps to create the mixture. If the uniform variable is less than 0.3 we choose an observation from the normal distribution with mean \\(-1\\), otherwise we choose from the distribution with mean \\(1.5\\). The decision boundary of \\(x=-0.08892\\) should have the greatest accuracy among all possible cutoffs if the experiment is repeated over and over.\n\nset.seed(345)\n\nx1 &lt;- rnorm(20000,-1,1)\nx2 &lt;- rnorm(20000,1.5,1)\nu &lt;- runif(20000,0,1)\n\nx1obs &lt;- subset(x1, u &lt; 0.3)\nx2obs &lt;- subset(x2, u &gt;= 0.3)\ndata &lt;- c(x1obs,x2obs)\n\nFigure 14.3 shows the accuracy of the classifier as the cutoffs are varied. At the cutoff value \\(-0.08992\\) the highest accuracy is achieved (acc=0.908).\n\n\n\n\n\n\n\n\nFigure 14.3: Accuracy of classifier as a function of the decision boundary. The Bayes decision rule is \\(-0.08892\\).\n\n\n\n\n\nTwo important methods that apply these ideas are discriminant analysis and the naïve Bayes classifier. They differ in assumptions about the distribution of the inputs.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification with Random Inputs</span>"
    ]
  },
  {
    "objectID": "class_random.html#sec-class-dca",
    "href": "class_random.html#sec-class-dca",
    "title": "14  Classification with Random Inputs",
    "section": "14.2 Discriminant Analysis",
    "text": "14.2 Discriminant Analysis\nClassification discriminant analysis (DA) was designed for the case of a qualitative target variable and one or more quantitative input variables. The typical assumption is that the input variables have a joint Gaussian distribution. When the distribution within each group is not known—or known not to be Gaussian—nonparametric forms of discriminant analysis that rely on kernel methods or nearest neighbor analysis are available.\nAt the heart of DA is the discriminant function, a decision rule used to classify observations into categories. The discriminant function is based on a generalized measure of distance between data points to the groups. This is a generalized distance because it takes into account the variances and covariances among the inputs. The posterior probabilities are functions of discriminant scores which are in turn based on those distances. An observation is assigned to the category which has the largest posterior probability.\nDiscriminant analysis is also a dimension reduction techniques, similar to principal component analysis (PCA, see Chapter 23). It computes one or more linear combinations between the quantitative variables that break down the variability between classes. In PCA, linear combinations are constructed that explain proportions of the total variability in the data.\n\nLinear and Quadratic DA\nSuppose we have \\(p\\) input variables \\(X_1, \\cdots, X_p\\). Discriminant analysis assumes that the joint distribution of the inputs is multivariate Gaussian with mean vector \\(\\boldsymbol{\\mu}= [\\mu_1, \\cdots, \\mu_p]^\\prime\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\). A different multivariate Gaussian distribution applies to each of the \\(k\\) categories.\nIn linear discriminant analysis (LDA), the category-specific Gaussian distributions differ only in their mean vectors and not their covariance matrices. In quadratic discriminant analysis, the distributions differ in their mean vectors and their covariance matrices.\n\nLDA: \\(f_j(\\textbf{x}) \\sim G(\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma})\\)\nQDA: \\(f_j(\\textbf{x}) \\sim G(\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j)\\)\n\nWe focus on LDA now.\nNote that there are two “dimensions” to the problem. We have \\(p\\) inputs and \\(k\\) categories. For each of the \\(j=1,\\cdots,k\\) categories we are dealing with a \\(p\\)-dimensional Gaussian distribution. With LDA, the p.m.f. for the \\(j\\)th distribution is\n\\[\n    f_j(\\textbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left\\{ -\\frac{1}{2} (\\textbf{x}-\\boldsymbol{\\mu}_j)^\\prime\\boldsymbol{\\Sigma}^{-1}(\\textbf{x}-\\boldsymbol{\\mu}_j)\\right \\}\n\\] where \\(\\boldsymbol{\\mu}_j = [\\mu_{j1},\\cdots,\\mu_{jp}]\\). With \\(p=3\\), the covariance matrix \\(\\boldsymbol{\\Sigma}\\) has this form: \\[\n\\boldsymbol{\\Sigma}= \\left [ \\begin{array}{ccc} \\ \\text{Var}[X_1] & \\text{Cov}[X_1,X_2] & \\text{Cov}[X_1,X_3] \\\\\n    \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] & \\text{Cov}[X_2,X_3] \\\\\n    \\text{Cov}[X_3,X_1] & \\text{Cov}[X_3,X_2] & \\text{Var}[X_3]\n    \\end{array} \\right ]\n\\]\nThe Bayes classifier assigns an observation with features \\(\\textbf{x}_0\\) to the category \\(j\\) for which \\[\n    \\delta_j(\\textbf{x}_0) = \\textbf{x}_0^\\prime\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_j - \\frac{1}{2}\\boldsymbol{\\mu}_j^\\prime \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_j + \\text{ln}(\\pi_j)\n\\] is largest.\nThe term linear describes this form of discriminant analysis because \\(\\delta_j(\\textbf{x}_0)\\) is a linear function of the Xs—that is, the LDA decision rule depends on \\(\\textbf{x}\\) only through a linear combination of its elements.\n\nThe assumption of a common variance across all categories helps to keep the number of parameters of the linear discriminant analysis in check. With \\(k\\) categories and \\(p\\) inputs the LDA has \\(kp + p(p+1)/2\\) parameters. This quickly escalates; with \\(k=3\\) and \\(p=50\\) we are dealing with 1,425 parameters.\nIn QDA the number of parameters is much larger because the covariance matrices are also category-specific. This leads to \\(kp + kp(p+1)/2\\) parameters. With \\(k=3\\) and \\(p=50\\) this results in 3,975 parameters.\nChoosing between LDA and QDA is a classical bias-variance tradeoff. LDA is less flexible than QDA because of the equal-covariance-matrix constraint. If that constraint does not hold, LDA has high bias. QDA, on the other hand has high variability, in particular for small data sets where it is difficult to estimate variances and covariances well. If you have very large data sets or the common variance assumption is clearly wrong, choose QDA. Otherwise, give LDA a try.\nDiscriminant analysis has another glaring issue. The assumption of a joint multivariate Gaussian distribution of the inputs is questionable for many data sets. The assumption is a stretch for binary inputs such as those from encoding factors. Many continuous input variables are far from symmetric. The Gaussian assumption is for mathematical convenience, not because it reflects reality.\n\n\nDiscriminant Analysis in R\nWe can perform LDA with the MASS::lda function and QDA with the MASS:qda function in R.\n\n\nExample: LDA for Iris Data\n\n\nIn Section 13.2.1.2, the Species variable of the Iris data was classified using a multinomial regression with one input, petal length. The model had an accuracy of 0.9583 on the test data set, only two I. versicolor were misclassified as I. virginica.\nHow does a linear discriminant analysis compare to the multinomial regression analysis?\nFirst, we create the same train:test data split as in Section 13.2.1.2.\n\nset.seed(654)\ntrainset &lt;- caret::createDataPartition(iris$Species, p=2/3, list=FALSE,times=1)\niris_train &lt;- iris[trainset,]\niris_test &lt;- iris[-trainset,]\n\ntable(iris_train$Species)\n\n\n    setosa versicolor  virginica \n        34         34         34 \n\ntable(iris_test$Species)\n\n\n    setosa versicolor  virginica \n        16         16         16 \n\n\nThe following statements compute the linear discriminant analysis and graph the results.\n\nlibrary(MASS)\niris_lda &lt;- lda(Species ~ Petal.Length, data=iris_train)\niris_lda\n\nCall:\nlda(Species ~ Petal.Length, data = iris_train)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Petal.Length\nsetosa         1.461765\nversicolor     4.247059\nvirginica      5.514706\n\nCoefficients of linear discriminants:\n                  LD1\nPetal.Length 2.258558\n\nplot(iris_lda)\n\n\n\n\n\n\n\n\nBy default, lda uses the class proportions as the prior probabilities. Since each species has the same number of observations, the prior probabilities are the same. The next table in the output shows the average value of the input(s) in the classes. Only one discriminant function was computed, it explains all the between-class variability.\nThe graph shows the separation of the groups. With a single discriminant function, the graph consists of a series of histograms, one for each category. There is no overlap in the discriminant scores between I. setosa and the other species. I. versicolor and I. virginica show some overlap.\nThe confusion matrix tells us how well the LDA classifier does. It is identical to the confusion matrix in the multinomial regression model.\n\npred &lt;- predict(iris_lda,newdata=iris_test)\ncaret::confusionMatrix(pred$class,iris_test$Species, mode=\"everything\")\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         14         0\n  virginica       0          2        16\n\nOverall Statistics\n                                          \n               Accuracy : 0.9583          \n                 95% CI : (0.8575, 0.9949)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9375          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8750           1.0000\nSpecificity                 1.0000            1.0000           0.9375\nPos Pred Value              1.0000            1.0000           0.8889\nNeg Pred Value              1.0000            0.9412           1.0000\nPrecision                   1.0000            1.0000           0.8889\nRecall                      1.0000            0.8750           1.0000\nF1                          1.0000            0.9333           0.9412\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.2917           0.3333\nDetection Prevalence        0.3333            0.2917           0.3750\nBalanced Accuracy           1.0000            0.9375           0.9688\n\n\n\n\n\nNext we return to the credit card default analysis from Section 13.1.\n\n\nExample: Credit Default–ISLR (Cont’d)\n\n\nRecall that the Default data is part of the ISLR2 library (James et al. 2021), a simulated data set with ten thousand observations. The target variable is default, whether a customer defaulted on their credit card debt. Input variables include a factor that indicates student status, account balance and income information.\nAs before, we randomly split the data into 9,000 training observations and 1,000 test observations.\n\nlibrary(ISLR2)\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\nset.seed(765)\nn &lt;- nrow(Default)\ntestset &lt;- sort(sample(n,n*0.1))\ntest &lt;- Default[testset,]\ntrain &lt;- Default[-testset,]\n\nLinear discriminant analysis, LDA with equal priors, and quadratic discriminant analysis follow:\n\nlda &lt;- lda(default ~ income + balance + student, data=train)\nlda\n\nCall:\nlda(default ~ income + balance + student, data = train)\n\nPrior probabilities of groups:\n        No        Yes \n0.96688889 0.03311111 \n\nGroup means:\n      income   balance studentYes\nNo  33549.41  804.4596  0.2926913\nYes 32318.24 1749.8002  0.3859060\n\nCoefficients of linear discriminants:\n                     LD1\nincome      4.854450e-06\nbalance     2.248323e-03\nstudentYes -1.354917e-01\n\nlda2 &lt;- lda(default ~ income + balance + student, data=train,\n            prior=c(0.5,0.5))\nlda2\n\nCall:\nlda(default ~ income + balance + student, data = train, prior = c(0.5, \n    0.5))\n\nPrior probabilities of groups:\n No Yes \n0.5 0.5 \n\nGroup means:\n      income   balance studentYes\nNo  33549.41  804.4596  0.2926913\nYes 32318.24 1749.8002  0.3859060\n\nCoefficients of linear discriminants:\n                     LD1\nincome      4.854450e-06\nbalance     2.248323e-03\nstudentYes -1.354917e-01\n\nqda &lt;- qda(default ~ income + balance + student, data=train)\nqda\n\nCall:\nqda(default ~ income + balance + student, data = train)\n\nPrior probabilities of groups:\n        No        Yes \n0.96688889 0.03311111 \n\nGroup means:\n      income   balance studentYes\nNo  33549.41  804.4596  0.2926913\nYes 32318.24 1749.8002  0.3859060\n\n\nDoes the choice of linear versus quadratic DA and the choice of prior affect the classification performance?\n\nlda_pred_test &lt;- predict(lda, newdata=test)\nlda_c &lt;- caret::confusionMatrix(lda_pred_test$class,test$default, \n                                positive=\"Yes\")\nlda_c$table\n\n          Reference\nPrediction  No Yes\n       No  961  29\n       Yes   4   6\n\nc(lda_c$overall[1],lda_c$byClass[1:2])\n\n   Accuracy Sensitivity Specificity \n  0.9670000   0.1714286   0.9958549 \n\nlda2_pred_test &lt;- predict(lda2, newdata=test)\nlda2_c &lt;- caret::confusionMatrix(lda2_pred_test$class,test$default, \n                                 positive=\"Yes\")\nlda2_c$table\n\n          Reference\nPrediction  No Yes\n       No  815   4\n       Yes 150  31\n\nc(lda2_c$overall[1],lda2_c$byClass[1:2])\n\n   Accuracy Sensitivity Specificity \n  0.8460000   0.8857143   0.8445596 \n\nqda_pred_test &lt;- predict(qda, newdata=test)\nqda_c &lt;- caret::confusionMatrix(qda_pred_test$class,test$default, \n                                positive=\"Yes\")\nqda_c$table\n\n          Reference\nPrediction  No Yes\n       No  959  27\n       Yes   6   8\n\nc(qda_c$overall[1],qda_c$byClass[1:2])\n\n   Accuracy Sensitivity Specificity \n  0.9670000   0.2285714   0.9937824 \n\n\nThe accuracy, sensitivity, and specificity of the default LDA and QDA are comparable, the QDA is slightly more sensitive, but neither model satisfies with respect to that metric.\nAssuming equal priors, (lda2 analysis) means that prior to accounting for the inputs we assume that defaults and no defaults are equally likely. Under that (unreasonable) assumption the accuracy of the LDA classifier is lower but the sensitivity is much improved. A comparison of the classified values in the test set shows that 171 observations predicted as no defaults were classified as defaults under the equal prior assumption.\n\ntable(lda_pred_test$class, lda2_pred_test$class)\n\n     \n       No Yes\n  No  819 171\n  Yes   0  10",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification with Random Inputs</span>"
    ]
  },
  {
    "objectID": "class_random.html#sec-class-nb",
    "href": "class_random.html#sec-class-nb",
    "title": "14  Classification with Random Inputs",
    "section": "14.3 Naïve Bayes Classifier",
    "text": "14.3 Naïve Bayes Classifier\n\nConditional Independence\nThe parametric discriminant analysis relies on a joint (multivariate) Gaussian distributions of the inputs. That does not accommodate qualitative (categorical) features. After converting those to 0–1 encoded binary columns, a Gaussian assumption is not reasonable. The naïve Bayes classifier (NBC) handles inputs of different types more gracefully—you can combine quantitative and qualitative inputs in the same classifier.\nHowever, it does so by replacing the multivariate Gaussian assumption with another strong assumption: within a class \\(j\\) the \\(f_j(\\textbf{x})\\) across the \\(p\\) inputs are independent (given the response \\(Y\\)).\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(f_j(\\textbf{x})\\) introduced earlier also has a conditional interpretation (\\(f_j(\\textbf{x}|y)\\)). The conditioning on \\(Y\\) does not add anything new here.\n\n\nThe new wrinkle is that under the independence assumption the \\(p\\)-dimensional joint distribution of \\([X_1,\\cdots,X_p]\\) for category \\(j\\) factors into the product of the marginal distributions \\[\nf_j(\\textbf{x}|y) = f_{1}(x_1|Y=j) \\times f_{2}(x_2 | Y=j) \\times \\cdots \\times f_{p}(x_p | Y=j)\n\\] Or, using our shorthand notation \\[f_j(\\textbf{x}|y) = f_{j1}(x_1) \\times f_{j2}(x_2) \\times \\cdots \\times f_{jp}(x_p)\n\\]\nDoes this mean we are assuming \\(X_1\\) and \\(X_2\\) are independent? Not quite. It means that if you know that \\(Y\\) is in category \\(j\\), then knowing \\(X_1\\) has no bearing on our beliefs about \\(X_2\\). In other words, if we know that \\(Y\\) belongs to category \\(j\\), then \\(f_j(x_2|y, x_1) = f_j(x_2|y)\\): the distribution of \\(X_2\\) is not affected by the values of \\(X_1\\).\nThis is a very strong assumption. What did we gain by making it?\n\nConsiderable simplification of the estimation procedure.\nWe no longer have to model the \\(p\\)-dimensional joint distribution among the \\(X\\)s\nFeatures with different properties can be accommodated\n\n\\(X_1\\) might be continuous\n\\(X_2\\) can be a count\n\\(X_3\\) can be qualitative\n\nThe distributions of the inputs can be estimated separately using different methods (kernel density, histogram, frequency distribution)\nThe distributions do not have to be the same across categories. \\(f_{j1}(x_1)\\), the distribution of \\(X_1\\) in category \\(j\\) can be from a different distributional family than \\(f_{k1}(x_1)\\), the distribution of \\(X_1\\) in category \\(k\\).\n\n\n\nThe Classifier\nThe naïve Bayes classifier chooses as the predicted category of an observation the label for which \\[\n\\Pr(Y= j | \\textbf{x}) = \\frac{\\pi_j \\times f_{j1}(x_1) \\times \\cdots \\times f_{jp}(x_p)}{f(\\textbf{x})}\n\\] is largest. Since the denominator does not depend on \\(Y\\), this rule is equivalent to finding the category label for which \\[\n\\pi_j \\times f_{j1}(x_1) \\times \\cdots \\times f_{jp}(x_p)\n\\] is largest.\nThe prior probabilities \\(\\pi_l\\) are estimated by the proportion of training observations in category \\(j\\). The densities \\(f_{jm}(x_m)\\) are estimated as follows:\n\nQuantitative \\(X_m\\):\n\nassume \\(N(\\mu_{jm},\\sigma^2_{jm})\\)\nor using a histogram of observations of the \\(m\\)th predictor in each class\nor using a kernel density estimator, essentially a smoothed histogram\n\nQualitative \\(X_m\\):\n\nproportion of training observations for the \\(m\\)th predictor in each class\n\n\n\n\nNaïve Bayes in R\nNaive Bayes is implemented in R in various packages. The naiveBayes() function in the e1071 library uses syntax similar to that of lda and assumes that the numeric variables are Gaussian distributed. For factors (qualitative variables) it computes the conditional discrete distribution in each target category.\n\n\nExample: Credit Default–ISLR (Cont’d)\n\n\n\nlibrary(e1071)\nnb &lt;- naiveBayes(default ~ income + balance + student, data=train)\nnb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n        No        Yes \n0.96688889 0.03311111 \n\nConditional probabilities:\n     income\nY         [,1]     [,2]\n  No  33549.41 13323.37\n  Yes 32318.24 13728.98\n\n     balance\nY          [,1]     [,2]\n  No   804.4596 454.9880\n  Yes 1749.8002 344.5415\n\n     student\nY            No       Yes\n  No  0.7073087 0.2926913\n  Yes 0.6140940 0.3859060\n\n\nThe information for the conditional probability distribution can be easily verified. For numeric variables (income, balance) the tables display the sample mean and sample standard deviation for each category. These are then used to calculate the \\(f_j(x)\\) as Gaussian densities.\n\nlibrary(dplyr)\n# Prior probabilities\nprior_freq &lt;- table(train$default)\nprior_freq/sum(prior_freq)\n\n\n        No        Yes \n0.96688889 0.03311111 \n\n# mean and standard deviation of income by default\ntrain %&gt;% group_by(default) %&gt;%\n    summarize(income_mn=mean(income),\n              income_sd=sd(income))\n\n# A tibble: 2 × 3\n  default income_mn income_sd\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 No         33549.    13323.\n2 Yes        32318.    13729.\n\n# mean and standard deviation of balance by default\ntrain %&gt;% group_by(default) %&gt;%\n    summarize(balance_mn=mean(balance),\n              balance_sd=sd(balance))\n\n# A tibble: 2 × 3\n  default balance_mn balance_sd\n  &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 No            804.       455.\n2 Yes          1750.       345.\n\n# Proportions of students by default\nt &lt;- table(train$default,train$student)\nt\n\n     \n        No  Yes\n  No  6155 2547\n  Yes  183  115\n\nt[1,]/sum(t[1,])\n\n       No       Yes \n0.7073087 0.2926913 \n\nt[2,]/sum(t[2,])\n\n      No      Yes \n0.614094 0.385906 \n\n\nFor example, the distribution of income given default=No is modeled as \\[\nf_{\\text{No}}(\\text{income}) = f(\\text{income} | \\text{default = No}) = G(33549.41,13323.37^2)\n\\] and \\[\nf_{\\text{Yes}}(\\text{income}) = f(\\text{income} | \\text{default = Yes}) = G(32318.24,13728.98^2)\n\\] The confusion matrix for the Naive Bayes estimates is identical to the confusion matrix of the logistic regression model:\n\nnb_pred_test &lt;- predict(nb,newdata=test)\nnb_c &lt;- caret::confusionMatrix(nb_pred_test,test$default, positive=\"Yes\")\nnb_c$table\n\n          Reference\nPrediction  No Yes\n       No  958  26\n       Yes   7   9\n\nc(nb_c$overall[1],nb_c$byClass[1:2])\n\n   Accuracy Sensitivity Specificity \n  0.9670000   0.2571429   0.9927461 \n\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Classification with Random Inputs</span>"
    ]
  },
  {
    "objectID": "supportvectors.html",
    "href": "supportvectors.html",
    "title": "15  Support Vectors",
    "section": "",
    "text": "15.1 Introduction\nClassification methods based on support vectors were introduced in the 1990s and made a big splash. Their approach to solving classification problems was novel and intuitive, they showed excellent performance, and worked well in high dimensions even when \\(p \\gg n\\). The approach was refreshing and distinctly different from traditional approaches such as logistic regression or linear discriminant analysis.\nThe performance of support vector methods, especially support vector machines (SVM, see Section 15.4 below) was so impressive that the technique became an off-the-shelf standard classifier for many data scientists. Much so as (extreme) gradient boosting has become an off-the-shelf default method for many data scientists solving regression problems. Support vector machines have since been extended to regression problems; we consider them primarily for classification here.\nHow impressive is the performance of support vector methods? Consider the following example, classifying bananas into “Good” and “Bad” quality based on fruit attributes such as size, weight, sweetness, ripeness, etc. You can find the data set for this analysis on kaggle.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "supportvectors.html#introduction",
    "href": "supportvectors.html#introduction",
    "title": "15  Support Vectors",
    "section": "",
    "text": "Example\n\n\nExample: Banana Quality\n\n\n4,000 observations are in the training set, 4,000 observations are in the test data set. Figure 15.1 displays histograms for the fruit attributes in the training data\n\nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nban_train &lt;- dbGetQuery(con, \"SELECT * FROM banana_train\")\nban_test &lt;- dbGetQuery(con, \"SELECT * FROM banana_test\")\n\nban_train$Quality &lt;- factor(ban_train$Quality)\nban_test$Quality &lt;- factor(ban_test$Quality)\ndbDisconnect(con)\nstr(ban_train)\n\n'data.frame':   4000 obs. of  8 variables:\n $ Size       : num  1.71 3.7 -3.89 -3.05 1.97 ...\n $ Weight     : num  -0.0369 1.1188 1.3261 -0.588 -1.3787 ...\n $ Sweetness  : num  -4.4634 -3.0434 0.0461 -1.6347 -3.1428 ...\n $ Softness   : num  -1.51 0.029 2.259 1.049 -3.246 ...\n $ HarvestTime: num  4.564 -0.971 0.507 -0.21 -0.612 ...\n $ Ripeness   : num  -0.0417 -1.47 0.7371 -1.8188 1.8153 ...\n $ Acidity    : num  4.327 -0.588 2.583 3.957 -0.5 ...\n $ Quality    : Factor w/ 2 levels \"Bad\",\"Good\": 2 2 1 1 1 2 2 1 1 2 ...\n\n\n\nlattice::histogram(~ Size + Weight + Sweetness + Softness + HarvestTime\n                   + Ripeness + Acidity , \n                   cex=0.5,\n                   as.table=TRUE,\n                   par.strip.text=list(cex=0.75),\n                   xlab=\"\",\n                   data=ban_train)\n\n\n\n\n\n\n\nFigure 15.1: Histogram of attributes in banana training data set.\n\n\n\n\n\nThe following statements use the e1071::svm function to train a support vector machine on the training data and compute the confusion matrix for the test data. Because the data are already scaled (see Figure 15.1), we set scale=FALSE. Otherwise, this is the default SVM analysis.\n\nlibrary(caret)\nlibrary(e1071)\nset.seed(176)\n\nban.svm &lt;- svm(Quality ~ . ,\n               data=ban_train, \n               scale=FALSE)\n\npred &lt;- predict(ban.svm,newdata=ban_test)\n\ncm &lt;- confusionMatrix(pred,ban_test$Quality,positive=\"Good\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Bad Good\n      Bad  1957   44\n      Good   37 1962\n                                          \n               Accuracy : 0.9798          \n                 95% CI : (0.9749, 0.9839)\n    No Information Rate : 0.5015          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9595          \n                                          \n Mcnemar's Test P-Value : 0.505           \n                                          \n            Sensitivity : 0.9781          \n            Specificity : 0.9814          \n         Pos Pred Value : 0.9815          \n         Neg Pred Value : 0.9780          \n             Prevalence : 0.5015          \n         Detection Rate : 0.4905          \n   Detection Prevalence : 0.4998          \n      Balanced Accuracy : 0.9798          \n                                          \n       'Positive' Class : Good            \n                                          \n\n\nThis out-of-the-box analysis achieves an impressive 97.975% accuracy on the test data set. With tuning of hyperparameters (see Section 15.4), this accuracy can be increased even further. The sensitivity and specificity of the model are also impressive. For comparisons, an out-of-the-box logistic regression achieves 88.1% accuracy, a finely grown and pruned decision tree achieves 89.5% accuracy.\n\n\n\n\nWhat is a Support Vector\nThe name of this family of methods stems from classification rules that are based on a subset of the observations, these observations are called the support vectors of the classifier. A support vector classifier with 10 support vectors, for example, uses the data from 10 observations to construct the classification rule, even if the training data contains 1 million observations. The number of support vectors is not a parameter you set in advance, however. Rather, you specify the constraints imposed on the classifier through other hyperparameters—such as the cost of a misclassification and the kernel function—and the number of support vectors is the result of training the classifier.\n\n\nTypes of Support Vector Methods\nChapter 9 of James et al. (2021) provides a great introduction into the support-vector based methods and we follow their general flow here. To arrive at support vector machines it is helpful to develop them from simpler methods, the maximal margin classifier and the support vector classifier (Figure 15.2).\n\n\n\n\n\n\nFigure 15.2: The family of support vector-based classifiers.\n\n\n\nThe maximal margin classifier (MMC) is a rather simple classifier for separable cases. Suppose you are dealing with a two-category problem. This is said to be separable if you can find a classification rule that assigns every observation in the training data without error to one of the two classes.\nAlthough intuitive, MMC is not a strong contender, most data are not completely separable. The support vector classifier (SVC) improves over the MMC by permitting a gray area of misclassification. It is OK to have a certain misclassification rate on the training data, if it improves generalizability to new observations. Finally, the support vector machine (SVM) generalizes the linear decision rule of the SVC to nonlinear decision boundaries by introducing kernel functions. The SVC emerges as a special case of SVM with a linear kernel function.\nIn the remainder of the chapter we discuss the three margin classifiers for binary data. As is customary with classification models not based on regression techniques, the target variable is coded \\(Y \\in \\{-1,1\\}\\), rather than \\(Y \\in \\{0,1\\}\\). A side effect of this target encoding is that the classification rule uses only the sign of the predicted value. If \\(Y = -1\\) encodes label \\(A\\) and \\(Y=1\\) encodes label \\(B\\), then we predict \\(A\\) if \\(\\widehat{y} &lt; 0\\) and predict \\(B\\) if \\(\\widehat{y}&gt; 0\\).",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "supportvectors.html#sec-mmc",
    "href": "supportvectors.html#sec-mmc",
    "title": "15  Support Vectors",
    "section": "15.2 Maximal Margin Classifier (MMC)",
    "text": "15.2 Maximal Margin Classifier (MMC)\nSuppose we have \\(p\\) input variables \\(X_1,\\cdots,X_p\\). For each observation in the training data we have \\([y_i, x_{i1}, \\cdots, x_{ip}]^\\prime = [y_i, \\textbf{x}_i]\\). Given a new data point for the inputs, \\(\\textbf{x}_0 = [x_{01}, \\cdots, x_{0p}]^\\prime\\), should we assign it to the \\(-1\\) or the \\(1\\) category?\nThe MMC asks to find a hyperplane \\[\nh(\\textbf{x}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n\\] that satisfies \\[\ny_i h(\\textbf{x}_i) = y_i \\, \\left ( \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} \\right )&gt; 0\\text{, } \\forall i=1, \\cdots, n\n\\] Recall that \\(y_i \\in \\{-1,1\\}\\). This condition states that all training observations fall either on the right or the left side of \\(h(\\textbf{x})\\). There are no misclassifications which would occur if \\(h(\\textbf{x}_i) &lt; 0\\) and \\(y = 1\\) or if \\(h(\\textbf{x}_i) &gt; 0\\) and \\(y = -1\\).\nOnce we have found \\(h(\\textbf{x})\\), we classify \\(\\textbf{x}_0\\) by computing \\(h(\\textbf{x}_0)\\) \\[\n    \\widehat{y} = \\left \\{ \\begin{array}{r l}\n    1 & h(\\textbf{x}_0) &gt; 0 \\\\ -1 & h(\\textbf{x}_0) &lt; 0\n    \\end{array} \\right .\n\\]\nFigure 15.3 shows the basic setup for a maximal margin classifier with two inputs. The hyperplane \\(h(\\textbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) in two dimensions is a line. The data are separable in the sense that we can find coefficients \\(\\beta_0, \\beta_1, \\beta_2\\) so that the line carves through the figure in such a way that all points with \\(y=1\\) fall on one side of the line (\\(h(\\textbf{x}) &gt; 0\\)), and all points with \\(y= -1\\) fall on the other side of the line (\\(h(\\textbf{x}) &lt; 0\\)). For all points that lie exactly on the hyperplane, we have \\(h(\\textbf{x}) = 0\\).\n\n\n\n\n\n\nFigure 15.3: Maximal margin classifier for \\(p=2\\).\n\n\n\nThis problem can have more than one solution. For example, Figure 15.4 depicts three hyperplanes \\(h_1(\\textbf{x})\\), \\(h_2(\\textbf{x})\\), and \\(h_3(\\textbf{x})\\) that satisfy the no-misclassification requirement. How should we define an optimal solution?\n\n\n\n\n\n\nFigure 15.4: Three possible solutions for the MMC problem.\n\n\n\nThe margin of a classifier is the shortest perpendicular distance between the training observations and the hyperplane. The best solution to the problem of classifying separable observations with a linear decision boundary is to find the hyperplane that has the largest margin (the largest minimum distance between training obs and hyperplane). The rationale is that a large margin on the training data will lead to a high accuracy on the test data, if the test data are a random subset of the training data.\nThe observations that have the shortest distance to the hyperplane—the observations that define the margin—are called the support vectors (Figure 15.5) because they support the hyperplane as the decision boundary of the classification problem. What happens if we were to move an observation in Figure 15.5, with the caveat that the observation has to stay on the same side of the hyperplane? If the observation is not a support vector, then the hyperplane is not affected, unless the observation is moved closer to the plane than any of the support vectors. Changing the support vectors will change the hyperplane.\n\n\n\n\n\n\nFigure 15.5: Example of an MMC that depends on two support vectors.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "supportvectors.html#sec-svc",
    "href": "supportvectors.html#sec-svc",
    "title": "15  Support Vectors",
    "section": "15.3 Support Vector Classifier",
    "text": "15.3 Support Vector Classifier\nThe MMC is based on a simple and intuitive idea and a solution can be found with a straightforward optimization algorithm. However, we will not consider this classifier much further, its utility lies in introducing the idea of a margin-based classifier, a linear decision boundary, and the concept of the support vector. It is not that useful in practical applications because most problems are not separable. How could we separate the points in Figure 15.6 without error by drawing a line through the plot?\n\n\n\n\n\n\n\n\nFigure 15.6: A non-separable case in two dimensions\n\n\n\n\n\nA perfect classification on the training data is also not generally desirable. It will likely lead to an overfit model that does not generalize well to new observations.\nThe support vector classifier (SVC) extends the MMC by finding a hyperplane that provides better classification of most observations and is more robust to individual data points (Figure 15.7).\nThe margin of the SVC is called a soft margin: some observations are allowed to be on the wrong side of the margin and some observations are allowed to be on the wrong side of the hyperplane (misclassified).\n\n\n\n\n\n\nFigure 15.7: Support vector classifier in two dimensions.\n\n\n\nThe support vectors of the SVC are the observations that lie on the margin or lie on the wrong side of the margin for their category. As with the MMC, the decision boundary depends only on these observations. Moving any of the other observations without crossing into the margin or across the decision boundary has no effect on the optimal hyperplane.\nHow do we find the optimal hyperplane for the support vector classifier? One approach is to give ourselves a budget of observations that violate the margin and make sure that the chosen hyperplane does not exceed the budget. Alternatively, we can specify the cost associated with a margin violation and find the hyperplane that minimizes the overall expense.\nUsing the cost approach, the optimization problem that finds the SVC can be expressed as follows: \\[\n\\begin{align*}\n\\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\beta},\\boldsymbol{\\epsilon}} &\\, \\frac12 \\boldsymbol{\\beta}^\\prime\\boldsymbol{\\beta}+ C\\sum_{i=1}^n \\epsilon_i \\\\\n\\text{subject to } &\\, y_i(\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_p x_p) \\ge 1-\\epsilon_i \\\\\n\\epsilon_i &\\ge 0, \\quad i=1,\\cdots,n\n\\end{align*}\n\\]\nThe \\(\\epsilon_i\\) are called the slack variables, each observation is associated with one. If \\(\\epsilon_i = 0\\), the \\(i\\)th observation does not violate the margin. If \\(0 &lt; \\epsilon_i \\le 1\\), the observation violates the margin but is not misclassified. Finally, misclassified observations in the training data have \\(\\epsilon_i &gt; 1\\).\n\\(C\\) is the hyperparameter representing the cost of a margin violation and is usually determined by some form of cross-validation. Choosing \\(C\\) is a typical bias-variance tradeoff.\n\n\\(C\\) small: the cost of a margin violation is low, encouraging more violations. The resulting classifier will have a wider margin and more support vectors. This results in classifiers that are more stable, with lower variance but potentially higher bias.\n\\(C\\) large: we have a low tolerance for margin violations. This encourages a small margin and few support vectors, resulting in classifiers that are closely fit to the training data, have low bias but potentially high variance.\n\nFigure 15.8 displays the decision rule for a support vector classifier fit to the data in Figure 15.6. For the particular choice of \\(C\\), the classification boundary depends on 14 support vectors (data points). These are shown in the plot as “x” symbols. Observations on which the hyperplane does not depend are shown as “o” symbols. You can see that some of the support vectors are on the correct side of the decision boundary and others are not. Given the chosen value of \\(C\\), this is the best one can achieve using a linear decision boundary for this non-separable problem.\n\n\n\n\n\n\n\n\nFigure 15.8: Support vector classifier trained on data in Figure 15.6\n\n\n\n\n\nWe have not shown the R code that leads to the classification plot in Figure 15.8 because the SVC turns out to be a special case of the next family of models, the support vector machines.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "supportvectors.html#sec-svm",
    "href": "supportvectors.html#sec-svm",
    "title": "15  Support Vectors",
    "section": "15.4 Support Vector Machines (SVM)",
    "text": "15.4 Support Vector Machines (SVM)\nThe support vector classifier is a marked improvement over the maximal margin classifier in that it can handle non-separable cases, allows cross-validation of the cost (or budget) hyperparameter, and is robust to observations that are far away from the decision boundary.\nThe shortcoming of the SVC is its linear decision boundary. If such a boundary applies in the case of two inputs, it means we can segment the \\(x_1\\)—\\(x_2\\) plane with a line as the classification rule. Consider the data in Figure 15.9. No linear decision boundary would slice the data to produce a good classification.\n\n\n\n\n\n\nFigure 15.9: Data for which a linear decision boundary does not work well.\n\n\n\nJust like linear regression models do not perform well if the relationship between target and inputs is nonlinear, a classifier with a linear decision rule will not classify well if the decision boundary should be nonlinear. One approach to introduce nonlinearity (in the inputs) in the regression context is to add transformations of the variables, for example, using polynomials. Similarly, we could consider revising the decision boundary in a margin classifier to include additional terms: \\[\nh(\\textbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_2^2\n\\] But where do we stop? What is the best order of the polynomial terms, and what about the transformations \\(\\log(X)\\) or \\(\\sqrt{X}\\) or \\(1/x\\) and what about interaction terms? This could get quickly out of hand.\nSupport vector machines rely on what is known as the kernel trick to essentially increase the number of features, introduce nonlinearity, but without increasing numerical complexity. The principal idea is to apply a nonlinear transformation to the \\(X\\)-space such that a linear decision boundary is reasonable in the transformed space. In other words, find a nonlinear decision boundary in \\(X\\)-space as a linear decision boundary in the transformed space.\n\nThe Kernel Trick\nFigure 15.10 from Zhang (2018) evokes how a problem that appears not linearly separable in a lower-dimensional space becomes linearly separable in a higher-dimensional space.\n\n\n\n\n\n\nFigure 15.10: A nonseparable problem in 2-D becomes linearly separable in 3-D. From Zhang (2018).\n\n\n\nIn 2-dimensional space, the linear decision boundary is a line, in 3-dimensional space the linear decision boundary (the hyperplane) is an actual plane.\nThe kernel trick allows us to make computations in higher-dimensional, nonlinear spaces based on only inner products of the coordinates in the original space. Wait, what?\n\nInner products\nA kernel \\(K(\\textbf{x}_j,\\textbf{x}_j)\\) is a generalization of the inner product \\[\n    \\langle \\textbf{x}_i, \\textbf{x}_j \\rangle = \\sum_{k=1}^p x_{ik} x_{jk}\n\\] Instead of applying the inner product to the original coordinates \\(\\textbf{x}_i\\) and \\(\\textbf{x}_j\\), we first transform the coordinates. \\[\nK(\\textbf{x}_i,\\textbf{x}_j) = \\langle g(\\textbf{x}_i), g(\\textbf{x}_j) \\rangle\n\\] Let’s look at an example. Suppose \\(\\textbf{x}\\) is two-dimensional, \\(\\textbf{x}= [x_1,x_2]\\) and we define the transformations \\[\n\\begin{align*}\n    g_1(\\textbf{x}) &= 1 \\\\\n    g_2(\\textbf{x}) &= \\sqrt{2}x_1 \\\\\n    g_3(\\textbf{x}) &= \\sqrt{2}x_2\\\\\n    g_4(\\textbf{x}) &= x_1^2 \\\\\n    g_5(\\textbf{x}) &= x_2^2 \\\\\n    g_6(\\textbf{x}) &= \\sqrt{2}x_1 x_2\n\\end{align*}\n\\tag{15.1}\\] The inner product of \\(g(\\textbf{x})\\) with itself is then \\[\ng(\\textbf{x})^\\prime g(\\textbf{x}) = 1 + 2 x_1^2 + 2 x_2^2 + x_1^4 + x_2^4 + 2x_1^2 x_2^2\n\\] The inner product in the transformed space includes higher-dimensional terms, without increasing the number of inputs in the calculation. That is neat, but have we gained that much? How are we to choose the transformations \\(g_1(\\textbf{x}),\\cdots, g_m(\\textbf{x})\\) in a meaningful way?\n\n\nKernel functions\nThis is where the following result comes to the rescue. We do not need to specify the functions \\(g_1(\\textbf{x}),\\cdots, g_m(\\textbf{x})\\) explicitly. Instead, we can start by choosing the kernel function \\(K(\\textbf{x}_i,\\textbf{x}_j)\\). Any valid kernel implies some transformation: \\[\nK(\\textbf{x}_i, \\textbf{x}_j) = \\langle \\phi(\\textbf{x}_i), \\phi(\\textbf{x}_j) \\rangle\n\\] for some function \\(\\phi()\\).\nPopular kernel functions in support vector machines include the following:\n\nLinear \\[\nK(\\textbf{x}_i, \\textbf{x}_j) = \\langle \\textbf{x}_i, \\textbf{x}_j \\rangle = \\textbf{x}_i^\\prime \\textbf{x}_j\n\\]\nPolynomial of degree \\(d\\) \\[K(\\textbf{x}_i, \\textbf{x}_j) = \\left (c_0 + \\gamma \\langle \\textbf{x}_i, \\textbf{x}_j \\rangle\\right )^d\n\\]\nRadial basis \\[\nK(\\textbf{x}_i, \\textbf{x}_j) = \\exp \\left \\{-\\gamma ||\\textbf{x}_i - \\textbf{x}_j||^2 \\right \\} \\quad \\gamma &gt; 0\n\\]\nSigmoid \\[K(\\textbf{x}_i, \\textbf{x}_j) = \\tanh\\left(\\gamma \\langle \\textbf{x}_i, \\textbf{x}_j \\rangle + c_0 \\right )\n\\]\n\nThe quantities \\(\\gamma\\) and \\(c_0\\) in these expressions are parameters of the kernel functions. They are treated as hyperparameters in training the models and often determined by a form of cross-validation.\nTo see the connection between choosing a kernel function and its implied transformations \\(g_1(\\textbf{x}), \\cdots, g_m(\\textbf{x})\\), consider the polynomial kernel of second degree (\\(d=2\\)), \\(c_0 = 1\\) and \\(\\gamma = 1\\). Then \\[\n\\begin{align*}\n    (1+\\langle [x_1, x_2], [x_1, x_2]\\rangle)^2 &= \\left ( 1 + x_1 x_1 + x_2 x_2\\right )^2 \\\\\n    &= \\left ( 1 + x_1^2 + x_2^2 \\right )^2 \\\\\n    &= 1 + 2x_1^2 + 2x_2^2 + x_1^4 + x_2^4 + 2x_1^2 x_2^2 \\\\\n\\end{align*}\n\\]\nThis kernel function implies the functions \\(g_1(\\textbf{x}), \\cdots, g_6(\\textbf{x})\\) in Equation 15.1.\nThe last piece of the puzzle is establishing the connection between kernels, SVMs and SVCs. It turns out that the decision rule in the support vector classifier (SVC) from the previous section can be written as a function of inner products, rather than as a linear equation in the \\(x\\)s: \\[\nf(\\textbf{x}_0) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle \\textbf{x}_0, \\textbf{x}_i \\rangle\n\\] for some coefficients \\(\\beta_0\\) and \\(\\alpha_1, \\cdots, \\alpha_n\\). However, this simplifies because \\(\\alpha_i = 0\\), unless the \\(i\\)th data point is a support vector. If \\(\\mathcal{S}\\) denotes the set of support vectors, then the decision boundary in the SVC can be written as \\[\nf(\\textbf{x}_0) = \\beta_0 + \\sum_{\\mathcal{S}} \\alpha_i \\langle \\textbf{x}_0, \\textbf{x}_i \\rangle\n\\] and for the support vector machine (SVM) it can be written as \\[\nf(\\textbf{x}_0) = \\beta_0 + \\sum_{\\mathcal{S}} \\alpha_i K(\\textbf{x}_0, \\textbf{x}_i)\n\\]\nThe support vector classifier is a special case of a support vector machine where the kernel function is the linear kernel. Now you know why we delayed training SVC in R until here. Training an SVC is a special case of training an SVM—just choose the linear kernel.\n\n\n\nPros and Cons of SVM\nWhat are some of the advantages and disadvantages of support vector machines? Among the advantages are:\n\nOften perform extremely well\nVery flexible through use of kernel functions\nHandle high-dimensional problems.\nHave been applied successfully in many fields\nCan be extended from classification to regression\n\nHowever, a support vector machine can be\n\ndifficult to explain.\nsensitive to noise in data and to outliers.\nslow, especially when cross-validating hyperparameters.\ndifficult to extend to more than two categories.\n\nUnlike boosting methods (Chapter 20) that have a built-in mechanism to judge the importance of an input in training the model, support vector machines have no such metric. The equivalent of variable importance is obtained by applying model-agnostic tools such as permutation-based variable importance. Although these can be applied to any type of model, they come with their own set of problems (computational intensity, high variability).\nThat leaves us in a bit of a quandary when it comes to support vector machines. While they frequently perform extremely well, they are also difficult to communicate. “It just works” is rarely a sufficient answer.\nExtending regression-based classification methods from \\(K=2\\) to \\(K &gt; 2\\) in Chapter 13 meant extending logistic regression to multinomial logistic regression. The key result was to replace the inverse logit link with the softmax function. With support vectors, extending to more than two categories is not possible. A decision boundary that separates two classes is fundamentally different from one that separates three classes. Two approaches are taken to handle \\(K &gt; 2\\):\n\none-versus-all: \\(K\\) separate SVMs are trained, each classifies one category against all others combined. For example, to classify between bananas, apples, oranges, and tomatoes, we fit 4 SMVS.\n\nbananas versus non-bananas\napples versus non-apples\noranges versus non-oranges\ntomatoes versus non-tomatoes\n\none-versus-one: A separate SMV is trained for each pair of categories:\n\nbananas versus apples\nbananas versus oranges\nbananas versus tomatoes\napples versus oranges\nand so forth\n\n\nWe encountered the one-versus-all approach in previous chapters on classification when caret::confusionMatrix computes the confusion matrix statistics for \\(K &gt; 2\\).",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "supportvectors.html#svm-in-r",
    "href": "supportvectors.html#svm-in-r",
    "title": "15  Support Vectors",
    "section": "15.5 SVM in R",
    "text": "15.5 SVM in R\nSupport vector machines (and classifiers) can be trained in R with the svm function in the e1071 package. We return to the banana quality data of Section 15.1.1.\n\n\nExample: Banana Quality (Cont’d)\n\n\nRecall that the training data set contains the quality (“Good”, “Bad”) of 4,000 bananas along with seven fruit attributes (see Figure 15.1). The training data set also contains 4,000 observations.\nWe start by training a support vector classifier using all fruit attributes by setting the kernel= option to \"linear. The only hyperparameter of this model is the cost of constraints violation. We set cost to 10, well, because. You have to start somewhere. The scale= option is set to FALSE because the inputs have already been scaled, see Figure 15.1.\nThe following code trains the SVC and computes the confusion matrix for this setting.\n\nlibrary(e1071)\nlibrary(caret)\n\nban.svc &lt;- svm(Quality ~ . ,\n              data=ban_train, \n              kernel=\"linear\",\n              cost=10,\n              scale=FALSE)\nban.svc\n\n\nCall:\nsvm(formula = Quality ~ ., data = ban_train, kernel = \"linear\", cost = 10, \n    scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  1228\n\n\nThe trained model has 1228 support vectors which seems like a lot. Almost one out of every three observation is needed to compute the decision boundary. The proportion should not be that high. How well does the model classify the bananas in the test data set?\n\npred &lt;- predict(ban.svc,newdata=ban_test)\n\nban.svc.cm &lt;- confusionMatrix(pred,ban_test$Quality,positive=\"Good\")\nban.svc.cm$overall\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n     0.8815000      0.7629915      0.8710795      0.8913595      0.5015000 \nAccuracyPValue  McnemarPValue \n     0.0000000      0.4348990 \n\n\nWith 1228 support vectors, the model achieves “only” 88.15 % accuracy on the test data. We know from Section 15.1.1 that we can do much better.\nSo let’s see if choosing a different cost value improves the model? To this end we use the tune function in the e1071 library. tune performs a grid search over the ranges of hyperparameters and computes mean squared error in regression problems or classification error in classification problems. We set a seed value for the random number generator because tune by default performs 10-fold cross-validation.\n\nset.seed(5432)\ntune.out &lt;- tune(svm,Quality ~ .,\n                 data=ban_train, \n                 kernel = \"linear\",\n                 scale  = FALSE,\n                 ranges = list(cost = c(0.1 , 1, 10, 100))\n                 )\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  0.1\n\n- best performance: 0.126 \n\n- Detailed performance results:\n   cost   error dispersion\n1   0.1 0.12600 0.01776388\n2   1.0 0.12675 0.01814486\n3  10.0 0.12700 0.01755151\n4 100.0 0.12675 0.01814486\n\ntune.out$best.model\n\n\nCall:\nbest.tune(METHOD = svm, train.x = Quality ~ ., data = ban_train, \n    ranges = list(cost = c(0.1, 1, 10, 100)), kernel = \"linear\", \n    scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.1 \n\nNumber of Support Vectors:  1243\n\npred &lt;- predict(tune.out$best.model,newdata=ban_test)\n\nban.svm.cm &lt;- confusionMatrix(pred,ban_test$Quality,positive=\"Good\")\nban.svm.cm$overall\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n     0.8812500      0.7624932      0.8708206      0.8911188      0.5015000 \nAccuracyPValue  McnemarPValue \n     0.0000000      0.5819094 \n\n\nThe tuned model does not perform any better than the first model with cost=10. This is an indication that the linear decision boundary implied by the linear kernel probably does not work well for these data. If we would believe that a linear kernel is correct, then we should continue to tune the model because the selected value falls on the edge of the supplied grid. The classification error might continue to fall below 0.126 for smaller values of cost.\nBut my belief in the appropriateness of the linear kernel is shaken and I move on to a support vector machine by modifying the kernel function from linear to a radial basis kernel:\n\ntune.out &lt;- tune(svm,Quality ~ . ,\n                 data   = ban_train, \n                 kernel = \"radial\",\n                 ranges = list(cost = c(0.1 , 1, 10, 100))\n                 )\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n   10\n\n- best performance: 0.018 \n\n- Detailed performance results:\n   cost  error  dispersion\n1   0.1 0.0280 0.007799573\n2   1.0 0.0225 0.008740074\n3  10.0 0.0180 0.006433420\n4 100.0 0.0240 0.006892024\n\ntune.out$best.model\n\n\nCall:\nbest.tune(METHOD = svm, train.x = Quality ~ ., data = ban_train, \n    ranges = list(cost = c(0.1, 1, 10, 100)), kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  10 \n\nNumber of Support Vectors:  358\n\n\nOnly one of the hyperparameters is cross-validated, the cost parameter. We could also include the gamma parameter in the ranges= list, I leave it up to you to further improve on this SVC.\nThe best choice of cost from the four values supplied is cost = 10 with a misclassification rate of 0.018 on the training data.\n\npred &lt;- predict(tune.out$best.model,newdata=ban_test)\n\nban.svm.cm &lt;- confusionMatrix(pred,ban_test$Quality,positive=\"Good\")\n\nban.svm.cm$overall\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n     0.9810000      0.9620002      0.9762757      0.9850017      0.5015000 \nAccuracyPValue  McnemarPValue \n     0.0000000      0.3018984 \n\n\nOn the test data, the accuracy of the model is only slightly smaller: 98.1 %.\n\n\n\n\n\nFigure 15.2: The family of support vector-based classifiers.\nFigure 15.3: Maximal margin classifier for \\(p=2\\).\nFigure 15.4: Three possible solutions for the MMC problem.\nFigure 15.5: Example of an MMC that depends on two support vectors.\nFigure 15.7: Support vector classifier in two dimensions.\nFigure 15.9: Data for which a linear decision boundary does not work well.\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nZhang, Grace. 2018. “What Is the Kernel Trick? Why Is It Important?” Medium. https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d.",
    "crumbs": [
      "Part III. Supervised Learning II: Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Support Vectors</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html",
    "href": "decisiontrees.html",
    "title": "16  Regression and Classification Trees",
    "section": "",
    "text": "16.1 Introduction\nWe decided to give decision trees its own part in the material for a number of reasons.\nThe trees described in this chapter were introduced by Breiman et al. (1984), and are known as CART (Classification and Regression Trees). Sutton (2005) gives an excellent overview of these type of trees, and discusses bagging and boosting as well.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regression and Classification Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#introduction",
    "href": "decisiontrees.html#introduction",
    "title": "16  Regression and Classification Trees",
    "section": "",
    "text": "Decision trees can be used in regression and classification settings which could place them in both Parts II and III. Trees are different from the regression approach to classification where one fits a regression-type model first to predict category probabilities and then uses them to classify the result. With decision trees, the tree is set up from the beginning as either a regression tree or a classification tree. The remaining logic of building the tree is mostly the same, apart from differences in loss functions and how predicted values are determined.\nDecision trees are among the statistical learning techniques that are easiest to explain to a non-technical audience. Trees are intrinsically interpretable, a tree diagram reveals completely how an input record is processed to arrive at a prediction or classification. This makes decision trees very popular.\nDecision trees can be applied to many problems—since they can predict and classify—but their performance is often lacking. Trees have a number of shortcomings such as high variability, sensitivity to small changes in the data, a tendency to overfit. You are trading these against advantages such as simplicity, interpretability, and the ability to handle missing values (to some extent).\nMany popular statistical learning techniques, which can perform extremely well, are based on decision trees and have been developed to overcome their shortcomings. Bagged trees, random forests, gradient boosted trees, and extreme gradient boosting are examples of such methods. To understand and appreciate these popular learning approaches, we need a basic understanding of how decision trees work. Trees are a prime example of a weak learner, a learning method with low bias and high variance that benefits from averaging results—growing many trees instead of a single one.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regression and Classification Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#partitioning-x-space",
    "href": "decisiontrees.html#partitioning-x-space",
    "title": "16  Regression and Classification Trees",
    "section": "16.2 Partitioning \\(X\\)-space",
    "text": "16.2 Partitioning \\(X\\)-space\nThe principle behind building decision trees can be visualized if we consider a problem with a target variable \\(Y\\) and two input variables \\(X_1\\) and \\(X_2\\). \\(Y\\) can be a continuous target or a classification target variable. Trees are frequently introduced with this scenario because we can visualize how observed values \\(\\textbf{x}= [x_1,x_2]\\) translate into a prediction or classification.\nThe basic idea is to carve up the \\([X_1,X_2]\\) space into \\(J\\) non-overlapping regions \\(R_1,\\cdots, R_J\\) based on training data. Each region is associated with a representative value, also determined from the training data. A new observation will fall into one of the regions, and the representative value for the region is returned as the predicted value.\n\n\n\n\n\n\nFigure 16.1: Partitioning a two-dimensional input space through binary splitting.\n\n\n\nAn example is shown in Figure 16.1. \\(J=5\\) regions are generated by first splitting \\(X_2 \\le a_1\\) followed by \\(X_1 \\le b_2\\). This creates regions \\(R_1\\) and \\(R_2\\). The next split is again applied to \\(X_2\\), now separating data points with \\(X_2 \\le a_3\\). This creates region \\(R_3\\). The final split is splitting points where \\(X_2 \\ge a_3\\) according to \\(X_1 \\le b_4\\). This creates regions \\(R_4\\) and \\(R_5\\).\nA display such as Figure 16.2 works for trees with \\(p \\le 2\\) inputs. In higher dimensions we cannot visualize the regions. The typical display for a decision tree is, well, a tree. However, the tree display resembles more a root system with the trunk at the top and the branches of the tree below; or an upside down tree with the leaves at the bottom. The decision tree implied by the partitioning in Figure 16.1 is shown in Figure 16.2.\n\n\n\n\n\n\nFigure 16.2: Decision tree implied by the partitioning in Figure 16.1.\n\n\n\nThe segments connecting levels of the tree are called branches, the points where the tree splits are the nodes (or internal nodes) and the end points of the tree are called the terminal nodes or the leaves. Note that having split on a variable higher up in the tree (closer to the trunk) does not preclude reusing that variable at a later point. This is evident on the right side of the tree where the tree splits on \\(X_2\\) a second time.\nThe particular form of creating the regions is called binary splitting, it leads to rectangular regions rather than irregularly shaped regions or polygons. Binary splitting is not the optimal partitioning strategy, but it leads to trees that are easy to describe and it is easier to find the regions.\nNote that the coordinate system in Figure 16.1 displays \\(X_1\\) and \\(X_2\\) and not the target variable \\(Y\\). Where and when does the target come into the picture? The values of the target variable in the training set are used to determine\n\nwhich variable to split on at any split point of the tree\nthe location of the splits \\((a_1, b_2, \\cdots)\\)\nthe number of regions \\(M\\).\nthe representative (predicted) value in each region\n\nIn the case of a regression tree, \\(Y\\) is continuous and the representative value in region \\(R_j\\) is typically the average of the training data points that lie in \\(R_j\\). In the case of a classification tree, a majority vote is taken, that is, the most frequently observed category in \\(R_j\\) is returned as the classified value.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regression and Classification Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#training-a-tree",
    "href": "decisiontrees.html#training-a-tree",
    "title": "16  Regression and Classification Trees",
    "section": "16.3 Training a Tree",
    "text": "16.3 Training a Tree\nTraining a tree involves multiple steps. First, we have to decide on a criterion by which to measure the quality of a tree. For a regression tree we could consider a least-squares type residual sum of squares (RSS) criterion such as \\[\n\\sum_{j=1}^J \\sum_{i \\in R_j} \\left(y_i - \\overline{y}_{R_j}\\right)^2\n\\]\nwhere \\(\\overline{y}_{R_j}\\) is the average of the observations in the \\(j\\)^th region.\nOnce we have the criterion (the loss function) for the tree, the next step is to grow the tree until some termination criterion is met. For example, we might continue splitting nodes until all leaves contain no more than \\(m\\) observations. At this point we have found a tree, but not necessarily a great one. The question is how complex a tree one should build? Too many nodes and leaves with few observations and the tree will overfit and not generalize well to new observations. Too few nodes and the tree will not take advantage of the information in the training data.\nSince the predictive accuracy of trees tends to improve initially as nodes are added, the typical process is to grow the tree until a termination criterion is met and in a subsequent step to remove leaves and branches to create a smaller tree that generalizes well. Another reason for initially growing a deep tree is that a best possible split may not reduce the RSS by much, but a subsequent split can produce an appreciable decrease in RSS. By stopping too early one would miss good later splits. The process of removing nodes and branches from a deep tree is known as pruning the tree and involves a test data set or cross-validation.\n\nGrowing\n\nRegression tree\nTo grow a regression tree that minimizes such a criterion we restrict the regions \\(R_j\\) to those obtained by recursive binary splitting as pointed out above. We first find the combination of input variable \\(X_j\\) and split point \\(c\\) such that separating the observations according to \\(X_j \\le c\\) leads to the greatest reduction in RSS. Formally, \\[\n\\min_{j,c} \\left \\{ \\sum_{i:x_i \\in R_1(j,c)} (y_i - \\overline{y}_{R_1(j,c)})^2 +\n                    \\sum_{i:x_i \\in R_2(j,c)} (y_i - \\overline{y}_{R_2(j,c)})^2\n            \\right \\}\n\\] At each step of the tree-building process, we choose the pair \\((j,c)\\) independent of any future splits further below in the tree. In other words, we only consider the best possible split for this step, and do not look ahead. This strategy is known as a greedy algorithm.\nBuilding the tree continues by repeating the above optimization, at each point considering the previously constructed regions instead of the entire input space. Splitting the tree continues until a termination criterion is met: for example as long as all leaves contain at least \\(m\\) observations.\n\n\nClassification tree\nA squared-error loss function works for a regression tree but not for classification problems. Besides the loss function, we also need to compute a representative value to classify observations. Let \\(\\widehat{p}_{jk}\\) denote the proportion of training observations in region \\(R_j\\) that fall into class \\(k\\); this is simply \\[\n\\widehat{p}_{jk} = \\frac{1}{n_j}\\sum_{x_i \\in R_j} I(y_i = k)\n\\] The representative value returned from region \\(R_j\\) is the category with the largest proportion, \\(\\mathop{\\mathrm{arg\\,max}}_k \\widehat{p}_{jk}\\). In other words, we take a majority vote and classify a leaf according to its most frequent category.\nBased on the \\(\\widehat{p}_{jk}\\) we can define several criteria:\n\nMisclassification error \\[ E = 1 - \\max_k(\\widehat{p}_{jk})\\]\nGini Index \\[ G = \\sum_{k=1}^K \\widehat{p}_{jk}(1-\\widehat{p}_{jk})\\]\nCross-Entropy \\[ D = - \\sum_{k=1}^{K} \\widehat{p}_{jk} \\text{log}(\\widehat{p}_{jk})\\]\nDeviance \\[ -2 \\sum_{j=1}^{J} \\sum_{k=1}^K n_{jk} \\text{log}(\\widehat{p}_{jk})\\]\n\nThe misclassification error might seem as the natural criterion for classification problems, but the Gini index and the cross-entropy criterion are preferred. The misclassification error is not sensitive enough to changes in the node probabilities and is not differentiable everywhere, making it less suitable for numerical optimization.\nThe Gini index and cross-entropy are sensitive to node purity, leading to trees having nodes where one class dominates. To see this, consider a Bernoulli(\\(\\pi\\)) distribution. The variance of the distribution is \\(\\pi(1-\\pi)\\) and is smallest when \\(\\pi \\rightarrow 0\\) or \\(\\pi \\rightarrow 1\\); it is highest for \\(\\pi= 0.5\\). Nodes where \\(\\widehat{p}_{jk}\\) is near 0 or 1 have high purity. We’d rather base classification on terminal nodes where the decision is “decisive” rather than “wishy-washy”.\nHastie, Tibshirani, and Friedman (2001, 271) give the following example of a two-category problem with 400 observations in each category. A tree with two nodes that splits observations [300,100] and [100,300] has a misclassification rate of 1/4 (200 out of 800) and a Gini index of \\(2 \\times 3/4 1/4 = 3/8\\). A tree with nodes that splits [200,0] and [200,400] has the same misclassification rate but a lower Gini index of 2/9. The second tree would be preferred for classification.\nAn interesting interpretation of the Gini index is in terms of the total variance over \\(K\\) binary problems, each with a variance of \\(\\widehat{p}_{jk}(1-\\widehat{p}_{jk})\\). Another interpretation of the index is to imagine that rather than classifying an observation to the majority category we assign an observation to category \\(k\\) with probability \\(\\widehat{p}_{jk}\\). The Gini index is the training error of this procedure.\nThe cross-entropy criterion also leads to nodes with higher purity than the misclassification rate. In neural networks (Chapter 31 and Chapter 32) for classification, cross-entropy is the loss function of choice.\nThe deviance function originates in likelihood theory and plays a greater role in pruning decision trees than in training classification trees.\n\n\n\nPruning\nAs mentioned earlier, simply applying a stop criterion based on the number of observations in the leaves (terminal nodes) does not guarantee that we have arrived at a great decision tree. Trees with many nodes and branches, also called deep trees, tend to overfit. If one were to split the tree all the way down so that each terminal leaf contains exactly one observation, the tree would be a perfect predictor on the training data.\nPruning a tree means to remove splits from a tree, creating a subtree. As subsequent splits are removed, this creates a sequence of trees, each a subtree of the previous ones.\n\n\n\n\n\n\nFigure 16.3: Tree before and after pruning.\n\n\n\nTo generate a useful sequence of trees to evaluate, a process called minimum cost-complexity pruning is employed, A nested sequence of the original (full) tree is created by lopping off all nodes that emanate from a non-terminal node and the node is chosen for which a criterion such as misclassification error decreases by the smallest amount. In other words, the nodes emanating from the node are pruned that worsens the tree the least. This is known as weakest-link pruning. The performance of the subtrees is evaluated by computing a tree complexity measure based on a test data set or \\(k\\)-fold cross-validation.\nFormally, let \\(Q(T)\\) be the cost criterion by which tree \\(T\\) is evaluated. \\(Q(T)\\) is RSS for a regression tree and any of the criteria listed above for classification trees (Gini index or deviance, for example). If \\(|T|\\) is the number of terminal nodes in \\(T\\), then we seek to minimize for each subtree the cost-complexity criterion \\[\nC_\\alpha(T) = Q(T) + \\alpha|T|\n\\] where \\(\\alpha\\) is a hyperparameter. If \\(\\alpha = 0\\) we end up with the full tree. As \\(\\alpha\\) increases, trees with more terminal nodes are penalized more heavily. For each value of \\(\\alpha\\) there is one tree \\(T_\\alpha\\) in the sequence of subtrees obtained by weakest-link pruning that minimizes \\(C_\\alpha(T)\\).\nWith regression trees, the criterion used to grow the trees (squared error) is also used to prune the tree. With classification trees, growing the tree often relies on Gini index or cross-entropy, while pruning a tree uses the deviance or misclassification rate.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regression and Classification Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#issues-related-to-decision-trees",
    "href": "decisiontrees.html#issues-related-to-decision-trees",
    "title": "16  Regression and Classification Trees",
    "section": "16.4 Issues Related to Decision Trees",
    "text": "16.4 Issues Related to Decision Trees\n\nCategorical Inputs\nMany introductions to decision trees focus on the case where the \\(X\\)s are quantitative (numeric) variables. Categorical inputs (factors) can be handled easily in trees.\nSuppose \\(X\\) is categorical with \\(k\\) levels in the set \\(D_k\\). The set of possible values can be divided into \\(2^{k-1}-1\\) possible partitions, which is trivial for \\(k=1\\) and leads to many possible partitions as \\(k\\) grows. However, for values of \\(k &gt; 2\\), if you split the set \\(D_k\\) into two proper subsets \\(D_s\\) and \\(D_l\\), where \\(s+l=k\\), then fewer possible splits need to be considered. In other words, at each split of the categorical variable, we try to split off groups containing 1, 2, or a small number of levels from the other set.\n\n\nMissing Values\nIt is often said that “trees can handle missing values” and by extension tree-based techniques such as random forests or gradient boosted trees are somehow robust to the presence of missing values. That is simply not true. So let us look into how missing values can be handled in decision trees.\nFirst, observations with missing values for the target variable are not automatically handled by tree constructing software. These observation are discarded. For input variables with missing values there are two approaches.\nWith categorical inputs one approach is to create a new missing category. This will be considered in determining splits just like any other value of the input variable. This approach can shed light on whether observations with missing values exhibit a different pattern from complete observations. Another strategy, that applies to categorical and quantitative inputs, is the use of surrogate splits. If we come to a split in the tree and the current observation has a missing value we split on a surrogate variable that does not have missing values. In choosing surrogate variables for splits we choose variables whose splits mimic those of the primary split variable—that is, variables that are highly correlated with the primary variable. If the surrogate split variable is missing, then we can have a surrogate for the surrogate variable and so on.\nSurrogate splits can be used during training and during prediction of new observations. They “handle” missingness in the sense that the observations do not need to be omitted from the analysis. However, this does not mean that the resulting analysis is OK. Unless the missing value is MCAR (missing completely at random), the analysis can be biased.\n\n\n\n\n\n\nMissing Value Processes\n\n\n\n\n\nA detailed study of missing values and their impact on statistical analysis is given in Little and Rubin (1987). They define missing value processes as MAR (missing at random) and MCAR (missing completely at random).\nSuppose we collected data on inputs \\(\\textbf{X}\\) and target \\(\\textbf{Y}\\). Some of the rows of \\(\\textbf{X}\\) contain missing values and \\(\\textbf{X}_{c}\\) are the rows with complete data. Define \\(\\textbf{R}\\) as a matrix with typical element \\(r_{ij} = 1\\) if the \\(j\\)th input in row \\(i\\) is missing, and \\(r_{ij} = 0\\) otherwise.\nThe missing value process is MAR if the distribution of \\(\\textbf{R}\\) depends on the data \\(\\textbf{Z}= [\\textbf{Y},\\textbf{X}]\\) only through \\(\\textbf{Z}_{c} = [\\textbf{Y}, \\textbf{X}_c]\\): \\[\n\\Pr(\\textbf{R}| \\textbf{Z}) = \\Pr(\\textbf{R}| \\textbf{Z}_{c})\n\\]\nThis essentially states that the mechanism that results in missingness is independent of the unobserved value. If furthermore the distribution of \\(\\textbf{R}\\) does not depend on either the observed or missing data, the process is MCAR: \\[\n\\Pr(\\textbf{R}| \\textbf{Z}) = \\Pr(\\textbf{R})\n\\] If a weight measurement is not taken because the scales unexpectedly malfunctioned the missing data is MAR or MCAR. If the measurement is not taken because the animal is too light or heavy for the scales, the missing value process is not MAR or MCAR. If in a longitudinal health study a patient drops out because their condition worsened, the missingness is not MAR or MCAR. We say that the missingness is informative.\nAn example of informative missingness is a model to predict cardiovascular risk. Because the risk is judged low, a cholesterol measurement is not taken, whereas for patients with expected higher risk for heart health such measurements are routinely done. The absence of the cholesterol measurement in electronic health records can have predictive power for a lower cardiovascular risk, its missingness is informative.\n\n\n\n\n\nOptimality\nDecision trees constructed with the CART method (or other methods) are not optimal in that they make concessions for reasons of computational expediency and interpretability.\n\nRecursive binary splitting leads to partitions of the \\(X\\)-space that are simple to interpret but are not optimal partitions in terms of a loss criterion.\nThe greedy splitting algorithm only considers a reduction in variability or increase in purity at the current level of the tree, it does not look ahead and gauge the impact of a split on nodes further down in the tree.\nWeakest-link cutting during cost-complexity pruning does not visit all subtrees, only a nested sequence of subtrees.\nWith large data sets, multiway splits rather than binary splits could be beneficial. We use binary splitting to prevent fragmentation of the data into small sets too quickly and hope that a subsequent split further down the tree will achieve the same as a multiway split higher up on the tree.\n\n\n\nStability\nWhile decision trees are easy to interpret, the lack of stability under small changes of the data reduces their interpretability in the sense that firm conclusions based on trees are difficult. If the tree could change greatly with small changes in the data, how can we be confident that inputs that appear high in the tree (early splits) are particularly important?\nOn the other hand, decision trees with single splits are stable in a different sense. If single variable splits are used, the tree is invariant with respect to monotone transformations of the inputs (Sutton 2005). You do not have to worry whether to use log or square root or reciprocal transformations of an input variable in constructing the tree. As long as large values consistenly map to large transformed values or large values map consistenly to smaller values, the tree is invariant.\n\n\nSmoothness\nThe prediction surface of decision trees is not smooth, it changes abruptly at the boundaries of the leaf nodes. In a regression application the underlying relationship \\(\\text{E}[Y] = f(\\textbf{x})\\) is normally assumed to be a smooth function of the inputs. In classification models, in particular with binary targets, this is less of an issue.\n\n\nNonlinearity and Interactions\nOne advantage of trees over linear model is their ability to model complex nonlinear relationships between inputs and target and non-homogeneous response. For example, the response surface can look different in some regions of the \\(X\\)-space compared to others. A linear model would have to use transformations and interactions to capture this in a single function across the space. A tree can easily adjust through the choice of split variables and split values in different regions of the \\(X\\)-space. So much so, that capturing linear relationships is actually difficult with decision trees. Hastie, Tibshirani, and Friedman (2001, 274) give the following example:\nSuppose that \\(Y = \\beta_1 I(X_1 &lt; t_1) + \\beta_2 I(X_2 &lt; t_2) + \\epsilon\\) is the underlying model. To maintain this additive relationship between \\(Y\\) and \\([X_1,X_2]\\), a tree that would first split on \\(X_1\\) near \\(t_1\\) would then have to split both nodes on \\(X_2\\) near \\(t_2\\). As Hastie, Tibshirani, and Friedman (2001) put it\n\n…the model is given no special encouragement to find such structure. If there were ten rather than two aditive effects, … the data analyst would be hard pressed to recognize [the additive structure] in the estimated tree.\n\nInteractions can be captured by trees. However, what appears as interaction between inputs can be caused by just correlation. Sutton (2005) gives the following example:\n\\(X_1\\) and \\(X_2\\) are highly correlated inputs. A tree first splits on \\(X_3\\), then on \\(X_1\\) in the left side of the tree and on \\(X_2\\) in the right side of the tree. This might suggest the presence of interactions, but it could be simply that a tree split on only \\(X_3\\) and \\(X_1\\) is equivalent to one split only on \\(X_3\\) and \\(X_2\\). Both of these trees would suggest an additive relationship rather than an interaction, their equivalence stems from the correlation between \\(X_1\\) and \\(X_2\\).\n\n\n\n\nBreiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Wadsworth, Pacific Grove, CA.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.\n\n\nLittle, R., and D. Rubin. 1987. Statistical Analysis with Missing Data. Wiley, New York.\n\n\nSutton, Clifton D. 2005. “Classification and Regression Trees, Bagging, and Boosting.” Handbook of Statistics 24: 303–29.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Regression and Classification Trees</span>"
    ]
  },
  {
    "objectID": "treesinR.html",
    "href": "treesinR.html",
    "title": "17  Trees in R",
    "section": "",
    "text": "17.1 Packages\nThere are several packages in R to compute decision trees.\nThe tree package is based on the original implementation in the S language (the precursor to R). The tree::tree() function fits classification and regression trees and provides modest control over the tree building process. Cost-complexity pruning the tree is performed with tree::prune.tree, using deviance as cost criterion for regression trees and either deviance or misclassification rate for classification trees. (Note that the deviance for regression trees is just squared error complexity). Pruning can be combined with cross-validation using tree::cv.tree. The tree package does not support surrogate splits to handle missing values.\nThe party package uses recursive partitioning as the building block for varied statistical models, including trees (party::ctree is the main function in the package). It also supports random forests through party::cforest and surrogate splits for missing values.\nA comprehensive and widely used package is rpart, which fits regression and classification trees following the CART philosophy of Breiman et al. (1984). rpart is the CART package in R. Because CART was the trademarked name of a commercial software implementation of Breiman’s ideas by Salford Systems and tree had already been taken, the authors settled on rpart as the package name. This vignette gives an introduction to the capabilities of the rpart package.\nrpart provides detailed control over the tree building process and supports surrogate splits for missing values. An interesting aspect of rpart is to evaluate subtrees for different values of the cost complexity parameter during the training phase and to apply cross-validation. You can thus select the desired value of the complexity parameter immediately after training and pass the value on to the rpart::prune function to get the optimal tree.\nGetting to know rpart is also useful because it is used by other R packages that depend on trees. For example, the ada package for adaptive boosting models uses rpart to compute boosted trees. To affect the construction of the trees during boosting you need to know how to control tree construction in rpart.\nMost tree packages have their own functions to plot trees and annotate them with labels for splits and nodes. rpart provides plot.rpart and text.rpart, tree provides plot.tree and text.tree. The rpart.plot package extends the functions provided by the rpart package. We use the rpart and rpart.plot packages in what follows.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trees in `R`</span>"
    ]
  },
  {
    "objectID": "treesinR.html#packages",
    "href": "treesinR.html#packages",
    "title": "17  Trees in R",
    "section": "",
    "text": "Note\n\n\n\n\n\nIronically, rpart is probably used more frequently today than the CART software by Salford Systems (acquired by Minitab in 2017).\nMinitab/Salford System claims that theirs is the only true implementation of CART, that they have worked with the inventors (Breiman and colleagues), that only they have access to the original source code, and that the famous papers described only 20% of the code.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trees in `R`</span>"
    ]
  },
  {
    "objectID": "treesinR.html#regression-trees",
    "href": "treesinR.html#regression-trees",
    "title": "17  Trees in R",
    "section": "17.2 Regression Trees",
    "text": "17.2 Regression Trees\n\nBasic Construction\nFor this application we use the Hitters data on performance and salaries of baseball players in the 1986/1987 seasons, the data are provided in the ISLR2 library. Because the salaries are highly skewed a log transformation is applied prior to constructing the tree (Figure 17.1). We will examine later the effect of the log-transformation on the tree.\n\nlibrary(ISLR2)\ndata(Hitters)\n\npar(mfrow=c(1,2))\nplot(density(na.omit(Hitters$Salary)), main=\"Salary\")\nplot(density(na.omit(log(Hitters$Salary))), main=\"log(Salary)\")\n\n\n\n\n\n\n\nFigure 17.1: Salaries and log(Salaries) for the Hitters data.\n\n\n\n\n\nThe following statements load the rpart library and request a decision tree according to the model formula. Note that not all variables specified in the model are necessarily used in constructing the tree. method=\"anova\" requests a regression tree, the complexity criterion is the residual sum of squares. Since the target variable is continuous, rpart defaults to a regression tree in this case and the method= specification is not necessary. However, because rpart has several tree-building methods to choose from, it is not a bad idea to state explicitly what is intended.\nNote that a random number seed is set prior to the call to rpart since the function performs 10-fold cross-validation by default. While the tree returned by rpart is not affected by the seed, the evaluations of the complexity (penalty) parameter during cross-validation depend on the random number stream.\n\n\nTree Summary\n\nlibrary(rpart)\nset.seed(87654)\nt1 &lt;- rpart(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            method=\"anova\")\n\nsummary(t1, cp=0.2)\n\nCall:\nrpart(formula = log(Salary) ~ Years + Hits + RBI + Walks + Runs + \n    HmRun + PutOuts + AtBat + Errors, data = Hitters, method = \"anova\")\n  n=263 (59 observations deleted due to missingness)\n\n          CP nsplit rel error    xerror       xstd\n1 0.44457445      0 1.0000000 1.0024572 0.06529349\n2 0.11454550      1 0.5554255 0.5936326 0.06177974\n3 0.04446021      2 0.4408800 0.5190135 0.06426742\n4 0.01831268      3 0.3964198 0.4639453 0.06373984\n5 0.01806689      4 0.3781072 0.4734726 0.06728376\n6 0.01678632      6 0.3419734 0.4776621 0.06718932\n7 0.01617738      7 0.3251871 0.4679886 0.06739159\n8 0.01000000      8 0.3090097 0.4570960 0.06731152\n\nVariable importance\n  Years    Hits    Runs   AtBat     RBI   Walks   HmRun PutOuts \n     38      13      12      10      10      10       5       4 \n\nNode number 1: 263 observations,    complexity param=0.4445745\n  mean=5.927222, MSE=0.7876568 \n  left son=2 (90 obs) right son=3 (173 obs)\n  Primary splits:\n      Years &lt; 4.5   to the left,  improve=0.4445745, (0 missing)\n      Hits  &lt; 117.5 to the left,  improve=0.2229369, (0 missing)\n      RBI   &lt; 43.5  to the left,  improve=0.2161883, (0 missing)\n      AtBat &lt; 472.5 to the left,  improve=0.1865249, (0 missing)\n      Walks &lt; 34.5  to the left,  improve=0.1678353, (0 missing)\n  Surrogate splits:\n      HmRun &lt; 1.5   to the left,  agree=0.696, adj=0.111, (0 split)\n      RBI   &lt; 24.5  to the left,  agree=0.688, adj=0.089, (0 split)\n      Walks &lt; 11.5  to the left,  agree=0.681, adj=0.067, (0 split)\n      Runs  &lt; 13.5  to the left,  agree=0.673, adj=0.044, (0 split)\n      Hits  &lt; 29.5  to the left,  agree=0.669, adj=0.033, (0 split)\n\nNode number 2: 90 observations\n  mean=5.10679, MSE=0.4705907 \n\nNode number 3: 173 observations\n  mean=6.354036, MSE=0.4202619 \n\n\nThe summary function produces a lengthy listing of the tree. The cp= option is used here to specify a cutoff value for displaying nodes that fall below the value of the complexity parameter, simply to limit the amount of output. The cp= option on the summary call prunes the output, it does not prune the tree.\nThe first table lists the result of 10-fold cross-validation of the cost-complexity parameter, more on this below. The variable importance listing ranks variables by a combination measure that accounts for the quality of split where the variable was the primary split variable, and an adjustment for splits where the variable was a surrogate. The sum of the variable importance measures is 100, but due to rounding it might sum to a slightly different value. A variable that has a relative high variable importance might not get used in the final tree, due to pruning.\nThe first node at the trunk of the tree contains 263 observation. The best split variable at this level is Years with split value of 4.5. Using this variable and split point leads to the largest improvement in the complexity criterion. rpart also constructs a list of the surrogate variables should the primary split variable be missing. For this node a missing value in Years would be split on HmRun. The numbers in parentheses tell you how many observations actually had missing values and which split variables were used as surrogates. For these data, only the target variable contains missing values.\nrpart removes observations with missing values from the analysis if the target is missing or if all inputs are missing. The missing values in this data set occur only in the target variable, which means that surrogates were not used in training the model.\nA more concise listing of the tree is obtained with the print method.\n\nprint(t1)\n\nn=263 (59 observations deleted due to missingness)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 263 207.153700 5.927222  \n   2) Years&lt; 4.5 90  42.353170 5.106790  \n     4) Years&lt; 3.5 62  23.008670 4.891812  \n       8) Hits&lt; 114 43  17.145680 4.727386 *\n       9) Hits&gt;=114 19   2.069451 5.263932 *\n     5) Years&gt;=3.5 28  10.134390 5.582812  \n      10) Runs&lt; 74.5 21   4.229230 5.379350 *\n      11) Runs&gt;=74.5 7   2.427815 6.193200 *\n   3) Years&gt;=4.5 173  72.705310 6.354036  \n     6) Hits&lt; 117.5 90  28.093710 5.998380  \n      12) Walks&lt; 21.5 26   5.001884 5.687706 *\n      13) Walks&gt;=21.5 64  19.562870 6.124591  \n        26) Years&lt; 6.5 18   6.411418 5.727128 *\n        27) Years&gt;=6.5 46   9.195161 6.280120 *\n     7) Hits&gt;=117.5 83  20.883070 6.739687  \n      14) Walks&lt; 60.5 50  10.225000 6.576444 *\n      15) Walks&gt;=60.5 33   7.306869 6.987024 *\n\n\nThere are a total of 15 nodes in the tree and 9 terminal nodes (leaves) as indicated by asterisks. For each node, the listing shows the split variable and split point, the number of observations, the residual sum of squares, and the representative value for the node. For example, at the root node (trunk) of the tree there are 263 observations, the sum of squares \\(\\sum (y-\\overline{y})^2 = 207.153\\) and the average log salary is 5.9272. Ninety observations are split off to the left at the first split on Years with split value of 4.5. The sum of squares of those ninety observations is 42.353 and their average is 5.1067.\n\nThe easiest way to consume the results of constructing a tree is by visualizing the tree. The rpart.plot function in the rpart.plot package creates good-looking trees and has many options to affect the rendering. Figure 17.2 displays the regression tree for the Hitters data built so far.\n\nlibrary(rpart.plot)\nrpart.plot(t1,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.2: Default regression tree for log(salary) constructed by rpart for Hitters data.\n\n\n\n\n\nThe boxes annotating the nodes display the representative (predicted) value for the node and the percent of observations that fall into the node. The intensity of the node color is proportional to the value predicted at the node.\nIf we were to choose a “stump”—that is, a tree with a single split, we would predict a log salary of 6.4 for players who have been in the league for more than 4.5 years and a salary for 5.1 for the players with less tenure. For a player with 7 years experience, 100 hits and 30 walks in 1986 we would predict a log salary of 6.3.\nNote that not all variables in the model are used in constructing the tree. Variables that do not improve a tree at any split in a meaningful way do not show up. For example, the number of RBIs or errors in 1986 are not used as primary split variables. This does not mean that these variables are unrelated to the target, they can enter the model indirectly through correlation with other variables. They might also be used as surrogate split variables, which the figure does not convey.\nAlso note that variables can be reused. A variable that serves as the primary split variable can later be used again as a split variable. For example, Years is the first split variable and is used again on both sides of the tree. The number of Walks in 1986 separates observations differently for longer-tenured players that have less than 118 Hits and those who have more.\n\nWhat would the tree look like if we had not transformed the target variable by taking logarithms?\n\nt2 &lt;- rpart(Salary ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            method=\"anova\")\n\nrpart.plot(t2,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.3: Default regression tree for salary constructed by rpart for Hitters data.\n\n\n\n\n\nClearly, Figure 17.3 is not the same tree as shown in Figure 17.2. Although the log transformation is monotonic for the positive salary values, it affects the distribution of the target variable and its relationship to the inputs.\nHowever, if we apply a monotone transformation to inputs, the constructed tree does not change. The following code fits the initial tree but uses log(years) instead of years. Only the values for the split variable change, but not the tree (Figure 17.4). Compare this to Figure 17.2.\n\nt3 &lt;- rpart(log(Salary) ~ log(Years) + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            method=\"anova\")\n\nrpart.plot(t3,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.4: Default regression tree for log(salary) with log-transformed year as input.\n\n\n\n\n\nChanging in the model from years to log(years) does not impact the splits or split values for any of the other inputs. This is fundamentally different from a linear regression model where transforming one input variable affects the coefficients of other inputs unless they are completely uncorrelated. Compare the following:\n\nround(\n    lm(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors,\n            data=Hitters)$coefficients,4)\n\n(Intercept)       Years        Hits         RBI       Walks        Runs \n     4.3797      0.0933      0.0128      0.0004      0.0066      0.0014 \n      HmRun     PutOuts       AtBat      Errors \n     0.0033      0.0003     -0.0025     -0.0029 \n\nround(\n    lm(log(Salary) ~ log(Years) + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors,\n            data=Hitters)$coefficients,4)\n\n(Intercept)  log(Years)        Hits         RBI       Walks        Runs \n     3.9394      0.6952      0.0108     -0.0002      0.0059      0.0024 \n      HmRun     PutOuts       AtBat      Errors \n     0.0057      0.0004     -0.0023      0.0000 \n\n\n\n\nRules\nA very helpful extension of rpart in rpart.plot is the rpart.rules function. It expresses the decisions captured in an rpart model as a set of text rules. The variables are listed in the order of frequency in the rules. A variable that appears in more rules is listed first. Adding the cover=TRUE option also displays the percentage of cases covered by each rule. For the initial tree constructed above, this yields:\n\nrpart.rules(t1, roundint=FALSE)\n\n log(Salary)                                                                  \n         4.7 when Years &lt;  3.5        & Hits &lt;  114                           \n         5.3 when Years &lt;  3.5        & Hits &gt;= 114                           \n         5.4 when Years is 3.5 to 4.5                             & Runs &lt;  75\n         5.7 when Years &gt;=        4.5 & Hits &lt;  118 & Walks &lt;  22             \n         5.7 when Years is 4.5 to 6.5 & Hits &lt;  118 & Walks &gt;= 22             \n         6.2 when Years is 3.5 to 4.5                             & Runs &gt;= 75\n         6.3 when Years &gt;=        6.5 & Hits &lt;  118 & Walks &gt;= 22             \n         6.6 when Years &gt;=        4.5 & Hits &gt;= 118 & Walks &lt;  61             \n         7.0 when Years &gt;=        4.5 & Hits &gt;= 118 & Walks &gt;= 61             \n\n\nThe information displayed in the rules can be affected through options, for example,\n\nrpart.rules(t1, cover=TRUE, style=\"tallw\", roundint=FALSE)\n\nlog(Salary) is 4.7 with cover 16% when\n                   Years &lt; 3.5\n                   Hits &lt; 114\n\nlog(Salary) is 5.3 with cover 7% when\n                   Years &lt; 3.5\n                   Hits &gt;= 114\n\nlog(Salary) is 5.4 with cover 8% when\n                   Years is 3.5 to 4.5\n                   Runs &lt; 75\n\nlog(Salary) is 5.7 with cover 10% when\n                   Years &gt;= 4.5\n                   Hits &lt; 118\n                   Walks &lt; 22\n\nlog(Salary) is 5.7 with cover 7% when\n                   Years is 4.5 to 6.5\n                   Hits &lt; 118\n                   Walks &gt;= 22\n\nlog(Salary) is 6.2 with cover 3% when\n                   Years is 3.5 to 4.5\n                   Runs &gt;= 75\n\nlog(Salary) is 6.3 with cover 17% when\n                   Years &gt;= 6.5\n                   Hits &lt; 118\n                   Walks &gt;= 22\n\nlog(Salary) is 6.6 with cover 19% when\n                   Years &gt;= 4.5\n                   Hits &gt;= 118\n                   Walks &lt; 61\n\nlog(Salary) is 7.0 with cover 13% when\n                   Years &gt;= 4.5\n                   Hits &gt;= 118\n                   Walks &gt;= 61\n\n\n\n\nCross-validation and Pruning\nAs mentioned earlier, rpart performs cross-validation for cost complexity values as part of the tree construction. This is the origin of the CP table that appears at the top of the summary.rpart output. You can print this information by itself with the printcp function.\n\ncp &lt;- printcp(t1)\n\n\nRegression tree:\nrpart(formula = log(Salary) ~ Years + Hits + RBI + Walks + Runs + \n    HmRun + PutOuts + AtBat + Errors, data = Hitters, method = \"anova\")\n\nVariables actually used in tree construction:\n[1] Hits  Runs  Walks Years\n\nRoot node error: 207.15/263 = 0.78766\n\nn=263 (59 observations deleted due to missingness)\n\n        CP nsplit rel error  xerror     xstd\n1 0.444574      0   1.00000 1.00246 0.065293\n2 0.114545      1   0.55543 0.59363 0.061780\n3 0.044460      2   0.44088 0.51901 0.064267\n4 0.018313      3   0.39642 0.46395 0.063740\n5 0.018067      4   0.37811 0.47347 0.067284\n6 0.016786      6   0.34197 0.47766 0.067189\n7 0.016177      7   0.32519 0.46799 0.067392\n8 0.010000      8   0.30901 0.45710 0.067312\n\n\nThe complexity table lists trees in order of the complexity criterion, smaller trees are on top (larger CP value), larger trees at the bottom. The number of (internal) nodes in the tree is nsplit plus one. The error columns are reported relative to the error at the first node. xerror is the error from cross-validation and xstd is the standard error. Scanning the xerror column we see that the error is minimized at 3 splits (4 nodes). The standard error is useful to determine the best number of split points by taking in the uncertainty in the error estimate and the fact that there is often a neighborhood of similar values. The 1-SE rule says to consider all values within one standard error of the achieved minimum. Those are essentially equivalent and we choose the simplest model. In Figure 17.5 this is a tree of size 4 (3 splits). The horizontal line is drawn 1 standard error above the smallest cross-validated error.\n\nplotcp(t1, upper=\"size\")\n\n\n\n\n\n\n\nFigure 17.5: Complexity parameter plot for Hitters data. The size of the tree refers to the number of nodes (number of splits + 1).\n\n\n\n\n\nTo produce the final tree, apply the prune function with the selected value of the complexity parameter (Figure 17.6).\n\nt_final &lt;- prune(t1,cp=cp[cp[,2]==3,1])\n\nrpart.plot(t_final,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.6: Final tree for Hitters data after pruning and cross-validation.\n\n\n\n\n\n\n\nControlling the Tree\nMany parameters and setting affect the construction of the tree. The split criterion, cost function, minimum number of observations before attempting a split or in terminal nodes, the use of surrogate variables, the maximum depth, cross-validation settings, and so on.\nChanging tree control parameters affects primarily the depth of the tree and the computational requirements. For example, rpart computes by default up to 5 surrogate split variables. Unless you have missing values in input variables, surrogate splits will not be used. If you are sure that you also do not have missing values in non-training data, then you can skip the determination of the surrogates. Approximately half of the computing time during tree construction is used to find surrogate splits.\nIn rpart, parameters that determine the tree construction are passed as a rpart.control structure to the control= parameter. The most important parameters that determine the depth of the tree are\n\nminsplit: The minimum number of observations in a node for which a split is even attempted, default 20. Setting this to a small number will save computation time since nodes that have fewer than minsplit observations will not be split and are likely chopped off during pruning.\n\n(Note: I have seen advice to use minsplit=0 to fit “stumps”, trees with a single split. That does not seem to work. I could not debug what was happening as the pertinent code is in C and not accessible with R debugging tools.)\nminbucket: The minimum number of observations in terminal nodes (leaves). The default is minsplit/3.\ncp: Threshold for the complexity parameter. Recall from Section 16.3.2 that the complexity of the tree is measured during pruning as \\[\nC_\\alpha(T) = Q(T) + \\alpha|T|\n\\] where \\(Q(T)\\) is the cost criterion by which tree \\(T\\) is evaluated, \\(|T|\\) is the number of terminal nodes in \\(T\\), and \\(\\alpha\\) is the penalty parameter. The rpart version of this is \\[\nC_\\alpha(T) = Q(T) + cp|T|\\, Q(T_1)\n\\] where \\(Q(T_1)\\) is a tree with no splits. A split that does not improve the fit by at least the cp= threshold value will not be attempted. Setting cp=1 results in a tree with no splits. The default is cp=0.01. During cost-complexity pruning rpart evaluates trees for values that are larger than the threshold parameter. Setting cp=0 asks rpart to evaluate very deep trees, subject to the other parameters such as minsplit, minbucket, and maxdepth.\nmaxdepth: the maximum depth of the final tree, with the root node counting as depth zero. For example, maxdepth=1 results in a single split, also known as a “stump”. (Technically, a stump would have no splits.)\n\nImportant parameters that control the memory and computational requirements beyond computing trees at various depths are\n\nmaxsurrogate: the number of surrogate splits, default is 5. Set it to 0 to prevent the search for surrogate split variables.\nxval: the number of cross-validations, default is 10.\ncp: The complexity parameter threshold also affects the computing requirements as splits that do not improve the overall fit by at least that value will not be attempted.\n\n\nFitting stumps\nA special application of controlling the construction of decision trees are trees with a single split, also called “stumps”. In most scenarios such trees will underfit the data. However, in adaptive boosting, we fit a large number of trees where subsequent trees are weighing observations according to the level of misclassification, those trees focus on observations poorly classified so far. The individual trees are not deep trees, relying only on the most important variables. The use of stumps is common in adaptive boosting.\nThe following code fits a stump to the Hitters data. minsplit=2 specifies that a split will not be attempted unless at least two observations are present. This guarantees that at least one split occurs (setting minsplit=0 as recommended in some places does not seem to work in guaranteeing a single split). Setting the threshold for the complexity parameter to -1 essentially turns off checking whether the split improves the overall fit. maxdepth=1 sets the maximum depth of the tree to two nodes; zero depth refers to the root node. Finally, xval=0 turns off the cross-validation.\n\nstump &lt;- rpart(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            control=rpart.control(maxdepth = 1,\n                                  cp       =-1,\n                                  minsplit = 2,\n                                  xval     = 0),\n            method=\"anova\")\n\nstump\n\nn=263 (59 observations deleted due to missingness)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 263 207.15370 5.927222  \n  2) Years&lt; 4.5 90  42.35317 5.106790 *\n  3) Years&gt;=4.5 173  72.70531 6.354036 *\n\nrpart.plot(stump,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.7: A stump fit to the Hitters data.\n\n\n\n\n\nThe result of the stump fit are as expected based on the previous analysis. The number of years in the league is the most important variable and the basis of the first split (Figure 17.7).",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trees in `R`</span>"
    ]
  },
  {
    "objectID": "treesinR.html#classification-trees",
    "href": "treesinR.html#classification-trees",
    "title": "17  Trees in R",
    "section": "17.3 Classification Trees",
    "text": "17.3 Classification Trees\nFitting a classification tree in rpart is simple, when the target variable is a factor the software defaults to building a classification tree. Choosing method=class makes that explicit.\n\nBinary Classification\nFor this example we use the stagec data frame that ships with the rpart package. It contains data on 146 patients with stage C prostate cancer from a study on the prognostic value of flow cytometry.\nInput variables include\n\nage: age of patient in years\neet: a binary variable indicating early endocrine therapy (1=no, 2=yes)\ng2: percent of cells in G2 phase as determined by flow cytometry\ngrade: the grade of the tumor according to the Farrow system\ngleason: the Gleason score of the tumor, higher values indicate a more aggressive cancer\nploidy: a three-level factor that indicates the tumor status according to flow cytometry as diplioid, tetraploid, or aneuploid.\n\nThe outcome (target) variable for this classification is pgstat, a binary variable that indicates progression of the cancer (pgstat=1). The variable is recoded as a factor for a nicer display on the tree. Similarly, the eet variable is recoded as a factor.\n\ndata(stagec)\nhead(stagec)\n\n  pgtime pgstat age eet    g2 grade gleason     ploidy\n1    6.1      0  64   2 10.26     2       4    diploid\n2    9.4      0  62   1    NA     3       8  aneuploid\n3    5.2      1  59   2  9.99     3       7    diploid\n4    3.2      1  62   2  3.57     2       4    diploid\n5    1.9      1  64   2 22.56     4       8 tetraploid\n6    4.8      0  69   1  6.14     3       7    diploid\n\nprogstat &lt;- factor(stagec$pgstat, levels = 0:1, labels = c(\"No\", \"Prog\"))\neetfac &lt;- factor(stagec$eet, levels=1:2, labels=c(\"No, Yes\"))\n\nset.seed(543)\ncfit &lt;- rpart(progstat ~ age + eetfac + g2 + grade + gleason + ploidy,\n              data = stagec, \n              method = 'class')\nprint(cfit)\n\nn= 146 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 146 54 No (0.6301370 0.3698630)  \n   2) grade&lt; 2.5 61  9 No (0.8524590 0.1475410) *\n   3) grade&gt;=2.5 85 40 Prog (0.4705882 0.5294118)  \n     6) g2&lt; 13.2 40 17 No (0.5750000 0.4250000)  \n      12) ploidy=diploid,tetraploid 31 11 No (0.6451613 0.3548387)  \n        24) g2&gt;=11.845 7  1 No (0.8571429 0.1428571) *\n        25) g2&lt; 11.845 24 10 No (0.5833333 0.4166667)  \n          50) g2&lt; 11.005 17  5 No (0.7058824 0.2941176) *\n          51) g2&gt;=11.005 7  2 Prog (0.2857143 0.7142857) *\n      13) ploidy=aneuploid 9  3 Prog (0.3333333 0.6666667) *\n     7) g2&gt;=13.2 45 17 Prog (0.3777778 0.6222222)  \n      14) g2&gt;=17.91 22  8 No (0.6363636 0.3636364)  \n        28) age&gt;=62.5 15  4 No (0.7333333 0.2666667) *\n        29) age&lt; 62.5 7  3 Prog (0.4285714 0.5714286) *\n      15) g2&lt; 17.91 23  3 Prog (0.1304348 0.8695652) *\n\nrpart.plot(cfit, roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.8: Classification tree for stagec data prior to pruning.\n\n\n\n\n\nFigure 17.8 shows the full tree fit to the data prior to pruning. The initial split is on the grade variable. Note that on the right hand side of the tree, at depth 2, the qualitative input variable ploidy is split into two groups: the left branch contains levels diploid and tetraploid, the right branch aneuploid. This confirms DNA ploidy as a major predictor variable in this study. From the rpart longintro vignette:\n\nFor diploid and tetraploid tumors, the flow cytometry method was also able to estimate the percent of tumor cells in a G2 (growth) stage of their cell cycle; G2% is systematically missing for most aneuploid tumors.\n\nThe boxes annotating the nodes contain three pieces of information:\n\nThe representative value (majority vote) in the node; this is the predicted category for the node. For example, there are 92 patients who did not progress and 54 patients who progressed. The majority vote in the root node would be no progression.\nThe proportion of events in the node. For example, in the first node that proportion is 54/146 = 0.37. There are 61 observations where grade &lt; 2.5, 9 of these progressed. That leads to the terminal node on the far left of the tree: its majority vote is no progression, 9/61 = 0.15 and 42% of the observations fall into this node.\nThe percentage of observations covered by the node.\n\nThis numeric breakdown can be seen easily from the printed tree:\n\ncfit\n\nn= 146 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 146 54 No (0.6301370 0.3698630)  \n   2) grade&lt; 2.5 61  9 No (0.8524590 0.1475410) *\n   3) grade&gt;=2.5 85 40 Prog (0.4705882 0.5294118)  \n     6) g2&lt; 13.2 40 17 No (0.5750000 0.4250000)  \n      12) ploidy=diploid,tetraploid 31 11 No (0.6451613 0.3548387)  \n        24) g2&gt;=11.845 7  1 No (0.8571429 0.1428571) *\n        25) g2&lt; 11.845 24 10 No (0.5833333 0.4166667)  \n          50) g2&lt; 11.005 17  5 No (0.7058824 0.2941176) *\n          51) g2&gt;=11.005 7  2 Prog (0.2857143 0.7142857) *\n      13) ploidy=aneuploid 9  3 Prog (0.3333333 0.6666667) *\n     7) g2&gt;=13.2 45 17 Prog (0.3777778 0.6222222)  \n      14) g2&gt;=17.91 22  8 No (0.6363636 0.3636364)  \n        28) age&gt;=62.5 15  4 No (0.7333333 0.2666667) *\n        29) age&lt; 62.5 7  3 Prog (0.4285714 0.5714286) *\n      15) g2&lt; 17.91 23  3 Prog (0.1304348 0.8695652) *\n\n\nTo prune the tree we print the CP table and plot the results including the 1-SE reference line.\n\nprintcp(cfit)\n\n\nClassification tree:\nrpart(formula = progstat ~ age + eetfac + g2 + grade + gleason + \n    ploidy, data = stagec, method = \"class\")\n\nVariables actually used in tree construction:\n[1] age    g2     grade  ploidy\n\nRoot node error: 54/146 = 0.36986\n\nn= 146 \n\n        CP nsplit rel error  xerror    xstd\n1 0.104938      0   1.00000 1.00000 0.10802\n2 0.055556      3   0.68519 0.98148 0.10760\n3 0.027778      4   0.62963 0.87037 0.10454\n4 0.018519      6   0.57407 0.87037 0.10454\n5 0.010000      7   0.55556 0.88889 0.10511\n\nplotcp(cfit)\n\n\n\n\n\n\n\n\nThe optimal tree is a tree with 5 nodes (4 splits). A tree with 6 splits has the same cross-validation error and standard error. We choose the simpler tree (Figure 17.9).\n\nrpart.plot(prune(cfit,cp=0.027778), roundint=FALSE)\n\n\n\n\n\n\n\nFigure 17.9: Classification tree for stagec data after pruning.\n\n\n\n\n\n\n\nNominal Classification\nThe data frame cu.summary ships with the rpart package. It contains data from the April, 1990 issue of Consumer Reports on makes of cars. One of the columns in the data frame is Reliability, an ordered factor with levels Much worse, worse, average, better, and Much better.\nThe following code fits a multi-classification decision tree and graphs the result.\n\ndata(cu.summary)\nmcfit &lt;- rpart(Reliability ~ ., data = cu.summary)\nrpart.plot(mcfit)\n\n\n\n\n\n\n\nFigure 17.10: Classification tree for car reliability prior to pruning.\n\n\n\n\n\nThe node annotation displays the predicted category for the node, the predicted probability in each category, and the percentage of observations in the node.\n\n\n\nFigure 17.2: Default regression tree for log(salary) constructed by rpart for Hitters data.\nFigure 17.3: Default regression tree for salary constructed by rpart for Hitters data.\nFigure 17.4: Default regression tree for log(salary) with log-transformed year as input.\nFigure 17.5: Complexity parameter plot for Hitters data. The size of the tree refers to the number of nodes (number of splits + 1).\nFigure 17.6: Final tree for Hitters data after pruning and cross-validation.\nFigure 17.7: A stump fit to the Hitters data.\nFigure 17.8: Classification tree for stagec data prior to pruning.\nFigure 17.9: Classification tree for stagec data after pruning.\nFigure 17.10: Classification tree for car reliability prior to pruning.\n\n\n\nBreiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Wadsworth, Pacific Grove, CA.",
    "crumbs": [
      "Part IV. Decision Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trees in `R`</span>"
    ]
  },
  {
    "objectID": "ensemble_intro.html",
    "href": "ensemble_intro.html",
    "title": "18  Introduction",
    "section": "",
    "text": "18.1 What is an Ensemble\nThe idea of ensemble learning is to make a prediction, classification, cluster assignment, etc., by combining the strengths of a collection of simpler base models (base learners). The base learners are often weak learners, models with high variability that do not perform well on their own.\nEnsembles are called homogeneous if all base learners are of the same type. e.g., decision trees, or heterogeneous when models of different types are being combined.\nFigure 18.1 displays the main approaches in ensemble learning. What they all have in common is a mechanism by which diversity is introduced into the system and that diversity is exploited to gain strength.\nIn stacking (also called stacked generalization), for example, base learners come from different model families. If the goal is classification of observations, base learners can be decision trees, logistic regression models, support vector machines, neural networks, naive Bayes classifiers, and so on. Each of these has its strengths and weaknesses. The diversity of classification methods, once combined into a single classifier, hopefully combines the strengths of the methods and allays their weaknesses.\nThe base learners, whether homogeneous or heterogeneous, need to be combined into an overall output. In heterogeneous ensembles, another model, the so-called meta learner is built for that purpose. In homogeneous ensembles the combination is often a simple summing, averaging, or majority voting. This comes at the expense of interpretability–a single decision tree is intrinsically interpretable, a decision based on 500 trees built with different inputs and splits is much less interpretable. On the other hand, ensemble techniques can perform extremely well. So well that random forests or boosting methods such as GBM or XGBoost have become de-facto standard approaches for many data scientists and machine learners.\nTo understand why ensembles are powerful, let’s take a look at a very simple situation: the sample mean in a random sample. Suppose we are drawing a random sample \\(Y_1, \\cdots, Y_n\\) from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\). To estimate \\(\\mu\\), we could use each of the \\(Y_i\\) as an estimator; after all, they are unbiased estimators for \\(\\mu\\) \\[\n\\text{E}[Y_i] = \\mu,  \\quad \\forall i\n\\]\nThe preferred estimator, however, is the sample mean \\[\n\\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\n\\]\nWe prefer \\(\\overline{Y}\\) over \\(Y_i\\) as an estimator of \\(\\mu\\) because \\[\n\\text{Var}[Y_i] = \\sigma^2 \\quad \\text{Var}[\\overline{Y}] = \\frac{\\sigma^2}{n}\n\\]\nThe variance of the sample mean can be made arbitrarily small by drawing a larger sample. The effect of reducing variability is particularly strong when \\(\\sigma^2\\) is large. When \\(\\sigma^2 \\rightarrow 0\\), \\(\\overline{Y}\\) provides less of an advantage as an estimator of \\(\\mu\\) over \\(Y_i\\). And if \\(\\sigma^2 = 0\\), then it makes no sense to draw more than a single observation. \\(Y_1 = Y_2 = \\cdots = Y_n = \\mu\\). After drawing one observation we know the mean.\nAveraging has a very powerful effect on reducing variability. And that effect is greater if the things being averaged are highly variable. Averaging things that do not vary does not buy as much. From these considerations flow two approaches to create a statistical model with small prediction error or high accuracy:\nYou can see how the concept of diversity plays into homogeneous ensembles. The more variable–the more diverse–the weak learners are, the more impact averaging has on their individual predictions or classifications.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ensemble_intro.html#what-is-an-ensemble",
    "href": "ensemble_intro.html#what-is-an-ensemble",
    "title": "18  Introduction",
    "section": "",
    "text": "Figure 18.1: An overview of approaches in ensemble learning\n\n\n\n\n\n\n\n\n\n\n\nCarefully craft a single model that captures the variability in the data very well\nAverage a bunch of models that have high variability and small bias.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ensemble_intro.html#sec-ensemble-bootstrap",
    "href": "ensemble_intro.html#sec-ensemble-bootstrap",
    "title": "18  Introduction",
    "section": "18.2 The Bootstrap",
    "text": "18.2 The Bootstrap\nThe bootstrap is a powerful (re-)sampling procedure through which one can build up the empirical distribution of any quantity of interest. By taking a sufficiently large number of bootstrap samples, the empirical distribution can be made close to the real distribution of the quantity. Such a procedure has many applications, for example, to estimate the variability of a statistic or to average ensembles.\nSuppose you have a data set of size \\(n\\). A bootstrap sample is a sample from the data, of the same size \\(n\\), drawn with replacement. The bootstrap procedure is to draw \\(B\\) bootstrap samples, repeat the analysis for each of them, and compute summary statistics (mean, standard deviation) across the \\(B\\) results.\n\nVariance Estimation\n\n\nExample: The Variance of \\(R^2\\)\n\n\nOne application of the bootstrap is to estimate the variability of a statistic. Take \\(R^2\\) in the classical linear model, for example. It is reported by software, but we never get an estimate of its standard error. Since \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2}{\\sum_{i=1}^n(Y_i - \\overline{Y})}\n\\] depends on random variables, it is also a random variable. The bootstrap allows us to build up an empirical distribution of \\(R^2\\) values by resampling the data set over and over, fitting the linear model and computing its \\(R^2\\) each time, and then simply computing the standard deviation of the \\(R^2\\) values.\nSuppose we wish to estimate the variance of \\(R^2\\) in a quadratic linear model predicting mileage per gallon from horsepower in the Auto data. The boot function in the boot library performs the bootstrap sampling based on a dataframe and a user-supplied function that returns the quantities of interest.\n\nlibrary(ISLR2)\nlibrary(boot)\n\nboot.fn &lt;- function(data, index) {\n    summary(lm(mpg ~ poly(horsepower,2), data=data, subset=index))$r.squared\n}\nset.seed(123)\nb &lt;- boot(data=Auto, boot.fn, R=1000)\n\nb$t[1:10]\n\n [1] 0.7067123 0.6761398 0.6256381 0.6829737 0.6998290 0.6928946 0.7250387\n [8] 0.6741485 0.7392279 0.6990890\n\nb\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n    original       bias    std. error\nt1* 0.687559 0.0007935389  0.02833526\n\n\nThe array b$t contains the results computed from the \\(B=1,000\\) bootstrap samples. The \\(R^2\\) of the polynomial regression was 0.7067 in the first bootstrap sample, 0.6761 in the second bootstrap sample, and so on.\nThe \\(R^2\\) statistic for the original data is 0.6876. The std. error is the standard deviation of the 1000 bootstrap estimates of \\(R^2\\), sd(b$t) = 0.0283. The bias reported by boot is the difference between the mean of the bootstrap result and the estimate based on the original data:\n\nmean(b$t) - b$t0\n\n[1] 0.0007935389\n\n\nHow many bootstrap samples should you draw? The good news is that there is no overfitting when increasing \\(B\\). You need to draw as many samples as necessary to stabilize the estimate. Increase \\(B\\) to the point where the estimate does not change in significant digits.\n\nsd(boot(data=Auto, boot.fn, R=2000)$t)\n\n[1] 0.02869823\n\nsd(boot(data=Auto, boot.fn, R=4000)$t)\n\n[1] 0.02810885\n\nsd(boot(data=Auto, boot.fn, R=5000)$t)\n\n[1] 0.02795916\n\n\nAfter several thousand bootstrap samples, the estimate of the standard deviation of \\(R^2\\) has stabilized to three decimal places. If we are satisfied with that level of accuracy, we have drawn enough bootstrap samples. If precision to four decimal places is required, \\(B\\) must be increased further.\n\n\nWhy are we introducing the bootstrap now, under ensemble learning? If the bootstrap procedure is useful to estimate a quantity, then we can use it to make predictions or classifications. The \\(B\\) bootstrap samples are variations of the original data, we can think of sampling with replacement as introducing diversity between the data sets and the resulting variability can be dampened by averaging. That is the idea behind bagging, a homogeneous ensemble method.\n\n\nOut-of-bag Error\nBefore we move to bagging, it is insightful to learn more about the structure of the bootstrap samples. Since sampling is with replacement, some observations will appear once in the bootstrap sample, some will appear more than once, and others will not appear at all.\nConsider a single bootstrap sample from a dataframe with \\(n\\) observations. The probability that the \\(n\\)th observation selected into the bootstrap sample is the \\(j\\)th observation in the data frame is \\(1/n\\), because sampling is with replacement. The complement, the probability that the \\(n\\)th bootstrap observation is not the \\(j\\)th obs is \\(1-1/n\\). It follows that in a bootstrap sample of size \\(n\\), the probability that a particular observation is not included is \\[\n(1 - 1/n)^n \\approx 1/3\n\\]\nAs \\(n\\) grows, this probability quickly approaches 1/3. Figure 18.2 displays \\((1-1/n)^n\\) as a function of \\(n\\). The asymptote near 1/3 is reached for small data set sizes (&gt; 10).\n\n\n\n\n\n\n\n\nFigure 18.2: Probability of an observation to be included in a bootstrap sample.\n\n\n\n\n\nA bootstrap sample will contain about 2/3 of the observations from the original dataframe, some with repeated values. The other third of the observations are a natural hold-out sample for this bootstrap sample. The bootstrap procedure thus also provides a mechanism to estimate the test error similar to a cross-validation procedure. Two methods come to mind to use the bootstrap samples to estimate the test error:\nIn each of the \\(B\\) bootstrap samples, compute the test error based on the \\(m\\) observations not included in this sample. This set of observations is called the out-of-bag set\nIf the criterion is mean-squared prediction error, this would be for the \\(j\\)th bootstrap sample \\[\n\\text{MSE}_{Te}^{(j)} = \\frac{1}{m} \\sum_{k=1}^m (y_k - \\widehat{y}_k)^2\n\\] The overall test error is the average of the \\(B\\) out-of-bag errors: \\[\n\\text{MSE}_{Te} = \\frac{1}{B}\\sum_{j=1}^B \\text{MSE}_{Te}^{(j)}\n\\]\nAnother method of computing the out-of-bag error is to compute the predicted value for an observation whenever it is out-of-bag. This yields about \\(B/3\\) predicted values for each observation, and when averaged, an overall out-of-bag prediction for \\(y_i\\). The overall error is then computed from the \\(n\\) out-of-bag predictions. This estimate, for \\(B\\) sufficiently large, is equivalent to the leave-one-out prediction error, but it is not identical to the leave-one-out error because bootstrapping involves a random element and LOOCV is deterministic.\nThe following function computes the out-of-bag error estimates both ways for the Auto data and the model \\[\n\\text{mpg} = \\beta_0 + \\beta_1\\text{horsepower} + \\beta_2\\text{horsepower}^2 + \\epsilon\n\\] and compares them to the LOOCV error:\n\nComputing \\(\\text{MSE}_{Te}^{(j)}\\) for each bootstrap sample and averaging those (a mean across \\(B\\) quantities)\nAveraging the individual \\(B/3\\) out-of-bag predictions and computing the mean of those (a mean across \\(n\\) quantities)\n\n\nOOB_error &lt;- function(B=1000) {\n    n &lt;- dim(Auto)[1]\n\n    # Compute LOOCV error first\n    reg &lt;- lm(mpg ~ poly(horsepower,2), data=Auto)\n    leverage &lt;- hatvalues(reg)\n    PRESS_res &lt;- reg$residuals / (1-leverage)\n    PRESS &lt;- sum(PRESS_res^2)\n    loocv_error &lt;- PRESS/length(leverage);\n\n    ind &lt;- seq(1,n,1)\n    MSE_Te &lt;- 0\n    oob_preds &lt;- matrix(0,nrow=n,ncol=2)\n    # draw the bootstrap samples\n    for(i in 1:B) {\n        bs &lt;- sample(n,n,replace=TRUE) # replace=TRUE is important here!\n        oob_ind &lt;- !(ind %in% bs)  # the index of out-of-bag observations\n        reg &lt;- lm(mpg ~ poly(horsepower,2), data=Auto[bs,])\n        # predict the response for the out-of-bag observations\n        oob_pred &lt;- predict(reg,newdata=Auto[oob_ind,])\n        # accumulate predictions of the out-of-bag observations\n        oob_preds[oob_ind,1] &lt;- oob_preds[oob_ind,1] + oob_pred\n        oob_preds[oob_ind,2] &lt;- oob_preds[oob_ind,2] + 1\n        # Accumulate mean-square prediction errors in the jth bootstrap sample\n        MSE_Te &lt;- MSE_Te + mean((oob_pred - Auto[oob_ind,\"mpg\"])^2)\n    }\n    # Average the MSE_Te^(j) across the B samples\n    MSE_Te &lt;- MSE_Te / B\n\n    # Compute the average predictions for the n observations\n    # oobs_preds[,2] will be approximately B/3 for each observation\n    oob_preds[,1] &lt;- oob_preds[,1] / oob_preds[,2]\n    oob_error &lt;- mean((oob_preds[,1]-Auto[,\"mpg\"])^2)\n\n    return(list(MSE_Te=MSE_Te, OOB_error=oob_error, LOOCV=loocv_error))\n}\n\nset.seed(765)\noe &lt;- OOB_error(B=1000)\noe\n\n$MSE_Te\n[1] 19.44796\n\n$OOB_error\n[1] 19.24568\n\n$LOOCV\n[1] 19.24821\n\n\nThe OOB_error estimate based on the averaged predictions is very close to the leave-one-out prediction error.\n\n\n\nFigure 18.1: An overview of approaches in ensemble learning",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "bagging.html",
    "href": "bagging.html",
    "title": "19  Bagging",
    "section": "",
    "text": "19.1 Relationship to Bootstrap\nBagging is a general technique–like cross-validation–that can be applied to any learning method. It is particularly effective in ensembles of base learners with high variability and low bias. The name is short for bootstrap aggregating and describes its relationship to the bootstrap. Draw \\(B\\) bootstrap samples from the dataframe with \\(n\\) observations, fit a base learner to each of the \\(B\\) samples, and combine the individual predictions or classifications.\nIn the regression case, the \\(B\\) predictions are averaged, in the classification case the majority vote is taken to assign the predicted category.\nNot surprisingly, bagging is popular with decision trees, unbiased regression or classification methods that tend to have high variability. A single decision tree might have a large classification or prediction error, an ensemble of 500 trees can be a very precise predictor. Random forests (Section 19.3) are a special version of bagged trees.\nBagging can be useful with other estimators.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#relationship-to-bootstrap",
    "href": "bagging.html#relationship-to-bootstrap",
    "title": "19  Bagging",
    "section": "",
    "text": "Example: Bagged Estimator based on\n\n\nSuppose we have 100 observations from a G(\\(\\mu\\),2) distribution.\nWe want to estimate the mean of the normal, so the sample mean \\[\n\\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\n\\] is the best estimator.\nWe can get close to that estimator by bagging a highly variable estimator, say the average of the first three observations. if \\(Y_{(i)}\\) denotes the \\(i\\)th observation in the data set, then \\[\nY^* = \\frac{1}{3} \\sum_{i=1}^3 Y_{(i)}\n\\] is also an unbiased estimator for \\(\\mu\\) and has standard deviation \\[\n\\sqrt{\\frac{\\sigma^2}{3}} = 0.816\n\\] compared to the standard deviation of the sample mean, \\[ \\sqrt{\\frac{\\sigma^2}{100}} = 0.1414\n\\]\n\\(Y^*\\) is more variable than \\(\\overline{Y}\\) and both are unbiased estimators. The precision of estimating \\(\\mu\\) based on \\(Y^*\\) can be improved by bagging the estimator: rather than compute \\(Y^*\\) once from the data, we compute it \\(B\\) times based on bootstrap samples from the data and take the average of the bootstrapped estimates as our estimate of \\(\\mu\\).\n\nset.seed(542)\nn &lt;- 100\nx &lt;- rnorm(n, mean=0, sd=sqrt(2))\n\nbagger &lt;- function(x,b=1000) {\n    n &lt;- length(x)\n    sum_bs &lt;- 0\n    sum_bs2 &lt;- 0\n    for (i in 1:b) {\n        # Draw a bootstrap sample\n        bsample &lt;- sample(n,n,replace=TRUE)\n        # Compute the estimator\n        est &lt;- mean(x[bsample][1:3])\n        # Accumulate sum and sum of squared value of estimator\n        # so we can calculate mean and standard deviation later\n        sum_bs &lt;- sum_bs + est\n        sum_bs2 &lt;- sum_bs2 + est*est\n    }\n    # Compute mean and standard deviation of the estimator\n    mn = sum_bs / b\n    sdev = sqrt((sum_bs2 - sum_bs*sum_bs/b)/(b-1))\n    return (list(B=b, mean=mn, sd=sdev))\n}\n\ncat(\"The best estimate of mu is \", mean(x),\"\\n\\n\")\n\nThe best estimate of mu is  -0.004722246 \n\nl &lt;- list()\nl[[length(l)+1]] &lt;- bagger(x,b=10)\nl[[length(l)+1]] &lt;- bagger(x,b=100)\nl[[length(l)+1]] &lt;- bagger(x,b=1000)\nl[[length(l)+1]] &lt;- bagger(x,b=10000)\nl[[length(l)+1]] &lt;- bagger(x,b=25000)\nl[[length(l)+1]] &lt;- bagger(x,b=50000)\n\ndo.call(rbind.data.frame, l)\n\n      B          mean        sd\n1    10  0.2239820300 0.6364328\n2   100 -0.1450445628 0.8497776\n3  1000 -0.0173021748 0.8271210\n4 10000  0.0006381345 0.8534333\n5 25000 -0.0028520425 0.8394222\n6 50000 -0.0073707649 0.8386747\n\n\nThe bagger function returns the mean and standard deviation of the \\(B\\) bootstrap estimates. Notice that by increasing \\(B\\), the bagged estimate gets closer to the optimal estimator, \\(\\overline{Y}\\). The standard deviation of the bagged estimate stabilizes around 0.8, close to the theoretically expected value of \\(\\sqrt{2/3} = 0.816\\). Both quantities returned from bagger are becoming more accurate estimates as \\(B\\) increases. The standard deviation of the bootstrap estimate does not go to zero as \\(B\\) increases, it stabilizes at the standard deviation of the estimator.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#sec-bagged-trees",
    "href": "bagging.html#sec-bagged-trees",
    "title": "19  Bagging",
    "section": "19.2 Bagged Trees",
    "text": "19.2 Bagged Trees\nBreiman (1996) introduced bootstrap aggregating (bagging) as a means to increase the accuracy of a predictor by applying the predictor to a sequence of learning sets, data sets of size \\(n\\), and combining the results from the sets. In case of prediction the obvious choice is to average the predictions across the learning sets. In case of classification, Breiman recommends majority voting: classify an observation in each learning set and assign the most frequently chosen category as the overall classification.\nThe learning sets are \\(B\\) bootstrap samples drawn from the original data. Breiman notes\n\n“The vital element is the instability of the prediction method.”\n\nIf the different learning sets do not lead to changes in the predictors the aggregation will not be effective. This is where weak learners are important because they have high variability. There is a second situation where bagging will not be effective: if the method being bagged is already near the limits of accuracy achievable for that data set no amount of bootstrapping and bagging will help.\nThe most frequent application of bagging is with decision trees for either regression or classification. In a non-ensemble situation one would build one tree, first by growing it deep and then pruning it back. During bagging trees are not pruned, the volatility of deep trees is desirable.\n\n\nExample: Banana Quality\n\n\nThe data for this example can be found on kaggle and comprises observations on the quality of bananas (“Good”, “Bad”) and seven attributes (Figure 19.1). The attributes are normalized although it is not clear how this was done. There are 4,000 training observations and 4,000 testing observations in separate data sets.\n\nlibrary(\"duckdb\")\n\nLoading required package: DBI\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nban_train &lt;- dbGetQuery(con, \"SELECT * FROM banana_train\")\nban_test &lt;- dbGetQuery(con, \"SELECT * FROM banana_test\")\n\nban_train$Quality &lt;- as.factor(ban_train$Quality)\nban_test$Quality &lt;- as.factor(ban_test$Quality)\n\ndbDisconnect(con)\n\n\nlattice::histogram(~ Size + Weight + Sweetness + Softness + HarvestTime\n                   + Ripeness + Acidity , \n                   cex     =0.5,\n                   as.table=TRUE,\n                   par.strip.text=list(cex=0.75),\n                   xlab    =\"\",\n                   data    =ban_train)\n\n\n\n\n\n\n\nFigure 19.1: Histograms of banana quality attributes (training data)\n\n\n\n\n\nWe first build a regular classification tree for the training data and compute its confusion matrix for the test data. The tree is grown with the tree::tree function and its complexity is cross-validated (10-fold) based on the misclassification rate.\n\nlibrary(tree)\ntree.ban &lt;- tree(Quality ~ ., data=ban_train)\nsummary(tree.ban)\n\n\nClassification tree:\ntree(formula = Quality ~ ., data = ban_train)\nVariables actually used in tree construction:\n[1] \"Sweetness\"   \"HarvestTime\" \"Ripeness\"    \"Weight\"      \"Size\"       \n[6] \"Softness\"   \nNumber of terminal nodes:  15 \nResidual mean deviance:  0.5658 = 2255 / 3985 \nMisclassification error rate: 0.09525 = 381 / 4000 \n\nset.seed(123)\ncv.ban &lt;- cv.tree(tree.ban, FUN=prune.misclass)\n\ncat(\"Optimial tree size after pruning: \",cv.ban$size[which.min(cv.ban$dev)],\"\\n\")\n\nOptimial tree size after pruning:  15 \n\n\nIt turns out that the size of the deep tree with 15 nodes is also the best size per cross-validation, so no further pruning is necessary and the tree can be used to predict the test data.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ntree.ban.pred &lt;- predict(tree.ban, newdata=ban_test, type=\"class\")\n\ntree.ban.cm &lt;- confusionMatrix(tree.ban.pred,ban_test$Quality)\ntree.ban.cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Bad Good\n      Bad  1844  270\n      Good  150 1736\n                                          \n               Accuracy : 0.895           \n                 95% CI : (0.8851, 0.9043)\n    No Information Rate : 0.5015          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.79            \n                                          \n Mcnemar's Test P-Value : 6.375e-09       \n                                          \n            Sensitivity : 0.9248          \n            Specificity : 0.8654          \n         Pos Pred Value : 0.8723          \n         Neg Pred Value : 0.9205          \n             Prevalence : 0.4985          \n         Detection Rate : 0.4610          \n   Detection Prevalence : 0.5285          \n      Balanced Accuracy : 0.8951          \n                                          \n       'Positive' Class : Bad             \n                                          \n\n\nThe decision tree achieves an accuracy of 89.5% on the test data.\nTo apply bagging we use the randomForest function in the package by the same name. As we will see shortly, bagging is a special case of a random forest where the mtry parameter is set to the number of input variables, here 7. By default, randomForest performs \\(B=500\\) bootstrap samples, that is, it builds \\(500\\) trees.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nset.seed(6543)\nbag.ban &lt;- randomForest(Quality ~ . , \n                        data      =ban_train, \n                        xtest     =ban_test[,-8],\n                        ytest     =ban_test[, 8],\n                        mtry      =7,\n                        importance=TRUE)\n\nbag.ban\n\n\nCall:\n randomForest(formula = Quality ~ ., data = ban_train, xtest = ban_test[,      -8], ytest = ban_test[, 8], mtry = 7, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 7\n\n        OOB estimate of  error rate: 3.88%\nConfusion matrix:\n      Bad Good class.error\nBad  1933   67      0.0335\nGood   88 1912      0.0440\n                Test set error rate: 3.35%\nConfusion matrix:\n      Bad Good class.error\nBad  1929   65  0.03259779\nGood   69 1937  0.03439681\n\n\nWhen a test set is specified, randomForest returns two types of predicted values. bag.ban$predicted are the out-of-bag predictions. Each observation is out-of-bag in about 1/3 of the trees built. The majority vote across the \\(B/3\\) classifications is the out-of-bag prediction for that observation. The predicted values based on the test data set are stored in bag.ban$test$predicted. Each observation in the test data set is passed through the 500 trees–the majority vote of the predicted quality is the overall prediction for that observation.\nThe confusion matrices shown by randomForest can be reconstructed as follows:\n\nconfusionMatrix(bag.ban$predicted,ban_train$Quality)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Bad Good\n      Bad  1933   88\n      Good   67 1912\n                                         \n               Accuracy : 0.9612         \n                 95% CI : (0.9548, 0.967)\n    No Information Rate : 0.5            \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.9225         \n                                         \n Mcnemar's Test P-Value : 0.1082         \n                                         \n            Sensitivity : 0.9665         \n            Specificity : 0.9560         \n         Pos Pred Value : 0.9565         \n         Neg Pred Value : 0.9661         \n             Prevalence : 0.5000         \n         Detection Rate : 0.4833         \n   Detection Prevalence : 0.5052         \n      Balanced Accuracy : 0.9612         \n                                         \n       'Positive' Class : Bad            \n                                         \n\nbag.ban.cm &lt;- confusionMatrix(bag.ban$test$predicted,ban_test$Quality)\nbag.ban.cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Bad Good\n      Bad  1929   69\n      Good   65 1937\n                                          \n               Accuracy : 0.9665          \n                 95% CI : (0.9604, 0.9719)\n    No Information Rate : 0.5015          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.933           \n                                          \n Mcnemar's Test P-Value : 0.7955          \n                                          \n            Sensitivity : 0.9674          \n            Specificity : 0.9656          \n         Pos Pred Value : 0.9655          \n         Neg Pred Value : 0.9675          \n             Prevalence : 0.4985          \n         Detection Rate : 0.4823          \n   Detection Prevalence : 0.4995          \n      Balanced Accuracy : 0.9665          \n                                          \n       'Positive' Class : Bad             \n                                          \n\n\nThe accuracy of the decision tree has increased from 89.5% for the single tree to 96.65% through bagging.\n\n\nAs with all ensemble methods, interpretability of the results suffers when methods are combined. Decision trees are intrinsically interpretable and among the most easily understood algorithms. The visual of a tree describes how data flows through the algorithm and how it arrives at a prediction or classification (Figure 19.2).\n\npar(mar = c(2, 2, 1, 2))\nplot(tree.ban)\ntext(tree.ban,cex=0.7)\n\n\n\n\n\n\n\nFigure 19.2: Decision tree for banana quality\n\n\n\n\n\nWith bagged trees we lose this interpretability; instead of a single tree the decision now depends on many trees and cannot be visualized in the same way. To help with the interpretation of bagged trees, the importance of the features can be calculated in various ways. One technique uses the Gini index, a measure of the node purity, another looks at the decrease in accuracy when the variable is permuted in the out-of-bag samples. Permutation breaks the relationship between the values of the variable and the target variable. A variable that changes the accuracy greatly when its values are permuted is an important variable for the tree.\n\n\nExample: Banana Quality (Cont’d)\n\n\nThe variable importance for the banana quality based on bagging 500 trees is shown in Figure 19.3.\n\nimportance(bag.ban)\n\n                  Bad      Good MeanDecreaseAccuracy MeanDecreaseGini\nSize         89.29303  89.07396            127.62487         318.5278\nWeight      100.00928 111.36404            139.91621         245.3550\nSweetness    89.51721 112.26886            158.96167         392.7300\nSoftness    108.32549 133.31219            168.30776         287.1453\nHarvestTime  96.58981 108.22739            143.83798         337.7313\nRipeness    101.76511  93.76190            133.84679         294.0230\nAcidity      57.39471  57.35883             77.66721         123.9741\n\npar(mar = c(2, 2, 0, 2))\nvarImpPlot(bag.ban,main=\"\")\n\n\n\n\n\n\n\nFigure 19.3: Variable importance plots for bagged trees.\n\n\n\n\n\n\n\nBagging is a powerful method to improve accuracy. In Breiman’s words:\n\n“Bagging goes a ways toward making a silk purse out of a sow’s ear, especially if the sow’s ear is twitchy. […] What one loses, with the trees, is a simple and interpretable structure. What one gains is increased accuracy.”",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#sec-random-forest",
    "href": "bagging.html#sec-random-forest",
    "title": "19  Bagging",
    "section": "19.3 Random Forests",
    "text": "19.3 Random Forests\nAre there any other downsides to bagging, besides a loss of interpretability and an increased computational burden? Not really, but another issue deserves consideration: bootstrap samples are highly correlated. Two thirds of the observations are in the bootstrap samples making the data frames similar to each other. An input variable that dominates the model will tend to be the first variable on which the tree is split, leading to similar trees. This reduces the variability of the trees and reduces the effectiveness of the ensemble.\nHow can we make the trees more volatile? Breiman (2001) advanced the concept of bagged trees through the introduction of random forests. The key innovation between bagged trees and random forests is that not all input variables are considered at each split of the tree. Instead, only a randomly selected set of predictors is considered, the set changes from split to split. The random split introduces more variability and allows non-dominant input variables to participate.\nIn other aspects, random forests are constructed in the same way as bagged trees introduced in Section 19.2: \\(B\\) trees are built based on \\(B\\) bootstrap samples, the trees are built deep and not pruned.\n\n\nExample: Banana Quality (Cont’d)\n\n\nFitting a random forest instead of bagging trees is simple based on the previous R code. Simply specify a value for the mtry parameter that is smaller than the number of input variables. By default, randomForest chooses \\(p/3\\) candidate inputs at each split in regression trees and \\(\\sqrt{p}\\) candidate inputs in classification trees.\n\nset.seed(6543)\nrf.ban &lt;- randomForest(Quality ~ . , \n                       data =ban_train, \n                       xtest=ban_test[,-8],\n                       ytest=ban_test[, 8])\nrf.ban\n\n\nCall:\n randomForest(formula = Quality ~ ., data = ban_train, xtest = ban_test[,      -8], ytest = ban_test[, 8]) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 3.3%\nConfusion matrix:\n      Bad Good class.error\nBad  1944   56       0.028\nGood   76 1924       0.038\n                Test set error rate: 3.15%\nConfusion matrix:\n      Bad Good class.error\nBad  1928   66  0.03309930\nGood   60 1946  0.02991027\n\n\nOnly two of the seven inputs are considered at each split in the random forest compared to the bagged trees. The accuracy improves slightly from 96.65% to 96.85%.\n\n\nIt might seem counterintuitive that one can achieve a greater accuracy by “ignoring” five out of the seven input variables at each split. It turns out that the results are fairly insensitive to the number of inputs considered at each split. The important point is that not all inputs are considered at each split. In that case, one might as well choose a small number to speed up the computations.\nBagged trees and random forests are easily parallelized, the \\(B\\) trees can be trained independently. This is different from other ensemble techniques, for example, boosting that builds the ensemble sequentially. Another advantage of bootstrap-based methods is that they do not overfit. Increasing \\(B\\), the number of bootstrap samples, simply increases the number of trees but does not change the model complexity.\nCross-validation is not necessary with bootstrap-based methods because the out-of-bag error can be computed from the observations not in a particular bootstrap sample.\nUsing a random selection of inputs at each split is one manifestation of “random” in random forests. Other variations of random forests take random linear combinations of inputs, randomizing the outputs in the training data or random set of weights. The important idea is to inject randomness into the system to create diversity.\nRandom forests excel at reducing variability and there is some evidence that they also reduce bias. Breiman (2001) mentions that the mechanism by which forests reduce bias is not obvious. The bootstrap mechanism is inherently focused on reducing variability. A different class of ensemble methods was developed to attack both bias and variance simultaneously. In contrast to the random forest, these boosting techniques are based on sequentially changing the training data set or the model (Chapter 20).\nA yet completely different approach is taken by Bayesian Model Averaging (BMA). In many situations there is no one clear winning model but a neighborhood of models that deserve consideration. Picking a single winner then seems not justified. Providing interpretations of a large number of models is also not advised. BMA combines many models to contribute to an overall prediction according to their proximity to the best-performing model (Chapter 21).\n\n\n\nFigure 19.2: Decision tree for banana quality\n\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24: 123–40.\n\n\n———. 2001. “Random Forests.” Machine Learning 45: 5–32.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "boosting.html",
    "href": "boosting.html",
    "title": "20  Boosting",
    "section": "",
    "text": "20.1 Adaptive Boosting (AdaBoost)",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "boosting.html#gradient-boosting",
    "href": "boosting.html#gradient-boosting",
    "title": "20  Boosting",
    "section": "20.2 Gradient Boosting",
    "text": "20.2 Gradient Boosting",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "boosting.html#extreme-gradient-boosting-xgboost",
    "href": "boosting.html#extreme-gradient-boosting-xgboost",
    "title": "20  Boosting",
    "section": "20.3 Extreme Gradient Boosting (XGboost)",
    "text": "20.3 Extreme Gradient Boosting (XGboost)",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "bma.html",
    "href": "bma.html",
    "title": "21  Bayesian Model Averaging",
    "section": "",
    "text": "21.1 Bayesian Inference\nThe Bayesian paradigm is arguably the more natural paradigm to draw conclusions about the world, compared to the frequentist approach that has permeated this material so far. A frequentist interprets probabilities as a long-run average of sample proportions: if a coin lands on “Head” 45 times out of 100 tosses, the frequentist estimate of the probability of “Head” is 45/100 = 0.45. In Bayesian statistics, probabilities reflect the degree of belief that something will occur. Furthermore, in Bayesian statistic, there are no parameters in the sense of the frequentist view. Parameters are random variables as well.\nSuppose we observe data \\(\\textbf{Y}\\) and its distribution depends on a parameter vector \\(\\boldsymbol{\\theta}\\). The data we observe is a realization of the distribution \\(p(\\textbf{Y}|\\boldsymbol{\\theta})\\), known as the likelihood of the data. The likelihood is the probability of observing the data given that \\(\\boldsymbol{\\theta}\\) is the true parameter. The goal of the inference, whether the viewpoint is frequentist or Bayesian, is to learn something about \\(\\boldsymbol{\\theta}\\). If \\(\\boldsymbol{\\theta}\\) is a constant, then what we need to do is to estimate its value based on the data. This is the frequentist approach used so far. The OLS estimator in a linear model, \\[\n\\widehat{\\boldsymbol{\\theta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] is an example of a (frequentist) estimator.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Bayesian Model Averaging</span>"
    ]
  },
  {
    "objectID": "bma.html#bayesian-inference",
    "href": "bma.html#bayesian-inference",
    "title": "21  Bayesian Model Averaging",
    "section": "",
    "text": "Bayes’ Rule\nIf \\(\\boldsymbol{\\theta}\\) is a random variable then we need to consider its distribution rather than trying to find a point estimator. But which distribution should we consider? We can look at \\(p(\\boldsymbol{\\theta})\\) and at \\(p(\\boldsymbol{\\theta}| \\textbf{Y})\\). The former is known as the prior distribution, reflecting our belief about \\(\\boldsymbol{\\theta}\\) before we have seen any data. The latter, \\(p(\\boldsymbol{\\theta}| \\textbf{Y})\\) is known as the posterior distribution, reflecting our belief in \\(\\boldsymbol{\\theta}\\) after having seen the data.\nTo get from \\(p(\\textbf{Y}|\\boldsymbol{\\theta})\\) to \\(p(\\boldsymbol{\\theta}| \\textbf{Y})\\) we have to reverse the conditioning. This can be done with Bayes’ Rule: \\[\np(\\boldsymbol{\\theta}|\\textbf{Y}) = \\frac{p(\\textbf{Y},\\boldsymbol{\\theta})}{p(\\textbf{Y})} = \\frac{p(\\textbf{Y}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{\\int p(\\textbf{Y}| \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}\n\\] The first fraction is simply the definition of a conditional probability, the ratio of the joint probability divided by the marginal probability of the conditioning event (here, the data). The second fraction expresses the joint probability in the numerator as the product of a conditional and a marginal probability, this time using the reverse condition and the prior distribution. The integral in the denominator computes the marginal distribution of \\(\\textbf{Y}\\) by integrating over the distribution of \\(\\boldsymbol{\\theta}\\); this term can be thought of as a normalizing constant to ensure we get a proper probability. The magic is in the numerator, we can write \\[\np(\\boldsymbol{\\theta}|\\textbf{Y}) \\propto p(\\textbf{Y}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n\\]\nThe posterior distribution is proportional to the likelihood times the prior distribution.\nOur prior beliefs about the world, before seeing any data, are represented by \\(p(\\boldsymbol{\\theta})\\). After we collected data about the world that follows the distribution \\(p(\\textbf{Y}|\\boldsymbol{\\theta})\\) we update our beliefs. This makes a lot of sense and reflects how most of us make decisions and draw conclusions. We have a notion and based on evidence we update our belief–at least that is how it is supposed to work.\n\n\nExample: Are trains on time?\n\n\nSuppose you need to catch a train at the station. Your prior belief that the train is late is \\(p(\\theta) = 0.25\\): a quarter of the time the train you need to catch will be late. When you arrive at the train station you see that all the other trains are running late today. Based on this new information, \\(p(Y|\\theta)\\), you update your belief; the posterior probability of your train running late is now \\(p(\\theta) &gt; 0.25\\).\n\n\nTo paraphrase L. J. Savage,\n\nYou are either Bayesian or irrational.\n\n\n\nInference about \\(\\boldsymbol{\\theta}\\)\nThe posterior distribution \\(p(\\boldsymbol{\\theta}| \\textbf{Y})\\) contains all information we need to make statements about the parameters. For example, to compute the Bayesian equivalent of a point estimate, standard error, and confidence interval for one of the parameters, \\(\\theta_1\\) say, we first derive the posterior of \\(\\theta_1\\) by integrating out the other parameters: \\[\np(\\theta_1 | \\textbf{Y}) = \\int \\cdots \\int p(\\boldsymbol{\\theta}| \\textbf{y}) d\\theta_2 d\\theta_3 \\cdots d\\theta_p\n\\] The square root of the variance of \\(p(\\theta_1|\\textbf{Y})\\) is the standard error, the 0.25 and 0.975 quantiles are the 95% confidence bounds, and the mode or mean of the posterior serves as the point estimate.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Bayesian Model Averaging</span>"
    ]
  },
  {
    "objectID": "bma.html#introducing-model-uncertainty",
    "href": "bma.html#introducing-model-uncertainty",
    "title": "21  Bayesian Model Averaging",
    "section": "21.2 Introducing Model Uncertainty",
    "text": "21.2 Introducing Model Uncertainty\nHow do we bring uncertainty about models, not just about parameters, into this picture? Suppose we have \\(K\\) candidate models, \\(f_1, \\cdots, f_K\\) and can quantify our prior belief in the models. Without any additional information the prior belief might be “uninformative”, all models have the same prior probability. The posterior probability of the parameters, \\(p(\\boldsymbol{\\theta}|\\textbf{Y})\\) can now be written as \\[\np(\\boldsymbol{\\theta}| \\textbf{Y}) = \\sum_{k=1}^K p(\\boldsymbol{\\theta}, f_k | \\textbf{Y}) = \\sum_{k=1}^K p(\\boldsymbol{\\theta}| \\textbf{Y}, f_k) p(f_k|\\textbf{Y})\n\\] integrating over the distribution of the models. The posterior is updated after evaluating the models against data.\n\n\nExample: Are trains on time?\n\n\nThis example draws on Hinne et al. (2020). Suppose you are at the train station and the train is running late. Should you continue to wait or consider an alternative mode of transportation?\nThere are different models about the world, with different consequences for the length of delay \\(t\\):\n\n\\(f_1\\): the railroad company went bankrupt overnight and abruptly stopped operations; \\(t\\) is very large, months to years.\n\\(f_2\\): railroad workers went on strike; \\(t\\) is large, weeks to months.\n\\(f_3\\): a railroad accident occurred; \\(t\\) is large, hours to days.\n\\(f_4\\): bad weather slowed down the train; \\(t\\) is short, minutes.\n\\(f_5\\): loading and unloading passengers at the previous station took longer than anticipated; \\(t\\) is very short, seconds to minutes.\n\nThe decision about choosing alternative transportation or waiting for the train to arrive depends on the probability of \\(f_j\\) and the distribution of the delays, \\(p(t|f_j)\\). We are interested in the overall distribution of the delay time, weighted by the likelihoods of the different models \\(f_1, \\cdots, f_4\\).\n\n\\(p(t|f_j)\\) is the distribution of delay times if \\(f_j\\) is the reason for the delay.\n\\(p(f_j)\\) is our prior belief that \\(f_j\\) occurs in the absence of additional information (when we leave the house in the morning).\n\\(p(f_j|\\textbf{Y})\\) is the posterior belief that \\(f_j\\) occurs after receiving information, for example, listening to a station announcement or a weather report.\n\\(p(t|\\textbf{Y})\\) is the distribution of the delay after receiving information and averaging across the possible reasons for the delay.\n\nIncorporating model uncertainty means taking into account all possible states of the world, weighted by the likelihood of their occurrence. The prior belief about \\(f_j\\) will differ from one person to another and that will affect the ultimate decision about choosing alternate transportation. If you wake up in the morning convinced that there will be either a strike or a railroad accident, you will feel differently about traveling by train than the optimist who believes nothing can slow them down.\n\n\n\nBayes Factors and BIC\nThe posterior distributions of the models, \\(p(f_k|\\textbf{Y})\\) are updated versions of our beliefs once we have seen data. You might not give the possibility of a railroad strike much credence one you learner that the morning did not mention any strike. The extent to which the data support one model over the other is expressed by the ratio of posterior probabilities \\[\n\\frac{p(f_j|\\textbf{Y})}{p(f_k|\\textbf{Y})}\n\\] This ratio is called the posterior odds for \\(f_j\\) versus \\(f_k\\). But \\(p(f_j|\\textbf{Y}) = p(\\textbf{Y}|f_j)p(f_j)/p(\\textbf{Y})\\) and the posterior odds expand to \\[\n\\frac{p(f_j|\\textbf{Y})}{p(f_k|\\textbf{Y})} = \\frac{p(\\textbf{Y}|f_j)}{p(\\textbf{Y}|f_k)} \\, \\frac{p(f_j)}{p(f_k)}\n\\] The first term on the right hand side is called a Bayes factor; a measure of the strength of evidence for \\(f_j\\) over \\(f_k\\): how much more likely is the observed data under the model \\(f_j\\) versus the model \\(f_k\\). In the situation where all models are equally likely a priori, \\(p(f_j) = 1/K\\), the Bayes factor \\(B_{jk}\\) is equal to the posterior odds: \\[\nB_{jk} = \\frac{p(f_j|\\textbf{Y})}{p(f_k|\\textbf{Y})}\n\\]\nTo judge the strength of evidence of one model against another, thresholds like the following are in use\n\n\\(1 \\le B_{jk} \\le 3\\): \\(f_j\\) is slightly better than \\(f_k\\) but deserves no more than a “bare mention”.\n\\(3 \\le B_{jk} \\le 10\\): there is some evidence for \\(f_j\\) over \\(f_k\\).\n\\(10 \\le B_{jk} \\le 100\\): there is strong evidence for \\(f_j\\) over \\(f_k\\).\n\\(B_{jk} &gt; 100\\): \\(f_j\\) is decisively better than \\(f_k\\).\n\nThe calculation of the distribution of the data for a particular model, \\(p(\\textbf{Y}|f_j)\\) is tricky, we have to integrate over the distribution of the parameters under the model. Fortunately, this can be approximated as \\[\n\\log p(\\textbf{Y}|f_j) \\approx \\log p(\\textbf{Y}|\\boldsymbol{\\theta}_j) - \\frac{p_j}{2}\\log n\n\\] The first term on the right hand side is simply the likelihood under model \\(f_j\\) and \\(p_j\\) denotes the number of parameters in the model. Minus twice the value of that approximation is known as the Bayesian Information Criterion (BIC). \\[\n\\text{BIC}_j = -2\\log p(\\textbf{Y}| \\boldsymbol{\\theta}_j) + p_j \\log n\n\\] It is used to select among competing models based on the maximized likelihood with a penalty term \\((p/2 \\log n)\\) that protects against overfitting. When comparing models, those with smaller BIC values are preferred.\n\n\n\n\n\n\nNote\n\n\n\nBIC can also be defined as the negative, \\(\\text{BIC}^*_j = 2\\log p(\\textbf{Y}|\\boldsymbol{\\theta}_j) - p_j\\log n\\). \\(\\text{BIC}_j\\) is interpreted in a smaller-is-better sense. \\(\\text{BIC}^*_j\\) is interpreted in a larger-is-better sense. Always check which version of BIC is reported by software to ensure correct interpretation.\n\n\nBIC is easy to calculate and helpful to approximate Bayes factors and posterior odds \\[\n2 \\log B_{jk} = \\log p(\\textbf{Y}| f_j) - \\log p(\\textbf{Y}| f_k) \\approx \\text{BIC}_k - \\text{BIC}_j\n\\]\n(Raftery 1995, 139) gives ranges for strength of evidence for one model over another based on the BIC difference:\n\n\\(0 \\le \\Delta\\text{BIC} \\le 2\\): weak evidence for \\(f_j\\) over \\(f_k\\).\n\\(2 \\le \\Delta\\text{BIC} \\le 6\\): positive evidence for \\(f_j\\) over \\(f_k\\).\n\\(6 \\le \\Delta\\text{BIC} \\le 10\\): strong evidence for \\(f_j\\) over \\(f_k\\).\n\\(\\Delta\\text{BIC} &gt; 10\\): very strong evidence for \\(f_j\\) over \\(f_k\\).\n\nIn the case of a uniform prior for the models, \\(p(f_j) = 1/K\\), the posterior probabilities can be approximated as \\[\np(f_j | \\textbf{Y}) \\approx \\frac{\\exp\\left\\{ -\\frac{1}{2} \\text{BIC}_j\\right\\}} {\\sum_{k=1}^K \\exp \\left\\{-\\frac{1}{2}\\text{BIC}_k\\right\\}}\n\\]\nWe now have the ingredients to define the Bayesian Model Averaging procedure for regression models.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Bayesian Model Averaging</span>"
    ]
  },
  {
    "objectID": "bma.html#bma-for-regression-models",
    "href": "bma.html#bma-for-regression-models",
    "title": "21  Bayesian Model Averaging",
    "section": "21.3 BMA for Regression Models",
    "text": "21.3 BMA for Regression Models\nSuppose we have \\(p\\) candidate input variables in a linear model family \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\). \\(2^p\\) possible models that can be formed based on the presence and absence of the \\(p\\) inputs. (This does not count interactions and transformations of the \\(p\\) variables.)\nIf the model errors are Gaussian distributed, we can calculate the BIC for each model and approximate posterior probabilities and Bayes factors. Instead of selecting one model as best, a larger number of models is evaluated and the models are ranked according to BIC. However, \\(2^p\\) is often larger than what we can and should accommodate, so the following selection rules apply: a reduced set of good candidate models is selected according to the leaps-and-bound algorithm. Within the set of models returned by the leaps-and-bound algorithm only those models are considered whose posterior probability is within a certain neighborhood of the model with the best BIC value. This neighborhood is called Occam’s window and is expressed as a ratio. For example, an Occam’s ratio of 20 means that models whose posterior probability is less than 1/20 of the posterior probability of the model with the best BIC are excluded.\nThe models within Occam’s window are the set of models over which predictions are averaged, weighted by their posterior probabilities. This shows us that BMA is an ensemble procedure. If there are \\(K\\) models within Occam’s window, the predicted value at \\(\\textbf{x}_0\\) is \\[\n\\widehat{y}(\\textbf{x}_0) = \\sum_{k=1}^K \\widehat{p}(f_k | \\textbf{Y}) \\, \\textbf{x}_{0k}^\\prime\\widehat{\\boldsymbol{\\beta}}_k\n\\] and \\(\\textbf{x}_{0k}\\) is the set of inputs in the \\(k\\)th model, \\(\\widehat{\\boldsymbol{\\beta}}_k\\) is the vector of coefficient estimates in the \\(k\\)th model.\n\n\nExample: Crime Rates Data\n\n\nThis example uses the UScrime data from the MASS library in R. The data set represents aggregate data on 47 states in the U.S. for 1960. In addition to the target variable, the rate of crimes per person in a particularly category, the data set contains information about income equality, GDP, population, education, unemployment, prison population, police expenditures, and demographics.\nBayesian model averaging for linear regression models can be performed with the bicreg function in the BMA library. bicreg uses the leaps all-subset algorithm to select variables into the models. By default, up to nbest=150 models of each size are being considered. nbest is not the total number of models being averaged, but the max number of models of any size returned by leaps.\nThe OR parameter specifies the maximum ratio for excluding models in Occam’s window. A model that is not at least within 1/OR of the BIC of the best model is excluded from consideration.\nbicreg uses the smaller-is-better formulation for BIC.\n\nlibrary(MASS)\nlibrary(BMA)\ndata(UScrime)\nx &lt;- UScrime[,-16]\ny &lt;- log(UScrime[,16])\nx[,-2]&lt;- log(x[,-2])  # log transform all except binary indicator in second col\n\ncrimeBMA &lt;- bicreg(x,y, OR=20)\n\nsummary(crimeBMA)\n\n\nCall:\nbicreg(x = x, y = y, OR = 20)\n\n\n  115  models were selected\n Best  5  models (cumulative posterior probability =  0.2039 ): \n\n           p!=0    EV        SD       model 1    model 2    model 3  \nIntercept  100.0  -23.45301  5.58897  -22.63715  -24.38362  -25.94554\nM           97.3    1.38103  0.53531    1.47803    1.51437    1.60455\nSo          11.7    0.01398  0.05640      .          .          .    \nEd         100.0    2.12101  0.52527    2.22117    2.38935    1.99973\nPo1         72.2    0.64849  0.46544    0.85244    0.91047    0.73577\nPo2         32.0    0.24735  0.43829      .          .          .    \nLF           6.0    0.01834  0.16242      .          .          .    \nM.F          7.0   -0.06285  0.46566      .          .          .    \nPop         30.1   -0.01862  0.03626      .          .          .    \nNW          88.0    0.08894  0.05089    0.10888    0.08456    0.11191\nU1          15.1   -0.03282  0.14586      .          .          .    \nU2          80.7    0.26761  0.19882    0.28874    0.32169    0.27422\nGDP         31.9    0.18726  0.34986      .          .        0.54105\nIneq       100.0    1.38180  0.33460    1.23775    1.23088    1.41942\nProb        99.2   -0.24962  0.09999   -0.31040   -0.19062   -0.29989\nTime        43.7   -0.12463  0.17627   -0.28659      .       -0.29682\n                                                                     \nnVar                                      8          7          9    \nr2                                      0.842      0.826      0.851  \nBIC                                   -55.91243  -55.36499  -54.69225\npost prob                               0.062      0.047      0.034  \n           model 4    model 5  \nIntercept  -22.80644  -24.50477\nM            1.26830    1.46061\nSo             .          .    \nEd           2.17788    2.39875\nPo1          0.98597      .    \nPo2            .        0.90689\nLF             .          .    \nM.F            .          .    \nPop         -0.05685      .    \nNW           0.09745    0.08534\nU1             .          .    \nU2           0.28054    0.32977\nGDP            .          .    \nIneq         1.32157    1.29370\nProb        -0.21636   -0.20614\nTime           .          .    \n                               \nnVar           8          7    \nr2           0.838      0.823  \nBIC        -54.60434  -54.40788\npost prob    0.032      0.029  \n\n\nOf the 150 models returned by the leaps algorithm, 115 were selected within Occam’s ratio of 1/20. The best model has a BIC of -55.9124 and a posterior probability of 0.0619. The posterior probability of the final model within Occam’s window is crimeBMA$postprob[115] = 0.0031.\nThe column EV displays the posterior means (expected values) of the model coefficients, averaged across the 115 models. The p!=0 column displays the proportion of models that include the particular input. For example, the intercept is included in all models, 100% of the models include the So variable.\nOnly five of the 115 models are shown on the output, information on all models can be retrieved from the return object of bicreg.\nThe \\(R^2\\) values of the models are also shown in the third row from the bottom of the output. Notice that the best model according to BIC does not have the highest \\(R^2\\) and the model with a higher \\(R^2\\) does not necessarily have a higher posterior probability.\nThe predict function computes statistics based on the posterior distribution of the model. The predicted values are calculated as the means of the posterior distribution, averaging all 115 models. Because we are computing summary statistics of the posterior distribution, standard deviation and quantiles are also immediately accessible.\nHere are the means, standard deviations, and 0.1, 0.5, 0.9 quantiles for the first 10 observations.\n\npredict(crimeBMA,x[1:10,])\n\n$mean\n       1        2        3        4        5        6        7        8 \n6.670113 7.353431 6.156241 7.682876 7.056718 6.493145 6.793854 7.280225 \n       9       10 \n6.618132 6.592609 \n\n$sd\n        1         2         3         4         5         6         7         8 \n0.2073859 0.2075745 0.2134277 0.2128672 0.2116684 0.2122920 0.2238612 0.2118520 \n        9        10 \n0.2069795 0.2070578 \n\n$quantiles\n        0.1      0.5      0.9\n1  6.401775 6.670140 6.938417\n2  7.084600 7.353481 7.622198\n3  5.872708 6.156290 6.439704\n4  7.400362 7.683734 7.964283\n5  6.777767 7.056512 7.335936\n6  6.212531 6.493102 6.773818\n7  6.484452 6.795024 7.101686\n8  7.000573 7.280314 7.559749\n9  6.350878 6.618084 6.885448\n10 6.325123 6.592608 6.860097\n\n\nTo validate the computation of a predicted value as a weighted average of predictions, where the weights are the posterior probabilities, we can extract the coefficients of the 115 models with crimeBMA$ols, multiply those into the \\(\\textbf{X}\\) matrix and compute the weighted average:\n\n# The predicted values for the first ten obs from all models\npred_1_10 &lt;- as.matrix(crimeBMA$ols) %*% as.matrix(t(cbind(1,x[1:10,])))\n# The final predicted value is weighted by the posterior probabilities\ncrimeBMA$postprob %*% pred_1_10\n\n            1        2        3        4        5        6        7        8\n[1,] 6.670113 7.353431 6.156241 7.682876 7.056718 6.493145 6.793854 7.280225\n            9       10\n[1,] 6.618132 6.592609\n\n\nThe plot function produces a series of plots of the posterior distributions for the coefficients (Figure 21.1). The height of the vertical line at zero indicates the probability that the coefficient is zero. The non-zero probability is scaled so that the maximum of the distribution equals the complement of the height of the line. Coefficients that appear in almost all models have a very short vertical line at zero.\n\n\n\n\nplot(crimeBMA,mfrow=c(2,3),include.intercept=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.1\n\n\n\nFigure 21.2 shows an image plot of the selected models. The order=probne0 option requests that the variables are shown in the order of their probability of inclusion in the model. Variables near the top of the image have the highest inclusion probability (appear in most or all models).\nThere are three colors in the image, one color for a positive coefficient (red), one color for a negative coefficient (blue), and one color indicating absence from the model. The width of the bars is proportional to the posterior probability of the models.\n\nimageplot.bma(crimeBMA,order=\"probne0\")\n\n\n\n\n\n\n\nFigure 21.2\n\n\n\n\n\n\n\n\n\n\n\nHinne, Max, Quentin F. Gronau, Don van den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3 (2): 200–215. https://doi.org/10.1177/2515245919898657.\n\n\nRaftery, Adrian E. 1995. “Bayesian Model Selection in Social Research.” Sociological Methodology 25: 111–63.",
    "crumbs": [
      "Part V. Ensemble Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Bayesian Model Averaging</span>"
    ]
  },
  {
    "objectID": "unsuper_intro.html",
    "href": "unsuper_intro.html",
    "title": "22  Introduction",
    "section": "",
    "text": "The methods discussed so far fall into the category of supervised learning, where a target variable \\(Y\\) is present and the goal is to predict or classify \\(Y\\) or to test hypothesis about \\(Y\\). The distinction between supervised and unsupervised methods of learning from data was introduced in Chapter 5.\nRecall that the term unsupervised learning seeks to evoke learning without a teacher who can judge the quality of an answer because the teacher is aware of the correct answer, the ground truth. In unsupervised learning there are no loss functions and no clear metrics on how to evaluate the quality of the analysis. The proliferation of method of unsupervised learning and the reliance on heuristics are a result of this condition.\nHowever, unsupervised learning is not a lesser form of learning from data and the questions it addresses can be more complex than estimating the mean of a random variable. In general, the data for unsupervised learning consists of \\(n\\) tuples \\([X_1, \\cdots, X_p]\\); because there is no target variable, the data structure is also referred to as unlabeled data. We are interested in discovering aspects of the joint distribution \\(p(X_1, \\cdots, X_p) = p(\\textbf{X})\\) (Figure 22.1). For example,\n\nare there low-dimensional regions where \\(p(\\textbf{X})\\) is large \\(\\rightarrow\\) principal component analysis.\nare there multiple regions in the \\(X\\)-space that contain modes of \\(p(\\textbf{X}) \\rightarrow\\) cluster analysis.\nare there values of \\(X_j\\) that are more likely to associate with values of \\(X_k\\) than would be expected under a random allocation \\(\\rightarrow\\) association analysis.\n\n\n\n\n\n\n\nFigure 22.1: An overview of statistical learning approaches\n\n\n\nBecause there is no target variable, unsupervised methods of learning rely less on model structures and probabilistic assumptions–with some exceptions such as model-based clustering. The field has more of an exploratory nature than supervised learning; that makes it less confirmatory and maybe more fun. For example, there are no strict rules on how to perform a hierarchical cluster analysis and different analysts will create different cluster assignments. Finding the optimal number \\(k\\) in \\(k\\)-means clustering escapes cross-validation because there is no loss function according to which errors can be measured.\n\n\n\nFigure 22.1: An overview of statistical learning approaches",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "",
    "text": "23.1 Introduction\nPrincipal component analysis (PCA) is one of the most important methods of unsupervised learning with many applications:\nGiven the many applications it is no surprise that PCA is a key method in the toolbox of data scientists and machine learners. It goes back to 1901, when it was invented by Karl Pearson.\nA principal component is a linear combination of the \\(p\\) inputs. The elements of the \\(j\\)th component score vector are calculated as follows: \\[\nz_{ij} = \\psi_{1j} x_{i1} + \\psi_{2j} x_{i2} + \\cdots + \\psi_{pj} x_{ip} \\quad i=1,\\cdots,n; j=1,\\cdots,p\n\\] The coefficients \\(\\psi_{kj}\\) are called the rotations or loadings of the principal components. In constructing the linear combination, the \\(x\\)s are always centered to have a mean of zero and are usually scaled to have standard deviation one. More on whether to scale the data for PCA follows below.\nThe scores \\(z_{ij}\\) are then collected into a score vector \\(\\textbf{z}_j = [z_{1j}, \\cdots, z_{nj}]\\) for the \\(j\\)th component. Note that for each observation \\(i\\) there are \\(p\\) inputs and \\(p\\) scores. So what have we gained? Instead of the \\(p\\) vectors \\(\\textbf{x}_1, \\cdots, \\textbf{x}_p\\) we now have the \\(p\\) vectors \\(\\textbf{z}_1, \\cdots, \\textbf{z}_p\\). Because of the way in which the \\(\\psi_{kj}\\) are found, the PCA scores have very special properties:\nThis is useful in many ways:\nAn alternative interpretation of the principal components is to find \\(p\\) vectors \\(\\textbf{z}_1,\\cdots,\\textbf{z}_p\\) as linear combinations of the \\(\\textbf{x}_1,\\cdots,\\textbf{x}_p\\) such that\nViewing the components as projections into perpendicular directions hints at one technique for calculating the rotation matrix \\(\\boldsymbol{\\Psi} = [\\psi_{kj}]\\) rotation and the variance decomposition: through a matrix factorization into eigenvalues and eigenvectors.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#introduction",
    "href": "pca.html#introduction",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "",
    "text": "Summarization: summarize a \\(p\\)-dimensional matrix \\(\\textbf{X}_{(n\\times p)}\\) through a lower-dimensional matrix \\(\\textbf{Z}_{(n\\times m)}\\). \\(m\\) is usually much smaller than \\(p\\), and in many cases \\(m=2\\), especially if the results are to be presented grahically.\nVisualization: Instead of all \\({p\\choose 2}\\) two-way scatterplots between \\(p\\) variables, display a single scatterplot of the first two principal components.\nDimension reduction: Use the first \\(m\\) principal components as inputs to a regression model (see PCR, Section 8.3.2). Apply a clustering algorithm such as \\(k\\)-means clustering to the first \\(m\\) principal components.\nImputation: Use principal components iteratively to complete a numerical matrix that contains missing values.\n\n\n\n\n\nThey have zero mean (if the data was centered):\\(\\sum_{i=1}^n z_{ij} = 0\\)\nThey are uncorrelated: \\(\\text{Corr}[z_j, z_k] = 0, \\forall j \\ne k\\)\n\\(\\text{Var}\\left[\\sum_{j=1}^p z_j\\right] = \\sum_{j=1}^p \\text{Var}[z_j] = p\\) (if data was scaled)\nThe components are ordered in terms of their variance \\(\\Rightarrow \\text{Var}[z_1] &gt; \\text{Var}[z_2] &gt; \\cdots &gt; \\text{Var}[z_p]\\)\n\n\n\nThe components provide a decomposition of the variability in the data. The first component explains the greatest proportion of the variability, the second component explains the next largest proportion of variability and so forth.\nThe contributions of the components are independent, each component is orthogonal to the other components. For example the second component is a linear combination of the \\(X\\)s that models a relationship not captured by the first (or any other) component.\nThe loadings for each component inform about the particular linear relationship. The magnitude of the loading coefficients reflects which inputs are driving the linear relationship. For example, when all coefficients are of similar magnitude, the component expresses some overall, average trend. When a few coefficients stand out it suggests that the associated attributes explain most of the variability captured by the component.\nTo summarize high-dimensional data, we can ignore those components that explain a small amount of variability and turn a high-dimensional problem into a lower-dimensional one. Often, only the first two principal components are used to visualize the data and for further analysis.\n\n\n\n\n\n\n\nPrincipal versus Principle.\n\n\n\n\n\nA principle refers to a fundamental law or rule, for example, “the principles of Christianity” or “the principle of judicial review”. You might agree with someone in principle, but disagree with their approach. A principal, on the other hand, refers to something head, chief, main, or leading. The principal of the school is the head of the school. They are leading the school. The principal reason for doing something is the main reason for doing it.\nPCA is principal component analysis because the leading components are the main accountants of variability.\n\n\n\n\n\n\\(\\textbf{z}_1\\) explains the most variability of the \\(X\\)s\n\\(\\textbf{z}_2\\) explains the second-most variability of the \\(X\\)s\n… and so forth\nand the \\(\\textbf{z}\\) are orthogonal: \\(\\textbf{z}_j^\\prime\\textbf{z}_k = 0, \\forall j,k\\).\n\\(\\textbf{z}_1\\) is a projection of the \\(X\\)s in the direction that explains the most variability in the data.\n\\(\\textbf{z}_2\\) is a projection of the \\(X\\)s in the direction that explains the second-most variability in the data.\nthe \\(\\textbf{z}_j\\) represent jointly perpendicular directions through the space of the original variables.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#eigenvalue-decomposition",
    "href": "pca.html#eigenvalue-decomposition",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.2 Eigenvalue Decomposition",
    "text": "23.2 Eigenvalue Decomposition\nThe PCA can be computed with a singular value decomposition (SVD) of the matrix \\(\\textbf{X}\\) or with an eigendecomposition of the covariance or correlation matrix of \\(\\textbf{X}\\). The SVD is numerically more stable than the eigendecomposition, but we can demonstrate the computations easily using eigenvalues and eigenvectors.\nThe eigendecomposition of a square \\(n \\times n\\) matrix \\(\\textbf{A}\\) is \\[\n\\textbf{A}= \\textbf{Q}\\boldsymbol{\\Lambda}\\textbf{Q}^{-1}\n\\] where \\(\\textbf{Q}\\) is the \\(n \\times n\\) matrix containing the eigenvectors of \\(\\textbf{A}\\) and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix with the eigenvalues of \\(\\textbf{A}\\) on the diagonal. If \\(\\textbf{A}\\) is symmetric, then \\(\\textbf{Q}\\) is orthogonal and \\(\\textbf{Q}^{-1} = \\textbf{Q}^\\prime\\). The eigendecomposition of a real symmetric matrix is thus\n\\[\n\\textbf{A}= \\textbf{Q}\\boldsymbol{\\Lambda}\\textbf{Q}^\\prime\n\\] How can we interpret eigenvectors and eigenvalues? The \\(n\\times 1\\) vector \\(\\textbf{q}\\) is an eigenvector of \\(\\textbf{A}\\), if it satisfies the linear equation \\[\n\\textbf{A}\\textbf{q} = \\lambda\\textbf{q}\n\\] for a scalar \\(\\lambda\\). This scalar is called the eigenvalue corresponding to \\(\\textbf{q}\\). There are \\(n\\) eigenvectors and when they are arranged as the columns of a matrix you get \\(\\textbf{Q}\\).\n\n\nExample: Eigenanalysis of Correlation Matrix\n\n\nIn R you can obtain the eigenvectors and eigenvalues of a square matrix with the eigen function. Principal components can be computed with the prcomp or the princomp functions, both included in the base stats package. prcomp uses the singular-value decomposition of \\(\\textbf{X}\\), princomp relies on the eigendecomposition of the covariance matrix.\nThe following data were used to illustrate PCA scores in Section 8.3.2.\n\ndata &lt;- data.frame(\"x1\"=c(0.344167 ,0.363478,0.196364,0.2     ,0.226957,\n                          0.204348 ,0.196522,0.165   ,0.138333,0.150833),\n                   \"x2\"=c(0.363625 ,0.353739,0.189405,0.212122,0.22927 ,\n                          0.233209 ,0.208839,0.162254,0.116175,0.150888),\n                   \"x3\"=c(0.805833 ,0.696087,0.437273,0.590435,0.436957,\n                          0.518261 ,0.498696,0.535833,0.434167,0.482917),\n                   \"x4\"=c(0.160446 ,0.248539,0.248309,0.160296,0.1869  ,\n                          0.0895652,0.168726,0.266804,0.36195 ,0.223267)\n                   )\ndata\n\n         x1       x2       x3        x4\n1  0.344167 0.363625 0.805833 0.1604460\n2  0.363478 0.353739 0.696087 0.2485390\n3  0.196364 0.189405 0.437273 0.2483090\n4  0.200000 0.212122 0.590435 0.1602960\n5  0.226957 0.229270 0.436957 0.1869000\n6  0.204348 0.233209 0.518261 0.0895652\n7  0.196522 0.208839 0.498696 0.1687260\n8  0.165000 0.162254 0.535833 0.2668040\n9  0.138333 0.116175 0.434167 0.3619500\n10 0.150833 0.150888 0.482917 0.2232670\n\n\nThere are ten observations of four variables. Let’s take as input for the eigendecomposition the correlation matrix of the data:\n\nc &lt;- cor(data)\nc\n\n           x1         x2         x3         x4\nx1  1.0000000  0.9832580  0.8359297 -0.2761887\nx2  0.9832580  1.0000000  0.8539085 -0.4397088\nx3  0.8359297  0.8539085  1.0000000 -0.2890121\nx4 -0.2761887 -0.4397088 -0.2890121  1.0000000\n\ne &lt;- eigen(c)\ne$values \n\n[1] 2.9570820972 0.8438197077 0.1985106437 0.0005875513\n\ne$vectors\n\n           [,1]       [,2]        [,3]        [,4]\n[1,] -0.5550620 0.23539598  0.45961271  0.65211273\n[2,] -0.5741828 0.05247306  0.33623520 -0.74465196\n[3,] -0.5297817 0.20752636 -0.82067111  0.05256496\n[4,]  0.2855723 0.94803382  0.04691458 -0.13220959\n\n\nThe eigen function returns the diagonal elements of \\(\\boldsymbol{\\Lambda}\\) in e$values and the matrix \\(\\textbf{Q}\\) of eigenvectors in e$vectors. The input matrix of the decomposition can be reconstructed from those:\n\nA &lt;- e$vectors %*% diag(e$values) %*% t(e$vectors)\nround((c - A),4)\n\n   x1 x2 x3 x4\nx1  0  0  0  0\nx2  0  0  0  0\nx3  0  0  0  0\nx4  0  0  0  0\n\n\nWhat happens if we multiply the \\(10 \\times 4\\) data matrix with the \\(4\\times 4\\) matrix of eigenvectors of the correlation matrix? First, we center and scale the data. The result is a \\(10 \\times 4\\) matrix of linear transformations. These are the scores of the PCA.\n\ndata_c &lt;- scale(data,center=TRUE,scale=TRUE)\nscores &lt;- data_c %*% e$vectors\nscores\n\n            [,1]       [,2]        [,3]          [,4]\n [1,] -3.2478623  0.2815532 -0.43998889 -0.0292581389\n [2,] -2.5101127  1.2574623  0.42484650  0.0254701453\n [3,]  0.9925370  0.1935176  0.46544557 -0.0005376776\n [4,] -0.1908526 -0.6286757 -0.49817675  0.0413147515\n [5,]  0.2550067 -0.4593212  0.77973866  0.0014573403\n [6,] -0.3285776 -1.6137133  0.05490332 -0.0226596733\n [7,]  0.2862026 -0.6907719  0.08654684 -0.0123166161\n [8,]  1.0581238  0.4785600 -0.48496730 -0.0088830663\n [9,]  2.3797940  1.3913628 -0.09775326 -0.0229367993\n[10,]  1.3057412 -0.2099739 -0.29059469  0.0283497342\n\n\nThe scores have mean 0, their variances equal the eigenvalues of the correlation matrix, their variances sum to \\(p=4\\), and they are uncorrelated.\n\napply(scores,2,mean)\n\n[1]  8.881784e-17 -4.690692e-16 -3.330669e-17 -1.134509e-16\n\napply(scores,2,var)\n\n[1] 2.9570820972 0.8438197077 0.1985106437 0.0005875513\n\nsum(apply(scores,2,var))\n\n[1] 4\n\nround(cor(scores),4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#computing-the-pca-in-r",
    "href": "pca.html#computing-the-pca-in-r",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.3 Computing the PCA in R",
    "text": "23.3 Computing the PCA in R\nWe prefer the prcomp function to compute the PCA over the princomp function because it is based on the singular value decomposition, which is numerically more stable than the eigendecomposition. (Singular values are the square roots of the eigenvalues and since eigenvalues can be very close to zero working with square roots stabilizes the calculations.)\nBy default, prcomp centers the data but does not scale it. Add scale.=TRUE to scale the data as well.\n\npca &lt;- prcomp(data,retx=TRUE,scale.=TRUE)\npca\n\nStandard deviations (1, .., p=4):\n[1] 1.71961685 0.91859660 0.44554533 0.02423946\n\nRotation (n x k) = (4 x 4):\n          PC1        PC2         PC3         PC4\nx1 -0.5550620 0.23539598  0.45961271 -0.65211273\nx2 -0.5741828 0.05247306  0.33623520  0.74465196\nx3 -0.5297817 0.20752636 -0.82067111 -0.05256496\nx4  0.2855723 0.94803382  0.04691458  0.13220959\n\n\nprcomp reports the standard deviations (pca$dev) and rotation matrix \\(\\boldsymbol{\\Psi}\\) (pca$rotation). You will recognize the rotation as the matrix of eigenvectors from the previous analysis and the standard deviations as the square roots of the variances of the scores computed earlier. When retx=TRUE is specified, the function returns the PCA scores in pca$x.\nThe sum of the variances of the principal components is \\(p=4\\) and it can be used to break down the variance into proportions explained by the components.\n\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7196 0.9186 0.44555 0.02424\nProportion of Variance 0.7393 0.2109 0.04963 0.00015\nCumulative Proportion  0.7393 0.9502 0.99985 1.00000\n\n\nThe first principal component, with somewhat equal loading coefficients for \\(X_1\\), \\(X_2\\), and \\(X_3\\), explains 73.927% of the overall variance. The second component, which is dominated by \\(X_4\\), explains an additional 21.095% of the variance.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#predicting-in-a-pca",
    "href": "pca.html#predicting-in-a-pca",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.4 Predicting in a PCA",
    "text": "23.4 Predicting in a PCA\nYou can predict a new observation using principal components by calculating the component scores for the new obs. For example, suppose the input values for the new data point are the square roots of the values for the first observation:\n\nxx &lt;- sqrt(data[1,]) # the \"new\" observation\npredict(pca, newdata=xx)\n\n        PC1      PC2      PC3       PC4\n1 -6.202193 4.362812 1.553425 0.5424985\n\n\nThe predicted value is a vector, with one element for each of the components. To reconstruct the predicted value, we can apply the rotation matrix to the new observation. Before doing so, the values need to be centered and scaled to match the derivation of the rotations. It is thus important to apply the same centering and scaling values that were used on the training data:\n\nxx_centered_and_scaled &lt;- (xx - pca$center)/pca$scale\nas.matrix(xx_centered_and_scaled) %*% pca$rotation\n\n        PC1      PC2      PC3       PC4\n1 -6.202193 4.362812 1.553425 0.5424985",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#scree-plot",
    "href": "pca.html#scree-plot",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.5 Scree Plot",
    "text": "23.5 Scree Plot\nHow should we choose the number of principal components to use in subsequent analyses? If the goal is summarization or visualization \\(m=2\\) is common. If the components are input to a regression model, cross-validation can be used in PCR (Section 8.3.2). In other situations you have to choose \\(m\\) such that a “fair” amount of variation is explained. To help with that, a scree plot is commonly used. Scree describes the small rock material that accumulates at the base of a mountain or steep rock formation (Figure 23.1). The scree forms a sharp transition from the rocky cliff to the area of the rock fragments.\n\n\n\n\n\n\nFigure 23.1: Scree at the base of a mountain. Source: Wikipedia\n\n\n\nIn principal component analysis the scree plot displays the proprtion of variance explained against the number of components.\n\n\nExample: Hitters Data (ISLR2)\n\n\nFor the Hitters baseball data set from James et al. (2021), the PCA scree plot is obtained as follows:\n\nlibrary(ISLR2)\n\npca_hit &lt;- prcomp(Hitters[,1:13],.scale=TRUE) \n\nplot(summary(pca_hit)$importance[2,],\n     type=\"b\",\n     ylab=\"Proportion of Variance Explained\",\n     xlab=\"Principal Component\",\n     las=1)\n\n\n\n\nScree plot for first thirteen attributes of Hitters data.\n\n\n\n\nThe sharp “knee” (“elbow”) at \\(m=2\\) shows that most of the variability is captured by the first principal component.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#biplot",
    "href": "pca.html#biplot",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.6 Biplot",
    "text": "23.6 Biplot\nThe biplot of a PCA displays the scores and loadings for two principal components in the same visualization. Typically, it is produced using the first two components as these account for the most variability. The scores \\(z_{ij}\\) are shown in the biplot as a scatter, the rotations are displayed as arrows. Points that are close to each other on the biplot have similar values of the inputs. The length of an arrow depicts the contribution of the variable, long arrows correspond to large coefficients in the rotation matrix. The angle of the arrows shows the correlation with other variables:\n\nuncorrelated variables have perpendicular arrows\nhighly correlated variables have arrows that point in the same direction\n\n\n\nExample: PCA for Iris Data\n\n\nFor the Iris data, the first two principal component account for 95% of the variability in the four flower measurements.\n\npca_iris &lt;- prcomp(iris[,1:4],retx=TRUE,scale.=TRUE)\npca_iris\n\nStandard deviations (1, .., p=4):\n[1] 1.7083611 0.9560494 0.3830886 0.1439265\n\nRotation (n x k) = (4 x 4):\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nsummary(pca_iris)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nThe raw data for the biplot (Figure 23.2) of the first two components are the first two columns of the rotation matrix and the first two columns of the score matrix:\n\npca_iris$rotation[,1:2]\n\n                    PC1         PC2\nSepal.Length  0.5210659 -0.37741762\nSepal.Width  -0.2693474 -0.92329566\nPetal.Length  0.5804131 -0.02449161\nPetal.Width   0.5648565 -0.06694199\n\npca_iris$x[1:20,1:2]\n\n            PC1         PC2\n [1,] -2.257141 -0.47842383\n [2,] -2.074013  0.67188269\n [3,] -2.356335  0.34076642\n [4,] -2.291707  0.59539986\n [5,] -2.381863 -0.64467566\n [6,] -2.068701 -1.48420530\n [7,] -2.435868 -0.04748512\n [8,] -2.225392 -0.22240300\n [9,] -2.326845  1.11160370\n[10,] -2.177035  0.46744757\n[11,] -2.159077 -1.04020587\n[12,] -2.318364 -0.13263400\n[13,] -2.211044  0.72624318\n[14,] -2.624309  0.95829635\n[15,] -2.191399 -1.85384655\n[16,] -2.254661 -2.67731523\n[17,] -2.200217 -1.47865573\n[18,] -2.183036 -0.48720613\n[19,] -1.892233 -1.40032757\n[20,] -2.335545 -1.12408360\n\n\n\nbiplot(pca_iris,cex=0.75)\n\n\n\n\n\n\n\nFigure 23.2: Biplot for first two principal components of Iris data.\n\n\n\n\n\nThere are two systems of axes on the biplot. The axes on the bottom and on the left are for the scores of the principal components. The axes on the right and on the top are for the variable arrows (the rotations). Note that the Sepal.Width rotation for the first two components is [-0.2693474, -0.9232957] but the arrow does not point at that coordinate. The biplot function applies a scaling to the scores and the arrows, which you can suppress by adding scale=0 to the function call.\nWe see from the biplot that the scores cluster in distinct groups. The arrow for Sepal.Width is pointing away from the arrows of the other variables, which suggests that it is not highly correlated with the other flower measurements. The length and width of the petals are very highly correlated, however; their arrows point into nearly the same direction. This can be verified by computing the correlation matrix of the measurements:\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nUsing the autoplot function in the ggfortify library allows you to color the scores in the biplot by another variable. We see that the distinct group of scores low in PC1 correspond to the Iris setosa species.\n\nlibrary(ggfortify)\nautoplot(pca_iris, data=iris, color=\"Species\", \n        loadings=TRUE, loadings.label=TRUE)\n\n\n\n\n\n\n\n\nI. setosa has large sepal widths and the negative rotation coefficient for Sepal.Width in PC1 separates the species in that dimension from the other species.\n\nlibrary(dplyr)\niris %&gt;% group_by(Species) %&gt;% \n         summarize(min=min(Sepal.Width), \n                   max=max(Sepal.Width),\n                   mean=mean(Sepal.Width))\n\n# A tibble: 3 × 4\n  Species      min   max  mean\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 setosa       2.3   4.4  3.43\n2 versicolor   2     3.4  2.77\n3 virginica    2.2   3.8  2.97\n\n\n\n\nLooking at the sign of the rotation coefficients is helpful to determine which input variables increase or decrease a component compared to other inputs. However, you cannot interpret the sign as increasing or decreasing a target variable. The rotation matrix of a principal component analysis is unique up to sign. Two software packages might produce the same rotation matrices, but with different signs of the coefficients. The princomp function in R has a fix_sign= option that controls the sign of the first loading coefficient to be positive.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#to-scale-or-not-to-scale",
    "href": "pca.html#to-scale-or-not-to-scale",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.7 To Scale or Not To Scale",
    "text": "23.7 To Scale or Not To Scale\nThe variables should always be centered to have mean 0, so that the loadings have the same origin. Should the variables also be scaled to have the same variance?\nIn general, scaling is recommended whenever the results of an analysis depends on units of measurements. For example, when calculating distances it matters whether time is measured in days, hours, minutes, or seconds. The choice of units should not affect the outcome of the analysis. Similarly, large things tend to have greater variability than small things. Since the principal components are arranged in the order of variability explained, variables that have very large variances (because of their units of measurement or size) will dominate the components unless scaling is applied.\n\n\nExample: U.S. Arrests\n\n\nThe USArrests data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 U.S. states in 1973. Also given is the percent of the population living in urban areas. The data set comes with R.\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\napply(USArrests,2,sd)\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nThe standard deviations of the four variables is quite different, that of Assault is 20-times larger than that of the Murder variable.\nWe now perform PCA without and with scaling of the data.\n\npca_unscaled &lt;- prcomp(USArrests,retx=TRUE,scale.=FALSE)\npca_unscaled\n\nStandard deviations (1, .., p=4):\n[1] 83.732400 14.212402  6.489426  2.482790\n\nRotation (n x k) = (4 x 4):\n                PC1         PC2         PC3         PC4\nMurder   0.04170432 -0.04482166  0.07989066 -0.99492173\nAssault  0.99522128 -0.05876003 -0.06756974  0.03893830\nUrbanPop 0.04633575  0.97685748 -0.20054629 -0.05816914\nRape     0.07515550  0.20071807  0.97408059  0.07232502\n\nsummary(pca_unscaled)\n\nImportance of components:\n                           PC1      PC2    PC3     PC4\nStandard deviation     83.7324 14.21240 6.4894 2.48279\nProportion of Variance  0.9655  0.02782 0.0058 0.00085\nCumulative Proportion   0.9655  0.99335 0.9991 1.00000\n\n\nThe first principal component in the unscaled analysis explains 96.55% of the variability. The component is dominated by the Assault variable, with a loading coefficient of 0.9952.\n\npca_scaled &lt;- prcomp(USArrests,retx=TRUE,scale.=TRUE)\npca_scaled\n\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\nsummary(pca_scaled)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000\n\n\nIn the scaled analysis the variables are on equal footing with respect to their dispersion. The loading coefficients are more evenly sized in the first principal component which explains 62.01% of variability in the scaled analysis. The Assault variable is not allowed to dominate the component analysis because of its large variance compared to the other variables.\nThe biplots for the scaled and unscaled analysis are markedly different. The dominance of Assault in the unscaled analysis makes it difficult (impossible) to meaningfully interpret the biplot (Figure 23.3).\n\npar(mfrow=c(1,2))\npar(mar = c(1, 2, 1, 2))\nbiplot(pca_unscaled,cex=0.6)\nbiplot(pca_scaled,cex=0.6)\n\n\n\n\n\n\n\nFigure 23.3: Biplots in unscaled (left) and scaled (right) analysis.\n\n\n\n\n\n\n\nIn the next section we encounter a situation where we center but do not scale the \\(\\textbf{X}\\) matrix, when values of the inputs are reconstructed.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "pca.html#imputation-through-matrix-completion",
    "href": "pca.html#imputation-through-matrix-completion",
    "title": "23  Principal Component Analysis (PCA)",
    "section": "23.8 Imputation through Matrix Completion",
    "text": "23.8 Imputation through Matrix Completion\nIn the introduction to this chapter we expressed the PCA scores as a linear function of the inputs: \\[\nz_{ij} = \\psi_{1j} x_{i1} + \\psi_{2j} x_{i2} + \\cdots + \\psi_{pj} x_{ip} = \\sum_{k=1}^p \\psi_{kj}x_{ik}\n\\]\nThis relationship can be reversed, expressing the inputs as a linear combination of the PCA scores and the rotations: \\[\nx_{ij} = \\sum_{k=1}^p z_{ik}\\psi_{kj}\n\\]\nTo demonstrate, we can reconstruct the values for the first observation for the Iris data set from its PCA results. First, we only center the data and do not scale:\n\npca_iris_ns &lt;- prcomp(iris[,1:4],retx=TRUE,scale.=FALSE)\npca_iris_ns$rotation\n##                      PC1         PC2         PC3        PC4\n## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\n## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\n## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\n## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\nsum(pca_iris_ns$x[1,] * pca_iris_ns$rotation[1,]) + pca_iris_ns$center[1]\n## Sepal.Length \n##          5.1\nsum(pca_iris_ns$x[1,] * pca_iris_ns$rotation[2,]) + pca_iris_ns$center[2]\n## Sepal.Width \n##         3.5\nsum(pca_iris_ns$x[1,] * pca_iris_ns$rotation[3,]) + pca_iris_ns$center[3]\n## Petal.Length \n##          1.4\nsum(pca_iris_ns$x[1,] * pca_iris_ns$rotation[4,]) + pca_iris_ns$center[4]\n## Petal.Width \n##         0.2\n\nIf the PCA was centered and scaled, the reconstruction of the variables needs to take the scaling factor into account:\n\nsum(pca_iris$x[1,] * pca_iris$rotation[1,])*pca_iris$scale[1] + pca_iris$center[1]\n## Sepal.Length \n##          5.1\nsum(pca_iris$x[1,] * pca_iris$rotation[2,])*pca_iris$scale[2] + pca_iris$center[2]\n## Sepal.Width \n##         3.5\nsum(pca_iris$x[1,] * pca_iris$rotation[3,])*pca_iris$scale[3] + pca_iris$center[3]\n## Petal.Length \n##          1.4\nsum(pca_iris$x[1,] * pca_iris$rotation[4,])*pca_iris$scale[4] + pca_iris$center[4]\n## Petal.Width \n##         0.2\n\nThese calculations match the first observation:\n\niris[1,]\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n\n\nIf only a subset of the principal components is used, we obtain an approximation of the input value. If \\(M &lt; p\\), then\n\\[\nx^*_{ij} = \\sum_{k=1}^M z_{ik}\\psi_{kj} \\approx  x_{ij}\n\\] In fact, \\(x^*_{ij}\\) is the best \\(M\\)-dimensional approximation to \\(x_{ij}\\) in terms of having the smallest Euclidean distance.\nThe sepal width of the first observation can be approximated based on the first two principal components as\n\nsum(pca_iris_ns$x[1,] * pca_iris_ns$rotation[2,1:2]) + pca_iris_ns$center[2]\n\nSepal.Width \n   3.513403 \n\n\nIf principal component analysis can be used to approximate the observed input values, then it can be used to predict unobserved values. This is the idea behind using PCA to impute missing values, a technique known as matrix completion. The advantage of using PCA to fill in missing values in matrices is to draw on the correlations between the variables, as compared to imputing missing values by column means, which ignores information from other variables.\nIt is important to point out, however, that this form of imputation is only appropriate if the missingness process is missing completely at random (MCAR): the missingness is unrelated to any study variable, including the target variable. For example, if a measurement cannot be taken because an instrument randomly stopped functioning, the missing value occurred completely at random. If the measurement cannot be taken because the instrument cannot record objects of that size, then the process is not MCAR.\nAn iterative algorithm for matrix completion is described in $12.3 of James et al. (2021) and implemented in the eimpute library in R. The algorithm starts with imputing missing values with the means of the observed data and iterates the following steps:\n\nPerform a low-rank PCA, that is, a PCA with \\(M &lt; p\\).\nReplace the unobserved values with approximations based on the PCA scores and rotation.\nCompute the mean squared error between observed \\(x_{ij}\\) and predicted \\(x^*_{ij}\\). As long as the mean squared error continues to decrease, return to step 1.\n\nFigure 23.4 displays observed and imputed values for the Iris data set when 20% of the observations are randomly set to missing (by assigning NA). Two methods of imputation are used: using the column means, and a low-rank PCA approximation with 2 principal components using eimpute(x=iris_miss,r=2).\nThere are only four possible imputed values when using the column means, all missing values for a particular variable are replaced with the same column mean. The PCA imputation produces much more variety in the imputed values since it draws on the correlations between the variables.\nBecause missing values were introduced artificially, we have access to the true values and can compute the correlation between original and imputed values. This correlation is 0.83 for the imputation based on column means and 0.93 for the imputation based on the PCA.\n\n\n\n\n\n\n\n\n\nFigure 23.4: Comparison of original and imputed values for Iris data with 20% missingness.\n\n\n\n\n\n\n\n\nFigure 23.2: Biplot for first two principal components of Iris data.\nFigure 23.3: Biplots in unscaled (left) and scaled (right) analysis.\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "24  Cluster Analysis",
    "section": "",
    "text": "24.1 Introduction\nThe term cluster appears throughout data analytics in different contexts. In the analysis of correlated data a cluster is a group of observations that belong together and group membership is known a priori. For example, a subsample that is drawn from a larger sampling unit creates a hierarchy of sampling units. The longitudinal observations collected on a subject over time form a cluster of subject-specific data. The data from different subjects might be independent while the longitudinal observations within a cluster (a subject) are correlated.\nIn unsupervised learning, a cluster is a group of observations that are somehow similar. Group membership is not known a priori and determining membership as well as the number of clusters is part of the analysis. Examples are\nCluster analysis seeks to find groups of data such that members within a group are similar to each other and members from different groups are dissimilar. It is an unsupervised method, there is no target variable, we simply are trying to find structure in the data. The number of clusters can be set a priori, for example in \\(k\\)-means clustering, or be determined as part of analysis, as in hierarchical clustering.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html#introduction",
    "href": "clustering.html#introduction",
    "title": "24  Cluster Analysis",
    "section": "",
    "text": "Students that are similar with respect to STEM achievement scores\nReal estate properties that share similar property attributes\nOnline shoppers with similar browsing and purchase history\n\n\n\n\n\n\n\n\nClustering Rows or Columns\n\n\n\nNote that we are looking for groups of “data”, we did not specify whether clustering applies to finding similar observations or similar features. Usually it is the former, and clustering columns versus rows can be performed by simply transposing the data. From here on we assume that clustering is seeking similar groups of observations.\n\n\n\n\n\n\n\n\nScaling and Centering\n\n\n\nKey to all clustering methods is some notion of similarity–or the opposite, dissimilarity–of data points. Measures of similarity (or dissimilarity) depend on a metric expressing distance. Squared Euclidean distance is a common choice, but other metrics such as the Manhattan (city-block) distance, correlation distance, or Gower’s distance are also important. Many distance measures depend on the units of measurement; variables with large values tend to dominate the distance calculations. It is highly recommended to scale data prior to cluster analysis to put features on equal footing.\nScaling is often not applied to binary variables, for example, variables that result from coding factors as a series of 0/1 variables.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html#k-means-clustering",
    "href": "clustering.html#k-means-clustering",
    "title": "24  Cluster Analysis",
    "section": "24.2 \\(K\\)-Means Clustering",
    "text": "24.2 \\(K\\)-Means Clustering\n\nIntroduction\n\\(K\\)-means clustering is an intuitive method to cluster \\(p\\) numeric input variables. The value \\(K\\) is the number of clusters and is set a priori. If you perform a \\(3\\)-means analysis, the algorithm will assign all observations to one of three clusters. If you perform a \\(100\\)-means analysis, the algorithm will assign all observations to one of 100 clusters. Choosing the appropriate number of clusters for a data set uses scree plots similar to choosing the number of components in principal component analysis.\nThe \\(K\\)-means algorithm has the following properties:\n\nThe analysis leads to \\(K\\) clusters\nEvery observation belongs to exactly one cluster\nNo observation belongs to more than one cluster\n\nFinding the optimal partitioning of \\(n\\) observations into \\(K\\) groups is a formidable computational problem, there are approximately \\(K^n\\) ways of partitioning the data. However, efficient algorithms exist to find at least a local solution to the global partitioning problem.\nTo introduce some notation for \\(K\\)-means clustering, let \\(C_i\\) denote the set of observations assigned to cluster \\(i=1,\\cdots,K\\). The \\(K\\)-means properties imply that\n\n\\(C_1 \\cup C_2 \\cup \\cdots \\cup C_K = \\{1,\\cdots, n\\}\\)\n\\(C_i \\cap C_j = \\emptyset\\) if \\(i \\neq j\\)\n\nThe number of observations in cluster \\(i\\) is called its cardinality, denoted \\(|C_i|\\).\nIf squared Euclidean distance is the dissimilarity measure of choice, the distance between two data points is \\[\nd(\\textbf{x}_i,\\textbf{x}_j) = ||\\textbf{x}_i - \\textbf{x}_j||_2^2 = \\sum_{m=1}^p \\left ( x_{im} - x_{jm} \\right )^2\n\\] The within-cluster variation in cluster \\(k\\) is the average dissimilarity of the observations in \\(C_k\\): \\[\nW(C_k) = \\frac{1}{|C_k|} \\sum_{i,j \\in C_k} ||\\textbf{x}_i - \\textbf{x}_j||_2^2\n\\] Let \\(\\overline{\\textbf{x}}_k = [\\overline{x}_{1k},\\cdots,\\overline{x}_{pk}]\\) be the vector of means of the inputs in the \\(k\\)th cluster. Finding the \\(K\\)-means solution requires to find the cluster allocations such that \\[\n\\min_{C_1,\\cdots, C_k} \\left \\{ \\sum_{k=1}^K W(C_k) \\right \\} \\Longleftrightarrow\n\\min_{C_1,\\cdots, C_k} \\left \\{ \\sum_{k=1}^K \\sum_{i \\in C_k} ||\\textbf{x}_i - \\overline{\\textbf{x}}_k||_2 ^2 \\right \\}\n\\]\nThis states that the cluster assignment that minimizes the sum of the within-cluster dissimilarity is the same assignment that minimizes the distances of data points from the cluster centroid. This is how \\(K\\)-means clustering gets its name; the cluster centroids are computed as the mean of the observations assigned to the cluster.\nThe within-cluster sum of squares is the sum of the squared distances between the data points in a cluster and the cluster centroid. For cluster \\(k\\) this sum of squares is \\[\n\\text{SSW}_k = \\frac{1}{2} W(C_k) = \\sum_{i \\in C_k} ||\\textbf{x}_i - \\overline{\\textbf{x}}_k||_2 ^2\n\\] This quantity is also called the inertia of the cluster. The average inertia, \\[\n\\frac{1}{|C_k|} \\text{SSW}_k\n\\] is called the distortion of cluster \\(k\\).\nA (local) solution is found by iterating from an initial cluster assignment: given cluster centroids \\(\\overline{\\textbf{x}}_k\\) assign each observation to the cluster whose center is closest. Following the assignment recompute the centers. Continue until the cluster assignment no longer changes. At the local solution no movement of a data point from one cluster to another will reduce the within-cluster sum of squares (Hartigan and Wong 1979).\nThe initial cluster assignment is done by either assigning observations randomly to the \\(k\\) clusters or by using \\(k\\) randomly chosen observations as the initial cluster centroids.\nBecause of this random element, and because the algorithm is not guaranteed to find a global solution, \\(K\\)-means is typically run with multiple random starts, and the best solution is reported.\n\n\nExample: \\(K\\)-Means for Iris Data\n\n\nTo show the basic calculations in \\(K\\)-means analysis, let’s first look at the familiar Iris data set. We have the luxury of knowing that the data set comprises three species, a \\(3\\)-means analysis of the flower measurements should be interesting: does it recover the iris species?\nThe kmeans function in R performs the \\(K\\)-means analysis. By default, it uses he algorithm of Hartigan and Wong (1979) with a single random start for the initial cluster assignment. Set nstart= to a larger number to increase the number of random starts. Because there are four inputs, Sepal.Length, Sepal.Width. Petal.Length, and Petal.Width, each observation and the centroids live in 4-dimensional space.\n\nset.seed(1234)\niris_s &lt;- scale(iris[,1:4])\nkm &lt;- kmeans(iris_s,centers=3,nstart=50)\n\nkm$size\n\n[1] 50 53 47\n\nkm$centers\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  -1.01119138  0.85041372   -1.3006301  -1.2507035\n2  -0.05005221 -0.88042696    0.3465767   0.2805873\n3   1.13217737  0.08812645    0.9928284   1.0141287\n\n\nThe algorithm finds three clusters of sizes 50, 53, and 47. The centroid of the first cluster is at coordinate [-1.0112, 0.8504, -1.3006, -1.2507].\nThe breakdown of the dissimilarities, the squared distances, in the data set is as follows.\n\nkm$totss\n## [1] 596\nkm$betweenss\n## [1] 457.1116\nkm$tot.withinss\n## [1] 138.8884\nkm$withinss\n## [1] 47.35062 44.08754 47.45019\n(km$tot.withinss/km$totss)*100\n## [1] 23.30342\n\nThe total sum of squares does not depend on the number of clusters. For \\(K=3\\), it is allocated to 138.8884 sum of squares units within the clusters and 457.1116 between the clusters.\nThe within-cluster sum of squares measures the average squared Euclidean distance between the points in a cluster and the cluster centroid. We can validate for any of the clusters as follows\n\nwithinss &lt;- function(x, center) {\n    tmp &lt;- sapply(seq_len(nrow(x)),function(i) sum((x[i,]-center)^2))\n    return (sum(tmp))\n}\nfor (i in 1:3) {\n    print(withinss(iris_s[km$cluster==i,],km$center[i,]))\n}\n\n[1] 47.35062\n[1] 44.08754\n[1] 47.45019\n\n\nThe distortions of the clusters are obtained by dividing the within-cluster sum of squares with the cluster sizes:\n\nkm$withinss / km$size\n\n[1] 0.9470124 0.8318405 1.0095786\n\n\nFigure 24.1 shows the cluster assignment in a bivariate plot of two of the flower measurements. The colored cluster symbols are overlaid with the species. The three clusters track the species fairly well, in particular I. setosa. The boundaries of the other two clusters align fairly well with species, but there is considerable overlap.\n\n\n\n\n\n\n\n\nFigure 24.1: Results of 3-means clustering for Iris data. Clusters are identified through colors, species are identified with plotting symbols.\n\n\n\n\n\nThe separation of these clusters is probably better than Figure 24.1 suggests, because two dimensions (Petal.Width and Petal.Length) are not represented in the figure.\n\npsym &lt;- ifelse(iris$Species==\"setosa\", 1, \n               ifelse(iris$Species==\"versicolor\" ,2,3))\ncm &lt;- caret::confusionMatrix(as.factor(km$cluster),as.factor(psym))\nround(cm$overall,4)\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n        0.8333         0.7500         0.7639         0.8891         0.3333 \nAccuracyPValue  McnemarPValue \n        0.0000            NaN \n\n\nThe confusion matrix between species and cluster assignment has an accuracy of 83.3333%.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\\(K\\)-means analysis is generally susceptible to outliers, as they contribute large distances. Also, \\(K\\)-means analysis is sensitive to perturbations of the data; when observations are added or deleted the results will change. Finally, \\(K\\)-means is affected by the curse of dimensionality (@#sec-curse-dimensionality).\n\n\n\n\nClustering Metrics\nTo choose the appropriate number of clusters in \\(K\\)-means clustering, we can apply various metrics that measure the tightness of the clusters and their separation. These metrics are plotted against the value of \\(k\\) in a scree plot. We do not look for a minimum of the criteria, but the “knee” or “elbow” where the increase/decrease of the metric abruptly changes.\nThe following criteria are commonly computed and plotted.\n\nInertia: this is the within-cluster sum of squares and measures the tightness of the clusters. It does not necessarily mean that clusters are well separated, it just means that the data points within the clusters are close to their centroid. The within-cluster sum of squares decreases as \\(K\\) increases, more clusters will lead to less variability within the clusters. That is why we do not look for minima with these criteria.\nDistortion: this is the average inertia within a cluster, obtained by dividing \\(\\text{SSW}_k\\) by the cluster cardinality.\nSilhouette score: measures how similar a data point is to its own cluster compared to other clusters. While inertia is based only on distances of data points from their cluster center, the silhouette takes into account the distances between points in one cluster and the nearest cluster center. The score ranges from \\(-1\\) to \\(+1\\); a high silhouette score means that we can easily tell the clusters apart–they are far from each other.\n\nInertia and silhouette measure different things: the former captures the tightness of the clusters, the latter how far apart (distinguishable) the clusters are. You can have a good (low) inertia but a bad (low) silhouette score if the clusters overlap or sit on top of each other.\n\n\nExample: Silhouette Scores\n\n\nYou can calculate and/or visualize silhouette scores in R in several ways: using the silhouette function in the cluster library or the fviz_nbclust function in the factoextra package. fviz_nbclust supports additional metrics, for example method=\"wss\" produced a scree plot of the within-cluster sum of squares (inertia).\n\nlibrary(cluster)\nset.seed(6345)\nsilhouette_score &lt;- function(k){\n  kmns &lt;- kmeans(iris_s, centers = k, nstart=50)\n  ss &lt;- silhouette(kmns$cluster, dist(iris_s))\n  mean(ss[, 3])\n}\nk &lt;- 2:10\nplot(k, \n     sapply(k, silhouette_score),\n     type='b',\n     xlab='Number of clusters', \n     ylab='Average Silhouette Scores',bty=\"l\")\n\n\n\n\n\n\n\n\n\nlibrary(factoextra)\nfviz_nbclust(iris_s, kmeans, method='silhouette')\n\n\n\n\n\n\n\nfviz_nbclust(iris_s, kmeans, method='wss')\n\n\n\n\n\n\n\n\nThe silhouette scores suggest \\(k=2\\) and the inertia scree plot $k=$3–5.\n\n\n\n\nPredicted Values\nAlthough \\(K\\)-means is an unsupervised learning method, we can use it to predict the cluster of a new observation. Calculate The distance of the coordinate of the new point to the cluster centers and assign the observation to the cluster whose center is closest. The cluster centroids serve as the predicted values. You can write a function in R that accomplishes that.\nIf the data were centered and/or scaled in the \\(K\\)-means analysis, make sure that the same treatment is applied before calculating distances to the cluster centroids.\n\nclusters &lt;- function(x, centers) {\n  # compute squared euclidean distance from \n  # each sample to each cluster center\n  tmp &lt;- sapply(seq_len(nrow(x)),\n                function(i) apply(centers, 1,\n                                  function(v) sum((x[i, ]-v)^2)))\n  max.col(-t(tmp))  # find index of min distance\n}\n\n# two new observations\nnewx = data.frame(\"Sepal.Length\"=c(4  , 6  ),\n                  \"Sepal.Width\" =c(2  , 3  ),\n                  \"Petal.Length\"=c(1.5, 1.3),\n                  \"Petal.Width\" =c(0.3, 0.5))\n\n#center and scales from training data\nmeans &lt;- attr(iris_s,\"scaled:center\")\nscales &lt;- attr(iris_s,\"scaled:scale\")\n \npred_clus &lt;- clusters((newx-means)/scales,km$centers)\npred_clus\n\n[1] 1 3\n\n# Using the cluster centers as the predicted values\nkm$centers[pred_clus,]\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1    -1.011191  0.85041372   -1.3006301   -1.250704\n3     1.132177  0.08812645    0.9928284    1.014129\n\n\n\n\nCombining \\(K\\)-Means and PCA\n\\(K\\)-means analysis finds groups of observations that are similar to each other in the inputs as judged by a distance metric. Principal component analysis finds independent linear combinations of the inputs that explain substantial amounts of information. In the Iris example analyzed earlier, we used 4 input variables but plotted the cluster assignment for two of the variables, because visualization in more dimensions is difficult (Figure 24.1).\nThere are two ways to combine PCA and \\(K\\)-means:\n\nPCA after \\(K\\)-means: Run a \\(K\\)-means analysis on \\(p\\) inputs, then calculate the first two principal components with the cluster assignment. This is a visualization techniques for clusters in high-dimensional data. It does not rectify the curse of dimensionality issue from which \\(K\\)-means suffers as \\(p\\) gets larger. When applied to visualize data in 2 dimensions, this technique reduces \\(p(p-1)/2\\) scatterplots to a singple biplot based on the first 2 components.\n\\(K\\)-means after PCA: Use PCA to reduce \\(p\\) inputs to \\(M &lt; p\\) principal components, then run a \\(K\\)-means analysis to find clusters in the components. This approach eliminates the curse of dimensionality.\n\n\n\n\nExample: Airbnb properties in Asheville, NC\n\n\nThe following data is Airbnb data about Asheville, NC. The data for this and other cities is available from http://insideairbnb.com/get-the-data/. We are using six numeric variables for the properties.\n\nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nairbnb &lt;- dbGetQuery(con, \"SELECT * FROM Asheville\")\n\ndbDisconnect(con)\n    \nairbnb2 &lt;- na.omit(airbnb[,c(\"price\",\n    \"number_of_reviews\",\"minimum_nights\",\n    \"reviews_per_month\",\"availability_365\",\n    \"calculated_host_listings_count\")])\n\nFigure 24.2 shows the distribution of prices as a function of the number of reviews. Many properties have accumulated hundreds of reviews over time. and most are toward the lower end of the price scale.\n\n\n\n\n\n\n\n\nFigure 24.2\n\n\n\n\n\nThe property with a rental price of more than $10,000 per day is a 1-bedroom, 1-bath guest suite in the middle of Asheville. The rental has a 2-night minimum and over 200 reviews. We are excluding this observation as an outlier.\nWe now perform a \\(K\\)-means analysis based on the first two principal components after limiting the data to properties with a daily rental price of less than $2,000.\n\nairbnb2 &lt;- airbnb2[airbnb2$price &lt; 2000,]\n\npca_asheville &lt;- prcomp(airbnb2,retx=TRUE,scale.=TRUE)\nsummary(pca_asheville)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4    PC5    PC6\nStandard deviation     1.3779 1.1772 0.9687 0.8906 0.8262 0.5488\nProportion of Variance 0.3164 0.2310 0.1564 0.1322 0.1138 0.0502\nCumulative Proportion  0.3164 0.5474 0.7038 0.8360 0.9498 1.0000\n\n\nWe use the first three principal components for the subsequent \\(K\\)-means analysis; they explain 70.383% of variability in the data.\nBased on the scree plot of the within-cluster sum of squares and the silhouette scores, \\(K\\)=5 or \\(K=6\\) seems like a reasonable number of clusters. The silhouette plot suggests \\(K\\)=7 instead (Figure 24.3). We compromise on \\(K=6\\).\n\nfviz_nbclust(pca_asheville$x[,1:3], kmeans, method='silhouette')\n\n\n\n\n\n\n\nFigure 24.3: Silhouette score and inertia scree plot.\n\n\n\n\n\n\nfviz_nbclust(pca_asheville$x[,1:3], kmeans, method='wss')\n\n\n\n\nInertia scree plot.\n\n\n\n\n\nkm &lt;- kmeans(pca_asheville$x[,1:3],centers=6,nstart=25)\n\nlibrary(ggfortify)\nautoplot(pca_asheville, \n         data=airbnb2, \n         color=km$cluster, \n         size=0.6,\n         loadings.label=TRUE, \n         loadings.label.size = 3,\n         loadings=TRUE)\n\n\n\n\n\n\n\n\nNot surprisingly, the reviews_per_month and the number_of_reviews are highly correlated. The six clusters separate pretty well. There is some overlap between black and green clusters, but the display is missing one of the principal components. The PCA rotation shows that PC1 is dominated by review-related attributes and PC2 by availability of the property and the number of listings that a host has in Asheville. PC3 has negative scores for pricey properties.\n\npca_asheville$rotation[,1:3]\n\n                                      PC1        PC2         PC3\nprice                           0.2902296  0.3294264 -0.62170726\nnumber_of_reviews              -0.6172910  0.1445229  0.12113455\nminimum_nights                  0.2131000 -0.4718076  0.53510059\nreviews_per_month              -0.6282131  0.2176421  0.03637559\navailability_365                0.1669516  0.5698854  0.42704350\ncalculated_host_listings_count  0.2584232  0.5252157  0.35886561\n\n\n\n\n\n\n\\(K\\)-Means on Random Data\nBefore we leave \\(K\\)-means clustering, a word of caution. K-Means clustering will always find \\(K\\) clusters even if the data have no structure. The following data perform a \\(3\\)-means analysis on 100 observations on 5 inputs drawn randomly from a standard Gaussian distribution. The correlation analysis shows no (pairwise) relationship among the inputs.\n\nset.seed(7654)\nx &lt;- matrix(rnorm(500,0,1),nrow=100,ncol=5)\nround(cor(x),4)\n\n        [,1]    [,2]    [,3]    [,4]    [,5]\n[1,]  1.0000 -0.1803 -0.0074  0.0591  0.0151\n[2,] -0.1803  1.0000 -0.1494 -0.0894  0.0121\n[3,] -0.0074 -0.1494  1.0000  0.0103 -0.0346\n[4,]  0.0591 -0.0894  0.0103  1.0000 -0.0004\n[5,]  0.0151  0.0121 -0.0346 -0.0004  1.0000\n\n\n\nkrand &lt;- kmeans(x,centers=3,nstart=20)\nkrand$center\n\n        [,1]       [,2]        [,3]       [,4]        [,5]\n1 -0.8585193  0.7523440 -0.47938437  0.2871084  0.05588092\n2  0.4961648 -0.8522573  0.27922916  1.3559955 -0.03425393\n3  0.2284897 -0.3133494  0.05111486 -0.6513347 -0.11558704\n\nkrand$withinss\n\n[1]  99.74124  92.39653 149.99366\n\n\nTo visualize, run a PCA and color the scores of the first two components with the cluster id. It appears that the algorithm found three somewhat distinct groups of observations. The cluster centroids are certainly quite different (Figure 24.4).\n\npca &lt;- prcomp(x,scale.=TRUE)\nproj_means &lt;- predict(pca,newdata=krand$centers)\n\n\n\n\n\n\n\n\n\nFigure 24.4: The first two principal components for \\(3\\)-means analysis on random data, \\(p=5\\). The diamonds are the cluster centroids.\n\n\n\n\n\nThere is a clue that something is amiss.\n\n(krand$betweenss/krand$totss)*100\n\n[1] 29.14485\n\n\nThe variability between the clusters accounts for only 29.145% of the variation in the data. If grouping explains differences between the data points, this percentage should be much higher.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html#hierarchical-clustering",
    "href": "clustering.html#hierarchical-clustering",
    "title": "24  Cluster Analysis",
    "section": "24.3 Hierarchical Clustering",
    "text": "24.3 Hierarchical Clustering\n\nIntroduction\nIn \\(K\\)-means clustering you specify \\(K\\), find the clusters, and examine the results. Metrics such as inertia, distortion, or the silhouette score are used to find an appropriate value for \\(K\\). Hierarchical clustering (HC) is a clustering technique where you do not specify the number of clusters in advance. Instead, the entire data set is organized between two extremes:\n\nat the top, all observations belong to a single cluster\nat the bottom, each observations is in a cluster by itself\n\nIf \\(c\\) denotes the number of clusters in hierarchical clustering, HC offers you to choose \\(1 \\le c \\le n\\). Between the two extremes, \\(c=1\\) and \\(c=n\\) lie many configurations where observations are combined into groups based on similarity measures and rules for combining groups, called linkage methods. The choice is typically made based on heuristics such as a visual inspection of the dendrogram, an upside-down tree display of the cluster arrangements (Figure 24.5). Algorithms exist that try to automate and optimize the determination of \\(c\\) based on criteria such as inertia.\n\n\n\n\n\n\nFigure 24.5: Example of a dendrogram in hierarchical clustering\n\n\n\n\n\nThe Dendrogram\nHierarchical clustering is popular because of the dendrogram, an intuitive representation of structure in the data. A word of caution is in order, however: just like \\(K\\)-means clustering will find \\(K\\) clusters–whether they exist or not–hierarchical clustering will organize the observations hierarchically in the dendrogram–whether a hierarchy makes sense or not.\nAt the lowest level of the dendrogram are the leaves, corresponding to observations. As you move up the tree, those merge into branches. Observations fuse first into groups, later on observations or groups merge with other groups. A common mistake in interpreting dendrograms is to assume similarity is greatest when observations are close to each other on the horizontal axis when fused. Observations are more similar if they are fused lower on the tree. The further up on the tree you go before merging branches, the more dissimilar are the members of the branches.\nIn Figure 24.6, observations 11 and 4 near the right edge of the tree appear “close” along the horizontal axis. Since they merge much higher up on the tree, these observations are more dissimilar as, for example, observations 23 and 25, which merge early on. Based on where they merge, observation 11 is no more similar to #4 than it is to observations 23, 25, 10, and 15.\n\n\n\n\n\n\n\n\nFigure 24.6: Example of a dendrogram in hierarchical clustering.\n\n\n\n\n\nThe name hierarchical clustering stems from the fact that clusters lower on the tree (near the bottom) are necessarily contained in clusters higher up on the tree (near the top), since clusters are formed by merging or splitting. This hierarchical arrangement can be unrealistic. James et al. (2021, 523) give the following example\n\nSuppose you have data on men and women from three countries.\nThe best division into three groups might be by country.\nThe best division into two groups might be by gender.\n\nThe best division into three groups does not result from taking the two gender groups and splitting one of those.\nThere are two general approaches to construct a dendrogram.\n\nThe agglomerative (bottom-up) approach starts with \\(c=n\\) clusters at the bottom of the dendrogram, where each observation is a separate cluster, and merges observations and branches based on their similarity. The pair chosen for merging at any stage consists of the least dissimilar (most similar) groups.\nThe divisive (top-down) approach starts at the trunk (the top) of the tree with a single cluster that contains all observations. Moving down the tree, branches are split into clusters to produce groups with the largest between-group dissimilarity (least similar clusters).\n\n\n\nCutting the Dendrogram\nIt is best to interpret the dendrogram as a data summary, not as a confirmatory tool. Based on the dendrogram you can choose any number of clusters. The typical approach is called cutting the tree, whereby you choose a particular height on the dendrogram and draw a line across. Depending on where you draw the line you end up with a different number of clusters (Figure 24.7). The number of clusters corresponds to the number of vertical lines the cut intersects.\n\n\n\n\n\n\nFigure 24.7: Dendrogram cut at different heights\n\n\n\n\n\nDissimilarity and Linkage\nBefore we can construct a dendrogram, we need to decide on two more things (besides whether the approach is top-down or bottom-up): a measure of dissimilarity and a rule on which groups are to be merged. These choices have profound effect on the resulting dendrogram, more so than the choice between top-down or bottom-up approach.\n\nDissimilarity measures\nLet \\(x_{ij}\\) denote the measurements for \\(i=1,\\cdots, n\\) observations on \\(j=1,\\cdots, p\\) inputs (variables). As before, the vector of inputs for the \\(i\\)th observation is denoted \\(\\textbf{x}_i = [x_{i1},\\cdots, x_{ip}]\\).\nThe dissimilarity (distance) matrix \\(\\textbf{D}\\) for the data is an \\((n \\times n)\\) matrix with typical element \\[\n    D(\\textbf{x}_i,\\textbf{x}_{i^\\prime}) = \\sum_{j=1}^p w_j \\, d_j(x_{ij},x_{i^\\prime j})\n\\] The \\(w_j\\) are weights associated with the \\(j\\)th attribute, \\(\\sum_{j=1}^p w_j = 1\\). \\(d_j(x_{ij},x_{i^\\prime j})\\) measures the dissimilarity between any two observations for the \\(j\\)th attribute.\nA number of dissimilarity measures are used, depending on the type of variable and the application. For quantitative variables, the following are most popular:\n\nSquared Euclidean distance \\[d_j(x_{ij},x_{i^\\prime j}) = (x_{ij} - x_{i^\\prime j})^2\\] Probably the most frequent choice, but it is sensitive to large distances due to squaring.\nAbsolute distance, also called “Manhattan” or city-block distance \\[d_j(x_{ij},x_{i^\\prime j}) = |x_{ij} - x_{i^\\prime j}|\\] Absolute distance is more robust to large differences compared to dissimilarity based on Euclidean distance.\nCorrelation-based distance \\[\n      d_j(x_{ij},x_{i^\\prime j}) = 1- \\rho(\\textbf{x}_i,\\textbf{x}_{i^\\prime})  = 1- \\frac{\\sum_j (x_{ij}-\\overline{x}_i) (x_{i^\\prime j} - \\overline{x}_{i^\\prime})}\n      {\\sqrt{\\sum_j (x_{ij}-\\overline{x}_i)^2 \\, (x_{i^\\prime j} - \\overline{x}_{i^\\prime})^2 }}\n\\] with \\(\\overline{x}_i = 1/p \\sum_j x_{ij}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(\\rho(\\textbf{x}_i, \\textbf{x}_{i^\\prime})\\) does not measure the correlation between two variables across a set of \\(n\\) observations–that would be the familiar way to calculate and interpret a correlation coefficient. \\(\\rho(\\textbf{x}_i, \\textbf{x}_{i^\\prime})\\) is the correlation between two observations across \\(p\\) attributes.\n\n\n\n\nExample: Effect of Distance Metrics\n\n\nWe use a simple data set with observations on the shopping behavior of four imaginary shoppers. Frank, Betsy, Julian, and Lisa make purchases of 5 possible items. The values for the item attributes are the number of times the item was purchased.\n\ndf &lt;- data.frame(shopper=c(\"Frank\",\"Besty\",\"Julian\",\"Lisa\"),\n                 item1=c(0,1,0,0),\n                 item2=c(0,0,4,1),\n                 item3=c(1,1,0,0),\n                 item4=c(1,3,0,1),\n                 item5=c(2,0,1,1)\n                 )\ndf\n\n  shopper item1 item2 item3 item4 item5\n1   Frank     0     0     1     1     2\n2   Besty     1     0     1     3     0\n3  Julian     0     4     0     0     1\n4    Lisa     0     1     0     1     1\n\n\nFirst, let’s calculate various distance metrics. These are represented as matrices of distances between the data points. The function dist() returns the lower triangular matrix of pairwise distances\n\ndist(df[,2:6],method=\"manhattan\")\n##    1  2  3\n## 2  5      \n## 3  7 10   \n## 4  3  6  4\ndist(df[,2:6],method=\"euclidean\")\n##          1        2        3\n## 2 3.000000                  \n## 3 4.358899 5.291503         \n## 4 1.732051 2.828427 3.162278\n\nThe dist function excludes the diagonal entries of the distance matrix by default, these are known to be zero. Because the input values are integers in this example, the city-block distances are also integers.\nThe correlation-based distances can be calculated with factoextra::get_dist(). This function adds methods for correlations based on Pearson (method=\"pearson\"), Spearman (method=\"spearman\") or Kendall (method=\"kendall\"). The variables can be centered and scaled with stand=TRUE (stand=FALSE is the default).\n\ncor(t(df[,2:6]))\n\n           [,1]       [,2]       [,3]      [,4]\n[1,]  1.0000000  0.0000000 -0.3450328 0.3273268\n[2,]  0.0000000  1.0000000 -0.5892557 0.0000000\n[3,] -0.3450328 -0.5892557  1.0000000 0.5270463\n[4,]  0.3273268  0.0000000  0.5270463 1.0000000\n\nget_dist(df[,2:6],method=\"pearson\",stand=FALSE)\n\n          1         2         3\n2 1.0000000                    \n3 1.3450328 1.5892557          \n4 0.6726732 1.0000000 0.4729537\n\n\nThe correlation-based dissimilarity is not equal to the correlation among the item purchases, it is one minus the correlation of the item purchases for each shopper.\nThe cluster::daisy() function can compute Euclidean, Manhattan, and Gower’s distance matrices. More on Gower’s distance after the example.\n\ndaisy(df[,2:6],metric=\"manhattan\")\n## Dissimilarities :\n##    1  2  3\n## 2  5      \n## 3  7 10   \n## 4  3  6  4\n## \n## Metric :  manhattan \n## Number of objects : 4\ndaisy(df[,2:6],metric=\"euclidean\")\n## Dissimilarities :\n##          1        2        3\n## 2 3.000000                  \n## 3 4.358899 5.291503         \n## 4 1.732051 2.828427 3.162278\n## \n## Metric :  euclidean \n## Number of objects : 4\ndaisy(df[,2:6],metric=\"gower\")\n## Dissimilarities :\n##           1         2         3\n## 2 0.5333333                    \n## 3 0.5666667 0.9000000          \n## 4 0.3500000 0.6833333 0.2166667\n## \n## Metric :  mixed ;  Types = I, I, I, I, I \n## Number of objects : 4\n\nNow let’s construct the dendrograms for the data based on Euclidean and Pearson correlation distance matrices using the hclust function. The input to hclust is a distance (dissimilarity) matrix as produced by dist(), get_dist(), daisy(). The actual values of the variables are no longer needed once the dissimilarities are calculated.\n\nh1 &lt;- hclust(dist(df[,2:6]))\nh2 &lt;- hclust(get_dist(df[,2:6],method=\"pearson\"))\n\npar(mfrow=c(1,2))\npar(cex=0.7) \npar(mai=c(0.6,0.6,0.2,0.3))\nplot(h1,labels=df[,1],sub=\"\",xlab=\"Shoppers\",main=\"Euclidean Dist.\")\nplot(h2,labels=df[,1],sub=\"\",xlab=\"Shoppers\",main=\"Correlation\")\n\n\n\n\n\n\n\n\nChoosing Euclidean distance groups together users who bought few items, because they appear as similar (close). Frank and Lisa bought 4 and 3 items, respectively. Betsy and Julian purchased 5 items. Choosing correlation-based distance groups users who bought items together. For example, Julian and Lisa bought items 2 and 5 together, Frank and Betsy purchased items 3 and 4 together.\n\n\nThe distance metrics discussed so far are not appropriate for categorical variables (nominal or ordinal) because differences between values are not defined. A four-star rating is not twice as much as a two-star rating and the “distance” between a one- and two-star rating is not the same as that between a four- and five-star rating.\nStill, for an ordinal variable with \\(M\\) categories it is not uncommon to replace the label for category \\(j\\) with \\[\n\\frac{j-1/2}{M}\n\\] and treat this as a quantitative score. With nominal variables it is common to assign a simple loss value depending on whether the values of two variables are the same (loss = 0) or different (loss=1).\nWhat should we do when the inputs are of mixed type, for example, \\(x_1\\) is continuous, \\(x_2\\) is binary, and \\(x_3\\) is nominal? Gower (n.d.) introduced a similarity metric to compute distances in this case, known as Gower’s distance. Suppose there are no missing values. Gower’s similarity coefficient is \\[\nS(\\textbf{x}_i,\\textbf{x}_{i^\\prime}) = \\frac{1}{p}\\sum_{j=1}^p s_{ii^\\prime j}\n\\] The \\(s_{ii^\\prime j}\\) is the score between observations \\(i\\) and \\(i^\\prime\\) for variable \\(j\\); The scores range \\(0 \\leq s_{ii^\\prime j} \\leq 1\\) and are calculated as follows:\n\nqualitative attributes: 0/1 loss\nquantitative attritbutes: \\[\ns_{i i^\\prime j} = 1 - \\frac{x_{ij} - x_{i^\\prime j}}{R_j}\n\\] where \\(R_j\\) is the range (max-min) for the \\(j\\)th variable. The Gower similarity coefficient has the following properties:\n\\(0 \\leq S(\\textbf{x}_i, \\textbf{x}_{i^\\prime}) \\leq 1\\)\n\\(S(\\textbf{x}_i, \\textbf{x}_{i^\\prime}) = 0 \\Rightarrow\\) records differ maximally\n\\(S(\\textbf{x}_i, \\textbf{x}_{i^\\prime}) = 1 \\Rightarrow\\) records do not differ\n\nFor purposes of clustering, the dissimilarity measure based on Gower’s distance is \\(1 - S(\\textbf{x}_i, \\textbf{x}_{i^\\prime})\\). This is implemented in the daisy function of the cluster package in R.\n\n\nLinkage methods\n\\(D(\\textbf{x}_i,\\textbf{x}_{i^\\prime})\\) measures the dissimilarity between two data points. In order to move up (or down) the tree in hierarchical clustering we also need to determine how to measure the similarity/dissimilarity between groups of points. Suppose \\(G\\) and \\(H\\) present two clusters and \\(D(G,H)\\) is the dissimilarity between the two, some function of the dissimilarity of the points in the clusters. The decision rule that determines how to merge (or split) clusters is called linkage. The three most common linkage methods are\n\nSingle linkage: \\(D(G,H) = \\min D(\\textbf{x}_i,\\textbf{x}_{i^\\prime}), i \\in G, i^\\prime \\in H\\)\nComplete linkage: \\(D(G,H) = \\max D(\\textbf{x}_i,\\textbf{x}_{i^\\prime}), i \\in G, i^\\prime \\in H\\)\nAverage linkage: \\(D(G,H) = \\text{ave} D(\\textbf{x}_i,\\textbf{x}_{i^\\prime}), i \\in G, i^\\prime \\in H\\)\n\nThe agglomerative clustering algorithm merges the cluster with the smallest linkage value.\nWhen clusters separate well, the choice of linkage is not that important. Otherwise, linkage can have substantial impact on the outcome of hierarchical clustering. Single linkage is know to cause chaining, combining observations that are linked by a series of close observations. Complete linkage tends to produce compact clusters, but observations can end up being closer to members of other clusters than to members of their own cluster. Average linkage is a compromise, but is not invariant to transformations of the dissimilarities. Centroid linkage uses distances between centroids of the clusters (Figure 24.8).\n\n\n\n\n\n\nFigure 24.8: Some linkage types in hierarchical clustering.\n\n\n\n\n\nExample: Hierarchical Cluster Analysis for Glaucoma Data.\n\n\nWhat can we learn about the 98 subjects in the Glaucoma data set who had glaucomatous eyes by way of hierarchical cluster analysis? The following statements create a data frame from the DuckDB table, filter the glaucotamous cases, remove Glaucoma column and scale the remaining 62 variables.\n\nlibrary(duckdb)\nlibrary(dplyr)\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nglauc &lt;- dbGetQuery(con, \"SELECT * FROM Glaucoma\")\n\ndbDisconnect(con)\n\nglauc &lt;- glauc %&gt;% filter(Glaucoma==1) %&gt;% \n    dplyr::select(-c(Glaucoma)) %&gt;% \n    scale()\n\nWe first perform agglomerative clustering with correlation-based dissimilarity and complete linkage.\n\nhc &lt;- hclust(get_dist(glauc,method=\"pearson\"),method=\"complete\")\n\nThe merge matrix on the hclust output object describes the \\(n-1\\) steps in which the observations/clusters were merged. Negative numbers refer to observations, positive numbers refer to clusters formed at that stage.\n\nhc$merge[1:25,]\n\n      [,1] [,2]\n [1,]  -34  -41\n [2,]   -4  -73\n [3,]  -16  -95\n [4,]  -58  -93\n [5,]  -72  -90\n [6,]  -24  -70\n [7,]  -76    1\n [8,]   -6  -75\n [9,]  -51    2\n[10,]  -43  -45\n[11,]  -27  -55\n[12,]   -9  -79\n[13,]  -20  -65\n[14,]  -19  -56\n[15,]  -15  -71\n[16,]  -29  -81\n[17,]   -7   12\n[18,]  -30  -57\n[19,]  -17  -54\n[20,]  -23  -25\n[21,]  -80    7\n[22,]  -61  -68\n[23,]    4    5\n[24,]    3   14\n[25,]  -13  -60\n\n\nThe first merge combines observations #34 and #41 into a group of two. The next merge combines #4 and #73 into another group of two. At the seventh merge, observation #76 is combined with the group created at the first merge. This cluster now contains observations [34, 41, 76]. The first time two groups are being merged is at step 23: the groups consisting of observations [58, 93] and [72, 90] are combined into a cluster of 4.\nThe height vector is a vector of \\(n-1\\) values of the height criterion; the actual values depend on the linkage method. For this analysis, the first 25 heights at which merges occurred are as follows:\n\nhc$height[1:25]\n\n [1] 0.1022034 0.1031823 0.1144513 0.1331928 0.1349688 0.1386951 0.1487625\n [8] 0.1510436 0.1540351 0.1547025 0.1657972 0.1679269 0.1770342 0.1806037\n[15] 0.1838289 0.1868672 0.2097274 0.2276110 0.2467682 0.2477665 0.2543348\n[22] 0.2682308 0.2684616 0.2737816 0.2779692\n\n\nThe dendrogram for this analysis is plotted in Figure 24.9 along with the bounding boxes for 4 and 8 clusters. The cut for the larger number of clusters occurs lower at the tree.\n\nplot(hc, cex=0.5, main=\"\",)\nrect.hclust(hc,k=4)\nrect.hclust(hc,k=8)\n\n\n\n\n\n\n\nFigure 24.9: Dendrogram for partial glaucoma data with correlation-based dissimilarity and complete linkage.\n\n\n\n\n\nThe sizes of the four clusters are as follows:\n\ntable(cutree(hc,k=4))\n\n\n 1  2  3  4 \n37 26 20 15 \n\n\nChanging the linkage method to single demonstrates the chaining effect on the dendrogram (Figure 24.10). Identifying a reasonable number of clusters is more difficult.\n\nhc_s &lt;- hclust(get_dist(glauc,method=\"pearson\"),method=\"single\")\nplot(hc_s, cex=0.5,main=\"\")\nrect.hclust(hc_s,k=4)\n\n\n\n\n\n\n\nFigure 24.10: Dendrogram for partial glaucoma data, single linkage.\n\n\n\n\n\nIf you create a dendrogram object from the hclust results, a number of plotting functions are available to visualize the dendrogram in interesting ways. For example:\n\nhc.dend &lt;- as.dendrogram(hc) # create dendrogram object\nplot(dendextend::color_branches(hc.dend,k=4),leaflab=\"none\",horiz=TRUE)\n\n\n\n\n\n\n\nfactoextra::fviz_dend(hc.dend,k=4,horiz=TRUE,cex=0.4,palette=\"aaas\",type=\"rectangle\")\n\n\n\n\n\n\n\nfactoextra::fviz_dend(hc.dend,k=4,cex=0.4,palette=\"aaas\",type=\"circular\")\n\n\n\n\n\n\n\nfactoextra::fviz_dend(hc.dend,k=4,cex=0.4,palette=\"aaas\",type=\"phylogenic\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.1: Results of 3-means clustering for Iris data. Clusters are identified through colors, species are identified with plotting symbols.\nFigure 24.5: Example of a dendrogram in hierarchical clustering\nFigure 24.6: Example of a dendrogram in hierarchical clustering.\nFigure 24.7: Dendrogram cut at different heights\nFigure 24.8: Some linkage types in hierarchical clustering.\nFigure 24.9: Dendrogram for partial glaucoma data with correlation-based dissimilarity and complete linkage.\nFigure 24.10: Dendrogram for partial glaucoma data, single linkage.\n\n\n\nGower, J. C. n.d. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHartigan, J. A., and M. A. Wong. 1979. “Algorithm AS 136: A k-Means Clustering Algorithm.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 28 (1): 100–108.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "mbc.html",
    "href": "mbc.html",
    "title": "25  Model-based Clustering",
    "section": "",
    "text": "25.1 Finite Mixture Models\nIn general, a finite mixture distribution is a weighted sum of \\(k\\) probability distributions where the weights sum to one: \\[\np(Y) = \\sum_{j=1}^k \\pi_j \\, p_j(Y) \\quad \\quad \\sum_{j=1}^k \\pi_j = 1\n\\] \\(p_j(Y)\\) is called the \\(j\\)th component of the mixture and \\(\\pi_j\\) is the \\(j\\)th mixing probability, also referred to as the \\(j\\)th mixture weight. The finite mixture is called homogeneous when the \\(p_j(Y)\\) are of the same family, and heterogeneous when they are from different distributional families. An important example of a heterogeneous mixture, discussed in Section 10.6, is a zero-inflated model for counts, a two-component mixture of a constant distribution and a classical distribution for counts such as the Poisson or Negative Binomial.\nThe class of finite mixture models (FMM) is very broad and the models can get quite complicated. For example, each \\(p_j(Y)\\) could be a generalized linear model and its mean can depend on inputs and parameters. The mixing probabilities can also be modeled as functions of inputs and parameters using logistic (2-component models) or multinomial model types.\nHow do mixtures of distributions come about? An intuitive motivation is through a latent variable process. A discrete random variable \\(S\\) takes on states \\(j=1,\\cdots,k\\) with probabilities \\(\\pi_j\\). It is called a latent variable because we cannot observe it directly. We only observe its influence on the distribution of \\(Y\\), the variable we are interested in, and that depends on \\(S\\). This dependence results in a different conditional distribution of \\(Y\\) for each of the possible states of \\(S\\), \\(p(Y| S=j)\\). We can then derive the marginal distribution of \\(Y\\), the distribution we are interested in, by integrating out (summing over) \\(S\\) in the joint distribution\n\\[\\begin{align*}\np(Y) &= \\sum_{j=1}^k \\Pr(Y, S=j) \\\\\n        &= \\sum_{j=1}^k \\Pr(S=j) \\, p(Y|S=j) = \\sum_{j=1}^k \\pi_j \\, p_j(Y)\n\\end{align*}\\]\nThis is the same as computing the weighted sum over the conditional distributions–the finite mixture formulation. We cannot observe \\(S\\) directly, it exerts its influence on \\(Y\\) through the likelihood of its states (\\(\\pi_j\\)) and the conditional distributions of \\(Y|S=j\\).",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model-based Clustering</span>"
    ]
  },
  {
    "objectID": "mbc.html#gaussian-mixture-models",
    "href": "mbc.html#gaussian-mixture-models",
    "title": "25  Model-based Clustering",
    "section": "25.2 Gaussian Mixture Models",
    "text": "25.2 Gaussian Mixture Models\nA special case of FMMs are Gaussian mixtures where the component distributions are multivariate Gaussian. (See Section 3.7 for a review of the multivariate Gaussian distribution.) The mixture distribution in model-based clustering is \\[\np(\\textbf{X}) = \\sum_{j=1}^k \\pi_j \\, G(\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j)\n\\]\nEach \\(G(bmu_j,\\boldsymbol{\\Sigma}_j)\\) is a \\(p\\)-variate Gaussian, \\(\\boldsymbol{\\mu}_j\\) and \\(\\boldsymbol{\\Sigma}_j\\) are the \\((p \\times 1)\\) mean vector and the \\((p \\times p)\\) covariance matrix of the \\(j\\)th component. These need to be estimated. In terms of clusters, \\(\\boldsymbol{\\mu}_j\\) is the center of the \\(j\\)th cluster and \\(\\boldsymbol{\\Sigma}_j\\) determines the volume, shape, and orientation of the \\(j\\)th cluster.\nFigure 25.1 shows contours of the density for two bivariate (\\(p=2\\)) Gaussian distributions. The distribution on the right has \\[\\boldsymbol{\\mu}_1 = \\left [ \\begin{array}{c}1 \\\\ 0\\end{array} \\right ]\n\\qquad\n\\boldsymbol{\\Sigma}_1 = \\left [ \\begin{array}{cc} 1 & 0.5 \\\\ 0.5 & 2\\\\\n\\end{array} \\right]\n\\] and the distribution on the left has\n\\[\\boldsymbol{\\mu}_2 = \\left [ \\begin{array}{c}-1.6 \\\\ 1\\end{array} \\right ]\n\\qquad\n\\boldsymbol{\\Sigma}_2 = \\left [ \\begin{array}{cc} 1.5 & 0 \\\\ 0 & 1\\\\\n\\end{array} \\right]\n\\] When the variance-covariance matrix is diagonal, the contours align with the axes of the coordinate system, stretching in the direction of greater variance. The covariance between \\(X_1\\) and \\(X_2\\) introduces a rotation of the contours in the graphic on the right.\n\n\n\n\n\n\nFigure 25.1: Contours of bivariate (\\(p=2\\)) Gaussian distributions. The mean vector centers the distribution in the coordinate system. The variances stretch in the direction of \\(X_1\\) and \\(X_2\\). The distribution on the right is rotated in the coordinate system because of the correlation between the variables.\n\n\n\n\n\n\n\n\n\nFigure 25.2: A two component mixture of bivariate Gaussian distributions.\n\n\n\nThe Gaussian mixture model (GMM) can be viewed as a soft version of \\(K\\)-means clustering. The latter assigns an observation to exactly one of \\(K\\) clusters—a hard assignment. The GMM assigns probabilities of cluster membership to each observation based on \\(G(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1), \\cdots, G(\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\). GMM allows for a “gray area”; an observation has a non-zero probability to belong to any of the \\(k\\) clusters (Figure 25.2).",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model-based Clustering</span>"
    ]
  },
  {
    "objectID": "mbc.html#modeling-variance-covariance-matrices",
    "href": "mbc.html#modeling-variance-covariance-matrices",
    "title": "25  Model-based Clustering",
    "section": "25.3 Modeling Variance-covariance Matrices",
    "text": "25.3 Modeling Variance-covariance Matrices\nTo train a Gaussian mixture model on data we need to estimate the following parameters: \\(k\\), the number of components, \\(\\boldsymbol{\\mu}_1, \\cdots, \\boldsymbol{\\mu}_k\\), the means of the distributions, and \\(\\boldsymbol{\\Sigma}_1, \\cdots, \\boldsymbol{\\Sigma}_k\\), the variance-covariance matrices of the distributions (covariance matrix for short). Each \\(\\boldsymbol{\\mu}_j\\) is a \\(p \\times 1\\) vector and each \\(\\boldsymbol{\\Sigma}_j\\) is a \\(p \\times p\\) matrix with up to \\(p(p+1)/2\\) unique entries. The total number of potential unknowns in a GMM is thus \\[\nk \\times (p + p(p+1)/2) = k\\frac{3p + p^2}{2}\n\\]\nA mixture with \\(k=5\\) components and \\(p=10\\) variables has 325 unknowns. For \\(p=20\\) this grows to 1,150 unknowns. That is not a large number compared to say, an artificial neural network, but it is a substantial number for estimation by maximum likelihood or Bayesian methods. To reduce the number of parameters and to add interpretability, the class of mixtures considered is constrained by imposing structure on the covariance matrices of the Gaussian distributions.\nThe most constrained covariance model is to assume that the variables are uncorrelated and have the same variance across the \\(k\\) clusters (an equal variance model): \\[\n\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\cdots = \\boldsymbol{\\Sigma}_k = \\sigma^2 \\textbf{I}\n\\]\nThe next most constrained model allows for a different variance in each cluster but maintains uncorrelated inputs: \\[\n\\boldsymbol{\\Sigma}_j = \\sigma^2\\textbf{I}\n\\] To capture the relevant covariance structures in model-based clustering, Fraley and Raftery (2002) start from the eigendecomposition of \\(\\boldsymbol{\\Sigma}\\) and use the general parameterization \\[\n\\boldsymbol{\\Sigma}_j = \\lambda_j \\textbf{D}_j \\textbf{A}_j \\textbf{D}_j^\\prime\n\\]\n\\(\\textbf{D}_j\\) is an orthogonal matrix of eigenvectors, \\(\\textbf{A}_j\\) is a diagonal matrix whose elements are proportional to the eigenvalues, and \\(\\lambda_j\\) is a proportionality constant. The highly constrained equal variance model corresponds to the special case \\(\\boldsymbol{\\Sigma}_j = \\lambda \\textbf{I}\\) and the heterogeneous variance model corresponds to \\(\\boldsymbol{\\Sigma}_j = \\lambda_j \\textbf{I}\\).\nThe idea behind this parameterization is to hold elements constant across clusters or vary them, and because \\(\\lambda\\), \\(\\textbf{D}\\), and \\(\\textbf{A}\\) represent different aspects of the shape of the covariance matrix, one arrives at a reasonable set of possible models to consider and to compare. The geometric aspects of the component distributions captured by the three elements of the decomposition are\n\n\\(\\textbf{A}_j\\) determines the shape of the \\(j\\)th mixture\n\\(\\textbf{D}_j\\) determines the orientation (rotation) of the \\(j\\)th mixture\n\\(\\lambda_j\\) determines the volume of the \\(j\\)th mixture\n\nFor example, the model \\[\n\\boldsymbol{\\Sigma}_j = \\lambda \\textbf{D}_j \\textbf{A}\\textbf{D}_j^\\prime\n\\] imposes equal volume and equal shape across the clusters, but allows for cluster-specific orientation of the distributions.\nOne could use other parameterizations of covariance matrices, for example, expressing covariances as functions of distance between values—the parameterization shown here is implemented in the Mclust function of the popular mclust package in R.\n‘mclust’ uses a three-letter code to identify a specific covariance model (Table 25.1)\n\n\n\nTable 25.1: Covariance models for \\(p &gt; 1\\) implemented in mclust.\n\n\n\n\n\n\n\n\n\n\n\n\n\nmclust Code\n\\(\\boldsymbol{\\Sigma}_j\\)\nVolume\nShape\nOrientation\nNotes\n\n\n\n\nEII\n\\(\\lambda \\textbf{I}\\)\nEqual\nIdentity\nNA\nSpherical\n\n\nVII\n\\(\\lambda_j \\textbf{I}\\)\nVariable\nIdentity\nNA\nSpherical\n\n\n\n\n\n\n\n\n\n\nEEI\n\\(\\lambda \\textbf{A}\\)\nEqual\nEqual\nCoord. axes\nDiagonal\n\n\nVEI\n\\(\\lambda_j \\textbf{A}\\)\nVariable\nEqual\nCoord. axes\nDiagonal\n\n\nEVI\n\\(\\lambda \\textbf{A}_j\\)\nEqual\nVariable\nCoord. axes\nDiagonal\n\n\nVVI\n\\(\\lambda_j \\textbf{A}_j\\)\nVariable\nVariable\nCoord. axes\nDiagonal\n\n\n\n\n\n\n\n\n\n\nEEE\n\\(\\lambda\\textbf{D}\\textbf{A}\\textbf{D}^\\prime\\)\nEqual\nEqual\nEqual\nElliptical\n\n\nEEV\n\\(\\lambda\\textbf{D}_j \\textbf{A}\\textbf{D}_j^\\prime\\)\nEqual\nEqual\nVariable\nElliptical\n\n\nVEV\n\\(\\lambda_j\\textbf{D}_j \\textbf{A}\\textbf{D}_j^\\prime\\)\nVariable\nEqual\nVariable\nElliptical\n\n\nVVV\n\\(\\lambda_j\\textbf{D}_j \\textbf{A}_j\\textbf{D}_j^\\prime\\)\nVariable\nVariable\nVariable\nElliptical\n\n\n\n\n\n\nThe first six models in Table 25.1 have diagonal covariance matrices, the inputs are independent (lack of correlation does equal independence for Gaussian random variables). Models EEE, EEV, VEV, and VVV capture correlations among the \\(X\\)s and vary different aspects of \\(\\boldsymbol{\\Sigma}_j\\) across the clusters. The EEE model applies the same covariance matrix in all clusters, EEV varies orientation, VEV varies orientation and volume, and VVV varies all aspects across clusters.\nThe choice of covariance model should not be taken lightly, it has considerable impact on the clustering analysis. Assuming independent inputs is typically not reasonable. However, if model-based clustering is applied to the components of a PCA, this assumption is met. Some of the models in Table 25.1 are nested and can be compared through a hypothesis test. But in addition to choosing the covariance model, we also need to choose \\(k\\), the number of clusters. One approach is to compare the model-\\(k\\) combinations by way of BIC, the Bayesian information criterion, introduced in Section 21.2.1.\nBIC is based on the log-likelihood, which is accessible because we have a full distributional specification, and a penalty term that protects against overfitting. Before applying model-based clustering in a full analysis to select \\(k\\) and the covariance models, let’s examine how to interpret the results of model-based clustering and some of the covariance structures for the Iris data.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model-based Clustering</span>"
    ]
  },
  {
    "objectID": "mbc.html#model-based-clustering-with-mclust-in-r",
    "href": "mbc.html#model-based-clustering-with-mclust-in-r",
    "title": "25  Model-based Clustering",
    "section": "25.4 Model-based Clustering with Mclust in R",
    "text": "25.4 Model-based Clustering with Mclust in R\nModel-based clustering based on a finite mixture of Gaussian distributions is implemented in the Mclust function of the mclust package in R. Maximum likelihood estimates are derived by EM (Expectation-Maximization) algorithm and models are compared via BIC. An iteration of the EM algorithm comprises two steps. The E-step computes at the current estimates an \\((n \\times k)\\) matrix \\(\\textbf{Z}\\) of conditional probabilities that observation \\(i=1,\\cdots,n\\) belongs to component \\(j=1,\\cdots,k\\). The M-step updates the parameter estimates given the matrix \\(\\textbf{Z}\\).\n\nFirst Look at Mclust\n\n\nExample: Equal Variance Structure for Iris Data\n\n\nThe G= and modelNames= option im Mclust are used to specify the number of components in the mixture (the number of clusters) and the covariance model. The defaults are G=1:9 and all available covariance models based on whether \\(p=1\\) or \\(p&gt;1\\) and whether \\(n &gt; p\\) or \\(n \\le p\\).\nThe following statements fit a 3-component mixture with EII covariance model to the numerical variables in the Iris data. The EII model is the most restrictive multivariate model, assuming diagonal covariance matrices and equal variances across variables and clusters (see Table 25.1). This model is probably not appropriate for the Iris data but simple enough to interpret the results from the analysis.\n\nlibrary(mclust)\nmb_eii &lt;- Mclust(iris[,-5],G=3,verbose=FALSE, modelNames=\"EII\")\nmb_eii$parameters$pro\n\n[1] 0.3333956 0.4135728 0.2530316\n\n\nThe three Gaussian distributions mix with probabilities \\(\\widehat{\\pi}_1 =\\) 0.3334, \\(\\widehat{\\pi}_2 =\\) 0.4136, and \\(\\widehat{\\pi}_3 =\\) 0.253.\n\nmb_eii$parameters$mean\n\n                  [,1]     [,2]     [,3]\nSepal.Length 5.0060172 5.903268 6.848623\nSepal.Width  3.4278263 2.748010 3.074750\nPetal.Length 1.4622880 4.400526 5.732651\nPetal.Width  0.2461594 1.432036 2.074895\n\n\nThe first component has a mean of \\(\\widehat{\\boldsymbol{\\mu}}_1\\) = [5.006, 3.4278, 1.4623, 0.2462]. The following lists the covariance matrices in the three clusters, the model forces them to be diagonal, to have the same variance for all variables and across the clusters, \\(\\widehat{\\lambda}_j\\) = 0.1331.\n\nmb_eii$parameters$variance\n\n$modelName\n[1] \"EII\"\n\n$d\n[1] 4\n\n$G\n[1] 3\n\n$sigma\n, , 1\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1330939   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1330939    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1330939   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1330939\n\n, , 2\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1330939   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1330939    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1330939   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1330939\n\n, , 3\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1330939   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1330939    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1330939   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1330939\n\n\n$Sigma\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1330939   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1330939    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1330939   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1330939\n\n$sigmasq\n[1] 0.1330939\n\n$scale\n[1] 0.1330939\n\n\n\n\n\n\nClassification and Uncertainty\nBased on the underlying \\(\\textbf{Z}\\) matrix of the EM algorithm at convergence, the observations can be classified based on the highest probability. For observation #53, for example, \\(\\textbf{z}_{53}\\) = [0, 0.3068, 0.6932]. The probability is highest that the observation belongs to the third component, hence its cluster membership is classified as 3.\n\nround(mb_eii$z[53,],4)\n\n[1] 0.0000 0.3068 0.6932\n\nmb_eii$classification[53]\n\n[1] 3\n\n\nOne of the big advantages of model-based clustering is the ability to quantify the uncertainty associated with a cluster assignment. How confident are we in assigning observation #53 to cluster 3? We are certain that it does not belong to cluster 1, but there is a non-zero probability that the observation belongs to cluster 2. You can retrieve the uncertainty associated with the classification of each observation in the uncertainty vector. For observations 50—55, for example:\n\nround(mb_eii$uncertainty[50:55],4)\n\n[1] 0.0000 0.3289 0.0015 0.3068 0.0000 0.0031\n\n\nThere is no uncertainty associated with classifying #50. An examination of \\(\\textbf{z}_{50}\\) shows that \\(z_{50,1} = 1.0\\). There is uncertainty associated with the next three observations, and it is higher for #51 and #53 than for #52.\nThe uncertainty is simply the complement of the highest component probability. For #53, this is 1-0.6932 = 0.3068. The following code validates this for all observations.\n\nc &lt;- apply(mb_eii$z,1,which.max)\nprobs &lt;- rep(0,length(c))\nfor (i in seq_along(c)) {probs[i] &lt;- mb_eii$z[i,c[i]]}\nsum(mb_eii$uncertainty - (1-probs))\n\n[1] 0\n\n\nFinally, you can plot the classification, uncertainty, and density contours for the analysis by choosing the what= parameter of the plot method.\n\nplot(mb_eii, what=\"classification\")\n\n\n\n\n\n\n\nplot(mb_eii, what=\"uncertainty\",dimens=c(1,2))\n\n\n\n\n\n\n\nplot(mb_eii, what=\"density\")\n\n\n\n\n\n\n\n\nThe classification and uncertainty plots are overlaid with the densities of the components. The parallel alignment of the densities with the coordinate axes reflects the absence of correlations among the variables. The volume of the densities does not capture the variability of most attributes, many points fall outsode of the density contour—the equal variance assumption across variables and clusters is not justified.\n\n\nExamining Covariance Structures\n\n\nExample: Other Covariance Structures for Iris Data\n\n\nNow let’s look at how the choice of covariance structure affects the results of the cluster analysis. The following statements fit 3-component models with four additional covariance structures:\n\nVII: Spherical Model, variance varies among clusters\nVEI: Diagonal Model, variance varies among attributes and clusters\nEEE: Elliptical Model, same non-diagonal covariance among clusters\nVEV: Elliptical Model, different covariance matrices with same shape\n\n\nmb_vii &lt;- Mclust(iris[,-5],G=3,verbose=FALSE, modelNames=\"VII\")\nmb_vei &lt;- Mclust(iris[,-5],G=3,verbose=FALSE, modelNames=\"VEI\")\nmb_eee &lt;- Mclust(iris[,-5],G=3,verbose=FALSE, modelNames=\"EEE\")\nmb_vev &lt;- Mclust(iris[,-5],G=3,verbose=FALSE, modelNames=\"VEV\")\n\nmb_vii$parameters$variance$sigma\n\n, , 1\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length     0.075755    0.000000     0.000000    0.000000\nSepal.Width      0.000000    0.075755     0.000000    0.000000\nPetal.Length     0.000000    0.000000     0.075755    0.000000\nPetal.Width      0.000000    0.000000     0.000000    0.075755\n\n, , 2\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1629047   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1629047    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1629047   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1629047\n\n, , 3\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1635866   0.0000000    0.0000000   0.0000000\nSepal.Width     0.0000000   0.1635866    0.0000000   0.0000000\nPetal.Length    0.0000000   0.0000000    0.1635866   0.0000000\nPetal.Width     0.0000000   0.0000000    0.0000000   0.1635866\n\nmb_vii$bic\n\n[1] -853.8144\n\n\nFor the VII model, the covariance matrices are uncorrelated and the variances of the inputs are the same. Compared to the EII model of the previous run, the variances now differ across the clusters.\nNote that mclust computes the BIC statistic in a “larger-is-better” form—see the note in Section 21.2.1 on the two versions of the BIC criterion.\n\nmb_vei$parameters$variance$sigma\n\n, , 1\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.1190231  0.00000000    0.0000000   0.0000000\nSepal.Width     0.0000000  0.07153324    0.0000000   0.0000000\nPetal.Length    0.0000000  0.00000000    0.0803479   0.0000000\nPetal.Width     0.0000000  0.00000000    0.0000000   0.0169908\n\n, , 2\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.2655657   0.0000000    0.0000000  0.00000000\nSepal.Width     0.0000000   0.1596058    0.0000000  0.00000000\nPetal.Length    0.0000000   0.0000000    0.1792731  0.00000000\nPetal.Width     0.0000000   0.0000000    0.0000000  0.03791006\n\n, , 3\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.3380558   0.0000000    0.0000000  0.00000000\nSepal.Width     0.0000000   0.2031725    0.0000000  0.00000000\nPetal.Length    0.0000000   0.0000000    0.2282084  0.00000000\nPetal.Width     0.0000000   0.0000000    0.0000000  0.04825817\n\nmb_vei$bic\n\n[1] -779.1566\n\n\nThe VEI model introduces different variances for the attributes and has a larger (better) BIC than the VEI model.\n\nmb_eee$parameters$variance$sigma\n\n, , 1\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26392812  0.08980761   0.16970269  0.03931039\nSepal.Width    0.08980761  0.11191917   0.05105566  0.02992418\nPetal.Length   0.16970269  0.05105566   0.18674400  0.04198254\nPetal.Width    0.03931039  0.02992418   0.04198254  0.03965846\n\n, , 2\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26392812  0.08980761   0.16970269  0.03931039\nSepal.Width    0.08980761  0.11191917   0.05105566  0.02992418\nPetal.Length   0.16970269  0.05105566   0.18674400  0.04198254\nPetal.Width    0.03931039  0.02992418   0.04198254  0.03965846\n\n, , 3\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26392812  0.08980761   0.16970269  0.03931039\nSepal.Width    0.08980761  0.11191917   0.05105566  0.02992418\nPetal.Length   0.16970269  0.05105566   0.18674400  0.04198254\nPetal.Width    0.03931039  0.02992418   0.04198254  0.03965846\n\nmb_eee$bic\n\n[1] -632.9647\n\n\nThe EEE model introduced covariances between the inputs but keeps the covariance matrices constant across clusters. Its BIC value of -632.965 is a further improvement over that of the VEI model.\n\nmb_vev$parameters$variance$sigma\n\n, , 1\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.13320850  0.10938369  0.019191764 0.011585649\nSepal.Width    0.10938369  0.15495369  0.012096999 0.010010130\nPetal.Length   0.01919176  0.01209700  0.028275400 0.005818274\nPetal.Width    0.01158565  0.01001013  0.005818274 0.010695632\n\n, , 2\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.22572159  0.07613348   0.14689934  0.04335826\nSepal.Width    0.07613348  0.08024338   0.07372331  0.03435893\nPetal.Length   0.14689934  0.07372331   0.16613979  0.04953078\nPetal.Width    0.04335826  0.03435893   0.04953078  0.03338619\n\n, , 3\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.42943106  0.10784274   0.33452389  0.06538369\nSepal.Width    0.10784274  0.11596343   0.08905176  0.06134034\nPetal.Length   0.33452389  0.08905176   0.36422115  0.08706895\nPetal.Width    0.06538369  0.06134034   0.08706895  0.08663823\n\nmb_vev$bic\n\n[1] -562.5522\n\n\nThe VEV model allows for correlated attributes and varies volume and orientation across clusters, keeping the shape of the densities the same. Its BIC value of -562.552 indicates a further improvement over the previous models.\n\n\n\n\nFull Analysis Choosing \\(k\\) and Covariance Structure\nThe choice of covariance structures and the number of mixture components can be combined into a single analysis.\n\n\nExample: Selecting Structure and Number of Components\n\n\nTo obtain a full analysis of relevant covariance structures for \\(1 \\le k \\le 9\\), simply do not specify the G= or modelNames= arguments. Alternatively, you can limit the number of combinations considered by specifying vectors for these options.\n\nmbc &lt;- Mclust(iris[,-5],verbose=FALSE)\nsummary(mbc)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VEV (ellipsoidal, equal shape) model with 2 components: \n\n log-likelihood   n df       BIC       ICL\n       -215.726 150 26 -561.7285 -561.7289\n\nClustering table:\n  1   2 \n 50 100 \n\n\nBased on \\(9 \\times 10 = 90\\) models considered, Mclust choose the VEV structure with \\(k=2\\) mixture components. Its BIC value is -561.7285. You can see the BIC values of all 90 models and a list of the top three models in the BIC object:\n\nmbc$BIC\n\nBayesian Information Criterion (BIC): \n         EII        VII        EEI        VEI        EVI        VVI       EEE\n1 -1804.0854 -1804.0854 -1522.1202 -1522.1202 -1522.1202 -1522.1202 -829.9782\n2 -1123.4117 -1012.2352 -1042.9679  -956.2823 -1007.3082  -857.5515 -688.0972\n3  -878.7650  -853.8144  -813.0504  -779.1566  -797.8342  -744.6382 -632.9647\n4  -893.6140  -812.6048  -827.4036  -748.4529  -837.5452  -751.0198 -646.0258\n5  -782.6441  -742.6083  -741.9185  -688.3463  -766.8158  -711.4502 -604.8131\n6  -715.7136  -705.7811  -693.7908  -676.1697  -774.0673  -707.2901 -609.8543\n7  -731.8821  -698.5413  -713.1823  -680.7377  -813.5220  -766.6500 -632.4947\n8  -725.0805  -701.4806  -691.4133  -679.4640  -740.4068  -764.1969 -639.2640\n9  -694.5205  -700.0276  -696.2607  -702.0143  -767.8044  -755.8290 -653.0878\n        VEE       EVE       VVE       EEV       VEV       EVV       VVV\n1 -829.9782 -829.9782 -829.9782 -829.9782 -829.9782 -829.9782 -829.9782\n2 -656.3270 -657.2263 -605.1841 -644.5997 -561.7285 -658.3306 -574.0178\n3 -605.3982 -666.5491 -636.4259 -644.7810 -562.5522 -656.0359 -580.8396\n4 -604.8371 -705.5435 -639.7078 -699.8684 -602.0104 -725.2925 -630.6000\n5        NA -723.7199 -632.2056 -652.2959 -634.2890        NA -676.6061\n6 -609.5584 -661.9497 -664.8224 -664.4537 -679.5116        NA -754.7938\n7        NA -699.5102 -690.6108 -709.9530 -704.7699 -809.8276 -806.9277\n8 -654.8237 -700.4277 -709.9392 -735.4463 -712.8788 -831.7520 -830.6373\n9        NA -729.6651 -734.2997 -758.9348 -748.8237 -882.4391 -883.6931\n\nTop 3 models based on the BIC criterion: \n    VEV,2     VEV,3     VVV,2 \n-561.7285 -562.5522 -574.0178 \n\n\nModel-\\(k\\) combinations shown as NA failed to converge based on the default settings of the EM algorithm; these can be tweaked with the control= option of Mclust. The BIC of the winning combination is\n\nmax(na.omit(apply(mbc$BIC,1,max)))\n\n[1] -561.7285\n\nwhich.max(na.omit(apply(mbc$BIC,1,max)))\n\n2 \n2 \n\n\nThe plot of the BIC values shows two distinct groups of models (Figure 25.3). The models with diagonal covariance structure generally have lower (worse) BIC values if \\(k &lt; 6\\). The difference between \\(k=2\\) and \\(k=3\\) is generally small for the models with correlations. [VEV, \\(k=3\\)] is a close competitor to the overall “best” model, [VEV, \\(k=2\\)]. Since we know that there are three species in the data set one might be tempted to go with \\(k=3\\). In fact, it seems that the \\(k=2\\) model separates I. setosa in one cluster from the other two species (Figure 25.4).\n\nplot(mbc, what=\"BIC\")\n\n\n\n\n\n\n\nFigure 25.3: BIC versus number of mixture components for all ten covariance models.\n\n\n\n\n\n\nplot(mbc, what=\"classification\",cex=0.75)\n\n\n\n\n\n\n\nFigure 25.4: Classification based on model-based clustering with VEV model, \\(k=2\\).\n\n\n\n\n\n\n\n\n\n\nFigure 25.1: Contours of bivariate (\\(p=2\\)) Gaussian distributions. The mean vector centers the distribution in the coordinate system. The variances stretch in the direction of \\(X_1\\) and \\(X_2\\). The distribution on the right is rotated in the coordinate system because of the correlation between the variables.\nFigure 25.3: BIC versus number of mixture components for all ten covariance models.\n\n\n\nFraley, Chris, and Adrian E Raftery. 2002. “Model-Based Clustering, Discriminant Analysis, and Density Estimation.” Journal of the American Statistical Association 97 (458): 611–31. https://doi.org/10.1198/016214502760047131.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model-based Clustering</span>"
    ]
  },
  {
    "objectID": "arules.html",
    "href": "arules.html",
    "title": "26  Association Rules",
    "section": "",
    "text": "26.1 Introduction\nLike other unsupervised learning methods, Association Rule Mining (ARM) tries to learn aspects of the joint density \\(p(\\textbf{X})\\) of the data. It specifically asks if in regions of \\(p(\\textbf{X})\\) where the density is high, are certain values of \\(X_j\\) associated with values of \\(X_j\\) more frequently than one would expect under a completely random allocation? In other words, are there associations between the values that tell us something interesting about the joint distribution of the data?\nThe most frequent form of ARM is market basket analysis, the name appeals to items a customer places in a shopping basket. \\(X_1, \\cdots, X_p\\) are the possible items offered for purchase, \\(X_j \\in \\{0,1\\}\\). \\(X_{ij}\\) takes on the value \\(1\\) if the \\(i\\)th customer selects the \\(j\\)th item into their basket, \\(0\\) otherwise. Market basket analysis is concerned with the properties of the \\((n \\times p)\\) matrix \\(\\textbf{X}\\) of zeros and ones, trying to understand with items are purchased together—the famous example is beer and diapers.\nSome applications of ARM are\nThe approach to association rule mining is essentially mode hunting, looking for areas of the joint distribution with high densities and finding associations of items that are more frequent than expected if shopping baskets were filled at random. This is conceptually pretty simple, it is just a matter of counting.\nAssociation rule mining is about finding associations that have a large lift factor. But that is not the only criterion by which associations are judged. A lift factor of 10 is very interesting when the items appear frequently in baskets, but not that interesting if they are purchased very rarely. In the former case one can focus promotional pricing on those items together whereas in the latter case this might not yield a lot of bang for the buck (literally).\nFinding meaningful associations can be challenging, since we are dealing with a \\(p\\)-dimensional distribution and \\(p\\) can be very large. Market basket analysis of Amazon.com involves more than 12 million products. The data are very sparse, many items are never purchased together. There is insufficient data to estimate proportions reliably. Also, we would not be interested only in associations between two items; what about sets of three or four or more items? We need a structured approach and simplify the procedure—this leads us down the path of item sets.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "arules.html#introduction",
    "href": "arules.html#introduction",
    "title": "26  Association Rules",
    "section": "",
    "text": "Market basket analysis. Source\n\n\n\n\nCustomer segmentation: grouping customers based on their buying behavior\nRetail allocation: which item should be on shelves and how should they be grouped\nRecommender systems: cross-marketing of products\nSurvey analysis: which responses go together?\nPrecursor analytics: Borne (2021) mentions this interesting application of ARM, where data are time stamped. The analysis seeks time lags at which the association strength peaks. Precursor analysis has found that solar storm events show the strongest geomagnetic effects 2–3 hours after the storm and that video cameras are bought 3 months after the purchase of a video (VHS) player.\n\n\n\n\nExample: Beers and Diapers\n\n\nBorne (2021) cites beer–diaper association as the classic example of market basket analysis, although the particular example might be more legend than reality.\nSuppose that a retail store has 600,000 transactions (shopping baskets). 7,500 (1.25%) of the shopping baskets contain diapers, 60,000 (10%) of the baskets contain beer. So we know that a basket is 8 times more likely to contain beer than diapers. In order to analyze the association of beer and diapers we need to know how many of the baskets contain both. Suppose that number is 6000 (1%).\nIf beer and diaper purchases were independent, then 10% of the diaper purchases would be accompanied by beer purchases, or, equivalently, 1.25% of beer buyers would pick up diapers. Either way, under independence of the two items we would expect \\(600,000 \\times 0.1 \\times 0.0125 = 750\\) baskets with both beer and diapers. The actual number of baskets containing both is much higher.\nThe ratio between what was observed and what was expected under independence is called the lift factor: \\[\nL = \\frac{6000}{750} = 8\n\\]",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "arules.html#association-rules-and-item-sets",
    "href": "arules.html#association-rules-and-item-sets",
    "title": "26  Association Rules",
    "section": "26.2 Association Rules and Item Sets",
    "text": "26.2 Association Rules and Item Sets\nItems are collected in item sets, for example {butter, bread} is a two-item set and {milk} is a single-item set. Association rules display relationships among item sets, for example\n\n{bread, butter} \\(\\Rightarrow\\) {jelly}\n\nThe left hand side (lhs) of the rule is called the head or the antecedent, the right hand side (rhs) is called the body or the consequent of the rule. The rule is read as follows: if the head (lhs) occurs then the body (rhs) also occurs.\nThe most important metrics by which to judge an association rule are its support, confidence, and lift.\n\nSupport\nSuppose we have \\(n\\) transactions and a rule $A \\(\\Rightarrow\\) B$ where A and B are item sets. Let \\(\\#A\\) denote the number of times A occurs in the database and \\(\\#(A,B)\\) the number of times the the sets A and B occur together. For example, if A is a three-item set, then \\(\\#A\\) is the number of records where all three items are present. if B is a single-item set, then \\(\\#(A,B)\\) is the number of records where all four items are present.\nThe support of the rule is the relative frequency with which head and body appear: \\[\nS(A \\Rightarrow B) = \\frac{\\#(A,B)}{n}\n\\]\n\n\nConfidence\nThe confidence of the rule is the estimate of the conditional probability to find the body (the consequent) of the rule in transactions given that they contain the head (the antecedent):\n\\[\nC(A \\Rightarrow B) = \\frac{\\#(A,B)}{\\#A}\n\\] It is a measure of predictability of the association between A and B.\nSupport can be beneficial for finding the connection between products in comparison to the whole database, whereas confidence looks at the connection between item sets. A support threshold is often used to focus on those item sets that are interesting because they are frequent. For example, a support threshold of 0.2 implies that only item set that appear in at least 20% of the transactions are considered. The confidence, on the other hand, tells us how likely it is that B occurs every time A occurs.\nIt is possible that confidence is high but support is low. For example, {caviar} \\(\\Rightarrow\\) {vodka} is a high-confidence rule, because given that caviar is purchased it is likely to be accompanied by a vodka purchase. It is a low-support rule because caviar is not purchased frequently. An algorithm that sets a support threshold might miss association rules with low-support and high confidence.\nA rule can have 100% confidence if the head requires the body—that is, every time the head occurs the body also occurs. For example, if opening a savings account requires having a checking account, then \\[\nC(\\text{savings} \\Rightarrow \\text{checking}) = \\frac{\\#(\\text{checking and savings})}{\\#\\text{savings}}=1\n\\] Knowing that a customer has a savings account, you can say with certainty that they also have a checking account. The body (checking account) is completely predictable from the head (savings account).\n\n\nLift\nThe lift of an association rule measured how much more likely the association is compared to independence of the items: \\[\nL(A \\Rightarrow B) = n \\frac{\\#(A,B)}{\\#A \\#B} = \\frac{C(A\\Rightarrow B)}{S(B)}\n\\] The lift factor is an estimate of the ratio of the joint probabilities \\[\n\\frac{\\Pr(A,B)}{\\Pr(A)\\Pr(B)}\n\\]\n\nWhen \\(L(A \\Rightarrow B) = 1\\), A and B are not associated with each other (independent).\nWhen \\(L(A\\Rightarrow B) &gt; 1\\), then A and B occur more frequently together than under a random selection of items—A and B have a positive association.\nWhen \\(L(A \\Rightarrow B) &lt; 1\\), then A and B occur less frequently together than expected under a random selection of items—A and B have a negative association.\n\n\n\nExample\n\n\nA supermarket has 80 shopping baskets, 60 of them contain bread, 50 contain peanut butter. 40 baskets contain both bread and peanut butter.\n\nSupport for {bread} = 60/80 = 0.75\nSupport for {peanut butter} = 50/80 = 0.625\nSupport for {bread, peanut butter} = 40/80 = 0.5\nConfidence for rule {bread} \\(\\Rightarrow\\) {peanut butter} = 40/60 = 0.666\nLift for rule {bread} \\(\\Rightarrow\\) {peanut butter} = 80 x 40 / (60 x 50) = 1.06\n\n\n\nAnother example calculation follows before we apply an efficient algorithm to find rules in a large database.\n\n\nExample\n\n\nSuppose we have indicator variables \\(X_1, \\cdots, X_5\\) for five items and the following baskets\n\n\n\nBasket\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\n\\(X_5\\)\n\n\n\n\n1\n1\n1\n1\n0\n0\n\n\n2\n1\n0\n1\n1\n0\n\n\n3\n0\n1\n1\n1\n0\n\n\n4\n1\n0\n0\n1\n1\n\n\n5\n0\n1\n1\n0\n1\n\n\n\nThe next table shows support, confidence, and lift for a series of rules\n\n\n\nRule\nSupport\nConfidence\nLift\n\n\n\n\n\\(X_1 \\Rightarrow X_4\\)\n2/5\n2/3\n10/9\n\n\n\\(X_3 \\Rightarrow X_1\\)\n2/5\n2/4\n5/6\n\n\n\\(X_1 \\Rightarrow X_3\\)\n2/5\n2/3\n5/6\n\n\n\\((X_2,X_3) \\Rightarrow X_4\\)\n1/5\n1/3\n5/9",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "arules.html#apriori-algorithm",
    "href": "arules.html#apriori-algorithm",
    "title": "26  Association Rules",
    "section": "26.3 Apriori Algorithm",
    "text": "26.3 Apriori Algorithm\nThe math to compute support, confidence, and lift of rules is simple, it just means counting occurrences in a database of transactions. What makes ARM tricky is the large number of rules. For \\(p\\) items, there are \\(2^p-1\\) non-empty item sets. An algorithmic approach is needed to find interesting rules, especially when \\(p\\) and \\(n\\) are large.\nAlso, we want to query the database with queries such as\n\nFind all rules that have “bagels” in the antecedent. Find all rules that have “sausage” in the antecedent and “mustard” in the consequent.\n\n\n\n\n\n\n\nCaution\n\n\n\nWhen analyzing large databases of baskets, you will find a large number of spurious associations. Those occur when items appear together more frequently compared to independence, but only by chance. Suppose we apply a 5% significance rule, meaning that we declare associations as “significant” if they appear beyond the 95th percentile of the frequency distribution. A market basket analysis with 1,000,000 two-item sets in which there are no associations whatsoever will declare 50,000 associations as “significant” although none of them are relevant.\n\n\nThe Apriori algorithm of Agrawal, Imieliński, and Swami (1993) is based on setting a support threshold \\(s\\) and a confidence threshold \\(c\\). Multiple passes over the data are made. In the first pass the support of single-item sets is computed and any items with \\(S(A) &lt; s\\) are discarded. In the second pass the support of two-item sets is computed and any sets with \\(S(A,B) &lt; s\\) are again discarded. This continues until all candidate rules in a pass have support less than \\(s\\).\nBased on the item sets determined so far, only those association rules are displayed for which the confidence exceeds the threshold \\(c\\).\nThe Apriori algorithm makes it feasible to find relevant association rules in very large databases. It works because \\(S(A,B) \\le \\min(S(A),S(B))\\), in other words the support of any sets found on a subsequent pass cannot exceed the support of the subsets of the previous pass. However, the algorithm also has some disadvantages:\n\nIt can create large candidate sets of rules and thus lead to many spurious rules.\nIt requires a query engine to filter and summarize candidate item sets.\nIt passes through the database multiple times, once for each cardinality of item sets.\nIt is unlikely to find rules with high confidence and low support because items are filtered first based on the support threshold—only item sets that pass this threshold are considered in computing confidence, which is a measure comparing item sets.\n\n\nExample Analysis with arules\n\n\nExample: Grocery Shopping\n\n\nThe apriori function in the arules package in R performs association rule mining based on the Apriori algorithm. The input data to the function is an object of class transactions, which is based on a sparse matrix representation of item occurrences.\nWe analyze here the Groceries data set that is part of the arules package. This data set contains 1 month (30 days) of real-world point-of-sale transaction data from a typical local grocery outlet. The data set contains 9835 transactions and the items are aggregated into 169 categories.\n\nlibrary(arules)\ndata(Groceries)\n\nsummary(Groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n       labels  level2           level1\n1 frankfurter sausage meat and sausage\n2     sausage sausage meat and sausage\n3  liver loaf sausage meat and sausage\n\n\nThe data are very sparse, which is typical for these applications. Only 2.6% of the 9835 x 169 possible item–transaction combinations are non-zero. The most frequently item is whole milk, its support is 2513/9835 = 0.255516. There are 2159 single-item sets, 1643 two-item sets, and one 32-item set. The median item set size is 3.\nYou can query the raw data for the transactions with the inspect function:\n\ninspect(Groceries[1:3])\n\n    items                 \n[1] {citrus fruit,        \n     semi-finished bread, \n     margarine,           \n     ready soups}         \n[2] {tropical fruit,      \n     yogurt,              \n     coffee}              \n[3] {whole milk}          \n\n\nThe first transaction involves four items, the second transaction two items, the third transaction was a purchase of whole milk alone.\nThe itemFrequencyPlot function helps to inspect the frequency distribution of the items in the database. The following call requests the 20 items with the highest absolute frequency.\n\nitemFrequencyPlot(Groceries,topN=20,type=\"absolute\",cex.names=0.75)\n\n\n\n\n\n\n\n\nBy default, the apriori function mines for rules with a support of \\(s &gt; 0.1\\) and a confidence of \\(c &gt; 0.8\\). The following call changes these values. Based on the summary above, it should only find three rules in the database, based on single-item sets for whole milk, other vegetables, and rolls/buns.\n\nrules &lt;- apriori(Groceries, \n                 parameter=list(support=0.18,conf=0))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n          0    0.1    1 none FALSE            TRUE       5    0.18      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 1770 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [3 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 done [0.00s].\nwriting ... [3 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\ninspect(rules)\n\n    lhs    rhs                support   confidence coverage lift count\n[1] {}  =&gt; {rolls/buns}       0.1839349 0.1839349  1        1    1809 \n[2] {}  =&gt; {other vegetables} 0.1934926 0.1934926  1        1    1903 \n[3] {}  =&gt; {whole milk}       0.2555160 0.2555160  1        1    2513 \n\n\nAs expected, only three rules are found by the algorithm since only three single items exceed the support threshold of 0.18. It would have been possible to find rules based on two-item sets constructed from whole milk, other vegetables or rolls\\buns on the second pass, but none of those combinations had support &gt; 0.18.\nThe head of the rules is displayed as lhs and the body as rhs for the left hand side and right hand side of the rule, respectively. The coverage is the support of the lhs which shows that the set {} is to be interpreted as all transactions. That also explains why the confidence of the single-item rules is equal to their support.\nChanging the value for support to a smaller number generates more rules.\n\nrules &lt;- apriori(Groceries, \n                 parameter=list(support=0.001,conf=0.8),\n                 control = list(verbose=F))\nsummary(rules)\n\nset of 410 rules\n\nrule length distribution (lhs + rhs):sizes\n  3   4   5   6 \n 29 229 140  12 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   4.000   4.000   4.329   5.000   6.000 \n\nsummary of quality measures:\n    support           confidence        coverage             lift       \n Min.   :0.001017   Min.   :0.8000   Min.   :0.001017   Min.   : 3.131  \n 1st Qu.:0.001017   1st Qu.:0.8333   1st Qu.:0.001220   1st Qu.: 3.312  \n Median :0.001220   Median :0.8462   Median :0.001322   Median : 3.588  \n Mean   :0.001247   Mean   :0.8663   Mean   :0.001449   Mean   : 3.951  \n 3rd Qu.:0.001322   3rd Qu.:0.9091   3rd Qu.:0.001627   3rd Qu.: 4.341  \n Max.   :0.003152   Max.   :1.0000   Max.   :0.003559   Max.   :11.235  \n     count      \n Min.   :10.00  \n 1st Qu.:10.00  \n Median :12.00  \n Mean   :12.27  \n 3rd Qu.:13.00  \n Max.   :31.00  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.8\n                                                                                                  call\n apriori(data = Groceries, parameter = list(support = 0.001, conf = 0.8), control = list(verbose = F))\n\n\nThis query generated 410 rules, four-item rules most frequent among them. Their confidence ranges from 0.8 to 1.0, their lift ranges from 3.131 to 11.235.\nThe top 10 rules by decreasing lift are\n\noptions(digits=5)\nknitr::kable(DATAFRAME(rules[1:10]))\n\n\n\n\n\n\n\n\n\n\n\n\n\nLHS\nRHS\nsupport\nconfidence\ncoverage\nlift\ncount\n\n\n\n\n{liquor,red/blush wine}\n{bottled beer}\n0.00193\n0.90476\n0.00214\n11.2353\n19\n\n\n{curd,cereals}\n{whole milk}\n0.00102\n0.90909\n0.00112\n3.5579\n10\n\n\n{yogurt,cereals}\n{whole milk}\n0.00173\n0.80952\n0.00214\n3.1682\n17\n\n\n{butter,jam}\n{whole milk}\n0.00102\n0.83333\n0.00122\n3.2614\n10\n\n\n{soups,bottled beer}\n{whole milk}\n0.00112\n0.91667\n0.00122\n3.5875\n11\n\n\n{napkins,house keeping products}\n{whole milk}\n0.00132\n0.81250\n0.00163\n3.1798\n13\n\n\n{whipped/sour cream,house keeping products}\n{whole milk}\n0.00122\n0.92308\n0.00132\n3.6126\n12\n\n\n{pastry,sweet spreads}\n{whole milk}\n0.00102\n0.90909\n0.00112\n3.5579\n10\n\n\n{turkey,curd}\n{other vegetables}\n0.00122\n0.80000\n0.00153\n4.1345\n12\n\n\n{rice,sugar}\n{whole milk}\n0.00122\n1.00000\n0.00122\n3.9137\n12\n\n\n\n\n\nThe association with the highest lift factor is the purchase of beer when the basket contains liquor and wine. The confidence of this transaction is very high, making beer purchase highly predictable from the presence of liquor and wine.\nThe top 10 rules by decreasing confidence are\n\nrules &lt;- sort(rules, by=\"confidence\", decreasing=TRUE)\nknitr::kable(DATAFRAME(rules[1:10]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLHS\nRHS\nsupport\nconfidence\ncoverage\nlift\ncount\n\n\n\n\n10\n{rice,sugar}\n{whole milk}\n0.00122\n1\n0.00122\n3.9137\n12\n\n\n16\n{canned fish,hygiene articles}\n{whole milk}\n0.00112\n1\n0.00112\n3.9137\n11\n\n\n33\n{root vegetables,butter,rice}\n{whole milk}\n0.00102\n1\n0.00102\n3.9137\n10\n\n\n60\n{root vegetables,whipped/sour cream,flour}\n{whole milk}\n0.00173\n1\n0.00173\n3.9137\n17\n\n\n62\n{butter,soft cheese,domestic eggs}\n{whole milk}\n0.00102\n1\n0.00102\n3.9137\n10\n\n\n68\n{citrus fruit,root vegetables,soft cheese}\n{other vegetables}\n0.00102\n1\n0.00102\n5.1682\n10\n\n\n126\n{pip fruit,butter,hygiene articles}\n{whole milk}\n0.00102\n1\n0.00102\n3.9137\n10\n\n\n133\n{root vegetables,whipped/sour cream,hygiene articles}\n{whole milk}\n0.00102\n1\n0.00102\n3.9137\n10\n\n\n135\n{pip fruit,root vegetables,hygiene articles}\n{whole milk}\n0.00102\n1\n0.00102\n3.9137\n10\n\n\n143\n{cream cheese ,domestic eggs,sugar}\n{whole milk}\n0.00112\n1\n0.00112\n3.9137\n11\n\n\n\n\n\nYou can limit the maximum length of rules generated with the maxlen= parameter. Setting the maximum length to 3 reduces the number of generated ruls to 29.\n\n# setting max length of rules to 3\nrules &lt;- apriori(Groceries, \n                 parameter = list(supp=0.001, \n                                  conf=0.8,\n                                  maxlen=3),\n                 control = list(verbose=F))\nsummary(rules)\n\nset of 29 rules\n\nrule length distribution (lhs + rhs):sizes\n 3 \n29 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3       3       3       3       3       3 \n\nsummary of quality measures:\n    support          confidence       coverage            lift      \n Min.   :0.00102   Min.   :0.800   Min.   :0.00112   Min.   : 3.13  \n 1st Qu.:0.00112   1st Qu.:0.812   1st Qu.:0.00122   1st Qu.: 3.26  \n Median :0.00122   Median :0.846   Median :0.00153   Median : 3.61  \n Mean   :0.00147   Mean   :0.861   Mean   :0.00173   Mean   : 4.00  \n 3rd Qu.:0.00173   3rd Qu.:0.909   3rd Qu.:0.00214   3rd Qu.: 4.20  \n Max.   :0.00254   Max.   :1.000   Max.   :0.00315   Max.   :11.24  \n     count     \n Min.   :10.0  \n 1st Qu.:11.0  \n Median :12.0  \n Mean   :14.5  \n 3rd Qu.:17.0  \n Max.   :25.0  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.8\n                                                                                                           call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.8, maxlen = 3), control = list(verbose = F))\n\n\nAn application of the querying engine is to target items. For example, if folks are buying whole milk, what are they also buying? Report the 5 associations with the highest lift:\n\nrules &lt;- apriori(data=Groceries,\n                 parameter=list(supp=0.001,conf=0.15,maxlen=3), \n                 appearance=list(lhs=\"whole milk\",default=\"rhs\"),\n                 control = list(verbose=F))\nrules &lt;- sort(rules, decreasing=TRUE,by=\"lift\")\nknitr::kable(DATAFRAME(rules[1:5]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLHS\nRHS\nsupport\nconfidence\ncoverage\nlift\ncount\n\n\n\n\n5\n{whole milk}\n{root vegetables}\n0.04891\n0.19140\n0.25552\n1.7560\n481\n\n\n4\n{whole milk}\n{tropical fruit}\n0.04230\n0.16554\n0.25552\n1.5776\n416\n\n\n7\n{whole milk}\n{yogurt}\n0.05602\n0.21926\n0.25552\n1.5717\n551\n\n\n9\n{whole milk}\n{other vegetables}\n0.07483\n0.29288\n0.25552\n1.5136\n736\n\n\n8\n{whole milk}\n{rolls/buns}\n0.05663\n0.22165\n0.25552\n1.2050\n557\n\n\n\n\n\narulesViz::plot has specialized plotting methods for association rules. method=\"scatterplot\" requires two quality measures and shades the rules by a third variable (lift is the default). method=\"matrix\" arranges the rules by antecedent (head, lhs) and consequent (body, rhs).\n\nlibrary(arulesViz)\nrules &lt;- apriori(Groceries, \n                 parameter=list(support=0.001,conf=0.5),\n                 control = list(verbose=F))\nplot(rules,\n     method=\"scatterplot\",\n     measure=c(\"support\",\"confidence\"),\n     shading=\"lift\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgrawal, Rakesh, Tomasz Imieliński, and Arun Swami. 1993. “Mining Association Rules Between Sets of Items in Large Databases.” In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, 207–16. SIGMOD ’93. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/170035.170072.\n\n\nBorne, Kirk. 2021. “Association Rule Mining — Not Your Typical ML Algorithm.” Medium. https://medium.com/@kirk.borne/association-rule-mining-not-your-typical-ml-algorithm-97acda6b86c2.",
    "crumbs": [
      "Part VI. Unsupervised Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Association Rules</span>"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "27  Generalized Linear Models",
    "section": "",
    "text": "27.1 Introduction\nWe have encountered generalized linear models (GLMs) throughout the material and one could argue why they belong into a segment of advanced supervised learning techniques. After all, generalized linear models are very common today. We have placed them here because this part of the material is more “mathy” than previous sections and we want to dive a bit more into the distributional properties of the models in this part of the material than in other parts.\nThe generalization of GLMs is found in a comparison to the classical linear model with Gaussian errors: \\[\nY = \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\epsilon, \\quad \\epsilon \\sim \\textit{ iid } G(0, \\sigma^2)\n\\] Because of the linearity property of Gaussian random variables, the normality of \\(\\epsilon\\) transfers to \\(Y\\): \\(Y \\sim \\textit{ iid } G(\\textbf{x}^\\prime\\boldsymbol{\\beta},\\sigma^2)\\). We are now relaxing several elements of this model:\nHowever, the relaxation of the model conditions is not without limits. Rather than allowing \\(Y\\) to have any distribution, its distribution has to be a member of a special family of probability distributions. Rather than allowing any arbitrary nonlinear relationship between inputs and target, only certain (invertible) transformations are permitted; the effect of the inputs remains linear on some scale, although it is usually not the scale of the mean. Rather than allowing any arbitrary variance, the variance of the targets can be unequal but it is determined through the distribution itself.\nThis sounds like we are placing many restrictions on the relaxations but it turns out that the class of generalized linear models is very broad and general. A generalized linear model has the following components:\nA further generalization of the generalization is achieved by modeling the effects of the inputs through smooth local elements (see Chapter 11), and not just through global effects. The linear predictor now is a combination of smooth functions of the inputs, \\[\n\\eta = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots\n\\] This is the structure of generalized additive models (GAMs). A semi-parametric version combines global effects with smooth local effects, for e example \\[\n\\eta = \\beta_0 + \\beta_1 x_1 + f_2(x_2)\n\\] These linear predictor types make up the family of generalized models in Figure 27.1.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-intro",
    "href": "glm.html#sec-glm-intro",
    "title": "27  Generalized Linear Models",
    "section": "",
    "text": "The distribution of \\(Y\\) does not have to be Gaussian.\nThe relationship between the inputs and the mean of \\(Y\\) does not have to be linear in the coefficients.\nThe model does not have an additive error structure.\nThe target variables do not have to have the same variance.\n\n\n\n\nThe distribution of \\(Y\\) is a member of the exponential family of distributions (Section 27.2).\nThe effects are a linear combination of the inputs, called the linear predictor \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\).\nThe linear predictor \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\) is related to the mean through a link function \\(g(\\mu) = \\eta\\) (Section 27.3).\n\n\n\n\n\n\n\n\nFigure 27.1: Generalized model mind map",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-Pexpo",
    "href": "glm.html#sec-glm-Pexpo",
    "title": "27  Generalized Linear Models",
    "section": "27.2 Exponential Family of Distributions",
    "text": "27.2 Exponential Family of Distributions\n\nDefinition\nThe exponential family of distributions, \\(P_{expo}\\) comprises important discrete and continuous distributions (Figure 27.1), the exponential distribution is one particular member of the family. How do we know whether a distribution belongs to the family? If you can write its mass or density function in a particular form, then the distribution is a member. We follow here (for the most part) the notation in the seminal work on GLMs by McCullagh and Nelder Frs (1989).\n\n\nDefinition: Exponential Family of Distributions\n\n\nThe distribution of random variable \\(Y\\) is a member of \\(P_{expo}\\), the exponential family of distributions, if its density or mass function can be written as \\[\np(y) = \\exp\\left\\{(y\\theta - b(\\theta))/\\psi + c(y,\\psi)\\right\\}\n\\tag{27.1}\\]\nfor some functions \\(b(\\cdot)\\) and \\(c(\\cdot)\\).\n\\(\\theta\\) is called the natural parameter and \\(\\psi\\) is related to the scale (dispersion) of \\(Y\\). Not all members of \\(P_{expo}\\) feature this parameter. Those that do are said to be in the two-parameter exponential family.\nWhen \\(\\theta\\) is expressed as a function of the mean \\(\\mu = \\text{E}[Y]\\), the function \\(\\theta(\\mu)\\) is called the canonical (natural) link function of the distribution.\nDistributions in the exponential family have many fascinating properties. Among them,\n\\[\\frac{\\partial b(\\theta(\\mu))}{\\partial \\theta} = b^\\prime(\\theta) = \\mu\\]\n\\[\\frac{\\partial^2 b(\\theta(\\mu))}{\\partial \\theta^2} = b^{\\prime\\prime}(\\theta) = \\text{Var}[Y]\\] The first result states that \\(b^\\prime(\\theta) = \\mu\\) is the inverse canonical link function. \\(b^{\\prime\\prime}(\\theta)\\), when written as a function of \\(\\mu\\), is called the variance function of the distribution.\n\n\n\n\nThe Bernoulli in Exponential Form\nThe expressions for mass functions or densities we have worked with so far are not written in the form of \\(P_{expo}\\). The mass function of the Bernoulli() distribution, for example, is usually written as \\[\np(y) = \\pi^y (1-\\pi)^{1-y}, \\quad y \\in \\{0,1\\}\n\\] If we rewrite this to fit Equation 27.1, we should be able to peel off \\(\\theta\\), \\(b(\\theta)\\), then we can identify link function and variance function of the generalized linear model. Let’s start by replacing \\(\\pi\\), the event probability of the Bernoulli with the symbol \\(\\mu\\), \\(\\pi\\) is the mean of \\(Y\\) after all. Then, put the mass function into an exponent while applying logarithms and collect terms: \\[\n\\begin{align*}\np(y) &= \\mu^y (1-\\mu)^{1-y} \\\\\n     &= \\exp\\{y \\log \\mu + (1-y) \\log (1-\\mu) \\} \\\\\n     &= \\exp\\left\\{y \\log\\left(\\frac{\\mu}{1-\\mu} \\right) + \\log(1-\\mu)\\right\\}\n\\end{align*}\n\\] Compare this expression to Equation 27.1 and identify:\n\n\\(\\theta(\\mu) = \\log(\\mu/(1-\\mu))\\)\n\\(b(\\theta) = -\\log(1-\\mu)\\)\n\\(\\psi = 1\\)\n\\(c(y,\\psi) = 0\\).\n\nSolve 1. for \\(\\mu\\) and plug into 2. to get \\(b(\\theta) = \\log(1+e^\\theta)\\). Now you can take first and second derivatives with respect to \\(\\theta\\): \\[\n\\begin{align*}\nb^\\prime(\\theta) &= e^\\theta/(1+e^{\\theta}) = \\mu\\\\\nb^{\\prime\\prime}(\\theta) &= \\mu(1-\\mu)\n\\end{align*}\n\\] The canonical (natural) link function for the Bernoulli distribution is the logit function: \\(\\text{logit}(\\mu) = \\log(\\mu/(1-\\mu))\\). The inverse link function \\(b^\\prime(\\theta)\\) is the logistic function \\[\nb^\\prime(\\theta) = \\frac{e^\\theta}{1+e^{\\theta}} = \\frac{1}{1+e^{-\\theta}}\n\\] Logistic regression derives its name from this inverse link function.\nThe specification of a generalized linear model for binary data is now complete by equating the natural parameter with the linear predictor: \\[\n\\begin{align*}\nY &\\sim \\text{Bernoulli}(\\mu) \\\\\n\\eta         &= \\textbf{x}^\\prime\\boldsymbol{\\beta}\\\\\ng(\\mu)       &= \\log \\left(\\frac{\\mu}{1-\\mu} \\right) = \\eta \\\\\ng^{-1}(\\eta) &= \\frac{1}{1+e^{-\\eta}} = \\mu\n\\end{align*}\n\\]\nWe generally use the notation \\(g(\\mu)\\) for the link function and \\(g^{-1}(\\mu)\\) for the inverse link function, rather than \\(\\theta(\\mu)\\) and \\(b^\\prime(\\theta)\\) as suggested by the \\(P_{expo}\\) notation. The reason for the general notation is that the canonical link is just one of the possible link functions, we are free to choose others (Section 27.3). Using the canonical link in the specification of a GLM typically has certain computational advantages, but for some distributions the canonical link is not a good mapping between mean and linear predictor.\n\n\nDistributions\nWe now review some of the discrete and continuous distributions in \\(P_{expo}\\)\n\nDiscrete distributions\n\nBernoulli\nThe most elementary random experiment is the Bernoulli experiment, two possible outcomes \\(Y=1\\) and \\(Y=0\\) occur with probability \\(\\pi\\) and \\(1-\\pi\\), respectively. The outcome coded \\(Y=1\\) is called the event of the experiment. The outcome coded \\(Y=0\\) is called the non-event. As we established earlier, the Bernoulli(\\(\\pi\\)) distribution is a member of the exponential family with mean \\(\\pi\\), canonical link \\(\\log(\\pi/(1-\\pi))\\) and variance function \\(\\pi(1-\\pi)\\) (Figure 27.2).\n\n\n\n\n\n\n\n\nFigure 27.2: Bernoulli(0.7) probability mass function.\n\n\n\n\n\nAlthough elementary, the Bernoulli(\\(\\pi\\)) is very important as the underlying model for logistic regression and because many other distributions are defined in terms of Bernoulli experiments.Consider a sequence \\(Y_1, Y_2, \\cdots\\) of independent Bernoulli(\\(\\pi\\)) experiments. The following members of \\(P_{expo}\\) are defined in terms of the \\(Y_i\\):\n\nBinomial\\((n,\\pi)\\): the number of events in \\(n\\) experiments.\nGeometric\\((\\pi)\\): the number of experiments until 1st event.\nGeometric\\(^*(\\pi)\\): the number of non-events before the 1st event\nNegative Binomial\\((k,\\pi)\\): the number of experiments until \\(k\\)th event.\nNegative Binomial\\(^*(k,\\pi)\\): the number of non-events before the \\(k\\)th event.\n\n\n\nBinomial\nThe number of events \\(X = \\sum_{i=1}^n Y_i\\) in a sequence \\(Y_1, Y_2, \\cdots, Y_n\\) of independent Bernoulli (\\(\\pi\\)) events is a Binomial(\\(n,\\pi\\)) random variable \\[\n\\begin{align*}\n    p(x) &= {n \\choose x} \\pi^x \\, (1-\\pi)^{n-x} \\quad x=0,1,\\cdots, n \\\\\n    \\text{E}[X] &= n\\pi \\\\\n    \\text{Var}[X] &= n\\pi(1-\\pi)\n\\end{align*}\n\\]\n\\({n \\choose x}\\) is the binomial coefficient: \\({n \\choose x} = \\frac{n!}{x!(n-x)!}\\), the number of ways in which an unordered subset of size \\(k\\) can be chosen from \\(n\\) items.\nThe support of the Binomial(\\(n,\\pi\\)) is \\(0,\\cdots,n\\). You cannot observe fewer than 0 events and you cannot observe more than \\(n\\) events (Figure 27.3). The Binomial is thus a suitable probability model for count data that represents the number of events out of a total number. For example, the number of livestock herds out of all herds in a state that show incidences of pneumonia or the number of defective items out of a lot of 100 items. Such count variables are sometimes referred to as counts with a natural denominator.\n\n\n\n\n\n\n\n\nFigure 27.3: Binomial(\\(8,0.3\\)) probability mass function.\n\n\n\n\n\n\n\nNegative Binomial and Geometric\nThe Geometric and Negative Binomial distributions have infinite support, \\(Y \\in \\{0,1, \\cdots\\}\\). Having to wait very long until the 1st (or \\(k\\)th) event occurs is unlikely if \\(\\pi\\) is large but it is not impossible. The Geometric distribution is obviously a special case of the Negative Binomial for \\(k=1\\). These models are frequently used to model counts that do not have a natural demoninator, for example, the number of occurrences per day, per 100,000 population, or per mile.\n\n\n\n\n\n\n\n\nFigure 27.4: Geometric(\\(0.5\\)) probability mass function.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.5: Negative Binomial(\\(5,0.7\\)) probability mass function.\n\n\n\n\n\n\n\nPoisson\nA random variable has a Poisson distribution with parameter \\(\\lambda\\) if its mass function is given by\n\\[\n\\begin{align*}\n\\Pr(Y=y) &= \\frac{e^{-\\lambda} \\lambda^y} {y!} \\quad y=0,1,\\cdots\\\\\n    &= \\exp\\{ y \\log(\\lambda) - \\lambda - \\log(y!) \\}\n\\end{align*}\n\\] From the last expression you see the components in the \\(P_{expo}\\) family:\n\nCanonical link: \\(\\theta = \\log(\\lambda)\\)\n\\(b(\\theta) = e^\\theta\\)\n\\(b^\\prime(\\theta) = b^{\\prime\\prime}(\\theta) = e^\\theta\\)\n\\(\\text{E}[Y] = \\text{Var}[Y] = \\lambda\\)\n\n\n\n\n\n\n\n\n\nFigure 27.6: Poisson(2.4) (left) and Poisson(0.2) (right) probability mass functions.\n\n\n\n\n\nIf events occur independently and at a constant rate, the number of events per unit is a Poisson random variable. The Poisson is not directly related to Bernoulli experiment but there is a connection through the Poisson approximation to the Binomial. In a Binomial\\((n,\\pi)\\) process, if \\(n \\rightarrow \\infty\\) and \\(n\\pi \\rightarrow \\lambda\\), then the probabilities can be approximated by those of a Poisson\\((\\lambda)\\) process (Figure 27.7).\n\n\n\n\n\n\n\n\nFigure 27.7: Binomial(200,0.05) (left) and Poisson(10) (right) probability mass functions.\n\n\n\n\n\n\n\nExample: Poisson Approximation to Binomial\n\n\nIf three dice are rolled the probability of three sixes turning up is \\(1/6^3 = 1/216 = 0.0046\\). If three dice are rolled 200 times, what is the probability of at least one triple six?\nThis is a \\(X \\sim\\) Binomial(200,1/216) experiment and we are interested in \\(1-\\Pr(X=0)\\) \\[\n    \\Pr(\\text{at least one triple six}) = 1 -  {200 \\choose 0} \\left ( \\frac{1}{216}\\right )^0  \\left(\\frac{215}{216}\\right)^{200} = 0.6046\n\\] The Poisson approximation of this probability relies on \\(Y \\sim \\text{Poisson}(\\lambda=200/216)\\) \\[\n    \\Pr(\\text{at least one triple six}) \\approx 1 - \\frac{(200/216)^0 } {0!} \\exp\\left\\{ -\\frac{200}{216} \\right \\} = 0.6038\n\\] The calculation is much simpler and accurate to 2 decimal places.\n\n\n\n\n\n\n\n\n\n\nFigure 27.8: istogram & Density of 10,000 Poisson(20) and Normal(20,20) draws.\n\n\n\n\n\nThe Poisson can in turn be approximated by a Gaussian distribution. As \\(\\lambda \\rightarrow \\infty\\), the Poisson\\((\\lambda)\\) p.m.f. approaches the density of a G\\((\\lambda,\\lambda)\\) random variable. The approximation holds for \\(\\lambda &gt; 100\\) but is not bad even for \\(\\lambda &gt; 20\\) (Figure 27.8).\n\n\n\n\n\n\n\nApproximation of Binomial and Poisson probabilities\n\n\n\n\n\nContinuous distributions\n\nGaussian\nThe most important continuous distribution in statistics is the Gaussian, denoted G\\((\\mu,\\sigma^2)\\). A random variable \\(Y\\) has a Gaussian distribution if its density function is given by \\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y-\\mu)^2 \\right \\} \\quad -\\infty &lt; y &lt; \\infty\n\\]\nThe parameters of the G(\\(\\mu,\\sigma^2)\\) represent the mean and variance of \\(Y\\): \\[\n\\text{E}[Y] = \\mu \\quad \\text{Var}[Y] = \\sigma^2\n\\]\nThe distribution with \\(\\mu = 0\\), \\(\\sigma^2 = 1\\) is called the standard Gaussian distribution.\n\n\n\n\n\n\nTip\n\n\n\nThe Gaussian distribution is sometimes parameterized in terms of the mean and standard deviation of \\(Y\\). When using software make sure you know what the quantities represent. For example, you specify the mean and standard deviation with the dnorm, pnorm, qnorm, and rnorm functions in R.\n\n\nThe Gaussian distribution is often referred to as the Normal distribution. We prefer the term Gaussian because there is nothing normal about the Normal distribution. Most attributes are not normally distributed, despite frequent assumptions to the contrary. The density has a classical, symmetric bell shape, centered at \\(\\mu\\) (Figure 27.9).\n\n\n\n\n\n\n\n\nFigure 27.9: Probability density functions of three Gaussian distributions.\n\n\n\n\n\n\n\n\n\n\n\nWhat is normal?\n\n\n\n\n\nWe eschew the use of the term normal to describe the Gaussian distribution for another reason: the connection of the concept of normality and this distribution to eugenics. The Belgian astronomer, statistician and mathematician Adolphe Quetelet (1796–1847) introduced the generalized notion of the normal. He studied the distribution of physical attributes and determined that the normal, the most representative, value of an attribute is its average. Discrepancies above and below the average were considered “errors”. Early applications of the Gaussian distribution were in the study of measurement errors. C.F. Gauss used the distribution to represent errors in the measurement of celestial bodies.\nPrior to Quetelet, the view of “norm” and “normality” was associated with carpentry. The carpenter square is also called the norm, and in normal construction everything is at right angles. With Quetelet, the middle of the distribution, the center, became the “new normal.” There is nothing objectionable so far.\nHowever, this view did not sit well with Fancis Galton, who introduced the term eugenics. Galton replaced the term “error” with standard deviation and considered variability within a human population as potential for racial progress (Grue and Heiberg 2006). The bell shape of the normal distribution was not used to focus our attention on the average, as Quetelet did. Galton introduced quartiles to categorize the area under the normal into sections of greater and lesser genetic worth. That is not normal!\n\n\n\nThe Gaussian has remarkable properties that make it stand out and that make it particularly easy to work with—compared to other distributions. For example,\n\nLinearity. A linear combination of Gaussian random variables is also Gaussian distributed. If \\(Y \\sim G(\\mu,\\sigma^2)\\), then \\(X = aY + b \\sim G(a\\mu + b, a^2\\sigma^2)\\). An immediate corollary is that the sample mean from a Gaussian distribution is also Gaussian distributed: if \\(Y_i \\sim \\textit{ iid } G(\\mu,\\sigma^2)\\), then \\(\\overline{Y} \\sim G(\\mu,\\sigma^2/n)\\)\nMean-variance decoupling. The mean and the variance of a Gaussian distribution are not linked. The variance is not a function of the mean, as is the case for other members of the exponential family of distributions.\nCorrelation and Independence. When two variables are jointly (bi-variate) Gaussian distributed, the absence of a correlation implies the independence of the random variables. This follows from the fact that with a zero covariance the joint distribution factors into the product of the marginal distributions (see Section 3.7)\nSampling Distributions. When drawing random samples from a G(\\(\\mu,\\sigma^2\\)), the probability distribution of simple transformations of sufficient statistics are well understood. For example, \\[\n\\frac{\\overline{Y}-\\mu}{s/\\sqrt{n}}\n\\] follows a \\(t_{n-1}\\) distribution with \\(n-1\\) degrees of freedom. This forms the basis of hypothesis tests about the mean \\(\\mu\\).\nAsymptotic Gaussianity. As sample size increases, the distribution of many estimators and statistics approaches a Gaussian distribution. This is the basis for using hypothesis tests, constructing confidence and prediction intervals, etc. based on a Gaussian distribution in sufficiently large samples.\n\nMaybe most importantly, the Central Limit Theorem tells us that even if we do not sample from a G(\\(\\mu, \\sigma^2\\)) distribution, the distribution of the sample mean asymptotically follows a G(\\(\\mu,\\sigma^2/n\\)) distribution.\n\n\nCentral Limit Theorem\n\n\nLet \\(Y_{1},\\cdots,Y_{n}\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^{2} &lt; \\infty.\\) The distribution of\n\\[\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}\\]\nconverges to that of a standard Gaussian random variable as \\(n \\rightarrow \\infty\\).\n\n\nIn summary, the importance of the Gaussian distribution does not stem from attributes being Gaussian distributed. It follows because statistics such as parameter estimators asymptotically follow a Gaussian distribution. Another interesting property of the Gaussian distribution is its support: \\(-\\infty &lt; Y &lt; \\infty\\). Many continuous attributes take on only positive values, length, weight, lifetime, etc. This does not rule out the use of the Gaussian distribution for those attributes, with a sufficiently large mean the probability of negative values is essentially zero. On the contrary, it triggers the question which continuous distributions in \\(P_{expo}\\) have support \\(Y &gt; 0\\), or \\(0 &lt; Y &lt; 1\\) and are possibly candidates\n\n\nExponential\nThe exponential distribution is a useful probability model for modeling continuous lifetimes (Figure 27.10). It is related to Poisson processes. If events occur continuously and independently at a constant rate \\(\\lambda\\), the number of events is a Poisson random variable. The time between the events is an exponential random variable, denoted \\(Y \\sim\\) Expo(\\(\\lambda\\)).\n\\[p(y) = \\lambda e^{- \\lambda y},\\ \\ \\ \\ y \\geq 0\\]\n\\[F(y) = 1 - e^{- \\lambda y},\\ \\ \\ y \\geq 0\\]\n\\[\\text{E}\\lbrack Y\\rbrack = \\frac{1}{\\lambda}\\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{1}{\\lambda^{2}}\\]\nLike the discrete Geometric(\\(\\pi\\)) distribution, the Expo(\\(\\lambda\\)) distribution is forgetful,\n\\[\\Pr{(Y &gt; s + t|Y &gt; t)} = \\Pr{(Y &gt; s)}\\]\nand it turns out that no other continuous function has this memoryless property. This property is easily proven using \\(\\Pr(Y &gt; y) = 1 - F(y) = e^{- \\lambda y}\\):\n\\[\\Pr\\left( Y &gt; t + s \\middle| Y &gt; t \\right) = \\frac{\\Pr{(Y &gt; t + s,Y &gt; t)}}{\\Pr{(Y &gt; t)}} = \\frac{Pr(Y &gt; t + s)}{Pr(Y &gt; t)} = \\frac{e^{- \\lambda(t + s)}}{e^{- \\lambda t}} = e^{- \\lambda s}\\]\nThe memoryless property of the exponential distribution makes it not a good model for human lifetimes. The probability that a 20-year-old will live another 10 years is not the same as the probability that a 75-year-old will live another 10 years. The exponential distribution implies that this would be the case. When modeling earthquakes, it might be reasonable that the probability of an earthquake in the next ten years is the same, regardless of when the last earthquake occurred—the exponential distribution would then be reasonable.\nYou don’t have to worry about whether other distributions have this memoryless property in applications where lack of memory would not be appropriate. The exponential distribution is defined by this property, it is the only continuous distribution with lack of memory.\n\n\n\n\n\n\n\n\nFigure 27.10: Probability density functions of Exponential random variables.\n\n\n\n\n\n\n\nGamma\nThe Expo(\\(\\lambda\\)) distribution is a special case of a broader family of distributions, the Gamma(\\(\\alpha,\\beta\\)) distribution. A random variable \\(Y\\) is said to have a Gamma(\\(\\alpha,\\beta\\)) distribution if its density function is\n\\[f(y) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}y^{\\alpha - 1}e^{- y/\\beta},\\ \\ \\ \\ \\ y \\geq 0,\\ \\alpha,\\beta &gt; 0\\]\nThe mean and variance of a Gamma(\\(\\alpha,\\beta\\)) random variable are given by\n\\[\\text{E}\\lbrack Y\\rbrack = \\alpha\\beta\\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\alpha\\beta^{2}\\]\n\\(\\alpha\\) is called the shape parameter of the distribution and \\(\\beta\\) is called the scale parameter. Varying \\(\\alpha\\) affects the shape and varying \\(\\beta\\) affects the units of measurement (Figure 27.11). A property of Gamma functions is a constant coefficient of variation (CV). CV is defined as the ratio of the standard deviation to the mean of a distribution. For the Gamma(\\(\\alpha,\\beta\\)) family this coefficient is \\[\n\\text{CV} = \\frac{\\sqrt{\\alpha\\beta^2}}{\\alpha\\beta} = \\frac{1}{\\sqrt{\\alpha}}\n\\]\nThe term \\(\\Gamma(\\alpha)\\) in the denominator of the density function is called the Gamma function,\n\\[\\Gamma(\\alpha) = \\int_{0}^{\\infty}{y^{\\alpha - 1}e^{- y}}dy\\]\n\n\n\n\n\n\n\n\nFigure 27.11: Probability density functions of Gamma random variables.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: if \\(\\alpha\\) is an integer, \\(\\Gamma(\\alpha) = (\\alpha - 1)!\\)\n\n\nThe exponential random variable introduced earlier is a special case of the Gamma family, the Expo(\\(1/\\beta\\)) is the same as the Gamma(1,\\(\\beta\\)).\nAnother special case of the gamma-type random variables is the Chi-square random variable. A random variable \\(Y\\) is said to have a Chi-squared distribution with \\(\\nu\\) degrees of freedom, denoted \\(\\chi_{\\nu}^{2}\\), if \\(Y\\) is a Gamma(\\(\\frac{\\nu}{2},2\\)) random variable.\n\n\nBeta\nA random variable has a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\), denoted \\(Y \\sim \\text{Beta}(\\alpha,\\beta)\\), if its density function is given by \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\] The family of beta distributions takes on varied shapes as seen in Figure 27.12.\n\n\n\n\n\n\n\n\nFigure 27.12: Probability density functions of Beta random variables.\n\n\n\n\n\nThe ratio of Gamma functions is known as the Beta function and the density can also be written as \\(f(y) = y^{\\alpha-1}(1-y)^{(\\beta-1)} / B(\\alpha,\\beta)\\) where \\(B(\\alpha,\\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)\\).\nThe mean of a \\(\\text{Beta}(\\alpha,\\beta])\\) random variable is \\[\\text{E}[Y] = \\frac{\\alpha}{\\alpha+\\beta}\n\\] and the variance is \\[\n\\text{Var}[Y] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)} = \\text{E}[Y]\\frac{\\beta}{(\\alpha+\\beta)(\\alpha+\\beta+1)}\n\\]\nThe support of a Beta random variable is continuous on \\([0,1]\\), which makes it an attractive candidate for modeling proportions, for example, the proportion of time a vehicle is in maintenance or the proportion of disposable income spent on rent or the market share of a company.\nThe Beta distribution can also be used for random variables that are defined on a different scale, \\(a &lt; Y &lt; b\\), by transforming to the [0,1] scale: \\(Y^* = (Y-a)/(b-a)\\).\nThe \\(\\text{Beta}(1,1)\\) is a continuous uniform random variable on [0,1].\nSince \\(Y\\) is continuous, we can define the support of the Beta random variable as \\(0 \\le y \\le 1\\) or as \\(0 &lt; y &lt; 1\\). The probability that the continuous random variable takes on exactly the value 0 or 1 is zero. However, in practice you can observe proportions at the extreme of the support; the proportion of income spent on rent by a homeowner is zero. That causes difficulties evaluating the log likelihood function at the boundaries of the support.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-link",
    "href": "glm.html#sec-glm-link",
    "title": "27  Generalized Linear Models",
    "section": "27.3 Link Functions",
    "text": "27.3 Link Functions\nThe link function in a GLM plays two important roles. It is the transformation of the mean to a scale where the input effects are linear. Because of this transformation, we can still work with linear structures \\(\\textbf{x}^\\prime\\boldsymbol{\\beta}\\) and do not have to resort to general nonlinear transformations. The linearity of the predictor simplifies parameter estimation, prediction, and hypothesis testing.\nThe second responsibility of the link function is to ensure that the model predictions are valid. The coefficients in the linear predictor can take on any value on the real line, \\(-\\infty &lt; \\beta_j &lt; \\infty\\). The predicted means have to comply with the support restrictions of the target variable. For example, the mean of a Bernoulli(\\(\\pi\\)) random variable is the probability \\(0 \\le \\pi \\le 1\\), for the Poisson the mean is \\(\\lambda &gt; 0\\). The inverse link function needs to ensure that \\(g^{-1}(\\eta)\\) is a valid quantity for all possible values of \\(\\eta\\) (Figure 27.13).\n\n\n\n\n\n\nFigure 27.13: The inverse link maps \\(\\eta\\) to valid values of the mean and the target data. If \\(Y \\in \\{0,1\\}\\), then \\(0 \\le \\text{E}[Y] \\le 1\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe link function is a transformation of the mean, it is not a transformation of the data. Modeling \\(\\log(\\mu) = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\) is very different from log-transforming the data and assuming a linear model holds on the transformed scale: \\(\\log(Y) = \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\epsilon\\). The link function is a transformation of a constant, \\(\\log(Y)\\) is the transformation of a random variable. Choosing a different link function does not alter the essential distributional properties of the target variable—a Poisson remains a Poisson, for example. Choosing a different transformation of the target variable changes the distributional properties. If \\(Y \\sim \\text{Poisson}(\\lambda)\\), neither \\(log(Y)\\) nor \\(\\sqrt{Y}\\) is Poisson distributed.\n\n\nTable 27.1 lists common link functions for important members of the exponential family. In most cases the canonical link is a good place to start. However, for the Gamma distribution, the reciprocal link is not adequate, although it is the natural link. Since the mean of a Gamma random variable is positive, choosing the reciprocal link does not guarantee that \\[\ng^{-1}(\\eta) = \\frac{1}{\\eta} &gt; 0\n\\] It is common to choose the log link instead in Gamma regression models. Similar for the Inverse Gaussian distribution, where the inverse canonical link is the square root function and does not permit negative values for \\(\\eta\\). The Gaussian distribution shows how special it is one more time: it is the only distribution for which the canoncial link is the identity function: there is no transformation of the mean, the input effects are linear on the scale of the data.\n\n\n\nTable 27.1: Common link functions for important members of \\(P_{expo}\\)\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLink Function\nName\nCanonical\n\n\n\n\nBernoulli\n\\(\\text{logit}(\\mu) = \\log\\left (\\frac{\\mu}{1-\\mu}\\right )\\)\nLogit\nYes\n\n\n\n\\(\\Phi^{-1}(\\mu)\\)\nProbit\n\n\n\n\n\\(\\log(-\\log(\\mu))\\)\nLog-log\n\n\n\n\n\\(\\log(-\\log(1-\\mu))\\)\nComplementary Log-log\n\n\n\nBinomial\nSame as Bernoulli\n\n\n\n\nNegative Binomial\n\\(\\log(\\mu)\\)\nLog\nYes\n\n\nPoisson\n\\(\\log(\\mu)\\)\nLog\nYes\n\n\nGaussian\n\\(\\mu = \\eta\\)\nIdentity\nYes\n\n\nGamma\n\\(\\mu^{-1}\\)\nReciprocal\nYes\n\n\n\n\\(\\log(\\mu)\\)\nLog\n\n\n\nInverse Gaussian\n\\(\\mu^{-2}\\)\nReciprocal squared\nYes\n\n\n\n\\(\\log(\\mu)\\)\nLog\n\n\n\nBeta\n\\(\\text{logit}(\\mu)= \\log\\left (\\frac{\\mu}{1-\\mu}\\right )\\)\nLogit\nYes\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can choose any link function that satisfies the properties stated above. The function must be invertible, however, so that we can switch back and forth between the scale of the mean and the scale of the linear predictor.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-estimation",
    "href": "glm.html#sec-glm-estimation",
    "title": "27  Generalized Linear Models",
    "section": "27.4 Parameter Estimation",
    "text": "27.4 Parameter Estimation\n\nMaximum Likelihood\nThe parameters of a generalized linear model are the coefficients in the mean function, \\(\\boldsymbol{\\beta}\\), and, where present, the scale parameter \\(\\psi\\). The standard (frequentist) approach is to estimate the parameters by maximum likelihood. This is reasonable and straightforward because the distribution of the data is specified and the observations are assumed independent. The log-likelihood of the data is then simply the sum of the log-likelihoods for the observations: \\[\n\\ell(\\boldsymbol{\\mu},\\psi;\\textbf{y}) = \\sum_{i=1}^n \\left(y_i\\theta(\\mu_i) - b(\\theta(\\mu_i)))/\\psi + c(y_i,\\psi)\\right)\n\\] To maximize this log likelihood function with respect to \\(\\beta_k\\) we can apply the chain rule of calculus: \\[\n\\frac{\\partial \\ell(\\mu,\\psi;\\textbf{y})}{\\partial \\beta_k} = \\sum_{i=1}^n \\frac{\\partial \\ell(\\mu_i,\\psi;y_i)}{\\partial \\theta(\\mu_i)}\n\\frac{\\partial \\theta(\\mu_i)}{\\partial \\mu_i}\n\\frac{\\partial \\mu_i}{\\partial \\eta_i}\n\\frac{\\partial \\eta_i}{\\partial \\beta_k}\n\\] Some manipulations lead to the following expression: \\[\n\\frac{\\partial \\ell(\\mu,\\psi;\\textbf{y})}{\\partial \\beta_k} = \\sum_{i=1}^n \\frac{y_i-\\mu_i}{\\text{Var}[Y_i]}\\frac{\\mu_i}{\\eta_i}x_k\n\\] This expression applies whether the link function is canonical or not. When you collect the derivatives for all parameters, finding the maximum likelihood estimates is equivalent to solving\n\\[\n\\textbf{F}^\\prime\\textbf{V}^{-1}(\\textbf{y}-\\boldsymbol{\\mu}) = \\textbf{0}\n\\] where \\(\\textbf{F}\\) is the \\((n \\times p+1)\\) matrix of the first derivatives of the mean with respect to the \\(\\beta\\)s: \\(\\textbf{F}\\) has typical element \\([\\partial \\mu/\\partial_\\beta]_{ij}\\). \\(\\textbf{V}\\) is a diagonal matrix that contains the variances of the targets on the diagonal.\nIf \\(\\textbf{V}\\) were known, this is a weighted least squares problem. Because \\(\\textbf{V}\\) depends on \\(\\boldsymbol{\\mu}\\) in a typical GLM, the ML estimates are found by an iterative procedure, called iteratively reweighted least squares (IRLS).\n\n\nIteratively Reweighted Least Squares (IRLS)\nThe IRLS algorithm can be used to find the maximum likelihood estimates of \\(\\boldsymbol{\\beta}\\). To motivate the algorithm we start by a linear approximation (a first-order Taylor series) of the linked observations about an estimate of the mean: \\[\n\\begin{align*}\ng(y) &= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[ \\frac{\\partial g(y)}{\\partial y}\\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= \\widehat{\\eta} + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}}  \\\\\n&= z\n\\end{align*}\n\\tag{27.2}\\]\n\\(z\\) is called an adjusted dependent variable or a working response variable or a pseudo-response. The final expression in Equation 27.2 can be viewed as a linear model with response variable \\(z\\), systematic part \\(\\widehat{\\eta} = \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}}\\) and error term \\((y-\\mu)[\\partial \\eta/\\partial \\mu]\\). The variance of this error term is \\[\n\\text{Var}[z] = \\left[\\frac{\\partial \\eta}{\\partial \\mu}\\right]^2 \\text{Var}[Y]\n\\]\nThe iterative procedure is as follows: given an initial value of \\(z\\), which requires an initial estimate of \\(\\mu\\), fit a weighted linear model with inputs \\(\\textbf{x}= [x_1,\\cdots,x_p]^\\prime\\) and weights given by the inverse of \\(\\text{Var}[z]\\). The solution to the weighted linear model is an updated parameter vector \\(\\boldsymbol{\\beta}\\). Re-calculate \\(z\\) and the weights and repeat the weighted linear regression fit. Continue until the relative change in the parameter estimates, the log likelihood function, the deviance, or some other criterion is negligible. McCullagh and Nelder Frs (1989) show that this procedure converges to the maximum likelihood estimates.\nA nice property of the IRLS algorithm is the ease of finding starting values. The initial value of \\(z\\) that kicks off the iterations does not necessarily require an estimate of \\(\\boldsymbol{\\beta}\\). If the link function can be evaluated at the data points (maybe after slight adjustments such as moving data points away from boundary values), then one can start the iterations using the observed data as the initial estimate of \\(\\mu\\). For binary data, for example, initial adjustments to the data values such as \\[\ny^* = \\frac{y+c}{y+2c}\n\\] can be used to avoid boundary problems. Another technique to get the iterations off the ground is to assume initially that \\(\\boldsymbol{\\beta}= \\textbf{0}\\).",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-gamma-example",
    "href": "glm.html#sec-glm-gamma-example",
    "title": "27  Generalized Linear Models",
    "section": "27.5 Example: Gamma Regression",
    "text": "27.5 Example: Gamma Regression\n\nThe Data, Real Estate Values in Albemarle County, Virginia\nThis example is based on this excellent tutorial at the University of Virginia.\nThe data represent real estate information from Albemarle County, Virginia. Of interest is modeling the total value of a home (totalvalue) as a function of predictors such as finished square feet, number of full bathrooms, lot size, etc.\n\nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nhomes &lt;- dbGetQuery(con, \"SELECT * FROM AlbemarleHomes\")\n\ndbDisconnect(con)\nstr(homes)\n\n'data.frame':   3025 obs. of  16 variables:\n $ column00   : num  1 2 3 4 5 6 7 8 9 10 ...\n $ yearbuilt  : chr  \"1754\" \"1968\" \"1754\" \"1934\" ...\n $ finsqft    : num  1254 1192 881 480 720 ...\n $ cooling    : chr  \"No Central Air\" \"No Central Air\" \"No Central Air\" \"No Central Air\" ...\n $ bedroom    : num  1 3 2 0 2 3 2 3 2 0 ...\n $ fullbath   : num  1 1 1 0 1 1 1 1 1 0 ...\n $ halfbath   : chr  \"0\" \"0\" \"0\" \"0\" ...\n $ lotsize    : num  4.93 1.09 195.93 10 1 ...\n $ totalvalue : num  124300 109200 141600 69200 139700 ...\n $ esdistrict : chr  \"Brownsville\" \"Scottsville\" \"Stony Point\" \"Crozet\" ...\n $ msdistrict : chr  \"Henley\" \"Walton\" \"Sutherland\" \"Henley\" ...\n $ hsdistrict : chr  \"Western Albemarle\" \"Monticello\" \"Albemarle\" \"Western Albemarle\" ...\n $ censustract: num  111 113 104 101 102 ...\n $ age        : num  265 51 265 85 56 265 87 59 69 265 ...\n $ condition  : chr  \"Substandard\" \"Substandard\" \"Substandard\" \"Substandard\" ...\n $ fp         : num  0 0 0 0 0 0 0 1 0 0 ...\n\n\nComputing the coefficient of variation within ranges of finsqft shows that the CV is pretty constant while the variability of home values increases with the square footage of the home. Increasing variability with increasing size is a common phenomenon for many data, biological or otherwise. If the increase in variability–as measured by the standard deviation–is proportional to the mean, the data exhibit a constant coefficient of variation, a property of the Gamma distribution.\n\nlibrary(dplyr)\n\nfinsqft_cut &lt;- cut(x=homes$finsqft, \n                   breaks = c(0, 1000, 2000, 3000, 4000, 5000, 8000))\n\nplot(finsqft_cut,homes$totalvalue,\n     ylab=\"Total Value\",\n     xlab=\"Finished Square Footage\",\n     las=1,\n     cex.axis=0.8)\n\n\n\n\n\n\n\nhomes %&gt;% mutate(sqft_cut = finsqft_cut) %&gt;%\n    group_by(sqft_cut) %&gt;% \n    summarize(count=n(), mean=mean(totalvalue), std=sd(totalvalue)) %&gt;%\n    mutate(CV = std/mean)\n\n# A tibble: 6 × 5\n  sqft_cut      count     mean      std    CV\n  &lt;fct&gt;         &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 (0,1e+03]       193  168621.   99722. 0.591\n2 (1e+03,2e+03]  1523  285751.  153949. 0.539\n3 (2e+03,3e+03]   901  479938.  184596. 0.385\n4 (3e+03,4e+03]   272  732660.  316818. 0.432\n5 (4e+03,5e+03]    82 1088504.  497282. 0.457\n6 (5e+03,8e+03]    54 1905324. 1112306. 0.584\n\n\n\npar(mfrow=c(1,2))\nhist(homes$totalvalue, \n     breaks=30, \n     cex.main=0.9,\n     main=\"Distribution of Home Values\", \n     xlab=\"value\")\n\nhist(log(homes$totalvalue), \n     breaks=30, \n     cex.main=0.9,\n     main=\"Distribution of Log Home Values\", \n     xlab=\"log(value)\" )\n\n\n\n\n\n\n\n\n\n\nFitting a Gamma Regression Model\nWe fit the Gamma GLM with the glm function in R. The default link for family=Gamma is the canonical link, which is the inverse. This is not a good link function to model positive target values, since there is no guarantee that the inverse link satisfies the range restriction for the mean of the target variable. We choose the log link instead. The following statements fit a regression model of the form \\[\n\\begin{align*}\n\\log \\text{E}[\\text{Total Value}] &= \\beta_0 + \\beta_1 \\, \\text{finished sq.ft} \\\\\n\\text{E}[\\text{Total Value}] &= \\exp\\{\\beta_0 + \\beta_1 \\, \\text{finished sq.ft}\\}\n\\end{align*}\n\\] assuming that totalvalue follows a Gamma distribution.\n\ngam_reg &lt;- glm(totalvalue ~ finsqft, \n               data=homes,\n               family=Gamma(link=\"log\"))\nsummary(gam_reg)\n\n\nCall:\nglm(formula = totalvalue ~ finsqft, family = Gamma(link = \"log\"), \n    data = homes)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.178e+01  1.986e-02  593.18   &lt;2e-16 ***\nfinsqft     5.081e-04  8.753e-06   58.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.2122692)\n\n    Null deviance: 1206.00  on 3024  degrees of freedom\nResidual deviance:  378.25  on 3023  degrees of freedom\nAIC: 79593\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model converges in 4 iterations and the maximum likelihood estimates of the coefficients are \\(\\widehat{\\beta}_0\\) = 11.7823 and \\(\\widehat{\\beta}_1\\) = 5.1^{-4}.\nThe addition of the single input variable finsqft reduces the deviance dramatically, compared to a null model; from 1205.995 to 378.25. The finsqft coefficient is highly significant.\n\nSuppose we wish to predict the value of a home with 2,500 finished square feet.\nFirst, let’s do it the hard way, using the coefficients from the output:\n\nround(gam_reg$coefficients,5)\n\n(Intercept)     finsqft \n   11.78227     0.00051 \n\npred_2500 &lt;- gam_reg$coefficients[1] + 2500 * gam_reg$coefficients[2]\ncat(\"Linear predictor for 2,500 finished square feet: \", pred_2500)\n\nLinear predictor for 2,500 finished square feet:  13.05243\n\ncat(\"Predicted home value with 2,400 finished square feet:\", exp(pred_2500))\n\nPredicted home value with 2,400 finished square feet: 466225.8\n\n\nThe linear combination \\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\times 2500\\) yields the linear predictor, 13.0524. This value is on the scale of the link function. To obtain the estimated total home value, we have to plug the estimated linear predictor into the inverse link function. The predicted total home value is 4.6622585^{5}. Especially when working with log links, analysts sometimes forget to apply the inverse link function. When the linear predictor is a positive number, it is easily mistaken for the prediction of the mean.\nYou can also use the predict function to obtain the predictions. The type= option determines whether the predicted values are calculated for the linear predictor (type=\"link\") or for the mean response (type=\"response\").\n\npredict(gam_reg, newdata=data.frame(finsqft=2500),type=\"link\")\n\n       1 \n13.05243 \n\np &lt;- predict(gam_reg, newdata=data.frame(finsqft=2500), type=\"response\")\ncat(\"Predicted home value for 2,500 finished square feet: \", p)\n\nPredicted home value for 2,500 finished square feet:  466225.8\n\n\n\n\nLikelihood Ratio Test\nIs our model sufficiently complex to capture the variability in total home values? We can compare it to more complex models with a likelihood ratio test, provided the models are nested. The following statements add four input variables to our basic model. The model summary confirms that their partial tests (adding each variable in the presence of the others) are significant.\n\ngam_reg_full &lt;- glm(totalvalue ~ finsqft + cooling + lotsize + fullbath + lotsize, \n                    data=homes,\n                    family=Gamma(link=\"log\"))\n\nsummary(gam_reg_full)\n\n\nCall:\nglm(formula = totalvalue ~ finsqft + cooling + lotsize + fullbath + \n    lotsize, family = Gamma(link = \"log\"), data = homes)\n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            1.171e+01  2.137e-02 547.703   &lt;2e-16 ***\nfinsqft                3.868e-04  1.032e-05  37.465   &lt;2e-16 ***\ncoolingNo Central Air -2.117e-01  2.339e-02  -9.048   &lt;2e-16 ***\nlotsize                7.411e-03  3.748e-04  19.773   &lt;2e-16 ***\nfullbath               1.310e-01  1.082e-02  12.104   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.1475242)\n\n    Null deviance: 1206.00  on 3024  degrees of freedom\nResidual deviance:  287.37  on 3020  degrees of freedom\nAIC: 78753\n\nNumber of Fisher Scoring iterations: 9\n\n\nWe can test the simultaneous addition of the four variables with a single LRT.\n\nlibrary(lmtest)\n\nlrtest(gam_reg,gam_reg_full)\n\nLikelihood ratio test\n\nModel 1: totalvalue ~ finsqft\nModel 2: totalvalue ~ finsqft + cooling + lotsize + fullbath + lotsize\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)    \n1   3 -39794                         \n2   6 -39370  3 846.39  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe lrtest function in the lmtest package constructs LRTs based on the result returned from calls to logLik for each model. If the result returned from logLik is not a true log likelihood, the results of lrtest are not meaningful. For example, you can request logLik for a model fit by lm, which performs least-squares estimation. There is no distributional assumption in lm, hence there is no likelihood or log likelihood function. The logLik function itself computes the log likelihood indirectly, for example, based on Akaike’s Information Criterion (AIC). For likelihood-based estimation, AIC is defined as \\[\n\\text{AIC} = -2\\log \\ell + 2 k\n\\] where \\(k\\) is the total number of model parameters, including scale and covariance parameters. Back-calculating \\(\\log ell\\) based on AIC is implicitly making the distributional assumption that underpins the computation of AIC for a particular class of models, even if the estimation is free of a distributional assumption.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#example-beta-regression",
    "href": "glm.html#example-beta-regression",
    "title": "27  Generalized Linear Models",
    "section": "27.6 Example: Beta Regression",
    "text": "27.6 Example: Beta Regression\nThe typical R functions to fit generalized linear models or generalized additive models are glm and gam::gam. Unfortunately, neither of them support beta-distributed data, although the Beta distribution is in the two-parameter exponential family. Recall from Section 27.2.3.2.4 that a Beta random variable is continuous on \\([0,1]\\) and a plausible model for ratios. Also, every random variable with finite support \\([a,b]\\) can be normalized to lie in the \\([0,1]\\) interval by the transformation \\[\nx_n = \\frac{x - a}{b - a}\n\\]\nThe parameterization of the Beta distribution in Section 27.2.3.2.4 is not the most convenient to fit a generalized linear model. As shown there, \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\]\nand \\(Y\\) has mean \\(\\alpha/(\\alpha + \\beta)\\). Ferrari and Cribari-Neto (2004) proposed a parameterization in terms of \\(\\mu=\\alpha/(\\alpha + \\beta)\\) and a scale parameter \\(\\phi= \\alpha + \\beta\\): \\[\nf(y) = \\frac{\\Gamma(\\phi)}{\\Gamma(\\mu\\phi)\\Gamma((1-\\mu)\\phi)}\\, y^{\\mu\\phi-1}(1-y)^{(1-\\mu)\\phi-1}\n\\]\nIn this Beta\\((\\mu,\\phi)\\) parameterization, \\(\\text{E}[Y] = \\mu\\) and \\(\\text{Var}[Y] = \\mu(1-\\mu)/(1+\\phi)\\). The variance of the random variable is inversely related to the magnitude of \\(\\phi\\), that is why it is called a precision parameter (\\(1/\\phi\\) is a dispersion parameter). The mean–variance relationship should look familiar, it is akin to that of the Bernoulli distribution, except for the involvement of the precision parameter. This is not too surprising as both Bernoulli and Beta random variables have means that range from 0 to 1. By the same token, the most common link function in Beta regressions is the logit link.\nYou can fit beta regression models in the Ferrari and Cribari-Neto (2004) parameterization in R with the betareg package and with the glam package. The glam package can also handle generalized additive models (Chapter 28) and grew out of a previous project in this course.\nWe start with the gasoline yield data by Prater (1956) that is supplied by the betareg package. The target variable is the proportion of crude oil converted to gasoline after distillation and fractionation. Potential input variables to the regression include the gravity and vapor pressure of the crude oil, the temperature at with 10% of the crude oil has vaporized, the temperature at which all gasoline has vaporized, and the batch number. The following code fits the model \\[\n\\begin{align*}\nY &\\sim \\text{Beta}(\\mu,\\phi) \\\\\n\\mu &= \\frac{1}{1+\\exp\\{-\\eta\\}} \\\\\n\\eta &= \\beta_0 + \\beta_1\\text{gravity} + \\beta_2\\text{pressure} + \\beta_3\\text{temp10} + \\beta_4\\text{temp} \\\\\n\\end{align*}\n\\]\n\nlibrary(betareg)\ndata(\"GasolineYield\", package = \"betareg\")\n\nbeta_glm1 &lt;- betareg(yield ~ gravity + pressure + temp10 + temp,\n                     data=GasolineYield)\ns &lt;- summary(beta_glm1)\ns\n\n\nCall:\nbetareg(formula = yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-1.9010 -0.6829 -0.0385  0.5531  2.1314 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.6949422  0.7625693  -3.534 0.000409 ***\ngravity      0.0045412  0.0071419   0.636 0.524871    \npressure     0.0304135  0.0281007   1.082 0.279117    \ntemp10      -0.0110449  0.0022640  -4.879 1.07e-06 ***\ntemp         0.0105650  0.0005154  20.499  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)   248.24      62.02   4.003 6.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 75.68 on 6 Df\nPseudo R-squared: 0.9398\nNumber of iterations: 144 (BFGS) + 5 (Fisher scoring) \n\n\nThe maximum likelihood estimates of the model coefficients are \\(\\widehat{\\beta}_0 =\\) -2.6949, \\(\\widehat{\\beta}_1 =\\) 0.0045, and so forth. The estimate of the precision parameter is \\(\\widehat{\\phi} =\\) 248.2419.\nFigure 27.14 displays the observed and fitted yield values against the temperature at which gasoline has vaporized (temp).\n\n\n\n\n\n\n\n\nFigure 27.14: Observed (empty circles) and fitted values (filled circles) in beta regression for gasoline yield.\n\n\n\n\n\nTo fit this model with the GLAM package, you would use\n\nlibrary(glam)\nY &lt;- GasolineYield$yield\nX &lt;- cbind(GasolineYield$gravity, \n           GasolineYield$pressure,\n           GasolineYield$temp10, \n           GasolineYield$temp)\nbeta_glm2 &lt;- glam(X, Y, model=\"linear\", family=\"beta\")",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#sec-glm-overdispersion",
    "href": "glm.html#sec-glm-overdispersion",
    "title": "27  Generalized Linear Models",
    "section": "27.7 Overdispersion",
    "text": "27.7 Overdispersion\nOverdispersion is the condition where the data appear more variable than is permissible under a reference model. If overdispersion is present some aspect of the model is not correct. It is important to address the specific shortcoming that induces the overdispersion. Causes can include one or more of the following\n\nimportant input variables are missing from the model\nthe data are autocorrelated, for example in time series or longitudinal data\nthe data do not follow the assumed probability model\nthe data were generated by a mixture of random processes rather than a single mechanism\n\nOverdispersion is addressed by turning the appropriate knob. If overdispersion is not addressed, the precision of the fitted model is overstated: standard error estimates are too small, confidence and prediction intervals are too narrow, \\(p\\)-values are too small.\nGeneralized linear models are notorious for overdispersion issues, because the variability of the data is partially or completely controlled by the mean function. With Poisson data, for example, the variance equals the mean of the distribution. By specifying a GLM for the mean function, we explicitly specify a model for the variance. Two-parameter exponential family models such as the Gaussian, Gamma, and Beta distribution have a scale parameter but there is still a linkage between the mean and the variance. The only distribution where the mean of \\(Y\\) is completely decoupled from the variance is the Gaussian distribution. Gaussian data can never be overdispersed because the reference model can combine any value for the mean with any value for the variance.\nThe following example shows how incorrectly modeling count data can induce overdispersion.\n\n\nExample: Poppy Counts in RCBD\n\n\nThe following data from (Mead, Curnow, and Hasted 1993, 144), are from a randomized complete block design (RCBD) with 4 blocks and 6 experimental treatments. The output variable was the number of poppies on the experimental unit.\n\nPoppy count data from (Mead, Curnow, and Hasted 1993)\n\n\nTreatment\nBlock 1\nBlock 2\nBlock 3\nBlock 4\n\n\n\n\nA\n538\n422\n377\n315\n\n\nB\n438\n442\n319\n380\n\n\nC\n77\n61\n157\n52\n\n\nD\n115\n57\n100\n45\n\n\nE\n17\n31\n87\n16\n\n\nF\n18\n26\n77\n20\n\n\n\nThere appears to be a strong treatment effect, the counts are generally higher for treatments A and B than for E and F. There also appears to be a block effect. Scanning the columns in the table, block 4 seems to have systematically smaller counts than the other blocks.\nIn a randomized complete block design with \\(t\\) treatments and \\(b\\) blocks, there are \\(t\\times b\\) experimental units. A block of units is characterized by greater similarity of the experimental units within the block than between the blocks. The \\(t\\) treatments are then randomly assigned to the experimental units within each block (Figure 27.15). This design captures systematic differences between the blocks and the randomization of treatments within a block balances out the unaccounted effects within the blocks.\n\n\n\n\n\n\nFigure 27.15: Randomized Complete Block Design\n\n\n\n\nlibrary(\"duckdb\")\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\npoppies &lt;- dbGetQuery(con, \"SELECT * FROM poppies\")\n\ndbDisconnect(con)\n\npoppies$block &lt;- as.factor(poppies$block)\npoppies$block &lt;- relevel(poppies$block,\"4\")\npoppies$tx &lt;- relevel(as.factor(poppies$treatment),\"F\")\n\nSuppose we analyze the data as a Poisson model with a treatment factor but leave the block effects out of the model. This is clearly not the correct model as the mean function does not reflect the experimental design.\n\npoi_tx &lt;- glm(count ~ tx, data=poppies, family=poisson)\nsummary(poi_tx)\n\n\nCall:\nglm(formula = count ~ tx, family = poisson, data = poppies)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.56247    0.08422  42.302  &lt; 2e-16 ***\ntxA          2.46098    0.08774  28.050  &lt; 2e-16 ***\ntxB          2.41579    0.08789  27.485  &lt; 2e-16 ***\ntxC          0.90056    0.09987   9.017  &lt; 2e-16 ***\ntxD          0.81014    0.10123   8.003 1.21e-15 ***\ntxE          0.06852    0.11711   0.585    0.558    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3868.48  on 23  degrees of freedom\nResidual deviance:  340.89  on 18  degrees of freedom\nAIC: 506.63\n\nNumber of Fisher Scoring iterations: 5\n\n\nTreatment F was set as the reference level, so the coefficients in the glm output measure differences to that treatment. All treatments are significantly different from F, except for treatment E.\nThe large residual deviance relative to its degrees of freedom is troubling, however. In a well-specified model, their ratio should be close to 1. Instead, the ratio is almost 19.\n\npoi_tx$deviance / poi_tx$df.residual\n\n[1] 18.93844\n\n\nThe model is highly overdispersed. Without block effects, the model is misspecified.\n\npoi_rcbd &lt;- glm(count ~ block + tx, data=poppies, family=poisson)\nsummary(poi_rcbd)\n\n\nCall:\nglm(formula = count ~ block + tx, family = poisson, data = poppies)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.32803    0.08978  37.067  &lt; 2e-16 ***\nblock1       0.37356    0.04516   8.273  &lt; 2e-16 ***\nblock2       0.22700    0.04659   4.873 1.10e-06 ***\nblock3       0.29939    0.04586   6.529 6.64e-11 ***\ntxA          2.46098    0.08774  28.050  &lt; 2e-16 ***\ntxB          2.41579    0.08789  27.485  &lt; 2e-16 ***\ntxC          0.90056    0.09987   9.017  &lt; 2e-16 ***\ntxD          0.81014    0.10123   8.003 1.21e-15 ***\ntxE          0.06852    0.11711   0.585    0.558    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3868.5  on 23  degrees of freedom\nResidual deviance:  264.7  on 15  degrees of freedom\nAIC: 436.44\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe RCBD Poisson model improves the overdispersion but not by much:\n\npoi_rcbd$deviance / poi_rcbd$df.residual\n\n[1] 17.6466\n\n\nThe mean function is now correctly specified; the explanation for the overdispersion is that the data are not Poisson distributed.\n\n\n\nScaled Analysis\nA “quick fix” of the overdispersion problem is to introduce a scaling factor that adjusts the variability of the data somehow. In the presence of overdispersion the standard errors reported by software are too small. As a consequence, \\(p\\)-values are too small, prediction and confidence intervals are too narrow.\nThe quick fix is to adjust the variance-covariance matrix of the model coefficients by the sum of the squared Pearson residuals divided by the residual degrees of freedom. The Pearson residual in a generalized linear model is \\[\n    \\frac{y_i - \\widehat{\\mu}_i}{\\sqrt{\\widehat{\\text{Var}}[y_i]}}\n\\] and the adjustment factor is \\[\n\\frac{1}{df_{\\text{res}}} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{\\widehat{\\text{Var}}[y_i]}\n\\]\nThis is sometimes called a quasi-glm analysis. The results of the quasi-poisson analysis for the poppies experiment follows.\n\n\nExample: Quasi-Poisson Model for Poppy Counts in RCBD\n\n\n\npoi_qp &lt;- glm(count ~ block + tx, data=poppies,\n           family=quasipoisson)\n\nThe coefficient estimates are identical to those in the Poisson model. The standard errors of the coefficients are increased by a constant factor\n\ns &lt;- summary(poi_rcbd)\ns_q &lt;- summary(poi_qp)\ns_q$coefficients[,2] / s$coefficients[,2]\n\n(Intercept)      block1      block2      block3         txA         txB \n   4.338883    4.338883    4.338883    4.338883    4.338883    4.338883 \n        txC         txD         txE \n   4.338883    4.338883    4.338883 \n\n\nThe factor scaling the standard errors is the square root of the sum of the squared Pearson residuals divided by the residual degrees of freedom.\n\npear_res &lt;- (poi_rcbd$fitted.values - poppies$count)/sqrt(poi_rcbd$fitted.values)\ndisp_factor &lt;- sum(pear_res^2)/poi_rcbd$df.residual\nsqrt(disp_factor)\n\n[1] 4.338883\n\n\nThe adjustment can be made directly by supplying the dispersion factor to the summary of the RCBD analysis:\n\nsummary(poi_rcbd,dispersion=disp_factor)\n\n\nCall:\nglm(formula = count ~ block + tx, family = poisson, data = poppies)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.32803    0.38956   8.543  &lt; 2e-16 ***\nblock1       0.37356    0.19592   1.907   0.0566 .  \nblock2       0.22700    0.20213   1.123   0.2614    \nblock3       0.29939    0.19897   1.505   0.1324    \ntxA          2.46098    0.38067   6.465 1.01e-10 ***\ntxB          2.41579    0.38137   6.335 2.38e-10 ***\ntxC          0.90056    0.43332   2.078   0.0377 *  \ntxD          0.81014    0.43921   1.845   0.0651 .  \ntxE          0.06852    0.50813   0.135   0.8927    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 18.8259)\n\n    Null deviance: 3868.5  on 23  degrees of freedom\nResidual deviance:  264.7  on 15  degrees of freedom\nAIC: 436.44\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nThe quasi-glm analysis is not the recommended approach for addressing overdispersion. It is a band-aid that covers over the real reasons behind the overdispersion mechanism. The estimates in the quasi-glm analysis are no longer maximum likelihood estimates. A more sophisticated and more appropriate approach to addressing overdispersion is to account for more variability in the system.\n\n\nMixing Models\nIf the Poisson distribution is not sufficiently dispersed for a count target, what other options are available?\nThe Negative Binomial distribution is also a model for count data and permits more variability than the Poisson distribution. It is a popular alternative for the Poisson model in the presence of overdispersion. The Negative Binomial is often derived from independent Bernoulli experiments, similar to the Binomial random variable. While the Binomial(\\(n,\\pi\\)) is the sum of \\(n\\) independent Bernoulli(\\(\\pi\\)) experiments, the NegBin(\\(k,\\pi\\)) random variable is the number of Bernoulli(\\(\\pi\\)) trials until the \\(k\\)th event occurs.\nAnother way of motivating the Negative Binomial is through a mixing process and this way is particularly insightful in the context of modeling overdispersion.\nIn a mixing process one or more parameters are assumed random. The base distribution is considered conditional on the mixing parameter and the mixing distribution is derived as the marginal distribution, integrating (or summing) over the distribution of the mixing parameter. An example will make that easier to understand. Suppose that \\(Y|\\lambda\\) has a Poisson(\\(\\lambda\\)) distribution and that \\(\\lambda\\) is a Gamma(\\(\\alpha,\\beta\\)) random variable. The mixing distribution, the marginal distribution, is obtained by integrating the conditional distribution over the distribution of the random parameter:\n\\[\np(y) = \\Pr(Y=y) = \\int_0^\\infty p(y|\\lambda) f(\\lambda) \\, d\\lambda\n\\] The conditional distribution in this integral, \\(p(y|\\lambda)\\), is a Poisson and \\(f(\\lambda)\\) is a Gamma distribution. Before looking at the marginal distribution of \\(Y\\) in this Poisson–Gamma mixing scheme, is there something we can learn about the mean and variance of \\(Y\\)?\nSuppose \\(Y\\) has a distribution that depends on parameters \\([\\alpha, \\beta]\\) and has conditional expectation and variance \\[\n\\text{E}[Y|\\alpha] = \\mu(\\alpha,\\beta) \\quad \\text{Var}[Y|\\alpha] = \\sigma^2(\\alpha,\\beta)\n\\] If \\(\\beta\\) is a constant and \\(\\alpha\\) is a random variable the marginal mean and variance of \\(Y\\) are\n\\[\\begin{align*}\n  \\text{E}[Y] &= \\text{E}_\\alpha[\\text{E}[Y|\\alpha]] = \\text{E}_\\alpha[\\mu(\\alpha,\\beta)] \\\\\n  \\text{Var}[Y] &= \\text{Var}_\\alpha[\\text{E}[Y|\\alpha]] + \\text{E}_\\alpha[\\text{Var}[Y|\\alpha]] \\\\\n  &= \\text{Var}_\\alpha[\\mu(\\alpha,\\beta)] + \\text{E}_\\alpha\\left[\\sigma^2(\\alpha,\\beta)\\right]\n\\end{align*}\\]\nWhat does that mean? The marginal variance is larger than the average variance of the conditional distribution. By introducing a randomly varying parameter, and integrating over its distribution, we arrive at a more dispersed distribution.\nNow back to the Poisson–Gamma mixing problem. It can be shown that if \\(Y|\\lambda \\sim \\text{Poisson}(\\lambda)\\), and \\(\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)\\) that the marginal distribution of \\(Y\\) is\n\\[\n\\Pr(Y=y)= {y+\\alpha-1\\choose\\alpha-1} \\left(\\frac{\\beta}{\\beta+1}\\right)^y \\left(\\frac{1}{\\beta+1}\\right)^\\alpha\n\\]\nThis is a Negative Binomial distribution with mean \\(\\text{E}[Y] = \\alpha\\beta\\) and variance \\(\\text{Var}[Y] = \\alpha\\beta(1+\\beta)\\). The Negative Binomial is overdispersed relative to the Poisson by the factor \\((1+\\beta)\\).\nDifferent parameterizations of the Gamma mixing distribution are in use. For example, the Gamma can be parameterized in terms of \\(\\text{E}[\\lambda] = \\alpha\\beta = \\mu\\) and \\(\\text{Var}[\\lambda] = \\mu/\\phi\\). The resulting Negative Binomial model is then overdispersed by a factor \\((1+\\phi)/\\phi\\).\n\n\nExample: Negative Binomial Analysis for Poppy Counts in RCBD\n\n\nYou can fit a Negative Binomial model in R with the glm.nb function in the MASS library.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nnbreg &lt;- glm.nb(count ~ block + tx, data=poppies,link=\"log\")\nsummary(nbreg)\n\n\nCall:\nglm.nb(formula = count ~ block + tx, data = poppies, link = \"log\", \n    init.theta = 11.03966246)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   3.0687     0.2129  14.413  &lt; 2e-16 ***\nblock1        0.3866     0.1894   2.041 0.041223 *  \nblock2        0.2676     0.1901   1.408 0.159181    \nblock3        0.8618     0.1872   4.603 4.16e-06 ***\ntxA           2.6020     0.2325  11.190  &lt; 2e-16 ***\ntxB           2.5906     0.2325  11.140  &lt; 2e-16 ***\ntxC           0.9301     0.2381   3.906 9.38e-05 ***\ntxD           0.8874     0.2384   3.723 0.000197 ***\ntxE           0.0465     0.2474   0.188 0.850899    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(11.0397) family taken to be 1)\n\n    Null deviance: 283.530  on 23  degrees of freedom\nResidual deviance:  23.774  on 15  degrees of freedom\nAIC: 253.97\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  11.04 \n          Std. Err.:  3.57 \n\n 2 x log-likelihood:  -233.966 \n\n\nThe ratio of the residual deviance to its degrees of freedom is about 1.6, much improved compared to any of the Poisson-based models.\n\nnbreg$deviance/nbreg$df.residual\n\n[1] 1.584948\n\n\nBoth the coefficient estimates and the standard errors change compared to the Poisson analysis. Recall that the quasi-Poisson analysis affected only the standard errors of the coefficients.\n\n\nOther forms of parameter mixing are popular. The Binomial–Poisson mixing scheme assumes that the number of trials \\(n\\) of a Binomial(\\(n,\\pi\\)) random variable follows a Poisson(\\(\\lambda\\)) distribution. If \\(Y|n \\sim \\text{Binomial}(n,\\pi)\\) and \\(n \\sim \\text{Poisson}(\\lambda)\\), then \\(Y\\) has a Poisson distribution with mean \\(\\lambda\\pi\\). This Poisson is overdispersed relative to the Binomial(\\(n,\\pi\\)) evaluated at the average sample size \\(\\lambda\\) because \\[\n\\lambda\\pi &gt; \\lambda\\pi(1-\\pi)\n\\]\nThe Beta-Binomial mixing model assumes that \\(Y|n \\sim \\text{Binomial}(n,\\pi)\\), \\(n\\) is fixed, and \\(\\pi\\) is a random variable with a Beta(\\(\\alpha,\\beta\\)) distribution. The resulting marginal distribution of \\(Y\\) is known as the beta-binomial distribution and has mean and variance \\[\n\\text{E}[Y] = \\frac{n\\alpha}{\\alpha+\\beta} \\quad \\text{Var}[Y] = \\frac{n\\alpha\\beta(n+\\alpha+\\beta)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n\\]\nA double mixing scheme assumes that multiple parameters are random. For example, starting with a Binomial(\\(n,\\pi)\\) random variable, we can define a Binomial–Poisson–Gamma mixing model as follows\n\\[\\begin{align*}\n    Y | n &\\sim \\text{Binomial}(n,\\pi) \\\\\n    n | \\lambda &\\sim \\text{Poisson}(\\lambda) \\\\\n    \\lambda &\\sim \\text{Gamma}(\\alpha,\\beta)\n\\end{align*}\\]\nThe marginal distribution of \\(Y\\) is again Negative Binomial with mean \\(\\text{E}[Y] = \\alpha\\beta\\pi\\) and variance \\(\\text{Var}[Y] = \\alpha\\beta\\pi(1+\\beta\\pi)\\).\n\n\n\nFigure 27.1: Generalized model mind map\nApproximation of Binomial and Poisson probabilities\nFigure 27.13: The inverse link maps \\(\\eta\\) to valid values of the mean and the target data. If \\(Y \\in \\{0,1\\}\\), then \\(0 \\le \\text{E}[Y] \\le 1\\).\nFigure 27.15: Randomized Complete Block Design\n\n\n\nFerrari, S. L. P., and F. Cribari-Neto. 2004. “Beta Regression for Modeling Rates and Proportions.” Journal of Applied Statistics 31 (7): 799–815.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of Normality–Reflections on the Work of Quetelet and Galton.” Scandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear Models, 2nd Ed. Chapman & Hall, New York.\n\n\nMead, R., R. N. Curnow, and A. M. Hasted. 1993. Statistical Methods in Agriculture and Experimental Biology. CRC Press, New York; Boca Raton, FL.\n\n\nPrater, N. H. 1956. “Estimate Gasoline Yields from Crudes.” Petroleum Refiner 35 (3).",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "gam.html",
    "href": "gam.html",
    "title": "28  Generalized Additive Models",
    "section": "",
    "text": "28.1 Introduction\nGeneralized linear models (GLMs) generalize the classical linear regression model in a number of ways, detailed in Section 27.1. The most important generalization is to allow the target variable to be distributed according to any of the members of the exponential family of distributions. An important property of the linear model retained in GLMs is the linear predictor \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\): the predictor is a linear function of the inputs.\nGeneralized additive models (GAMs) relax the linear predictor assumption of GLMs. There is still a predictor and it is additive, but it is not additive in the \\(\\beta_j x_j\\) components. Instead, the predictor is additive in functions of the inputs. Mathematically, the change from GLMs to GAMs is a change from \\[\n\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n\\] to \\[\n\\eta = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p)\n\\]\nEach input is modeled with its own function. Where did the \\(\\beta\\)s go? There still is an overall intercept that adjusts for the level of \\(Y\\), any remaining parameters that need to be determined depend on the particulars of the functions \\(f_j(x_j)\\). For example, if \\(f_1()\\) is a natural spline in \\(x_1\\), then the spline basis expands into a certain number of columns depending on the degrees of freedom of the spline.\nOnce the predictor of the GAM has been determined, predicting the mean proceeds as in the GLM, invoking the inverse link function, \\(\\mu = g^{-1}(\\eta)\\).\nHow do we choose the functions \\(f_1(),\\cdots,f_p()\\)? If \\(x_j\\) is discrete (a factor), then it is common to estimate \\(f_j(x_j)\\) with a separate constant value for each of the factor levels. If \\(x_j\\) is continuous, local methods from Chapter 11 are candidates. In principle, you could use any technique for expressing \\(Y\\) as a function of \\(x\\) as one of the \\(f_j\\).",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Additive Models</span>"
    ]
  },
  {
    "objectID": "gam.html#pros-and-cons-of-gams",
    "href": "gam.html#pros-and-cons-of-gams",
    "title": "28  Generalized Additive Models",
    "section": "28.2 Pros and Cons of GAMs",
    "text": "28.2 Pros and Cons of GAMs\nGAMs are another generalization of the classical model on top of GLMs. In a sense, they bring the concept of local models into the GLM framework and add flexibility to capture non-linear relationships. Rather than trying out different transformations of \\(x_j\\) such as logarithms, square roots, inverses, polynomials, etc., you can leave it to the non-parametric nature of \\(f_j()\\) to find the appropriate trend.\nYet the contributions of the input variables remain additive: you can evaluate the contribution made by adding \\(f_2(x_2)\\) to a model containing \\(f_1(x_1)\\). Model interpretation is simple in terms of additive effects of functions of the inputs.\nOne downside of the GAM formulation is that the model is additive in functions of individual inputs. When \\(x_1\\) and \\(x_2\\) interact, how can you capture this in an additive model? One approach is to consider the product \\(x_1x_2\\) and add it to the GAM as another variable: \\[\n\\eta = \\beta_0 + f_1(x_1) + f_2(x_2) + f_3(x_1x_2)\n\\]\nAnother approach would be to add a two-dimensional smooth function of \\(x_1\\) and \\(x_2\\): \\[\n\\eta = \\beta_0 + f_1(x_1) + f_2(x_2) + f^*_3(x_1, x_2)\n\\] Notice the subtle difference between \\(f_3(x_1x_2)\\) and \\(f^*_3(x_1,x_2)\\): the former is a one-dimensional smoother in the product of the two variables, the latter is a two-dimensional smoother.\nAnother downside of the GAM formulation is a more complicated fitting procedure. If \\(Y\\) is Gaussian and all \\(f_j\\) are simple natural cubic splines, then we can set up one big \\(\\textbf{X}\\) matrix with the columns of the splines and compute the solution directly, as in a linear regression. If the \\(f_j\\) are smoothing splines, or loess models, or other functions that require a local fitting routine, then a different algorithm is needed. The solution is known as the backfitting algorithm (Hastie and Tibshirani 1990).\nThe backfitting algorithm, more fully described below, is an iterative algorithm. Fitting a GLM is typically also an iterative procedure (unless \\(Y\\) is Gaussian and the link is the identity function). A GAM for non-Gaussian data is thus a doubly iterative procedure. The IRLS algorithm described in Section 27.4.2 derives the maximum likelihood estimates by repeatedly fitting a weighted linear model, updating the pseudo-response and the weights of the model after each iteration. With a GAM component, fitting the weighted model at each of the IRLS iteration uses a weighted iterative backfitting algorithm. The IRLS algorithm defines the outer loop of the doubly-iterative algorithm, the backfitting algorithm comprises the inner loop.\nWhile doubly-iterative algorithms are not unusual in statistical modeling, they do present additional challenges:\n\nThe outer and the inner iteration might not converge.\nConvergence criteria are needed for both iterations, and they can be different. Tolerances for the inner iterations can change as the outer iterations progress. It makes sense to have more lax convergence criteria for the inner loop at early stages of the outer iteration than toward the end.\nIncreased computational burden..\n\n\nBackfitting Algorithm\nThe idea of the backfitting algorithm is straightforward. Fit a scatterplot smoother \\(f_k\\) to the partial residuals that extract the signal of all other functions \\(f_j, j \\neq k\\). Replace the previous values of \\(f_k\\) with the new values. Do this in turn for all functions until some convergence criterion is met.\nSuppose the response is continuous and the observational generalized additive model can be written as \\[\nY_i = \\beta_0 + \\sum_{j=1}^p f_j(x_j) + \\epsilon_i \\qquad i=1,\\cdots,n\n\\] with mean-zero errors. This GAM applies when \\(Y_i\\) is Gaussian distributed. The backfitting algorithm takes the following steps\n\nStep 0: Estimate \\(\\widehat{\\beta}_0\\) as \\(\\frac{1}{n}Y_i\\)\nStep 1: \\(\\widehat{f}_j = 0 \\quad \\forall j\\)\nStep 2: Cycle through the \\(k=1,\\cdots,p\\) functions and replace \\(\\widehat{f}_k\\) with a smoother of the partial residuals \\[\ny - \\widehat{\\beta}_0 - \\sum_{j=1, \\ne k}^p \\widehat{f}_j(x_j)\n\\]\nStep 3: Stop the process if the functions \\(\\widehat{f}_j\\) are changing by less than a chosen threshold, otherwise return to step 2.\n\nAt the end of step 2 for each of the functions, the estimated functions can be re-centered as \\(\\widehat{f}_j - \\frac{1}{n}\\sum \\widehat{f}_j(x_{ij})\\). While this is not necessary from a theoretical standpoint, the finite-precision calculations can benefit from re-centering.\nAmong the issues with the backfitting algorithm are\n\nthe arbitrary stop criterion: monitoring the change in function norms or the predicted values, a sum of squares criterion, etc. It is not clear what is the best (right) choice.\nthe order-dependence: the solution of the backfitting algorithm depends on the order in which the \\(f_j(x_j)\\) enter the model.\nnon-uniqueness: the solution of the algorithm is not unique (unless expressed in terms of eigendecomposition of projections).",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Additive Models</span>"
    ]
  },
  {
    "objectID": "gam.html#example-real-estate-values-in-albemarle-county-virginia",
    "href": "gam.html#example-real-estate-values-in-albemarle-county-virginia",
    "title": "28  Generalized Additive Models",
    "section": "28.3 Example: Real Estate Values in Albemarle County, Virginia",
    "text": "28.3 Example: Real Estate Values in Albemarle County, Virginia\nIn Section 27.5 we fit a Gamma regression model to the home values in Albemarle County, VA. For comparison with a GAM, we repeat here the model from that section, it expresses total home value as a function of the finished square footage of the home:\n\nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nhomes &lt;- dbGetQuery(con, \"SELECT * FROM AlbemarleHomes\")\n\ndbDisconnect(con)\nstr(homes)\n\n'data.frame':   3025 obs. of  16 variables:\n $ column00   : num  1 2 3 4 5 6 7 8 9 10 ...\n $ yearbuilt  : chr  \"1754\" \"1968\" \"1754\" \"1934\" ...\n $ finsqft    : num  1254 1192 881 480 720 ...\n $ cooling    : chr  \"No Central Air\" \"No Central Air\" \"No Central Air\" \"No Central Air\" ...\n $ bedroom    : num  1 3 2 0 2 3 2 3 2 0 ...\n $ fullbath   : num  1 1 1 0 1 1 1 1 1 0 ...\n $ halfbath   : chr  \"0\" \"0\" \"0\" \"0\" ...\n $ lotsize    : num  4.93 1.09 195.93 10 1 ...\n $ totalvalue : num  124300 109200 141600 69200 139700 ...\n $ esdistrict : chr  \"Brownsville\" \"Scottsville\" \"Stony Point\" \"Crozet\" ...\n $ msdistrict : chr  \"Henley\" \"Walton\" \"Sutherland\" \"Henley\" ...\n $ hsdistrict : chr  \"Western Albemarle\" \"Monticello\" \"Albemarle\" \"Western Albemarle\" ...\n $ censustract: num  111 113 104 101 102 ...\n $ age        : num  265 51 265 85 56 265 87 59 69 265 ...\n $ condition  : chr  \"Substandard\" \"Substandard\" \"Substandard\" \"Substandard\" ...\n $ fp         : num  0 0 0 0 0 0 0 1 0 0 ...\n\ngamma_glm &lt;- glm(totalvalue ~ finsqft, \n               data=homes,\n               family=Gamma(link=\"log\"))\nsummary(gamma_glm)\n\n\nCall:\nglm(formula = totalvalue ~ finsqft, family = Gamma(link = \"log\"), \n    data = homes)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.178e+01  1.986e-02  593.18   &lt;2e-16 ***\nfinsqft     5.081e-04  8.753e-06   58.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.2122692)\n\n    Null deviance: 1206.00  on 3024  degrees of freedom\nResidual deviance:  378.25  on 3023  degrees of freedom\nAIC: 79593\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model converges in 4 iterations and the maximum likelihood estimates of the coefficients are \\(\\widehat{\\beta}_0\\) = 11.7823 and \\(\\widehat{\\beta}_1\\) = 5.1^{-4}.\nThe addition of the single input variable finsqft reduces the deviance dramatically, compared to a null model; from 1205.995 to 378.25. The finsqft coefficient is highly significant.\n\nHow does this compare to a generalized additive model where the effect of finsqft is modeled as a smoothing spline? We use the gam::gam function to fit the GAM via backfitting algorithm. The gam function gives you flexibility in the formula for the predictor. You can specify effects of inputs as linear effects by simply listing the variable name, or as smooth effects by wrapping the variable in one of the possible smoother expressions. Currently, those expressions are s() for a smoothing spline, and lo() for a local regression (loess) smoother.\n\nlibrary(gam)\ngamma_gam &lt;- gam(totalvalue ~ s(finsqft), \n              data=homes,\n              family=Gamma(link=\"log\"))\nsummary(gamma_gam)\n\n\nCall: gam(formula = totalvalue ~ s(finsqft), family = Gamma(link = \"log\"), \n    data = homes)\nDeviance Residuals:\n     Min       1Q   Median       3Q      Max \n-2.13053 -0.20686 -0.06341  0.09829  4.19350 \n\n(Dispersion Parameter for Gamma family taken to be 0.2119)\n\n    Null Deviance: 1205.995 on 3024 degrees of freedom\nResidual Deviance: 362.1354 on 3020 degrees of freedom\nAIC: 79464.99 \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ns(finsqft)    1 711.30  711.30  3356.7 &lt; 2.2e-16 ***\nResiduals  3020 639.95    0.21                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n            Npar Df Npar F     Pr(F)    \n(Intercept)                             \ns(finsqft)        3  26.48 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nround(gamma_gam$coefficients,5)\n\n(Intercept)  s(finsqft) \n   11.78244     0.00051 \n\ngamma_gam$nl.df\n\ns(finsqft) \n  3.000253 \n\n\ngam adds a global parametric effect for each input variable in addition to the smooth nonparametric effects. The term s(finsqft) in the model formula triggers the addition of two model elements: a term \\(\\beta \\,\\text{finsqft}\\) and a smoothing spline \\(f_1(\\text{finsqft})\\). That is why s(finsqft) appears twice in the summary output: under the parametric effects with a single degree of freedom for the coefficient, and under the nonparametric effects. By default, the spline smoother in gam has target degrees of freedom of 4, and the output reports the degrees of freedom as one less than the trace of the smoother matrix; these degrees of freedom can be fractional numbers.\nThe residual deviance of the GAM model is smaller than that of the GLM model, 362.135 versus 378.25. Does this mean the GAM model provides a significantly better fit? We can test this hypothesis, but first a few words on how gam constructs predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe smoothing spline and loess function invoked by gam are gam.s and gam.lo, and are not the same as smooth.spline and loess. The default for gam.s are a smoothing spline with 4 target degrees of freedom and for gam.lo a loess fit of degree 1 with a span of 0.5. You can change these values, for example,\n\n    gam(totalvalue ~ s(finsqft,df=6) + lo(lotsize,span=0.25,degree=1), ...)\n\nAlso note that the smoothing spline in gam does not perform cross-validation selection of the optimal number of degrees of freedom (or knots) of the spline.\n\n\nTo compute predicted values for a GLM or GAM model, we can use the predict function, making sure we request the predictions of interest: on the scale of the linear predictor or on the scale of the response (the mean)\n\np_glm &lt;- predict(gamma_glm, newdata=data.frame(finsqft=2500), type=\"response\")\np_gam &lt;- predict(gamma_gam, newdata=data.frame(finsqft=2500), type=\"response\")\ncat(\"GLM: Predicted home value for 2,500 finished square feet: \", p_glm,\"\\n\")\n\nGLM: Predicted home value for 2,500 finished square feet:  466225.8 \n\ncat(\"GAM: Predicted home value for 2,500 finished square feet: \", p_gam,\"\\n\")\n\nGAM: Predicted home value for 2,500 finished square feet:  493006.3 \n\n\nHow do you construct the predicted value for an observation in the GAM analysis? Suppose we are interested in the predicted value for the 36th observation.\n\nhomes[36,]\n\n   column00 yearbuilt finsqft     cooling bedroom fullbath halfbath lotsize\n36       36      1977    2212 Central Air       4        3        0       2\n   totalvalue esdistrict msdistrict        hsdistrict censustract age condition\n36     212400     Murray     Henley Western Albemarle         110  42      Poor\n   fp\n36  1\n\nround(gamma_gam$coefficients,5)\n\n(Intercept)  s(finsqft) \n   11.78244     0.00051 \n\nparametric &lt;- gamma_gam$coefficients[1] + gamma_gam$coefficients[2]*homes[36,\"finsqft\"]\nas.numeric(parametric)\n\n[1] 12.90323\n\nas.numeric(parametric + gamma_gam$smooth[36])\n\n[1] 12.96615\n\n\nThe predictions comprise a parametric contribution from \\(\\beta\\,\\text{finsqft}\\) and a nonparametric contribution from the smoothing spline. The sum of the two is the predicted value on the scale of the predictor (the scale of the link function). You can also find this value in the additive.predictors vector.\nExponentiating this value—because we chose a log link function—produces the predicted home value for that observation. The fitted.values vector contains those values for all observations\n\ngamma_gam$additive.predictors[36]  # parametric + nonparametric elements\n\n      36 \n12.96615 \n\nas.numeric(exp(parametric + gamma_gam$smooth[36]))\n\n[1] 427687.9\n\ngamma_gam$fitted.values[36]\n\n      36 \n427687.9 \n\n\n\nFigure 28.1 displays the observed and predicted home values for the GLM and GAM models. The GAM predictions are generally lower than those of the GLM model. Interestingly, although we added a spline component in the finished square feet variable, the predictions are still very smooth.\n\nxvals &lt;- seq(1000,8000,by=100)\npred_glm &lt;- predict(gamma_glm,type=\"response\",newdata=data.frame(\"finsqft\"=xvals))\npred_gam &lt;- predict(gamma_gam,type=\"response\",newdata=data.frame(\"finsqft\"=xvals))\n\n\n\n\n\n\n\n\n\nFigure 28.1: Fitted home values for GAM and GLM models.\n\n\n\n\n\nThis changes when you increase the degrees of freedom of the smoothing spline. Figure 28.2 shows the fitted values for smoothing splines with 4 and 100 df, respectively.\n\n\n\n\n\n\n\n\nFigure 28.2: GAM models with smoothing splines with 4 and 100 df, respectively.\n\n\n\n\n\n\nBoth GLMs and GAMs are fit by maximum likelihood and the gam function adds parametric terms for the input variables to the model. We can thus compare the following models with a likelihood-ratio test because the GLM model is nested within the GAM model.\nglm(totalvalue ~ finsqft, ... )\ngam(totalvalue ~ s(finsqft), ... )\nThe difference in degrees of freedom between the two models should be equal to the number of degrees of freedom attributed to the smooth (nonparametric) components. This can be verified with the anova function:\n\nanova(gamma_glm,gamma_gam,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: totalvalue ~ finsqft\nModel 2: totalvalue ~ s(finsqft)\n  Resid. Df Resid. Dev     Df Deviance  Pr(&gt;Chi)    \n1      3023     378.25                              \n2      3020     362.14 3.0003   16.115 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe GAM model provides a significantly better fit to the data compared to the generalized linear model.\n\n\n\n\nHastie, T. J., and R. Tibshirani. 1990. Generalized Additive Models. Chapman & Hall, London.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Additive Models</span>"
    ]
  },
  {
    "objectID": "corrdata.html",
    "href": "corrdata.html",
    "title": "29  Correlated Data",
    "section": "",
    "text": "29.1 Introduction\nThroughout much of the previous chapters we have encountered correlation in various forms and scenarios. For example,\nCorrelation is typically a good thing, it indicates relationships and patterns that we want to find. Correlations among the target values has so far been largely ignored or brushed aside by assuming that model errors are uncorrelated. We know that this assumption is not always correct.\nSuppose we are to collect data from individuals in the state of Virginia. The sampling is carried out in two stages to make sure that counties and individuals within counties are represented. First, we draw at random twenty entities from the 95 counties and 39 independent cities in the state. Then, in each of the twenty selected cities or counties, we sample 100 residents. If \\(\\tau_i\\) denotes the effect of the \\(i\\)th city/county and \\(e_{ij}\\) the effect of the \\(j\\)th resident sampled within the city/county \\(i\\), then a model for the overall mean attribute is \\[\nY_{ij} = \\mu + \\tau_i + e_{ij}\n\\]\nBecause we sampled at the stage of the city/county and at the stage of residents, the \\(\\tau_i\\) and the \\(e_{ij}\\) are random variables. If there are no systematic effects, their basic stochastic properties are \\(\\tau_i \\sim (0,\\sigma^2_\\tau)\\), \\(e_{ij} \\sim (0,\\sigma^2_e)\\).\nThis is an example of a hierarchical sampling scheme, we select one sample within the draws of another sample. What does this hierarchy of random variables have to do with correlated data? To answer this, consider two covariances:\nThe former is \\[\\begin{align*}\n\\text{Cov}[Y_{ij},Y_{kl}] &= \\text{Cov}[\\mu + \\tau_i + e_{ij},\\mu + \\tau_k + e_{kl}] \\\\\n&= \\text{Cov}[\\tau_i + e_{ij},\\tau_k + e_{kl}] \\\\\n&= \\text{Cov}[\\tau_i,\\tau_k] + \\text{Cov}[\\tau_i,e_{kl}] + \\text{Cov}[e_{ij},\\tau_k] + \\text{Cov}[e_{ij},e_{kl}] \\\\\n&= 0 + 0 + 0 + 0 \\\\\n&= 0\n\\end{align*}\\]\nThe covariance between two observations from the same city/county, on the other hand, is \\[\\begin{align*}\n\\text{Cov}[Y_{ij},Y_{il}] &= \\text{Cov}[\\mu + \\tau_i + e_{ij},\\mu + \\tau_i + e_{il}] \\\\\n&= \\text{Cov}[\\tau_i + e_{ij},\\tau_i + e_{il}] \\\\\n&= \\text{Cov}[\\tau_i,\\tau_i] + \\text{Cov}[\\tau_i,e_{il}] + \\text{Cov}[e_{ij},\\tau_i] + \\text{Cov}[e_{ij},e_{il}] \\\\\n&= \\sigma^2_\\tau + 0 + 0 + 0 \\\\\n&= \\sigma^2_\\tau\n\\end{align*}\\]\nTwo observations from the same city/county share the random effect \\(\\tau_i\\); this sharing induces a correlation. Notice that if the \\(\\tau_i\\) were fixed effects, rather than random variables, there would be no induced correlation. That would be the case if samples were taken from all cities/counties in Virginia.\nShared random effects due to random sampling is one mechanism that creates correlations among observations. Figure 29.1 displays other ways in which correlations can come about.\nHierarchical random processes can also at work when treatments are assigned to experimental units. A common experimental design in agricultural application is the split-plot design in which two different treatment factors are deployed on experimental units of different size (Figure 29.2). One factor might be a soil treatment that requires large areas for its application. Another factor might be four variants of a crop species that can be planted on smaller areas than the soil treatment. A split-plot design assigns soil treatments to large experimental units and assigns the four crop variants to small experimental units within each of the large units.\nA natural mechanism through which correlations occur is the observation of a stochastic process in time or space. In the subsampling example above, correlation is induced because two observations share something: they were sampled from the same city or county.\nAn extreme version of sharing is when the same entity is measured repeatedly. Suppose you measure the length of an item at time \\(t\\) and immediately afterwards. The two measurements will be very similar, due to measurement errors they might not be identical. If \\(\\epsilon\\) is the small time interval between the two measurements, then we expect \\(Y(t)\\) and \\(Y(t+\\epsilon)\\) to be highly correlated. If \\(Y\\) is the length of an inanimate object, an iron rod, say, then we actually expect the correlation to be very high over long time intervals, since \\(Y\\) is essentially invariable. If \\(Y\\) is an attribute subject to change, then we expect the correlation between \\(Y(t)\\) and \\(Y(t+\\delta)\\) to somehow be a function of the time difference \\(\\delta\\). The delay times of trains at the station are more highly correlated between now and ten minutes from now than between now and six months from now.\nThis type of correlation, of a random variable with itself, is called autocorrelation.\nHierarchical random processes and autocorrelated processes create clustered data structures. We need to distinguish the use of “cluster” in this context from the cluster analyses in Chapter 24. What they have in common is that a cluster is a group of observations that have something in common. In cluster analysis the commonality is similarity of attribute values; the number and size of the clusters is not known a priori, finding the clusters is the goal of the analysis.\nIn the context of correlated data the clusters are defined through the process in which the data are collected; observations from the same cluster are correlated and observations from different clusters are (typically) uncorrelated (Figure 29.3). A longitudinal study where \\(i=1,\\cdots,k\\) subjects are measured \\(j=1,\\cdot n_i\\) times, creates a clustered data structure with \\(k\\) clusters of sizes \\(n_1,\\cdots,n_k\\). Cluster membership is known a priori and not subject to analysis.\nMultiple causes for clustering can occur in the same study. For example, one might have a hierarchical sampling scheme and the elements drawn into the subsample are then measured repeatedly over time. The apple data which we introduce here and revisit throughout this and the following chapter are an example of such a data structure.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Correlated Data</span>"
    ]
  },
  {
    "objectID": "corrdata.html#introduction",
    "href": "corrdata.html#introduction",
    "title": "29  Correlated Data",
    "section": "",
    "text": "A common assumptions about the model errors is that they are not correlated. The homoscedastic (equi-variance) and uncorrelated situation, often called the iid case, is characterized by \\(\\text{Var}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\textbf{I}\\).\nThe correlation between a target \\(Y\\) and an input \\(X\\) is a measure of their linear dependency.\nThe \\(R^2\\) statistic in linear models is the square of the correlation coefficient between \\(Y\\) and \\(\\widehat{Y}\\).\nPrincipal component analysis (Chapter 23) constructs uncorrelated linear projections of the input variables.\nCorrelation does not imply causation.\nRandom variables that are independent are also uncorrelated. The reverse is not generally true, but it is true for Gaussian random variables (Section 3.7).\nThe covariance/correlation structure between inputs is explicitly modeled in model-based clustering (Chapter 25).\nMulticollinearity is a broader concept than pairwise correlations. In the presence of weak pairwise correlations you can have strong multicollinearity (Section 7.4.4).\n\n\n\n\n\n\nbetween observations from different cities/counties\nbetween observations from the same city/county\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.1: Correlated data mind map.\n\n\n\n\n\n\n\n\n\n\nFigure 29.2: Split-plot design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.3: Example of a clustered data structure. Observations from the same cluster are correlated, observations from different clusters are uncorrelated.\n\n\n\n\n\n\nExample: Apple Diameters\n\n\nThe data for this analysis were collected at the Winchester Agricultural Experiment Station of Virginia Tech and are analyzed in (Schabenberger and Pierce 2001, 466–74). Ten apple trees were randomly selected at the experiment station and 25 apples were randomly chosen on each tree. The data analyzed here comprise only the apples in the largest size class, those apples with an initial diameter equal or greater than 2.75 inches. Over a period of 12 weeks diameter measurements of the apples were taken at 2-week intervals. The variables in the data set are\n\nTree: the tree number\nappleid: the number of the apple within the tree. Note that the same appleid can appear on multiple trees and only apples in the largest diameter size class appear in the data set.\nmeasurement: the index of the measurements. Measurements are taken in two-week intervals, so that measurement=1 refers to the state of the apple after 2 weeks and measurement=6 refers to the state of the apple at the end of the 12-week period\ndiameter the diameter of the apple at the time of measurement\n\n\nlibrary(\"duckdb\")\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\napples &lt;- dbGetQuery(con, \"SELECT * FROM apples\")\n\ndbDisconnect(con)\n\nFigure 29.4 displays the diameter measurements on each tree. The data are longitudinal, each apple is measured one or more time during the 12-week period. The data also exhibit a subsampling structure: trees were randomly selected in the orchard and apples were randomly selected on each tree. There is clustering at two levels:\n\napples are clustered within trees due to subsampling\nlongitudinal measurements are creating a temporal cluster for each apple.\n\n\nlibrary(lattice)\nxyplot(diameter ~ measurement | Tree, \n       data=apples,\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =c(T,T), \n                         strip.levels=c(T,T),\n                         sep=\" \")\n       },\n       xlab=\"Measurement index\",\n       ylab=\"Diameter (inches)\",\n       type=c(\"p\"),\n       as.table=TRUE,\n       layout=c(4,3,1))\n\n\n\n\n\n\n\nFigure 29.4: Trellis plot of the apple diameters for all ten trees.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Correlated Data</span>"
    ]
  },
  {
    "objectID": "corrdata.html#autocorrelation",
    "href": "corrdata.html#autocorrelation",
    "title": "29  Correlated Data",
    "section": "29.2 Autocorrelation",
    "text": "29.2 Autocorrelation\nAs the name suggests, autocorrelation is the correlation of a random variable with itself. Rather than investigating the relationship between two different attributes, say \\(Y\\), the weight of an animal, and \\(X\\), the age of the animal, we are studying the behavior of the animal’s weight over time. Instead of \\(\\text{Corr}[Y,X]\\), we are interested in \\(\\text{Corr}[Y(t),Y(t+\\delta)]\\).\nIn time series analysis this type of correlation is also called serial correlation or temporal correlation. In the analysis of data in a spatial context it is referred to as spatial correlation. In longitudinal studies we encounter autocorrelation when the same entity is measured more than once; a typical example is taking repeated health measurements on patients in a study. When this occurs in the context of a designed experiment, the data are also referred to as repeated measures.\nAutocorrelation is typically positive. Observations of an attribute that are close temporally or spatially are similar to each other. As the temporal or spatial separation increases, the correlation tends to decline. This evolution of the correlation over time or space is captured by the autocorrelation function.\n\n\nExample: Autocorrelation of Apple Share Price\n\n\nFigure 29.5 displays the weekly closing share price for Apple stock (ticker symbol AAPL) between 2010 and 2023. This is a time series \\(Y(t)\\), where \\(Y\\) represents the close price and \\(t\\) is the point in time that marks the end of a particular week. A longer version of this time series was analyzed in #sec-smoothing-splines.\n\nlibrary(lubridate)\nlibrary(\"duckdb\")\n\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nweekly &lt;- dbGetQuery(con, \"SELECT * FROM AAPLWeekly\")\nweekly$date &lt;- make_date(weekly$year,weekly$month,weekly$day)\n\ndbDisconnect(con)\nweekly_2010 &lt;- subset(weekly,(weekly$year &gt;= 2010) & (weekly$year &lt;= 2023))\n\n\n\n\n\n\n\n\n\nFigure 29.5: Weekly AAPL close share prices from January 2010 through December 2023.\n\n\n\n\n\nAlthough the share price goes up and down, there is positive autocorrelation between the prices, \\(\\text{Corr}[Y(t),Y(t+\\delta)] &gt; 0\\). Figure 29.6 displays the empirical estimates of the autocorrelation for lags ranging from 1 to 20.\n\nlibrary(tseries)\npar(mar=c(5,5,2,2))\nautocorr &lt;- acf(weekly_2010$Close,lag.max=20,main=\"\",las=1)\n\n\n\n\n\n\n\nFigure 29.6: Empirical autocorrelations for lags 1–20 of AAPL weekly close prices between 2010 and 2023.\n\n\n\n\n\n\nknitr::kable(data.frame(lag=autocorr$lag,acf=autocorr$acf),align=\"l\",format=\"simple\")\n\n\n\n\nlag\nacf\n\n\n\n\n0\n1.0000000\n\n\n1\n0.9939698\n\n\n2\n0.9878988\n\n\n3\n0.9815283\n\n\n4\n0.9750561\n\n\n5\n0.9689264\n\n\n6\n0.9627808\n\n\n7\n0.9568324\n\n\n8\n0.9512378\n\n\n9\n0.9464483\n\n\n10\n0.9422454\n\n\n11\n0.9377848\n\n\n12\n0.9327817\n\n\n13\n0.9280758\n\n\n14\n0.9237923\n\n\n15\n0.9192121\n\n\n16\n0.9144181\n\n\n17\n0.9093639\n\n\n18\n0.9034697\n\n\n19\n0.8981824\n\n\n20\n0.8933590\n\n\n\n\n\nThe estimate of the autocorrelation at lag 1, \\(\\widehat{\\text{Corr}}[Y(t),Y(t+1)]\\) = 0.994. The correlation between two share prices twenty weeks apart is lower but still pretty high: \\(\\widehat{\\text{Corr}}[Y(t),Y(t+20)]\\) = 0.8934. (The first entry of the autocorrelation results corresponds to lag=0).\n\n\nThe calculation of the autocorrelation estimates in this example reveals that we think of the autocorrelation as a function of the distance between data points, the lag as measured in weeks. We do not think of it as a function of \\(t\\) itself. In other words, the correlation between two share prices \\(k\\) weeks apart is the same, whether the first date is in 2010, in 2015, or in 2023. It does not depend on the origin of time. This is a typical assumption in the analysis of stochastic processes, called stationarity. Before we explore stationarity properties further in Section 29.2.2, let’s spend a bit of time on what it means for data analytics if the data are autocorrelated.\n\nConsequences of Autocorrelation\nIs autocorrelation good or bad? Does it matter if the data are autocorrelated? In a sense, autocorrelation is neither good nor bad, it just is what it is. It is unavoidable if we measure the same attribute in temporal or spatial proximity. Soil samples taken close to each other will be more similar than soil samples taken far apart. They will be spatially correlated and more so the closer they are.\nIf you want to assess the growth in market capitalization of a company, you study their books at the beginning and at the end of the fiscal year. You could also take the difference between the average market cap of a random sample of companies taken at the beginning of the year and the average market cap of a random sample of companies taken at the end of the fiscal year. If the random samples are representative of our target company, this difference is an unbiased estimate of the growth–and a horrible estimate at that. If you want to measure change, measure the entity that changes! The result will be correlated observations.\nThe more interesting question to ask is what happens if data are autocorrelated and we do not take this into account in the analysis? The kind of thing we did in Section 11.3.4 when the AAPL share price data was analyzed with smoothing splines under an iid assumption.\nTo demonstrate the effect of ignoring correlation, we look at a special case, the equi-correlation model, also called the compound symmetry model of correlation. Under this correlation structure, all observations have the same correlation.\nSuppose that \\(Y(t_1), \\cdots, Y(t_n)\\) is a sequence of autocorrelated observations with common mean \\(\\mu\\) and variance \\(\\sigma^2\\). The (auto-)covariance is given by \\[\n    \\text{Cov}[Y(t_i),Y(t_j)] = \\left \\{ \\begin{array}{ll} \\sigma^2\\rho & i \\neq j \\\\ \\sigma^2 & i=j \\end{array} \\right .\n\\] and the autocorrelation is \\(\\text{Corr}[Y(t_i),Y(t_j)] = \\rho\\). Now suppose we estimate the mean \\(\\mu\\) of the sequence using the sample mean \\[\n\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y(t_i)\n\\] What is the variance of \\(\\overline{Y}\\)? It is probably not \\(\\sigma^2/n\\), the variance of \\(\\overline{Y}\\) if the \\(Y(t_i)\\) are uncorrelated.\n\\[\n\\begin{align*}\n        \\text{Var}[\\overline{Y}] &= \\frac{1}{n^2} \\text{Var}\\left [ \\sum_{i=1}^n Y(t_i) \\right ] = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}[Y(t_i),Y(t_j)] \\\\\n        &= \\frac{1}{n^2}\\left \\{ \\sum_{i=1}^n\\text{Var}[Y(t_i)] + \\sum_{i=1}^n\\sum_{j \\neq i}^n \\text{Cov}[Y(t_i),Y(t_j)]\\right \\} \\\\\n        &= \\frac{1}{n^2}\\left \\{ n\\sigma^2 + n(n-1)\\sigma^2\\rho \\right \\} \\\\ \\\\\n        &= \\frac{\\sigma^2}{n} \\left \\{ 1 + (n-1)\\rho \\right \\}\n\\end{align*}\n\\tag{29.1}\\]\nThis is a very interesting result: \\(\\text{Var}[\\overline{Y}] = \\{1+(n-1)\\}\\rho \\sigma^2/n\\). The variance of the sample mean of the equi-correlated observations is \\(\\{1+(n-1)\\rho\\}\\) times the variance of the sample mean of uncorrelated observations. Since autocorrelation is typically positive, we have \\[\n\\text{Var}[\\overline{Y}] &gt; \\frac{\\sigma^2}{n}\n\\]\nIf you analyze data as if they were independent (uncorrelated), when in fact they exhibit positive autocorrelation, the estimates of the variability of the statistics are too small. By pretending that the data are uncorrelated we are pretending that the statistics are more precise than they really are. As a consequence, standard error estimates and \\(p\\)-values are too small, confidence and prediction intervals are too narrow.\nNote that \\(\\overline{Y}\\) is not an unreasonable estimator of \\(\\mu\\) here. It is after all an unbiased estimator, \\[\n\\text{E}[\\overline{Y}] = \\frac{1}{n}\\text{E}[\\sum_{i=1}^n Y(t_i)] = \\frac{1}{n}\\sum_{i=1}^n\\text{E}[Y(t_i)] = \\frac{1}{n}\\sum_{i=1}^n\\mu = \\mu\n\\] Can we attach some intuition to the fact that ignoring positive autocorrelation leads to inflated statements of precision (underflated statements of variability)? The concept of the effective sample size helps: what is the equivalent number \\(n^\\prime\\) of independent observations that leads to the same precision as \\(n\\) observations of the correlated kind? Based on the derivation above, for the compound symmetry model this number is \\[\nn^\\prime = \\frac{n}{1+(n-1)\\rho}\n\\]\nFigure 29.7 displays the effective sample size under compound symmetry for various values of \\(n\\) and \\(\\rho\\).\n\n\n\n\n\n\n\n\nFigure 29.7: Effective sample size for compound symmetry correlation structure for various values of \\(\\rho\\).\n\n\n\n\n\nThis is quite sobering. If you collect 10 observations with \\(\\rho = 0.4\\), their sample mean will be as precise as the sample mean of 10/(1+9*0.4) = 2.17 uncorrelated observations! It seems like we are losing a lot of information. In the extreme situation where \\(\\rho = 1\\), the effective sample size is \\(n^\\prime = 1\\). How do we make sense of that?\nIf the observations are perfectly correlated, then \\(Y(t_1) = Y(t_2) = \\cdots = Y(t_n)\\) and there is no variability. Once you observe one data point, you have all the information you could possibly gather. Collecting more data does not add more information in this situation. Another way to look at it: if \\(\\sigma^2 = 0\\), then it does not matter what the denominator is in \\(\\sigma^2/n\\). The sample mean will have zero variance because the observations have zero variance.\nIn practice, the effective sample size reduction is not quite as harsh as in the compound symmetry example because autocorrelations diminish with increasing separation of data points. We chose the compound symmetry model since the calculation of the variance in Equation 29.1 is relatively straightforward.\n\n\nStationary Random Processes\nA stochastic process is called stationary if it is self-replicating, it looks similar in different parts of the domain \\(D\\). The domain \\(D\\) for a temporal process are points in time and for a spatial process the domain consists of all possible spatial coordinates (latitudes and longitudes). For a spatial process the observations are indexed with a two-dimensional vector of coordinates, \\(\\textbf{s}= [x_1,x_2]\\), for a temporal process the index is a scalar \\(t\\) and the processes themselves are expressed as \\(Y(\\textbf{s})\\) and \\(Y(t)\\).\nSo what does it mean for \\(Y(\\textbf{s})\\) to be self-replicating? \\(Y(\\textbf{s})\\) and \\(Y(\\textbf{s}+\\textbf{h})\\) are random variables separated by the lag vector \\(\\textbf{h}\\). As random variables they have a distribution, a mean, a variance, and so forth. Strict stationarity implies that the distribution is the same at all points, a condition more restrictive than what we need to make inferences about the random process. If we wish to estimate the covariance or correlation between two points of the process, then it would help if we can consider all pairs \\((Y(\\textbf{s}), Y(\\textbf{s}+\\textbf{h}))\\) to construct an estimator. But in order to do so, it must be true that \\[\n(Y(\\textbf{s}_1), Y(\\textbf{s}_1+\\textbf{h}))\n\\] and \\[\n(Y(\\textbf{s}_2), Y(\\textbf{s}_2+\\textbf{h}))\n\\] contribute the same information to the estimation. If that assumption does not hold, the autocorrelation function will not just be a function of \\(\\textbf{h}\\), but it will depend on the location itself.\nLess restrictive than strict stationarity—but a sufficiently strong condition for our purposes—second-order stationarity of a stochastic process implies that \\[\n\\begin{align*}\n\\text{E}[Y(\\textbf{s})] &= \\mu \\\\\n\\text{Cov}[Y(\\textbf{s}),Y(\\textbf{s}+\\textbf{h})] &= C(\\textbf{h})\n\\end{align*}\n\\] The first property states that the mean is constant and does not depend on location. The second property states that the covariance function is a function of the lag \\(\\textbf{h}\\) but not of the location \\(\\textbf{s}\\). In a second-order stationary random process absolute coordinates do not matter, the origin does not matter. For a time series that means you can talk about a difference of two days without worrying whether the first day was a Sunday or a Thursday.\nStationarity assumptions are important in the analysis of correlated data. You can say that the iid assumption has been replaced with a second-order stationarity assumption. The assumption of a constant mean seems more restrictive than the existence of a covariance function invariant to location. How can we apply these assumptions to modeling the AAPL share prices?\nThe statistical models entertained so far have focused on modeling the mean structure, \\[\nY = f(x_1,x_2, \\cdots,x_p,\\theta_1,\\cdots,\\theta_k) + \\epsilon\n\\] but implicitly we also modeled the random structure of \\(\\epsilon\\). Assuming that \\(\\epsilon \\textit{ iid } (0,\\sigma^2)\\) is placing a model on the variance-covariance structure on the model errors. Modeling correlated data applies the same ideas. The changes in the mean of the target are captured by \\(f(x_1,x_2, \\cdots,x_p,\\theta_1,\\cdots,\\theta_k)\\). The model errors are assumed to follow a second-order stationarity process with some covariance function. The assumption of a constant (zero) mean for \\(\\epsilon\\) makes sense in this situation as much as it makes sense in the iid case.\n\n\nAutocorrelation Functions\nThe autocorrelation function (or correlation function, for short) expresses the evolution of the correlation with increasing spatial or temporal separation of the observations. How does it relate to the covariance function \\(C(\\textbf{h})\\) or \\(C(t)\\) of a second-order stationary process mentioned in the previous section?\nLet’s look at observations measured at times $t_1, , t_n. The covariance between observations at time \\(t_i\\) and \\(t_j\\) is \\[\n\\text{Cov}[Y(t_i),Y(t_j)] = \\text{E}[Y(t_i)Y(t_j)] - \\text{E}[Y(t_i)]\\text{E}[Y(t_j)]\n\\] This is simply the definition of a covariance. If \\(Y(t)\\) is second-order stationary, then the means are the same and the covariance is only a function of \\(|t_i - t_j|\\) but not of the absolute values \\(t_i\\) or \\(t_j\\). The autocovariance function is \\[\n\\text{Cov}[Y(t_i),Y(t_j)] = C(|t_i - t_j|)\n\\] When \\(t_i = t_j\\), at lag 0, this function is the variance of \\(Y(t)\\): \\[\nC(0) = \\text{Cov}[Y(t_i),Y(t_i)] = \\text{Var}[Y(t_i)]\n\\] Note that second-order stationarity implies the absence of an origin, if \\(C(h)\\) does not depend on absolute coordinates \\(C(0)\\) does not depend on absolute coordinates. The second-order stationary process has constant mean and constant variance.\n\n\nDefinition: Autocorrelation Function\n\n\nThe autocorrelation function at lag \\(h\\) of a second-order stationary process is the ratio of the autocovariances at lag \\(h\\) and at lag \\(0\\) \\[\nR(h) = \\frac{C(h)}{C(0)}\n\\]\n\n\nBecause of this relationship we can switch between the autocorrelation and the autocovariance function easily: \\(C(h) = C(0)\\times R(h)\\) and models can be specified in terms of either function.\nModels for autocorrelations parameterize \\(R(h)\\), usually in such a way that the correlations decay with increasing lag. Some common autocorelation models are the following:\n\nFirst-order autoregressive (AR(1)) model \\[R(h) = \\rho^{|i-j|}\\]\nContinuous AR(1) (exponential) model \\[R(h) = \\rho^{h} = \\exp\\left \\{ - h/\\phi \\right \\}\\]\nGaussian model \\[R(h) = \\exp\\left \\{ -h^2/\\phi^2\\right \\}\\]\nSpherical model \\[R(h) = \\left \\{\n     \\begin{array}{ll}\n       1- \\frac{3}{2}\\left(\\frac{h}{\\phi} \\right) + \\frac{1}{2}\\left( \\frac{h}{\\phi}\\right)^3 & h \\leq \\phi \\\\\n       0 & h &gt; \\phi\n      \\end{array}\n      \\right .\n     \\]\n\n\n\n\n\n\n\nAR(1) Model\n\n\n\n\n\nThe name “AR(1) model” is a shorthand for the autoregressive model of first order, a basic model in the study of time series data. Let \\(\\{Y(t): t=\\cdots,-1,0,1,\\cdots\\}\\) be a time series of attribute \\(Y\\) observed at times \\(t=\\cdots,-1,0,1,\\cdots\\). The autoregressive series of first order is generated according to \\[\nY(t) = \\rho Y(t-1) + \\epsilon(t)\n\\] The \\(\\epsilon(t)\\) are uncorrelated random variables with mean 0 and variance \\(\\sigma^2_e\\), also called the random shocks or innovations of the time series process. The innovation at time \\(t\\) is not correlated with the past realizations, \\(\\text{Cov}[Y(t-s),\\epsilon(t)] = 0, \\forall s &gt; 0\\), and the \\(Y(t)\\) process has a constant mean \\(\\mu\\). For \\(\\rho &gt; 0\\), the AR(1) process has runs of positive and negative residuals that vary about \\(\\mu\\). The length of the runs increases with \\(\\rho\\).\nThe recursive relationship built into the AR(1) model leads to a covariance model of the form \\(C(h) = \\rho C(h-1)\\) and ultimately \\(C(h) = \\rho^h C(0)\\). The autocorrelation function of the AR(1) process is \\(R(h) = \\rho^h\\).\n\n\n\nThe AR(1) model specifies correlation in terms of the positions \\(i\\) and \\(j\\) of observations in the observed sequence. The lag \\(h\\) does not correspond to a difference in observation times in this model. When the observation times are equally spaced, this model is still appropriate and leads to a re-scaled estimate of \\(\\rho\\) compared to the continuous AR(1) model. For equally-spaced measurements, the possible lags are \\(|t_i - t_j| = c|i-j|\\) for some factor \\(c\\). The correlation function of the AR(1) process is a step function, it does not decrease smoothly with \\(h\\). The correlation matrix of this process with 4 observations is \\[\n\\left [\\begin{matrix}\n   1   & \\rho   & \\rho^2 & \\rho^3 \\\\\n\\rho   & 1      & \\rho   & \\rho^2 \\\\\n\\rho^2 & \\rho   & 1      & \\rho   \\\\\n\\rho^3 & \\rho^2 & \\rho   & 1\n\\end{matrix}\\right]\n\\] The same matrix for the continuous AR(1) model is \\[\n\\left [\\begin{matrix}\n   1   & \\rho^{|t_1-t_2|}   & \\rho^{|t_1-t_3|} & \\rho^{|t_1-t_4} \\\\\n\\rho^{|t_2-t_1|}   & 1      & \\rho^{|t_2-t_3|}   & \\rho^{|t_2-t_4|} \\\\\n\\rho^{|t_3-t_1|} & \\rho^{|t_3-t_2|}   & 1      & \\rho^{|t_3-t_4|}   \\\\\n\\rho^{|t_4-t_1|} & \\rho^{|t_4-t_2|} & \\rho^{|t_4-t_3|}   & 1\n\\end{matrix}\\right]\n\\]\nThe continuous AR(1) model appears in two common parameterizations, \\(\\rho^h\\) and \\(\\exp\\{-h/\\phi\\}\\). The former is convenient because \\(0 \\le \\rho \\le 1\\) and has interpretation as a correlation coefficient. The second parameterization has nice numerical properties and you can easily convert between the two: \\[\n\\begin{align*}\n\\rho &= e^{-1/\\phi} \\\\\n\\phi &= \\frac{-1}{\\log \\rho}\n\\end{align*}\n\\] The “gaussian” model is not related to the Gaussian distribution, it gets its name from the term in the exponent that resembles (somewhat) the term in the Gaussian density. For a given value of \\(\\phi\\), the exponential model decreases correlations initially more rapidly than the gaussian model but levels out sooner (Figure 29.8).\nThe spherical model is popular in the analysis of geospatial data. Unlike the exponential and the gaussian models, which do not reach \\(R(h) = 0\\), the spherical model has \\(R(\\phi) = 0\\). The lag at which autocorrelations have become negligbile is called the range of the function. For the exponential and gaussian models the practical range is defined as the lag at which \\(R(h) = 0.05\\). Those values are \\(3\\phi\\) and \\(\\sqrt{3}\\phi\\) for the exponential and gaussian models, respectively (Figure 29.8).\n\n\n\n\n\n\n\n\nFigure 29.8: Correlation functions with \\(\\phi = 20\\). Vertical reference lines are drawn at the respective practical range for the exponential and the gaussian models. The range of the spherical model is \\(\\phi = 20\\).",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Correlated Data</span>"
    ]
  },
  {
    "objectID": "corrdata.html#sec-corr-error-models",
    "href": "corrdata.html#sec-corr-error-models",
    "title": "29  Correlated Data",
    "section": "29.3 Correlated Error Models",
    "text": "29.3 Correlated Error Models\n\nSetting up the Model\nCorrelated error models parameterize the variance-covariance matrix (covariance matrix, for short) of the model errors. In longitudinal, time series, or spatial applications, the autocorrelation or autocovariance functions are used to structure this matrix. Suppose the data are clustered, the index \\(i\\) denotes the cluster, and \\(\\textbf{Y}_i\\) is the \\((n_i \\times 1)\\) vector of target values for the \\(i\\)th cluster. Clusters can be of different sizes, and there can be one or more clusters. In spatial data applications and in time series analysis it is common to treat the entire geospatial data set as a single cluster: all observations are correlated. In longitudinal applications clusters typically refer to the subjects of the study; the apple data is an example of that.\nIf the basic model structure is that of a linear model, we can write the model for the \\(i\\)th cluster as \\[\n\\begin{align*}\n\\textbf{Y}_i &= \\textbf{X}_i \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_i \\quad i=1,\\cdots,k\\\\\n\\boldsymbol{\\epsilon}_i &\\sim (\\textbf{0},\\textbf{V}_i) \\\\\n\\text{Cov}[ \\boldsymbol{\\epsilon}_i,\\boldsymbol{\\epsilon}_j] &= \\textbf{0}\\quad i \\neq j\n\\end{align*}\n\\tag{29.2}\\]\n\\(\\textbf{V}_i\\) is the \\((n_i \\times n_i)\\) covariance matrix of the model errors for the \\(i\\)th cluster: \\[\n\\textbf{V}_i = \\left [\n    \\begin{array}{cccc}\n    \\text{Var}[\\epsilon_{i1}] & \\text{Cov}[\\epsilon_{i1},\\epsilon_{i2}] & \\cdots & \\text{Cov}[\\epsilon_{i1},\\epsilon_{in_i}] \\\\\n    \\text{Cov}[\\epsilon_{i2},\\epsilon_{i1}] & \\text{Var}[\\epsilon_{i2}] &  \\cdots & \\text{Cov}[\\epsilon_{i2},\\epsilon_{in_i}] \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\text{Cov}[\\epsilon_{in_i},\\epsilon_{i1}] & \\text{Cov}[\\epsilon_{in_i},\\epsilon_{i2}] & \\cdots & \\text{Var}[\\epsilon_{in_i}]\n    \\end{array}\n\\right ]\n\\] The expression \\(\\text{Cov}[ \\boldsymbol{\\epsilon}_i,\\boldsymbol{\\epsilon}_j] = \\textbf{0}, i \\neq j\\), in Equation 29.2 states that data from different clusters are uncorrelated. If you were to write down the variance-covariance matrix of the entire data vector, where \\(\\textbf{Y}_1\\) is stacked on top of \\(\\textbf{Y}_2\\) and so on, the covariance matrix would be a block-diagonal matrix: \\[\n\\textbf{V}= \\text{Var}\\left[\\begin{array}{c} \\textbf{Y}_1 \\\\ \\textbf{Y}_2 \\\\ \\vdots \\\\ \\textbf{Y}_k \\end{array} \\right] =\n\\left [ \\begin{matrix}\n\\textbf{V}_1  & \\textbf{0}& \\cdots \\textbf{0}\\\\\n\\textbf{0}& \\textbf{V}_2  & \\cdots \\textbf{0}\\\\\n\\vdots & \\vdots & \\ddots \\textbf{0}\\\\\n\\textbf{0}& \\textbf{0}& \\cdots \\textbf{V}_k\n\\end{matrix}\\right]\n\\]\nTo reduce the number of unknown quantities in \\(\\textbf{V}_i\\), it is assumed that the \\(\\boldsymbol{\\epsilon}_i\\) are second-order stationary with autocovariance function \\(C(h,\\boldsymbol{\\theta})\\). The assumption about the error distribution now becomes \\[\n\\boldsymbol{\\epsilon}\\sim (0,\\textbf{V}_i(\\boldsymbol{\\theta}))\n\\] and the covariance matrix for the \\(i\\)th cluster is \\[\n\\textbf{V}_i(\\boldsymbol{\\theta}) = \\left [\n        \\begin{array}{cccc}\n        C(0,\\boldsymbol{\\theta}) & C(h_{12},\\boldsymbol{\\theta}) & \\cdots & C(h_{1{n_i}},\\boldsymbol{\\theta}) \\\\\n        C(h_{21},\\boldsymbol{\\theta}) & C(0,\\boldsymbol{\\theta}) & \\cdots & C(h_{2{n_i}},\\boldsymbol{\\theta}) \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        C(h_{{n_i}1},\\boldsymbol{\\theta}) & C(h_{{n_i}2,\\boldsymbol{\\theta}}) &  \\cdots & C(0,\\boldsymbol{\\theta})\n        \\end{array}\n\\right ]\n\\]\nThe overall model now comprises two sets of parameters, \\(\\boldsymbol{\\beta}\\) are the parameters of the mean function, \\(\\boldsymbol{\\theta}\\) are the parameters of the covariance structure. The former (\\(\\boldsymbol{\\beta}\\)) are called the fixed effects of the model, a term that will become clearer in Chapter 30. The latter (\\(\\boldsymbol{\\theta}\\)) are frequently called the covariance parameters of the model. We write \\(\\boldsymbol{\\theta}\\) as a vector of parameters since it contains at least two quantities: a parameter related to the strength of the correlation and a scale parameter that measures the variance of the process (\\(C(0)\\)). For example, if \\(n_i = 4\\) and the covariance structure follows the exponential (=continuous AR(1)) model, the covariance matrix takes the following form with two parameters \\(\\boldsymbol{\\theta}= [\\sigma^2, \\phi]\\):\n\\[\n\\textbf{V}_i(\\boldsymbol{\\theta}) = \\sigma^2 \\left [ \\begin{array}{cccc}\n    1                                     & e^{-|t_{i1}-t_{i2}|/\\phi} & e^{-|t_{i1}-t_{i3}|/\\phi} & e^{-|t_{i1}-t_{i4}|/\\phi} \\\\\n    e^{-|t_{i2}-t_{i1}|/\\phi} & 1                                     & e^{-|t_{i2}-t_{i3}|/\\phi} & e^{-|t_{i2}-t_{i4}|/\\phi} \\\\\n    e^{-|t_{i3}-t_{i1}|/\\phi} & e^{-|t_{i3}-t_{i2}|/\\phi} & 1                                     & e^{-|t_{i3}-t_{i4}|/\\phi} \\\\\n    e^{-|t_{i4}-t_{i1}|/\\phi} & e^{-|t_{i4}-t_{i2}|/\\phi} & e^{-|t_{i4}-t_{i3}|/\\phi} & 1\n    \\end{array} \\right ]\n\\]\nOnly 2 parameters are needed compared to a completely unstructured covariance matrix which has \\(4 \\times 5 / 2 = 10\\) unique elements.\n\n\nParameter Estimation\n\nEstimating the fixed effects\nOur correlated error model now takes the form \\[\n\\begin{align*}\n\\textbf{Y}_i &= \\textbf{X}_i \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_i \\quad i=1,\\cdots,k\\\\\n\\boldsymbol{\\epsilon}_i &\\sim (\\textbf{0},\\textbf{V}_i(\\boldsymbol{\\theta})) \\\\\n\\text{Cov}[ \\boldsymbol{\\epsilon}_i,\\boldsymbol{\\epsilon}_j] &= \\textbf{0}\\quad i \\neq j\n\\end{align*}\n\\]\nHow do we estimate the fixed-effects \\(\\boldsymbol{\\beta}\\) and covariance parameters \\(\\boldsymbol{\\theta}\\)? If \\(\\textbf{V}_i = \\sigma^2\\textbf{I}\\), we are in the ordinary least squares situation and would estimate \\(\\boldsymbol{\\beta}\\) as \\[\n\\widehat{\\boldsymbol{\\beta}}_{OLS} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}=\n\\left(\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{X}_i \\right)^{-1}\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{Y}_i\n\\] This estimate of \\(\\boldsymbol{\\beta}\\) does not involve the lonely covariance parameter \\(\\sigma^2\\), which is quite remarkable, see Section 4.2.1. Regardless of how variable the data are, we estimate the fixed effects coefficients the same way.\nIn the correlated error model this no longer holds. The optimal estimator now is the generalized least squares estimator (Section 4.2.2) \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\beta}}_{GLS} &= \\left(\\textbf{X}^\\prime\\textbf{V}(\\boldsymbol{\\theta})^{-1}\\textbf{X}\\right)^{-1} \\textbf{X}^\\prime\\textbf{V}(\\boldsymbol{\\theta})^{-1}\\textbf{Y}\\\\\n&= \\left(\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{V}_i(\\boldsymbol{\\theta})^{-1}\\textbf{X}_i \\right)^{-1}\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{V}_i(\\boldsymbol{\\theta})^{-1}\\textbf{Y}_i\n\\end{align*}\n\\] If we know \\(\\boldsymbol{\\theta}\\), this estimator can be readily calculated. But if we do not know \\(\\boldsymbol{\\theta}\\) we have a problem. In order to estimate \\(\\boldsymbol{\\theta}\\), one needs to know \\(\\boldsymbol{\\beta}\\), because we have to remove the mean function from the data in because the second-order process \\(\\boldsymbol{\\epsilon}\\) has a constant (zero) mean. Removing the mean requires some estimate of \\(\\boldsymbol{\\beta}\\). But in order to estimate \\(\\boldsymbol{\\beta}\\) we need to know \\(\\boldsymbol{\\theta}\\). This is somewhat of a cat-and-mouse game.\nThe tension is resolved by the estimated generalized least squares principle. Given a consistent estimate of \\(\\boldsymbol{\\theta}\\), you compute the estimated GLS estimator \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\beta}}_{EGLS} &= (\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{Y}\\\\\n&= \\left(\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{V}_i(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{X}_i \\right)^{-1}\\sum_{i=1}^k\\textbf{X}_i^\\prime\\textbf{V}_i(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{Y}_i\n\\end{align*}\n\\]\nWe are almost there! We just need an estimate of the covariance parameters.\n\n\nEstimating the covariance parameters\nMany principles can be applied to derive estimates of \\(\\boldsymbol{\\theta}\\) that can be plugged into the formula for the EGLS estimator of \\(\\boldsymbol{\\beta}\\). The usual approach is to estimate the covariance parameters \\(\\boldsymbol{\\theta}\\) by a likelihood-based principle after adding a distributional assumption for the model errors:\n\\[\n\\begin{align*}\n\\textbf{Y}_i &= \\textbf{X}_i \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_i \\quad i=1,\\cdots,k\\\\\n\\boldsymbol{\\epsilon}_i &\\sim G(\\textbf{0},\\textbf{V}_i(\\boldsymbol{\\theta})) \\\\\n\\text{Cov}[ \\boldsymbol{\\epsilon}_i,\\boldsymbol{\\epsilon}_j] &= \\textbf{0}\\quad i \\neq j\n\\end{align*}\n\\tag{29.3}\\]\nThis seems like a subtle change, we simply added the assumption that the errors are Gaussian distributed in Equation 29.3. It has significant implications. If \\(\\boldsymbol{\\epsilon}_i\\) is a vector of Gaussian random variables, then, by the linearity property of Gaussian variables, \\(\\textbf{Y}_i\\) is also Gaussian distributed with mean \\(\\textbf{X}_i\\boldsymbol{\\beta}\\) and variance \\(\\textbf{V}_i\\) (Section 3.7).\n\n\n\n\n\n\nImportant\n\n\n\nKeep in mind that adding a correlated error structure to your model invariably means that you make a Gaussian assumption for your data. This is implicit in algorithms that estimate the covariance parameters by the likelihood principle, whether that is maximum likelihood or restricted maximum likelihood. For example, the gls function in the nlme package in R estimates parameters by ML or REML.\n\n\nTwo likelihood-based approaches are maximum likelihood (ML) and restricted maximum likelihood (REML). The former is the familiar principle that maximizes the distribution of \\(\\textbf{Y}\\) as a function of the parameters. Restricted maximum likelihood estimation does not seek the values that maximize the log likelihood of \\(\\textbf{Y}\\) but that of \\(\\textbf{K}\\textbf{Y}\\). The matrix \\(\\textbf{K}\\) is chosen so that \\(\\textbf{K}\\textbf{X}= \\textbf{0}\\).\nML and REML solve the problem of the dependency between \\(\\boldsymbol{\\theta}\\) on \\(\\boldsymbol{\\beta}\\) in different ways. With ML estimation in the Gaussian case you can apply a technique called profiling the likelihood to\n\nfind an expression for \\(\\widehat{\\boldsymbol{\\beta}}\\) that depends on \\(\\boldsymbol{\\theta}\\)\nsubstitute this expression in the formula for the m log likelihood\nsolve the resulting function–the profiled log likelihood function–which is now only a function of \\(\\boldsymbol{\\theta}\\) for the covariance parameters, using numerical methods\n\nIn REML estimation the fixed effects are removed from the log likelihood by the matrix \\(\\textbf{K}\\), which causes \\(\\textbf{K}\\textbf{Y}\\) to have mean \\[\n\\text{E}[\\textbf{K}\\textbf{Y}] = \\textbf{K}\\text{E}[\\textbf{Y}] = \\textbf{K}\\textbf{X}\\boldsymbol{\\beta}= \\textbf{0}\n\\] The resulting REML log-likelihood is only a function of \\(\\boldsymbol{\\theta}\\) and can be maximized by numerical methods. Once the REML estimator of \\(\\boldsymbol{\\theta}\\) is obtained, \\(\\boldsymbol{\\beta}\\) is computed as the EGLS estimate.\nInterestingly, the profiled estimate of \\(\\boldsymbol{\\beta}\\) in ML estimation also has the form of a GLS estimate. On the surface, ML and REML estimates of \\(\\boldsymbol{\\beta}\\) are very similar: \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\beta}}_{ML} &= \\left(\\textbf{X}^\\prime\\textbf{V}^{-1}(\\widehat{\\boldsymbol{\\theta}}_{ML})\\textbf{X}\\right)^{-1} \\textbf{X}^\\prime\\textbf{V}^{-1}(\\widehat{\\boldsymbol{\\theta}}_{ML})\\textbf{Y}\\\\\n\\widehat{\\boldsymbol{\\beta}}_{REML} &= \\left(\\textbf{X}^\\prime\\textbf{V}^{-1}(\\widehat{\\boldsymbol{\\theta}}_{REML})\\textbf{X}\\right)^{-1} \\textbf{X}^\\prime\\textbf{V}^{-1}(\\widehat{\\boldsymbol{\\theta}}_{REML})\\textbf{Y}\n\\end{align*}\n\\] Both have the form of EGLS estimators with the respective estimates of the covariance parameters plugged into the evaluation of \\(\\textbf{V}(\\boldsymbol{\\theta})\\). Which should you prefer?\nThe relationship between ML and REML estimators of covariance parameters can be made more tangible by comparing them for a very simple model. Suppose \\(Y_1, \\cdots, Y_n\\) are a random sample from a G\\((\\mu,\\sigma^2)\\) distribution. The maximum likelihood estimator of the variance \\(\\sigma^2\\) (the covariance parameter in this case) is \\[\n\\widehat{\\sigma}^2_{ML} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\overline{Y})^2\n\\] We know that this estimator is biased. It would be unbiased if \\(\\mu\\) were known, but since we substitute the estimate \\(\\overline{Y}\\) for \\(\\mu\\), some bias is introduced. The REML estimator accounts for the fact that the mean is estimated by removing the mean from the (restricted) log likelihood and basing that function on \\(n-1\\) rather than \\(n\\) observations. The REML estimator of \\(\\sigma^2\\) is \\[\n\\widehat{\\sigma}^2_{REML} = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\overline{Y})^2\n\\] and is unbiased. A similar effect takes place when instead of the log likelihood of \\(\\textbf{Y}\\) you consider the log likelihood of \\(\\textbf{K}\\textbf{Y}\\). The contrast matrix \\(\\textbf{K}\\) effectively reduces the \\(n\\)-dimensional likelihood to a \\(n-p\\) dimensional likelihood where \\(p\\) is the number of parameters in the mean function. As a result, restricted maximum likelihood leads to less biased estimators of \\(\\boldsymbol{\\theta}\\) compared to ML estimation which leads to less biased estimators of \\(\\boldsymbol{\\beta}\\)–often, those estimators are unbiased. In general, ML estimates of covariance parameters have negative bias, they are smaller than the REML estimates.\nShould we thus prefer REML over ML estimation? It depends. Because the REML log likelihood does not contain information about the fixed effects, you cannot use the REML log-likelihood in a likelihood ratio test (LRT; see Section 10.4) to test hypotheses about the \\(\\beta\\)s. You can use the REML log likelihood to test hypotheses about the elements of \\(\\boldsymbol{\\theta}\\). If you want to use the LRT to test hypotheses about \\(\\boldsymbol{\\beta}\\) in a correlated error model, the estimation must be based on ML, not REML.\n\n\n\n\n\n\nCaution\n\n\n\nCheck the default estimation method of software for correlated error models. Because the covariance parameters are less biased, this is frequently REML estimation. The (log) likelihood reported cannot be used in LRTs about the fixed effects.\n\n\n\n\n\nWorked Example\n\n\nExample: Apple Diameters (Cont’d)\n\n\nSuppose we ignore the subsampling structure in the apple data for the time being and consider the data as comprising 80 clusters of longitudinal measurements–each of the 80 apples is one cluster.\nBecause appleid is not unique within Tree id, we create a unique identifier for each apple by concatenating the apple id to the tree id. This will serve as the cluster id in the models.\n\nlibrary(dplyr)\napples &lt;- apples %&gt;% \n    mutate(TreeApp = paste(as.character(Tree),\"|\",as.character(appleid)))\nhead(apples)\n\n  Tree appleid measurement diameter TreeApp\n1    1       1           1     2.90   1 | 1\n2    1       1           2     2.90   1 | 1\n3    1       1           3     2.90   1 | 1\n4    1       1           4     2.93   1 | 1\n5    1       1           5     2.94   1 | 1\n6    1       1           6     2.94   1 | 1\n\n\nYou can fit linear models with correlated errors by restricted maximum likelihood (REML) or maximum likelihood (ML) with the gls() function in the nlme package in R.\nThe following statements fit a linear model with continuous AR(1) errors for repeated measurements on apples by REML. The fixed-effects model is a linear regression of diameter on measurement occasion.\nThe correlation model is \\[\nR(h) = \\phi^h\n\\]\nThe expression corCAR1(form = ~measurement | TreeApp) specifies the correlation model and its formula. form = ~measurement specifies that the correlation is a function of the measurement variable. The variable specified after the vertical slash is the grouping variable that identifies the clusters. The observations that share the same value of TreeApp are considered members of the same group (cluster); the distance between their observations depends on the values of the measurement variable.\n\nlibrary(nlme)\ngls_car1 &lt;- gls(diameter ~ measurement, \n            data=apples, \n            corCAR1(form = ~measurement | TreeApp),\n            na.action=na.omit,\n            method=\"REML\") # this is the default\n\ngls_car1\n\nGeneralized least squares fit by REML\n  Model: diameter ~ measurement \n  Data: apples \n  Log-restricted-likelihood: 946.0671\n\nCoefficients:\n(Intercept) measurement \n 2.82442419  0.02899978 \n\nCorrelation Structure: Continuous AR(1)\n Formula: ~measurement | TreeApp \n Parameter estimate(s):\n      Phi \n0.9777128 \nDegrees of freedom: 451 total; 449 residual\nResidual standard error: 0.1052637 \n\n\nThe estimates of the fixed effects are \\(\\widehat{\\boldsymbol{\\beta}}_{REML}\\) = [2.82442, 0.029].\nThe estimate of the correlation parameter is \\(\\widehat{\\phi}_{REML}\\) = 0.9777\nThe estimate of the error variance is \\(\\widehat{\\sigma}^2_{REML}\\) = 0.1053^2 = 0.0111.\nThe fitted model for measurement \\(j\\) on apple \\(i\\) \\[\n\\widehat{y}_{ij} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1x_{ij} = 2.8244 + 0.028999 \\, x_{ij}\n\\] and the estimated covariance matrix is \\[\n\\widehat{\\text{Var}}[\\textbf{Y}_i] = 0.1052^2 \\times\n\\left [ \\begin{array}{cccccc}\n      1            & 0.977^1 & 0.977^2 & 0.977^3 & 0.977^4 & 0.977^5 \\\\\n      0.977^1 & 1            & 0.977^1 & 0.977^2 & 0.977^3 & 0.977^4 \\\\\n      0.977^2 & 0.977^1 & 1            & 0.977^1 & 0.977^2 & 0.977^3 \\\\\n      0.977^3 & 0.977^2 & 0.977^1 & 1            & 0.977^1 & 0.977^2 \\\\\n      0.977^4 & 0.977^3 & 0.977^2 & 0.977^1 & 1            & 0.977^1 \\\\\n      0.977^5 & 0.977^4 & 0.977^3 & 0.977^2 & 0.977^1 & 1            \\\\\n\\end{array} \\right ]\n\\] The autocorrelations between successive diameter measurements are very high.\nWe can also fit the same autocovariance structure as an equivalent exponential model with \\[ R(h) = \\exp(-h/\\phi)\\]\n\ngls_exp &lt;- gls(diameter ~ measurement, \n            data=apples, \n            corExp(form = ~measurement | TreeApp),\n            na.action=na.omit,\n            method=\"REML\")\n\ngls_exp\n\nGeneralized least squares fit by REML\n  Model: diameter ~ measurement \n  Data: apples \n  Log-restricted-likelihood: 946.0671\n\nCoefficients:\n(Intercept) measurement \n 2.82442419  0.02899978 \n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~measurement | TreeApp \n Parameter estimate(s):\n   range \n44.36699 \nDegrees of freedom: 451 total; 449 residual\nResidual standard error: 0.1052637 \n\n\nThe fixed-effects coefficients and the REML log-likelihood are the same as in the previous model. Note that \\(\\exp(-1/44.36699) = 0.97771\\), the AR(1) coefficient from the corCAR1 model.\nIn this case, since the measurements are evenly spaced, the standard autoregressive correlation model (AR(1)) also gives the same results\n\ngls_ar1 &lt;- gls(diameter ~ measurement, \n            data=apples, \n            corAR1(form = ~measurement | TreeApp),\n            na.action=na.omit,\n            method=\"REML\")\n\ngls_ar1\n\nGeneralized least squares fit by REML\n  Model: diameter ~ measurement \n  Data: apples \n  Log-restricted-likelihood: 946.0671\n\nCoefficients:\n(Intercept) measurement \n 2.82442419  0.02899978 \n\nCorrelation Structure: AR(1)\n Formula: ~measurement | TreeApp \n Parameter estimate(s):\n      Phi \n0.9777128 \nDegrees of freedom: 451 total; 449 residual\nResidual standard error: 0.1052637 \n\n\nNote that the three models have the same REML log likelihood of 946.0671. The models are equivalent.\nOverlaying the fitted line on the trellis plots shows that we fit a marginal model–all apples share the same overall linear trend.\n\nxyplot(diameter ~ measurement | Tree, \n       data=apples,\n       xlab=\"Measurement index\",\n       ylab=\"Diameter (inches)\",\n       type=c(\"p\"),\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =c(T,T), \n                         strip.levels=c(T,T),\n                         sep=\" \")\n       },\n       as.table=TRUE,\n       panel=function(x,y,...) {\n           panel.xyplot(x,y,...)\n           panel.abline(coef=gls_car1$coefficients,col=\"blue\")\n           },\n       layout=c(4,3,1)) \n\n\n\n\n\n\n\nFigure 29.9: Fitted trend from correlated error models.\n\n\n\n\n\n\n\nA common linear trend over time applies to all apples (Figure 29.9). This trend works for some apples but the figure suggest for most apples a different intercept and/or a different slope. Accommodating such cluster-to-cluster variation is done efficiently with mixed models (Chapter 30).\n\n\n\nFigure 29.1: Correlated data mind map.\nFigure 29.2: Split-plot design.\nFigure 29.3: Example of a clustered data structure. Observations from the same cluster are correlated, observations from different clusters are uncorrelated.\nFigure 29.4: Trellis plot of the apple diameters for all ten trees.\nFigure 29.5: Weekly AAPL close share prices from January 2010 through December 2023.\nFigure 29.6: Empirical autocorrelations for lags 1–20 of AAPL weekly close prices between 2010 and 2023.\nFigure 29.8: Correlation functions with \\(\\phi = 20\\). Vertical reference lines are drawn at the respective practical range for the exponential and the gaussian models. The range of the spherical model is \\(\\phi = 20\\).\nFigure 29.9: Fitted trend from correlated error models.\n\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary Statistical Models for the Plant and Soil Sciences. CRC Press, Boca Raton.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Correlated Data</span>"
    ]
  },
  {
    "objectID": "mixed.html",
    "href": "mixed.html",
    "title": "30  Mixed Models for Longitudinal Data",
    "section": "",
    "text": "30.1 Introduction",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Models for Longitudinal Data</span>"
    ]
  },
  {
    "objectID": "mixed.html#introduction",
    "href": "mixed.html#introduction",
    "title": "30  Mixed Models for Longitudinal Data",
    "section": "",
    "text": "Motivation\nIn the previous chapter we analyzed longitudinal data with correlated error models. The example in Section 29.3.3 analyzed diameter measurements of 80 apples over a 12-week period. The fixed-effect model was a linear relationship between diameter and measurement time. If subscript \\(i\\) identifies the apple and subscript \\(j\\) the measurement occasion, the model for an individual observation is \\[\nY_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\epsilon_{ij}, \\quad i=1,\\cdots,80\n\\] The graph of the fitted model with an autocorrelation structure of the \\(\\epsilon_{ij}\\) is shown in Figure 29.9. All apples share the same intercept and slope. That seems problematic, some apples are clearly not represented well by that overall trend. In the vernacular of mixed models, this overall trend is called the population average trend for reasons that will become apparent soon.\nHow can we introduce more subject-specific behavior to capture that apples differ in their growth behavior? An obvious step would be to vary the intercepts and slopes between apples. The model \\[\nY_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij}, \\quad i=1,\\cdots,80\n\\tag{30.1}\\]\nhas a cluster-specific intercept and a cluster-specific intercept. These 80 intercepts and 80 slopes capture the variability in the fixed-effects among the clusters (apples).\nIs there a more parsimonious way to express this variability? Replace in Equation 30.1 \\(\\beta_{0i}\\) with \\(\\beta_0 + b_0\\) and \\(\\beta_{1i}\\) with \\(\\beta_1 + b_1\\): \\[\nY_{ij} = (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) x_{ij} + \\epsilon_{ij}, \\quad i=1,\\cdots,80\n\\tag{30.2}\\]\nThe \\(b_{0i}\\) and \\(b_{1i}\\) terms in this model are random variables, not fixed effects. Specifically, we assume that \\(b_{0i} \\sim G(0,\\sigma^2_0)\\) and \\(b_{1i} \\sim G(0,\\sigma^2_1)\\). We are back to having only two fixed effects: \\(\\beta_0\\) and \\(\\beta_1\\). A specific apple’s intercept is expressed as a random deviation from the overall intercept. A specific apple’s slope is expressed as a random deviation from the overall slope.\nAnother way of thinking about the intercepts and slopes in Equation 30.2 is as follows: an effect consists of two elements: a fixed, overall, contribution and a random draw from a Gaussian distribution that is specific to the cluster (to the apple). Because the random effect is centered at 0, the overall contribution describes the behavior of the average apple. Adding the random draw yields the apple-specific effect. This explains the terms population-average and subject-specific effects. The behavior of the average apple is described by the population-average model \\[\n\\text{E}[Y_{ij}] = \\beta_0 + \\beta_1 x_{ij}\n\\] The apple-specific behavior is described by the conditional model \\[\n\\text{E}[Y_{ij} | b_{0i}, b_{1i}] = (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) x_{ij}\n\\]\nWhat have we gained by introducing random effects \\(b_{0i}\\) and \\(b_{1i}\\) in Equation 30.2 compared to Equation 30.1?\n\nA very natural way of expressing variability as normal random variation about a fixed value.\nA more parsimonious way of expressing the variability in intercepts and slopes. Instead of 160 parameters in the mean function (80 \\(\\beta_{0i}\\)s and 80 \\(\\beta_{1i}\\)s) we have only four parameters: \\(\\beta_0, \\beta_1, \\text{Var}[b_{0i}] = \\sigma^2_0, \\text{Var}[b_{1i}] = \\sigma^2_1\\).\nThe ability to model the population average behavior and the cluster-specific behavior within the same model.\nA straightforward way to address questions such as “do the slopes vary between clusters”? This is simply a test of \\(H: \\sigma^2_1 = 0\\). If a random variable with mean 0 has variance 0, then it is the constant 0. If $= 0, then \\(\\beta_1 + b_{1i} = \\beta_1\\) and a common slope for all subjects is appropriate.\n\n\n\nDefinition\n\n\nDefinition: Mixed Model\n\n\nA mixed model is a statistical model that contains multiple sources of random variation in addition to systematic (=fixed) effects. The random sources of variation separate into those describing the variability of model components and the variability of the target given all other effects.\nEvery major family of statistical model features a mixed model version. There are, for example, linear mixed models, nonlinear mixed models, generalized linear mixed models, generalized additive mixed models, and so on.\n\n\nCounting the number of sources of random variation in the definition of a mixed model deserves some explanation. The issue arises because some models have a natural formulation in terms of additive errors. For example, the linear model for clustered data \\[\n\\textbf{Y}_i = \\textbf{X}_i\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_i\n\\] expresses the distributional properties of cluster target vector \\(\\textbf{Y}_i\\) through the distributional properties of the model errors \\(\\boldsymbol{\\epsilon}_i \\sim (\\textbf{0},\\textbf{R}_i)\\). This model contains one source of random variation, \\(\\boldsymbol{\\epsilon}_i\\). Adding a second random component, \\(\\textbf{Z}_i\\textbf{b}_i\\) where \\(\\textbf{b}_i \\sim (\\textbf{0},\\textbf{D})\\) is a random vector, creates a (linear) mixed model: \\[\n\\textbf{Y}_i = \\textbf{X}_i\\boldsymbol{\\beta}+ \\textbf{Z}_i \\textbf{b}_i + \\boldsymbol{\\epsilon}_i\n\\] This model now has two random sources of variation, \\(\\boldsymbol{\\epsilon}_i\\) and \\(\\textbf{b}_i\\). The distribution of \\(\\textbf{Y}_i\\) now can be interpreted in a population-average sense and in a cluster-specific sense: \\[\n\\begin{align*}\n\\textbf{Y}_i &\\sim (\\textbf{X}_i\\boldsymbol{\\beta}, \\textbf{Z}_i\\textbf{D}\\textbf{Z}_i^\\prime + \\textbf{R}_i) \\\\\n\\textbf{Y}_i | \\textbf{b}_i &\\sim (\\textbf{X}_i\\boldsymbol{\\beta}+ \\textbf{Z}_i\\textbf{b}_i, \\textbf{R}_i)\n\\end{align*}\n\\]\n\nIn a generalized linear model the errors do not have an additive formulation. Instead, the distributional properties of the target are specified directly. For example, suppose that \\(\\textbf{Y}_i\\) is a vector of (0,1) values that specify the disease state of an individual over time. For example, \\(Y_{i1} = 1\\) if subject \\(i\\) has the disease at the first observation occasion, and \\(Y_{ij} = 0\\) if the subject does not have the disease at the \\(j\\)th occasion. A generalized linear mixed model for this situation might specify \\[\n\\begin{align*}\n\\textbf{Y}_i | \\textbf{b}_i &\\sim \\text{Bernoulli}(\\boldsymbol{\\pi}_i) \\\\\n\\text{logit}(\\pi_{ij} | \\textbf{b}_i) &= \\textbf{x}_{ij}^\\prime \\boldsymbol{\\beta}+ \\textbf{z}_{ij}^\\prime\\textbf{b}_i \\\\\n\\end{align*}\n\\] The expression for the linear predictor contains one random variable, \\(\\textbf{b}_i\\). The second source of random variation is implied through assuming that \\(\\textbf{Y}_i\\) conditional on the random effects, has a Bernoulli distribution.\n\n\nPredicting Random Effects\nLet’s return to the model with random intercept and random slopes in the apple example, Equation 30.2. The conditional (cluster-specific) and marginal (population-average) interpretations are elegant. With an estimate of \\(\\beta_0\\) and \\(\\beta_1\\), we can predict the population-average trend of apples as \\[\n\\widehat{Y}_{ij} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{ij}\n\\] How do we compute a cluster-specific prediction for the \\(i\\)th apple when \\(b_{0i}\\) and \\(b_{1i}\\) are unobservable random variables? We cannot estimate random variables, but fortunately, we can predict them. Based on the observed data and the distributional assumptions, values \\(\\tilde{b}_{0i}\\) and \\(\\tilde{b}_{1i}\\) can be found that are in some sense “best” guesses for the unobservable random variables. With those values the cluster-specific predictions can be computed: \\[\n(\\widehat{Y}_{ij} | b_{0i}=\\tilde{b}_{0i}, \\,b_{1i}=\\tilde{b}_{1i}) = \\widehat{\\beta}_0 + \\tilde{b}_{0i} + (\\widehat{\\beta}_1 + \\tilde{b}_{1i}) x_{ij}\n\\] Predictors of the random effects can be motivated in a number of ways, for example, by maximizing the joint density of \\(\\boldsymbol{\\epsilon}\\) and \\(\\textbf{b}\\) (Henderson 1950, 1984) or by considering optimal unbiased predictors under squared-error loss (Harville 1976).\n\n\n\n\n\n\nPrediction of Random Variables\n\n\n\n\n\nWe have encountered the issue of estimating parameters versus predicting random variables before. When predicting the target value for a new observation in regression models, we have a choice of estimating \\(\\text{E}[Y | x_0]\\), the mean of the target at \\(x_0\\), or predicting \\(Y | x_0\\), the value of the random variable \\(Y\\) at \\(x_0\\). In discussing regression models, this is often presented as the difference between predicting the average at \\(x_0\\) or a single observation at \\(x_0\\).\nIn the classical linear model, the predicted values are the same for both, \\(\\widehat{y} = \\textbf{x}_0\\widehat{\\boldsymbol{\\beta}}\\). What differs is the precision of that quantity. If we look for unbiased estimators that minimize squared error loss, then the relevant loss for predicting the mean is \\(\\text{Var}[\\widehat{Y} - \\text{E}[Y|x_0]]\\) and for predicting \\(Y|x_0\\) it is \\(\\text{Var}[\\widehat{Y}-Y]\\). The former is the variance of \\(\\widehat{Y}\\), the latter is the variance of a difference (since \\(Y\\) is a random variable)\n\n\n\nSo the good news is that by modeling the variation in the model terms through random effects, rather than through many fixed effects, we can calculate the solutions for the random effects and predict a cluster-specific response. This has an interesting implication for forecasting new values. We can forecast an apple’s diameter at any time point \\(x_0\\), even if it falls beyond the measurement period: \\[\n(\\widehat{Y}_{ij} | b_{0i}=\\tilde{b}_{0i}, \\,b_{1i}=\\tilde{b}_{1i}, x=x_0) = \\widehat{\\beta}_0 + \\tilde{b}_{0i} + (\\widehat{\\beta}_1 + \\tilde{b}_{1i}) x_0\n\\] How would we predict the response of a new apple, one that did not participate in the study? Our best estimates of the intercept and slope for this new apple are the population-average estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\). Unless we have reason to believe the new apple is not represented by the training data, we predict its diameter as \\[\n\\widehat{Y}^* | x=x_0 = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_0\n\\] What values for \\(\\tilde{b}_0\\) and \\(\\tilde{b}_1\\) are best for this new apple? If diameter measurements are available, and the apple did not contribute to the analysis, we can compute the predictors of the random effects. If diameter measurements are not available—the more realistic case—the random effects are best predicted with their mean, which is zero. In other words, the population-average prediction is the best we can do for new observations.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Models for Longitudinal Data</span>"
    ]
  },
  {
    "objectID": "mixed.html#linear-mixed-models",
    "href": "mixed.html#linear-mixed-models",
    "title": "30  Mixed Models for Longitudinal Data",
    "section": "30.2 Linear Mixed Models",
    "text": "30.2 Linear Mixed Models\n\n\n\n\n\n\nNote\n\n\n\nBefore “LMM” became the acronym for Large Language Model, it was the acronym for the Linear Mixed Model. LMM in this chapter refers to the linear mixed model.\n\n\n\nSetup\nMixed models apply to many data structures. Longitudinal clustered data is a special situation where the data breaks into \\(k\\) clusters of size \\(n_1, \\cdots, n_k\\). Split-plot experimental designs also have a linear mixed model representation, for example. To cover the more general case of the linear mixed model we do not explicitly call out the clusters with index \\(i\\). This makes the notation a bit cleaner. If you are thinking about a clustered data structure, where \\(\\textbf{Y}_i\\) is the \\((n_i \\times 1)\\) vector of responses for cluster \\(i\\), \\(\\textbf{X}_i\\) is an associated matrix of inputs for the fixed effects, and \\(\\textbf{Z}_i\\) is the matrix of inputs for the random effects, we now stack and re-arrange these as follows: \\[\n\\textbf{Y}= \\left [\\begin{array}{c} \\textbf{Y}_1 \\\\ \\vdots \\\\ \\textbf{Y}_k \\end{array} \\right]\n\\quad\n\\textbf{X}= \\left [ \\begin{array}{c} \\textbf{X}_1 \\\\ \\vdots \\\\ \\textbf{X}_k \\end{array}\\right]\n\\quad\n\\textbf{b}= \\left [\\begin{array}{c} \\textbf{b}_1 \\\\ \\vdots \\\\ \\textbf{b}_k \\end{array} \\right]\n\\quad\n\\boldsymbol{\\epsilon}= \\left [\\begin{array}{c} \\boldsymbol{\\epsilon}_1 \\\\ \\vdots \\\\ \\boldsymbol{\\epsilon}_k \\end{array} \\right]\n\\] The overall input matrix for the random effects is a block-diagonal matrix \\[\n\\textbf{Z}= \\left[\n    \\begin{array}{cccc}\n        \\textbf{Z}_1 & \\textbf{0}& \\cdots & \\textbf{0}\\\\\n        \\textbf{0}& \\textbf{Z}_2  & \\cdots & \\textbf{0}\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\textbf{0}& \\textbf{0}& \\cdots & \\textbf{Z}_k\n    \\end{array}\n\\right]\n\\] and the covariance matrices of the random effects and the model errors also take on a block-diagonal structure:\n\\[\n\\text{Var}[\\textbf{b}] = \\textbf{G}= \\left[\n    \\begin{array}{cccc}\n        \\textbf{G}^* & \\textbf{0}& \\cdots & \\textbf{0}\\\\\n        \\textbf{0}& \\textbf{G}^* & \\cdots & \\textbf{0}\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\textbf{0}& \\textbf{0}& \\cdots & \\textbf{G}^*\n    \\end{array}\n\\right]\n\\qquad\n\\text{Var}[\\textbf{b}] = \\textbf{R}= \\left[\n    \\begin{array}{cccc}\n        \\textbf{R}_1 & \\textbf{0}& \\cdots & \\textbf{0}\\\\\n        \\textbf{0}& \\textbf{R}_2 & \\cdots & \\textbf{0}\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\textbf{0}& \\textbf{0}& \\cdots & \\textbf{R}_k\n    \\end{array}\n\\right]\n\\] The variance matrix \\(\\text{Var}[\\textbf{b}_i] = \\textbf{G}^*\\) is the same for all clusters. The variance matrix of the \\(\\boldsymbol{\\epsilon}_i\\) can differ from cluster to cluster, for example, when the observation times differ between clusters or in the presence of missing values.\nWith the re-arrangement from the clustered case out of the way, the general linear mixed model can be written as follows: \\[\n\\begin{align*}\n\\textbf{Y}&= \\textbf{X}\\boldsymbol{\\beta}+ \\textbf{Z}\\textbf{b}+ \\boldsymbol{\\epsilon}\\\\\n\\textbf{b}&\\sim G(\\textbf{0},\\textbf{G}) \\\\\n\\boldsymbol{\\epsilon}&\\sim G(\\textbf{0},\\textbf{R}) \\\\\n\\text{Cov}[\\textbf{b},\\boldsymbol{\\epsilon}] &= \\textbf{0}\n\\end{align*}\n\\tag{30.3}\\]\nThe assumption of Gaussian distributions for the random effects and the errors is common, it makes the subsequent analysis simple compared to any alternative specification. It should be noted that this implies a Gaussian distribution for \\(\\textbf{Y}\\) as well. Applying the results for the multivariate Gaussian (Section 3.7), the conditional and marginal (population-average) distribution of \\(\\textbf{Y}\\) is\n\\[\n\\begin{align*}\n\\textbf{Y}| \\textbf{b}&\\sim G(\\textbf{X}\\boldsymbol{\\beta}+ \\textbf{Z}\\textbf{b}, \\textbf{R}) \\\\\n\\textbf{Y}&\\sim G(\\textbf{X}\\boldsymbol{\\beta}, \\textbf{V}) \\\\\n\\textbf{V}&= \\textbf{Z}\\textbf{G}\\textbf{Z}^\\prime + \\textbf{R}\n\\end{align*}\n\\]\n\n\nEstimation and Prediction\nThe general linear mixed model Equation 30.3 contains parameters that need to be estimated and random variables that need to be predicted. The parameters are the fixed effects \\(\\boldsymbol{\\beta}\\) and the covariance parameters that define \\(\\textbf{G}\\) and \\(\\textbf{R}\\). In the longitudinal, clustered case, \\(\\textbf{R}\\) has an autocovariance structure and \\(\\textbf{G}\\) is often an unstructured matrix. For example, with a random intercept and a random slope, an unstructured \\(\\textbf{G}^*\\) takes on the form \\[\n\\textbf{G}^* = \\left[\\begin{array}{cc} \\sigma^2_0 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma^2_1 \\end{array}\\right]\n\\] \\(\\sigma^2_0\\) and \\(\\sigma^2_1\\) are the variances of the random intercept and slope, respectively, and \\(\\sigma_{01}\\) is their covariance. Many other covariance structures are possbile for \\(\\textbf{G}\\), including autocovariance structures when the random effects have a temporal or spatial structure.\nWe collect all covariance parameters into the vector \\(\\boldsymbol{\\theta}= [\\boldsymbol{\\theta}_G, \\boldsymbol{\\theta}_R]\\) and write the variance matrix of the data as a function of \\(\\boldsymbol{\\theta}\\): \\[\n\\text{Var}[\\textbf{Y}] = \\textbf{V}(\\boldsymbol{\\theta}) = \\textbf{Z}\\textbf{G}(\\boldsymbol{\\theta}_G)\\textbf{Z}^\\prime + \\textbf{R}(\\boldsymbol{\\theta}_R)\n\\]\n\nFixed effects and random effects\nThe mixed model equations are estimating equations for \\(\\boldsymbol{\\beta}\\) and \\(\\textbf{b}\\) given that we know the covariance parameters \\(\\boldsymbol{\\theta}\\). These equations gp back to [Henderson (1950); Henderson1984] who derived them in the context of animal breeding. Henderson’s thought process was as follows: to derive estimating equations similar to the normal equations in least-squares estimation, consider the joint distribution of \\([\\boldsymbol{\\epsilon}, \\textbf{b}]\\). We want to find the values \\(\\widehat{\\boldsymbol{\\beta}}\\) and \\(\\widehat{\\textbf{b}}\\) which maximize this distribution.Since \\(\\boldsymbol{\\epsilon}= \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}- \\textbf{Z}\\textbf{b}\\) and \\(\\textbf{b}\\) are jointly Gaussian distributed and because \\(\\text{Cov}[\\boldsymbol{\\epsilon},\\textbf{b}] = \\textbf{0}\\), maximizing their joint distribution for known \\(\\textbf{G}\\) and \\(\\textbf{R}\\) is equivalent to minimizing \\[\nQ = \\textbf{b}^\\prime\\textbf{G}^{-1}\\textbf{b}+ (\\textbf{y}- \\textbf{X}\\boldsymbol{\\beta}- \\textbf{Z}\\textbf{b})^\\prime\\textbf{R}^{-1}(\\textbf{y}- \\textbf{X}\\boldsymbol{\\beta}- \\textbf{Z}\\textbf{b})\n\\] \\(Q\\) is the sum of two quadratic forms, one in the random effects, the other in the model errors. Taking derivatives of \\(Q\\) with respect to \\(\\textbf{b}\\) and \\(\\boldsymbol{\\beta}\\), setting to zero and rearranging yields the mixed model equations:\n\\[\n\\left[\\begin{array}{cc} \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{X}& \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{X}& \\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{Z}+ \\textbf{G}^{-1}\\end{array}\\right]\n\\left[\\begin{array}{c} \\boldsymbol{\\beta}\\\\ \\textbf{b}\\end{array}\\right] =\n\\left[\\begin{array}{c} \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{y}\\\\ \\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{y}\\end{array}\\right]\n\\tag{30.4}\\]\nThe solutions to Equation 30.4 are \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\beta}} &= \\left(\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{y}\\\\\n\\widehat{\\textbf{b}} &= \\textbf{G}\\textbf{Z}^\\prime\\textbf{V}^{-1}(\\textbf{y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}})\n\\end{align*}\n\\] The estimator of the fixed effects has the form of a generalized least squares estimator. The predictor of the random effects is a linear combination of the fitted residuals. Both results make sense. The random effects measure how much the conditional means differ from the marginal (population-average) prediction.\n\n\n\n\n\n\nNote\n\n\n\nHenderson’s method is not maximum likelihood although it was initially referred to as “joint likelihood estimation”. It is simply an intuitive way to motivate estimating equations for \\(\\boldsymbol{\\beta}\\) and \\(\\textbf{b}\\). Harville (1976) established that the solution to the mixed model equations is a best linear unbiased predictor (BLUP) for \\(\\textbf{b}\\) and remains the BLUP if the generalized least squares estimator for \\(\\boldsymbol{\\beta}\\) is substituted.\n\n\nThe mixed model equations Equation 30.4 are fascinating for another reason. Suppose we choose to estimate the inputs in \\(\\textbf{Z}\\) not as random effects, but as fixed effects. That is, the model would be a linear model \\[\n\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}_x + \\textbf{Z}\\boldsymbol{\\beta}_z + \\boldsymbol{\\epsilon}, \\qquad \\boldsymbol{\\epsilon}\\sim (\\textbf{0},\\textbf{R})\n\\] The generalized least squares estimating equations for \\(\\boldsymbol{\\beta}= [\\boldsymbol{\\beta}_x, \\boldsymbol{\\beta}_z]\\) would be\n\\[\n\\left[\\begin{array}{cc} \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{X}& \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{X}& \\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{Z}\\end{array}\\right]\n\\left[\\begin{array}{c} \\boldsymbol{\\beta}_x \\\\ \\boldsymbol{\\beta}_z \\end{array}\\right] =\n\\left[\\begin{array}{c} \\textbf{X}^\\prime\\textbf{R}^{-1}\\textbf{y}\\\\ \\textbf{Z}^\\prime\\textbf{R}^{-1}\\textbf{y}\\end{array}\\right]\n\\] These equations are very similar to the mixed model equations, except for the term \\(+\\textbf{G}^{-1}\\) in the lower right cell of the first matrix. What does that mean? The inverse variance matrix \\(\\textbf{G}^{-1}\\) acts like a shrinkage factor in the prediction of \\(\\textbf{b}\\). If the variances of the random effects are small, the effect of \\(\\textbf{G}^{-1}\\) on the estimating equations is large and the \\(\\textbf{b}\\) will be shrunk toward zero compared to the fixed-effect estimates. On the contrary, when the variances of the random effects are large, there will be less shrinkage in the random effects solution.\n\n\nCovariance parameters\nCovariance parameters can be derived by a number of principles, once the parameter estimates are obtained you plug them into \\(\\textbf{G}\\) and \\(\\textbf{R}\\) and solve the mixed model equations. The most important estimation principles for the estimation of covariance parameters are maximum likelihood (ML) and restricted (residual) maximum likelihood (REML) based on the marginal distribution of \\(\\textbf{Y}\\), as for the general correlated error model discussed in Section 29.3.\nSince \\(\\textbf{Y}\\) is multivariate Gaussian, minus twice the negative log likelihood is \\[\n-2 \\ell(\\boldsymbol{\\theta},\\boldsymbol{\\beta};\\textbf{y}) = \\log |\\textbf{V}(\\boldsymbol{\\theta})| + (\\textbf{y}-\\textbf{X}\\boldsymbol{\\beta})^\\prime\\textbf{V}(\\boldsymbol{\\theta})^{-1}(\\textbf{y}-\\textbf{X}\\boldsymbol{\\beta}) + c\n\\tag{30.5}\\] where \\(c\\) is a constant that does not depend on the parameters. Given \\(\\boldsymbol{\\theta}\\), the minimum of \\(-2 \\ell(\\boldsymbol{\\theta},\\boldsymbol{\\beta};\\textbf{y})\\) with respect to \\(\\boldsymbol{\\beta}\\) has a closed-form solution in the GLS estimator \\(\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{y}\\). Plugging this estimator back into Equation 30.5 leads to a function in \\(\\boldsymbol{\\theta}\\) only. The ML estimates of \\(\\boldsymbol{\\theta}\\) are found by numerically minimizing this profiled log-likelihood.\nIn the case of REML, instead of the log-likelihood of \\(\\textbf{Y}\\) one maximizes the log-likelihood of \\(\\textbf{K}\\textbf{Y}\\) where \\(\\textbf{K}\\) is a matrix of error contrasts chosen such that \\(\\textbf{K}\\textbf{X}= \\textbf{0}\\). A common choice of \\(\\textbf{K}\\) leads to the following expression for minus twice the restricted log-likelihood \\[\n-2 \\ell(\\boldsymbol{\\theta};\\textbf{y}) = \\log |\\textbf{V}(\\boldsymbol{\\theta})| + \\log |\\textbf{X}^\\prime\\textbf{V}(\\boldsymbol{\\theta})^{-1}\\textbf{X}| + (\\textbf{y}-\\textbf{X}\\boldsymbol{\\beta})^\\prime\\textbf{V}(\\boldsymbol{\\theta})^{-1}(\\textbf{y}-\\textbf{X}\\boldsymbol{\\beta}) + c_R\n\\tag{30.6}\\]\nThis is the same objective function as Equation 30.5 except for the second log determinant term (and the inconsequential constant terms). Also notice that the REML log-likelihood is a function of only the covariance parameters. The fixed effects are not profiled from Equation 30.6. The fixed effects are removed by considering the special transformation \\(\\textbf{K}\\textbf{Y}\\).\nREML estimators of the covariance parameters are generally less biased than ML estimates, and are sometimes unbiased. They are preferred for this reason. However, the restricted log-likelihood in Equation 30.6 does not contain information about the fixed effects. You cannot use the restricted log-likelihood to perform likelihood ratio tests for hypotheses about the fixed effects.\n\n\n\nChoosing Random Effects\nWhen a mixed model arises in the context of an experimental design—such as a split-plot design—the assignment of fixed and random effects is dictated by the experimental design. In other situations, there is more latitude in choosing fixed and random effects.\n\n\nExample: Sleep Study.\n\n\nThe data for this example are part of the lme4 library in R and represent the average reaction time (in milliseconds) for subjects in a sleep deprivation study. The data are a subset of a larger study described in Belenky et al. (2003).\nFigure 30.1 shows the response profiles for the 18 subjects in the study. In general, the reaction times increase with increasing sleep deprivation. However, there is clearly substantial subject-to-subject variability in the profiles. Some subjects (#309, #310) do not respond to sleep deprivation very much compared to others (#308, #337, for example). The trend of reaction time versus duration of sleep deprivation could be linear for most subjects, quadratic for some.\nIf we assume a linear trend for all subjects, do the intercepts and slopes vary across subjects or only the intercepts or only the slopes?\n\nlibrary(lme4)\nlibrary(lattice)\nxyplot(Reaction ~ Days | Subject, \n       data=sleepstudy,\n       xlab=\"Days of Sleep Deprivation\",\n       ylab=\"Reaction Time (ms)\",\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =TRUE, \n                         strip.levels=TRUE,\n                         sep=\" \")\n       },\n       par.strip.text = list(cex = 0.8), \n       type=c(\"p\"),\n       layout=c(6,3,1),\n       as.table=TRUE\n       )\n\n\n\n\n\n\n\nFigure 30.1: Reaction times by subject in sleep deprivation study\n\n\n\n\n\n\n\nAs a general rule, if a coefficient of a linear model appears in the random part of the model it should be included in the fixed effects part. Otherwise it might not be reasonable to assume that the random effects have a zero mean. In the sleep deprivation study, this means a model such as \\[\nY_{ij} = \\beta_0 + b_{1i} x_{ij} + \\epsilon_{ij}\n\\] with a random slope but no fixed-effect slope is not appropriate. In order for the \\(b_{1i}\\) to satisfy a zero-mean assumption, there needs an overall (population-average) slope in the model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1 x_{ij} + b_{1i} x_{ij} + \\epsilon_{ij}\n\\] In other words, the columns in \\(\\textbf{Z}\\) also appear in \\(\\textbf{X}\\). The reverse is not necessarily so. Not varying the intercepts or slopes by subject is perfectly acceptable.\n\n\nLinear Mixed Models in R\nLinear mixed models cam be fit in R with the lme4 and nlme packages. nlme fits LMMs with the lme function and nonlinear mixed models with the nlme() function by ML or REML. lme4 fits LMMs with the lmer() function by ML or REML, generalized linear models with the glmer() function by ML, and nonlinear mixed models with the nlmer() function by ML.\n\n\nExample: Sleep Study (Cont’d)\n\n\nWe use lme for the analysis of the sleep study data because it allows us to specify random effects and a correlated within-subject error structure. The model we have in mind is \\[\n\\begin{align*}\nY_{ij} &= \\beta_0 + \\beta_1 d_{j} + b_{0i} + b_{1i}d_j + \\epsilon_{ij} \\\\\n\\textbf{G}^* &= \\left [\\begin{array}{cc}\\sigma^2_0 & \\sigma_{01} \\\\ \\sigma_{01} & \\sigma^2_1 \\end{array}\\right] \\\\\n\\text{Cov}[\\epsilon_{ij},\\epsilon_{ik}] &= \\sigma^2 \\rho^{|d_j - d_k|}\n\\end{align*}\n\\] where \\(Y_{ij}\\) is the sleep deprivation of subject \\(i\\) on day \\(d_j\\) (\\(i=1,\\cdots,18\\)). This is a linear mixed model with random intercept and slopes and correlated errors. The autocovariance structure is that of a continuous AR(1) process. There are a total of seven parameters in this model:\n\nthe population-average intercept \\(\\beta_0\\) and the population-average slope \\(\\beta_1\\)\nthe variances of the random effects, \\(\\sigma^2_0\\) and \\(\\sigma^2_1\\) and their covariance \\(\\sigma_{01}\\)\nrhe variance of the errors, \\(\\sigma^2\\)\nthe autocorrelation \\(\\phi\\)\n\nThe following statements fit this model by REML (the default) using lme.\nThe fixed= argument specifies the fixed effects of the model, an intercept is included automatically. The random= argument specifies the random effects and the clusters after a vertical slash. The intercept is not automatically included in the random effect specification. The covariance structure is specified with the correlation= argument using the same syntax as the correlated error model in Section 29.3.3. The control= argument changes the numerical optimizer from the default to a general purpose algorithm.\n\nlibrary(nlme)\nlmixcorr &lt;- lme(fixed = Reaction ~ Days,\n                random = ~ 1 + Days | Subject,\n                data=sleepstudy, \n                correlation=corAR1(form = ~Days | Subject),\n                control=lmeControl(opt=\"optim\"),\n                method=\"REML\")\n\nsummary(lmixcorr)\n\nLinear mixed-effects model fit by REML\n  Data: sleepstudy \n       AIC     BIC    logLik\n  1738.187 1760.46 -862.0935\n\nRandom effects:\n Formula: ~1 + Days | Subject\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr  \n(Intercept) 14.879196 (Intr)\nDays         4.759861 0.897 \nResidual    30.500694       \n\nCorrelation Structure: AR(1)\n Formula: ~Days | Subject \n Parameter estimate(s):\n      Phi \n0.4870368 \nFixed effects:  Reaction ~ Days \n                Value Std.Error  DF  t-value p-value\n(Intercept) 252.24315  6.851137 161 36.81770       0\nDays         10.46701  1.532133 161  6.83166       0\n Correlation: \n     (Intr)\nDays -0.131\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.18606983 -0.63340364  0.06086905  0.49755434  4.78980880 \n\nNumber of Observations: 180\nNumber of Groups: 18 \n\n\nThe estimates of the fixed effects are \\(\\widehat{\\beta}_0\\) = 252.2432 and \\(\\widehat{\\beta}_1\\) = 10.467.\nThe estimates of the covariance parameters are \\(\\widehat{\\sigma}^2_0\\) = 221.3905, \\(\\widehat{\\sigma}^2_1\\) = 22.6563, and \\(\\widehat{\\sigma}_{01}\\) = 63.509. The lme output reports the square roots of the variances (Std Dev) and reports the correlation between \\(b_{0i}\\) and \\(b_{1i}\\) rather than the covariance.\nThe estimate of the autocorrelation parameter is \\(\\widehat{\\phi}\\) = 0.487.\n\nThe model with random effects and autocorrelated errors has a restricted log-likelihood of -862.0935. We can test if the addition of the correlation process significantly improves the model by comparing this model against one without the correlation structure. If we fit both models by REML, this is a valid likelihood ratio test of \\(H: \\phi = 0\\).\n\nlmix &lt;- lme(fixed = Reaction ~ Days,\n            random = ~ 1 + Days | Subject,\n            data=sleepstudy, \n            control=lmeControl(opt=\"optim\"),\n            method=\"REML\")\n\na &lt;- anova(lmix,lmixcorr)\na\n\n         Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nlmix         1  6 1755.628 1774.719 -871.8141                        \nlmixcorr     2  7 1738.187 1760.459 -862.0935 1 vs 2 19.44123  &lt;.0001\n\n\nThe model without correlated errors has a restricted log likelihood of -871.8141. Twice the difference of the log-likelihoods is the test statistic for the LRT, 19.4412. There is one restriction on the full model, \\(\\phi=0\\), hence this is a 1-degree of freedom test. The \\(p\\)-value is very small, we reject the hypothesis that \\(\\phi = 0\\). The correlated error structure is significant, we continue our investigation with the lmixcorr model.\n\nThe fixed and random effects can be extracted with the respective access functions:\n\nfixed.effects(lmixcorr)\n\n(Intercept)        Days \n  252.24315    10.46701 \n\nrefs &lt;- random.effects(lmixcorr)\nround(refs,5)\n\n    (Intercept)     Days\n308    17.57941  6.31918\n309   -27.85018 -9.02972\n310   -22.97529 -7.23899\n330     0.87286 -0.39008\n331     3.12671  0.60686\n332    -1.75398 -0.87031\n333     5.14211  1.39114\n334     0.56455  0.33448\n335   -19.38623 -6.87002\n337    26.38532  8.50471\n349    -5.63593 -1.42460\n350     6.91575  2.69492\n351    -3.08503 -1.12210\n352    11.69349  3.92972\n369     2.89212  0.90771\n370    -0.06864  0.60010\n371    -0.28057 -0.13069\n372     5.86352  1.78769\n\n\nThe random effects are organized as a matrix, the columns correspond to the intercept and slope, the rows correspond to subjects. To predict the response for the average study participant, we use intercept and slope estimates \\(\\widehat{\\beta}_0\\) = 252.2432 and \\(\\widehat{\\beta}_1\\) = 10.467.\nTo predict the sleep deprivation of subject #333, we use the subject-specific intercept and slope \\(\\widehat{\\beta}_0 + b_{0,333}\\) = 252.2432 + 5.1421 = 257.3853, \\(\\widehat{\\beta}_1 + b_{1,333}\\) = 10.467 + 1.3911 = 253.6343.\n\nWith increasing time of sleep deprivation, which subject has the smallest increase in reaction time and which subject has the largest increase?\n\nTo answer this question we can look at the min and max of the random slope predictors:\n\nmin(random.effects(lmixcorr)[,2])\n\n[1] -9.029723\n\nwhich.min(random.effects(lmixcorr)[,2]) # subject 309\n\n[1] 2\n\nmax(random.effects(lmixcorr)[,2])\n\n[1] 8.504712\n\nwhich.max(random.effects(lmixcorr)[,2]) # subject 337\n\n[1] 10\n\n\nSubjects 330, 334, 370, and 371 are similar to the population average. They have the smallest subject-specific intercept and slope adjustment.\nThis is also confirmed in Figure 30.2 below. The subject-specific and population-average predictions for these four subjects overlay closely.\n\nWhat is the predicted reaction time on day 4 for an individual who did not participate in the study?\n\nTo answer this question we have to rely on the population average effects since we do not have a solution for the random effects predictor.\n\npred &lt;- fixed.effects(lmixcorr)[1] + 4*fixed.effects(lmixcorr)[2]\ncat (\"Predicted reaction time of a new subject at day 4 = \", pred)\n\nPredicted reaction time of a new subject at day 4 =  294.1112\n\n\nYou can also use the predict() function but when you do that for a subject that is not in the study you get a NA. To get around that and obtain the population-average predictions, specify the level= option of predict(). Check the doc on predict.lme() for details. If you specify level=0 it does not matter what you pass for the Subject variable. You can even leave it out of the data frame.\n\n# Subject in the study, population-average predictions\ndata &lt;- data.frame(\"Days\"=4,\"Subject\"=308)\npredict(lmixcorr,newdata=data,level=0)\n\n[1] 294.1112\nattr(,\"label\")\n[1] \"Predicted values\"\n\n# Subject not in the study, population-average predictions\ndata &lt;- data.frame(\"Days\"=4,\"Subject\"=1000)\npredict(lmixcorr,newdata=data,level=0)\n\n[1] 294.1112\nattr(,\"label\")\n[1] \"Predicted values\"\n\n# No subject specified, population-average predictions\ndata &lt;- data.frame(\"Days\"=4) #,\"Subject\"=1000)\npredict(lmixcorr,newdata=data,level=0)\n\n[1] 294.1112\nattr(,\"label\")\n[1] \"Predicted values\"\n\n\nTo obtain population-average and subject-specific predictions for all subjects specify a vector of level values\n\npredvals &lt;- predict(lmixcorr,level=c(0:1),na.action=na.omit)\npredvals[1:20,] #showing only the first 20 of 180 records\n\n   Subject predict.fixed predict.Subject\n1      308      252.2432        269.8226\n2      308      262.7102        286.6088\n3      308      273.1772        303.3950\n4      308      283.6442        320.1812\n5      308      294.1112        336.9673\n6      308      304.5782        353.7535\n7      308      315.0452        370.5397\n8      308      325.5122        387.3259\n9      308      335.9792        404.1121\n10     308      346.4463        420.8983\n11     309      252.2432        224.3930\n12     309      262.7102        225.8303\n13     309      273.1772        227.2676\n14     309      283.6442        228.7048\n15     309      294.1112        230.1421\n16     309      304.5782        231.5794\n17     309      315.0452        233.0167\n18     309      325.5122        234.4540\n19     309      335.9792        235.8913\n20     309      346.4463        237.3286\n\n\n\nFigure 30.2 displays the population-average and subject-specific predictions of the reaction times for all 18 subjects. The population-average trend (blue) is the same in all panels. The subject-specific trends vary from panel to panel. Subject 330 behaves very much like the population average. Subject 337 has a very different slope, their reaction time increases much more quickly than that of the average subject. The linear mixed model with autocorrelated error captures a remarkable degree of variability, based on only seven parameters.\n\nxyplot(Reaction ~ Days | Subject, \n       data=sleepstudy,\n       xlab=\"Days of sleep deprivation\",\n       ylab=\"Reaction time (ms)\",\n       type=c(\"p\"),\n       as.table=TRUE,\n       layout=c(6,3,1),\n       panel=function(x,y,...) {\n           grpname &lt;- dimnames(trellis.last.object())[[1]][packet.number()]\n           panel.xyplot(x,y,...)\n           panel.lines(x,predvals$predict.Sub[predvals$Sub == grpname],\n                       col=\"red\",lwd=1.5)\n           panel.lines(x,predvals$predict.fixed[predvals$Sub == grpname],\n                       col=\"blue\",lwd=1.5)\n           },\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =TRUE, \n                         strip.levels=TRUE,\n                         sep=\" \")\n       },\n       par.strip.text = list(cex = 0.8), \n       ) \n\n\n\n\n\n\n\nFigure 30.2: Population-average (blue) and subject-specific (red) predictions in sleep study.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Models for Longitudinal Data</span>"
    ]
  },
  {
    "objectID": "mixed.html#sec-glmm",
    "href": "mixed.html#sec-glmm",
    "title": "30  Mixed Models for Longitudinal Data",
    "section": "30.3 Generalized Linear Mixed Models (GLMM)",
    "text": "30.3 Generalized Linear Mixed Models (GLMM)\n\nIntroduction\nThe transition from the classical linear model to generalized linear models (GLM) in Chapter 27 added to the linear model\n\nthat the distribution of \\(Y\\) is a member of the exponential family of distributions\nan invertible link function \\(g(\\mu)\\) that maps between the scale of the data (the scale of the response) and the input variables.\n\nCommon to both model families is to express the effect of the input variables through a linear predictor \\[\n\\eta = \\textbf{x}^\\prime \\boldsymbol{\\beta}\n\\] A GLM, written in vector form, can thus be summarized as follows: \\[\n\\begin{align*}\nY       &\\sim P_{expo} \\\\\n\\text{E}[Y] &= \\mu(\\boldsymbol{\\beta}) \\\\\ng(\\mu)  &= \\eta \\\\\n\\eta    &= \\textbf{x}^\\prime\\boldsymbol{\\beta}\n\\end{align*}\n\\]\nThe extension to a mixed-model version, the generalized linear mixed model (GLMM) seems obvious: extend the linear predictor with random variables \\(\\textbf{b}\\): \\[\n\\begin{align*}\nY | \\textbf{b}&\\sim P_{expo} \\\\\ng(\\mu)  &= \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\textbf{z}^\\prime \\textbf{b}\\\\\n\\eta    &= \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\textbf{z}^\\prime \\textbf{b}\n\\end{align*}\n\\] where \\(\\textbf{b}\\) follows some distribution, a \\(G(\\textbf{0},\\textbf{G})\\), say. Similar to the GLM and the LMM, we then estimate the parameters based on maximizing the log likelihood of the marginal distribution of \\(Y\\).\nThis works well on paper!\nCompared to the linear mixed model, where \\(\\textbf{b}\\) and \\(Y\\) followed a Gaussian distribution and deriving the marginal distribution of \\(Y\\) is straightforward, the same is not true for the GLMM family of models. Here are some of the complications we run into:\n\nThe marginal distribution of \\(Y\\) is obtained by integrating over the distribution of the random effects, \\[\np(y) = \\int \\cdots \\int p(y|\\textbf{b}) p(\\textbf{b}) \\, d\\textbf{b}\n\\] \\(p(y)\\) does not generally have a closed form (as in the Gaussian-Gaussian case) and this multidimensional integral must be computed by numerical methods.\nA valid joint distribution of the data might not actually exist. Gilliland and Schabenberger (2001) show, for example, that if \\(\\textbf{Y}\\) is an \\((n\\times 1)\\) vector of equicorrelated binary variables with common success probability \\(\\pi\\), then the lower bound of the correlation parameter depends on \\(n\\) and \\(\\pi\\). In other words, if you obtain an estimate of the correlation parameter that falls below that lower bound, there is no probability model that could have possibly generated the data.\nThe incorporation of correlated errors with or without the presence of random effects in the model is questionable when data do not follow a Gaussian distribution. Achieving desired properties of the marginal distribution and the conditional distribution might not be possible if the data are non-Gaussian.\nBecause the marginal distribution is difficult to come by, population-average inference is difficult in GLMM; we do not know the functional form of the marginal mean. The obvious attempt to simply evaluate the inverse link function at the mean of the random effects, \\[\ng^{-1}(\\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\textbf{z}^\\prime\\text{E}[\\textbf{b}]) = g^{-1}(\\textbf{x}^\\prime\\boldsymbol{\\beta}+ 0) = g^{-1}(\\textbf{x}^\\prime\\boldsymbol{\\beta})\n\\] does not yield the estimate of the marginal mean. Predictions in GLMMs thus focus on the conditional mean, \\(\\mu|\\textbf{b}= g^{-1}(\\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\textbf{z}^\\prime\\textbf{b})\\). In longitudinal studies, this is the cluster-specific mean.\n\nIn short, training generalized linear mixed models is much more complicated than working with their linear counterpart, and we typically have to restrict the types of models under consideration somehow.\n\n\nEstimation Approaches\nThe approaches to estimating parameters in GLMMs fall broadly into two categories:\n\nLinearization methods approximate the model based on Taylor series. Similar to fitting a nonlinear or generalized linear model as a series of approximate linear models based on a vector of pseudo data, these methods approximate the GLMM through a series of LMMs. The advantage of the linearization methods is that the approximate LMM can accommodate complex random effects structures and correlated errors. There is no limit to the types of covariance structure you can specify with this approach. The disadvantages of linearization are the absence of a true objective function, the log likelihood of \\(Y\\) is never calculated, the estimates are not maximum likelihood estimates, and they are often biased.\nIntegral approximation methods compute the log likelihood using numerical techniques such as Laplace approximation, Gauss-Hermite quadrature, Monte Carlo integration and other methods. The advantage of this approach is that the log likelihood of \\(Y\\) is being computed and maximum likelihood estimates are obtained. The disadvantage is the computational demand, increasing quickly with the number of random effects and the number of abscissas in the integral approximation. Integral approximations are thus used for relatively simple models that have only few random effects.\n\nThe Laplace and Gauss-Hermite quadrature integral approximations are related. The former is a Gauss-Hermite quadrature with a single quadrature node. The marginal joint distribution of the data in a mixed model is \\[\n\\begin{align*}\np(\\textbf{y}) &= \\int \\cdots \\int p(\\textbf{y}| \\textbf{b}, \\boldsymbol{\\beta}) p(\\textbf{b}| \\boldsymbol{\\theta}) \\, d\\textbf{b}\\\\\n&= \\int \\cdots \\int \\exp\\{c f(\\textbf{y},\\boldsymbol{\\beta},\\boldsymbol{\\theta},\\textbf{b}\\} \\, d\\textbf{b}\n\\end{align*}\n\\] for some function \\(f()\\) and constant \\(c\\). If \\(c\\) is large, the Laplace approximation of \\(p(\\textbf{y})\\) is \\[\n\\left(\\frac{2\\pi}{c} \\right)^{n_b/2} |-f^{\\prime\\prime}(\\textbf{y}, \\boldsymbol{\\beta},\\boldsymbol{\\theta},\\tilde{\\textbf{b}})|^{-1/2}\\exp\\left\\{cf(\\textbf{y},\\boldsymbol{\\beta},\\boldsymbol{\\theta},\\textbf{b})\\right\\}\n\\] where \\(f^{\\prime\\prime}\\) is the second derivative of \\(f\\) evaluated at \\(\\tilde{\\textbf{b}}\\) at which \\(f\\) has a minimum. This looks like a messy expression and requires a sub-optimization to find \\(\\tilde{\\textbf{b}}\\) such that \\(f^\\prime(\\textbf{y},\\boldsymbol{\\beta},\\boldsymbol{\\theta},\\tilde{\\textbf{b}})=0\\), but it is simpler than computing the integral by other means. The Laplace approximation is thus a common method to solving the maximum likelihood problem in GLMMs.\nWith longitudinal data, the quality of the Laplace approximation increases with the number of clusters and the number of observations within a cluster.\n\n\nGLMM in R\nThe glmer function in the lme4 package fits generalized linear mixed models by adaptive gaussian quadrature and Laplace approximation for relatively simple random effects structures.\n\n\nExample: Contagious Bovine Pleuropneumonia\n\n\nThe data set cbpp comes with the lme4 library and contains serological incidences of contagious bovine pleuropneumonia (CBPP) in 15 commercial cattle herds in Africa. Blood samples were collected quarterly from the animals of each herd. The data were used to compute the new cases that occurred during the time period.\n\nlibrary(lme4)\ncbpp\n\n   herd incidence size period\n1     1         2   14      1\n2     1         3   12      2\n3     1         4    9      3\n4     1         0    5      4\n5     2         3   22      1\n6     2         1   18      2\n7     2         1   21      3\n8     3         8   22      1\n9     3         2   16      2\n10    3         0   16      3\n11    3         2   20      4\n12    4         2   10      1\n13    4         0   10      2\n14    4         2    9      3\n15    4         0    6      4\n16    5         5   18      1\n17    5         0   25      2\n18    5         0   24      3\n19    5         1    4      4\n20    6         3   17      1\n21    6         0   17      2\n22    6         0   18      3\n23    6         1   20      4\n24    7         8   16      1\n25    7         1   10      2\n26    7         3    9      3\n27    7         0    5      4\n28    8        12   34      1\n29    9         2    9      1\n30    9         0    6      2\n31    9         0    8      3\n32    9         0    6      4\n33   10         1   22      1\n34   10         1   22      2\n35   10         0   18      3\n36   10         2   22      4\n37   11         0   25      1\n38   11         5   27      2\n39   11         3   22      3\n40   11         1   22      4\n41   12         2   10      1\n42   12         1    8      2\n43   12         0    6      3\n44   12         0    5      4\n45   13         1   21      1\n46   13         2   24      2\n47   13         0   19      3\n48   13         0   23      4\n49   14        11   19      1\n50   14         0    2      2\n51   14         0    3      3\n52   14         0    2      4\n53   15         1   19      1\n54   15         1   15      2\n55   15         1   15      3\n56   15         0   15      4\n\n\nThere are 56 observations for the 15 herds. The outcome of interest is the number of incidences in the herd, a Binomial random variable. Figure 30.3 shows a trellis plot of the incidence proportions over time for each herd. \n\nlibrary(lattice)\nxyplot(incidence/size ~ period | herd, \n       data=cbpp, \n       ylab=\"CBPP Incidences/Size of herd\",\n       xlab=\"Period\",\n       type=c('g','p','l'),\n       layout=c(3,5), \n       as.table=TRUE,\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =TRUE, \n                         strip.levels=TRUE,\n                         sep=\" \")\n       },\n       par.strip.text = list(cex = 0.8), \n       )\n\n\n\n\n\n\n\nFigure 30.3: Proportion of CBPP incidences over time by herd. Herds are ordered by the max incidence.\n\n\n\n\n\nThe following code fits a Binomial GLMM with a linear fixed-effects trend in measurement period and a random intercept for each herd.\n\nglmm_bin1 &lt;- glmer(cbind(incidence, size-incidence) ~ as.numeric(period) + (1 | herd),\n                   data = cbpp, \n                   family = binomial)\n\nsummary(glmm_bin1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(incidence, size - incidence) ~ as.numeric(period) + (1 |  \n    herd)\n   Data: cbpp\n\n     AIC      BIC   logLik deviance df.resid \n   192.6    198.6    -93.3    186.6       53 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2566 -0.7991 -0.4361  0.5804  3.0664 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n herd   (Intercept) 0.4402   0.6635  \nNumber of obs: 56, groups:  herd, 15\n\nFixed effects:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -0.9323     0.3017  -3.090    0.002 ** \nas.numeric(period)  -0.5592     0.1219  -4.589 4.46e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nas.nmrc(pr) -0.721\n\n\nThe estimates of the fixed effects are \\(\\widehat{\\boldsymbol{\\beta}}\\) = [-0.9323, -0.5592]. The variance of the herd-specific random intercept is \\(\\text{Var}[b_0]\\) = 0.4402.\nThe sum of the fixed and random coefficients for each level of the subject variable can be accessed with the coef function:\n\ncoef(glmm_bin1)$herd\n\n   (Intercept) as.numeric(period)\n1  -0.34835215         -0.5591817\n2  -1.24765551         -0.5591817\n3  -0.51230864         -0.5591817\n4  -0.90306342         -0.5591817\n5  -1.15990160         -0.5591817\n6  -1.34165149         -0.5591817\n7  -0.03187951         -0.5591817\n8  -0.25253439         -0.5591817\n9  -1.16998259         -0.5591817\n10 -1.48970916         -0.5591817\n11 -1.03196921         -0.5591817\n12 -1.00348124         -0.5591817\n13 -1.64774468         -0.5591817\n14  0.10556969         -0.5591817\n15 -1.47191143         -0.5591817\n\n\nAll herds have the same slope coefficient, -0.5592. The intercept varies from herd to herd.\n\nIs it reasonable to add a random slope in addition to the random intercept? We can test this with a likelihood ratio test (LRT):\n\nglmm_bin2 &lt;- glmer(cbind(incidence, size-incidence) ~ as.numeric(period) + \n                         (1 + as.numeric(period) | herd) ,\n                   data = cbpp, \n                   family = binomial)\n\na &lt;- anova(glmm_bin1,glmm_bin2,type=\"LRT\")\na\n\nData: cbpp\nModels:\nglmm_bin1: cbind(incidence, size - incidence) ~ as.numeric(period) + (1 | herd)\nglmm_bin2: cbind(incidence, size - incidence) ~ as.numeric(period) + (1 + as.numeric(period) | herd)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nglmm_bin1    3 192.57 198.65 -93.285   186.57                     \nglmm_bin2    5 194.18 204.30 -92.088   184.18 2.3946  2      0.302\n\n\nThe model with a random slope adds two parameters, the variance of the random slope, \\(\\sigma^2_1\\) and the covariance between random intercept and random slope,\\(\\sigma_{01}\\). The LRT has a test statistic of 2.3946 with \\(p\\)-value of 0.302. The addition of the random slope does not significantly improve the model; we stick with the random intercept model.\nFigure 30.4 shows the predicted probabilities along with the observed incidence ratios for each herd. These predictions on the scale of the data (the probability scale) are obtained with\n\npred_re &lt;- predict(glmm_bin1,re.form=NULL,type=\"response\")\npred_fe &lt;- predict(glmm_bin1,re.form=NA  ,type=\"response\")\n\nre.form=NULL adds all random effects in the linear predictor, re.form=NA sets the random effects to zero.\nThe blue line is the “population average” which is the predicted value on the data scale obtained by setting the random effects to zero. This is not an unbiased estimate of the incidence probability of the average herd. The linear trend in observation period on the logit scale maps to a nonlinear trend on the probability scale.\nNotice that predicted values can be computed for all periods, even if a herd was measured during only some periods. For example, herd #8 was observed only once, but a subject-specific prediction is possible for all periods because a subject-specific intercept \\(\\widehat{b}_{0(8)}\\) is available.\n\n\n\n\n\n\n\n\nFigure 30.4: “Population-average” (blue) and subject-specific (red) predictions in CBPP study.\n\n\n\n\n\n\n\n\n\n\nFigure 30.1: Reaction times by subject in sleep deprivation study\nFigure 30.2: Population-average (blue) and subject-specific (red) predictions in sleep study.\nFigure 30.3: Proportion of CBPP incidences over time by herd. Herds are ordered by the max incidence.\nFigure 30.4: “Population-average” (blue) and subject-specific (red) predictions in CBPP study.\n\n\n\nBelenky, Gregory, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo, and Thomas J. Balkin. 2003. “Patterns of Performance Degradation and Restoration During Sleep Restriction and Subsequent Recovery: A Sleep Dose-Response Study.” Journal of Sleep Research 12: 1--12.\n\n\nGilliland, D., and O. Schabenberger. 2001. “Limits on Pairwise Association for Equi-Correlated Binary Variables.” Journal of Applied Statistical Sciences 10: 279--285.\n\n\nHarville, D. A. 1976. “Extension of the Gauss-Markov Theorem to Include the Estimation of Random Effects.” The Annals of Statistics 4: 384–95.\n\n\nHenderson, C. R. 1950. “The Estimation of Genetic Parameters.” The Annals of Mathematical Statistics 21 (2): 309–10.\n\n\n———. 1984. Applications of Linear Models in Animal Breeding. University of Guelph.",
    "crumbs": [
      "Part VII. Supervised Learning III: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Models for Longitudinal Data</span>"
    ]
  },
  {
    "objectID": "ann.html",
    "href": "ann.html",
    "title": "31  Artificial Neural Networks",
    "section": "",
    "text": "31.1 Introduction",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "ann.html#sec-ann-intro",
    "href": "ann.html#sec-ann-intro",
    "title": "31  Artificial Neural Networks",
    "section": "",
    "text": "A Look Back\nFew analytic methods have occupied conversations in recent years as neural networks. They seem at times inscrutable and complex, and mysteriously powerful. Yet neural networks, or better artificial neural networks (ANN), had their ups and downs since the perceptron was described by McCulloch and Pitts (1943).\nOnce, ANNs were thought to hold the key to artificial intelligence (AI) and that within a short period of time machines based on neural networks would be capable to perform any task. These exuberant expectations could not be met. From the peak of inflated expectations the field of AI slumped into the trough of disillusionment and the first AI winter had set in. It was not fashionable in the 1980s or 1990s to say you were working on artificial intelligence. Neural networks were just another supervised learning approach to predict or classify a target variable. Compare the situation to today, when it is not fashionable to say you are working on anything but AI.\nThen, around 2010, everything changed—again. Classification models built on specialized neural networks improved greatly in performance. In 2012, AlexNet, a convolutional neural network (CNN, a special type of artificial neural network for image data), had the best top-5 error rate in the annual ImageNet competition, beating out human interpreters for the first time. The task in the ImageNet competition is to recognize the objects on images and correctly associate them with 1,000 categories.\nThis was a watershed moment in the history of artificial intelligence. Algorithms outperforming their human masters is not too surprising to us. After all, we build algorithms of all kinds for exactly that purpose: to perform a task repeatedly and reliably at a scale beyond human capacity. Who would want to trade a calculator for doing arithmetic themselves? The watershed moment occurred because an ANN bested human interpreters in a task that many previously thought was squarely in the human domain: sensing the world.\nUp until now, computerized systems and algorithms were great at solving problems that are difficult for humans but easy for computers. Those are problems that can be described by formal rules and logic. Performing mathematical operations and capturing the tax code in software are examples.\nThe CNN that won ImageNet solved a problem that is easy for humans but was so far difficult for computers: interpreting the natural world. We can look at the world and immediately recognize the objects in our field of vision: a tree here, a person there–we might know their name–, a flower pot on top of a table. This task requires a large amount of knowledge about the world, yet it is something we can do intuitively. How was it possible that now an algorithm trained on data had become better at this task than us?\nHad it really?\nAlexNet had not become more knowledgeable about the world. The ANN was not capable of synthesizing information and concepts and deriving the nature of objects from it. It was not intelligent. The ANN was trained to be a pattern matching machine. Using many labeled training examples, where the objects on an image had been identified by humans for the benefit of the algorithm, the network learned to associate labels with certain patterns in the image pixels. When presented with a new image, the algorithm can then map the presented pixels against the learned associations and predict with surprising accuracy which objects it “sees” on the image.\nThe pattern matching that occurs in our brains intuitively based on colors, shapes, edges, movement, context, etc., had been replaced with a crude pattern matching based on pixels. Exchange the pixels with some other means of encoding the visual information, and the neural network would fail miserably, unless it is retrained on the new type of data.\nStill, training an algorithm to perform a complex task as image recognition so well was remarkable. Other examples of neural network-based algorithms moving into the domain of easy-for-human specialties came from the fields of robotics, autonomous operations (driving), natural language understanding (reading, writing, summarization, interpretation), etc. That begged the questions\n\nHow was this possible?\nWhy now? Neural networks had been around since the 1940s? Why did it take another 6 decades to improve their performance. Regression has been around for a long time as well and it is not as if we are suddenly experiencing a massive “regression revolution”.\n\n\n\nWhat Changed?\nSeveral major developments came together to raise the performance and profile of neural networks.\n\nThe availability of large training data sets\nThe availability of computing resources\nSpecialized network architectures\nBetter algorithms to train neural networks with greater depth\nDedicated software packages for deep learning\n\nThe performance of neural networks to this point had been limited by the size of the networks and the computational resources required to train them. While a large statistical model might have a thousand parameters, this would constitute a pretty small neural network by today’s standards. Networks consist of layer and those with more layers, so-called deep neural networks, tend to abstract concepts better than shallow networks. This increases the number of parameters dramatically, however. Millions of parameters is not uncommon in deep neural networks. GPT-1, released in June 2018 had 117 million parameters. GPT-2, released in February 2019 had 1.5 billion parameters. GPT-3.5, released in November 2022 featured 175 billion parameters.\nThe internet facilitated the collection (vacuuming) of data. In object classification, acceptable performance can be achieved with about 5,000 training observations for each category to be classified. If you want an algorithm to be good at identifying cats, you need 5,000 pictures of cats. To achieve human-level performance in categorization you need about 10 million observations. The massive data sets needed to train deep neural networks well are now available. The rise of the large language models (LLMs) such as GPT would probably not have been possible without the internet as the training data.\nEstimating such a large number of parameters is a formidable problem, made more complex by the nonlinear nature of neural networks and their over-parameterization. A single computer limits the size of networks that can be trained. Distributed computing platforms in which relatively cheap compute servers are stitched together into large high-performance computing arrays provided the necessary horsepower to train large networks. Cloud computing made these computing resources widely available to anyone with a credit card.\nMaybe more importantly, the use of GPUs (graphical processing units) instead of CPUs changed the computational equations. It turns out that much of the math in training neural networks is similar to the type of vector operations that occur in processing images. The GPU chips that accelerated graphics operations have become the dominating chip architecture for AI models. NVIDIA, maker of GPUs, evolved from a company supporting video gaming and graphics rendering to one of the most valuable companies on the planet. Its market capitalization was about $10 billion in 2014. It has eclipsed $3 trillion in June 2024. As of this writing, NVIDIA is worth more than Meta, Tesla, Netflix, AMD, Intel, and IBM combined.\nInitially, neural networks were used to process continuous input data, not unlike traditional regression or classification methods. The earliest applications of neural networks was as binary classifiers. Specialized networks were developed to process unstructured data such as text, images, audio. Convolutional neural networks revolutionized image processing and recurrent neural networks had a large impact on natural language processing. Increasingly, these architectures are now being replaced by transformer models such as GPT, BERT, or LLama.\n\nBecause sensing the world and operating in it takes more brain than brawn, and partly because the term neural network invoked comparisons with human brain function, it was thought that—finally—machines are besting us and coming for us. The era of artificial general intelligence (AGI) was finally upon us. Not so fast.\nWell, that did not happen, and it probably never will based on algorithms just trained on data. Although we are today experiencing another massive shift in AI capabilities thanks to transformer models. These are basically very complex forms of artificial neural networks. Still, I firmly believe that no artificial general intelligence (AGI) will be based on a probabilistic prediction algorithm trained on historical data.\nNeural networks do not mimic the functioning of the human brain. Their fundamental building block is called a neuron, and that is where the similarities end. A neuron in an ANN does not at all work like a neuron in the brain. At best a parallel can be drawn because neurons in an ANN are connected and “activated” through weights.\nSo let’s start figuring out what makes artificial neural networks tick.\n\n\nThe Basics\nArtificial neural networks are supervised learning tools. Like other techniques in this family of algorithms they convert an input material (data) through an algorithm into the prediction or classification of a target variable (Figure 31.1).\n\n\n\n\n\n\nFigure 31.1: Supervised learning in a nutshell.\n\n\n\nNeural networks can take as input any type of data and produce any type of output. Figure 31.1 displays this as predicting or classifying a target variable \\(Y\\), but keep in mind that such a prediction could also be the most likely word in the translation of an input sequence of words into an output sequence of words in a different language. The stuff that happens in between input and output is organized in layers of transformations. The output of one layer is the input to the next layer. Each layer is associated with parameters, linear predictors, and nonlinear transformations.\nA network with a single hidden layer is shown in Figure 31.2. This brings the total number of layers in the network to three:\n\nan input layer with data\na hidden layer that transforms the input data somehow and passes it on to an output layer\nan output layer that transforms the results from the hidden layer into the goal of the analysis, a prediction or a classification.\n\n\n\n\n\n\n\nFigure 31.2: Supervised learning with single layer ANN.\n\n\n\nThe earliest networks were called perceptrons, they did not have a hidden layer. A single layer perceptron thus processes the input data and transforms it directly into the output (Figure 31.3).\n\n\n\n\n\n\nFigure 31.3: Perceptron and single layer network.\n\n\n\nThat sounds a lot like any other statistical model where the mean of the target is modeled as a function of inputs and parameters: \\[\n\\text{E}[Y] = f(x_1,x_2,\\cdots,x_p,\\theta_1, \\cdots,\\theta_k)\n\\] Neural networks are in fact often viewed as tools to approximate unknown functions. The basic idea is that the approximation can be achieved by using a nonlinear transformation, called an activation, of a linear function. Simplifying our model we can then write \\(f(x_1,x_2,\\cdots,x_p,\\theta_1, \\cdots,\\theta_k)\\) as \\[\ng(\\beta_0 + \\beta_1 x_1 + \\cdots\\beta_p x_p)\n\\] for some function \\(g()\\). It is customary in neural network analysis to refer to the coefficients of the linear predictor as bias and weights, rather than as intercept and slope coefficients (parameters). The notation \\[\ng(b + w_1 x_1 + \\cdots w_p x_p)\n\\] for bias (\\(b\\)) and weights (\\(w_j\\)) is common. An ANN with 1 million parameters has a combined 1 million weights and biases. Since \\(g()\\) operates on the predictor of the output layer, it is called the output activation function.\nWe now see that a simple linear regression model is a special case of an artificial neural network:\n\nthere are no hidden layers\nthe output activation function is the identity (also called linear activation)\n\nA multinomial logistic regression model for \\(k\\) categories works similarly, but now the output is a probability of class membership. Instead of a single linear predictor, you estimate a separate predictor \\[\n\\eta_j = b_j + w_{j1} x_1 + \\cdots + w_{jp} x_p \\qquad j=1,\\cdots,k\n\\] The output activation function is \\[\ng_j(\\eta) = \\frac{\\exp\\{\\eta_j\\}}{\\sum_{l=1}^k\\exp\\{\\eta_l\\}}\n\\] known as the softmax activation. The \\(g_j()\\) are now interpreted as category probabilities and an observation is classified into the category for which \\(g_j\\) is largest. Note that \\(g()\\) is a nonlinear function that serves two purposes: it makes the relationship between input variables and category probabilities nonlinear and it maps the linear predictors from the real line to the \\([0,1]\\) interval.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "ann.html#single-layer-networks",
    "href": "ann.html#single-layer-networks",
    "title": "31  Artificial Neural Networks",
    "section": "31.2 Single Layer Networks",
    "text": "31.2 Single Layer Networks\nPerceptrons proved to be limited in the number of categories (classes) they can learn. As a binary classifier, they required linear separability of the classes. Handling more complex problems was solved by the introduction of additional layers, known as hidden layers. The basic form of an artificial neural network today is the single layer network that consists of an input layer, a single hidden layer, and an output layer (Figure 31.4).\n\n\n\n\n\n\nFigure 31.4: Single layer ANN.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between single layer and multi layer network has to do with the number of hidden layers, not the total number of layers. As we will see in the discussion of deep learning, what constitutes a layer is not always clear. In convolutional neural networks, for example, a layer has multiple stages: convolution, activation, and pooling. These stages are often presented as separate layers, adding to the difficulty of exactly defining what we mean by deep learning based on the number of layers in a network.\n\n\nThe single layer ANN is also called a feedforward network, information flows from the input to the output layer. It is a fully connected network when all of the inputs are connected to all nodes of the hidden layer and all nodes of the hidden layer feed into the output layer.\nThe terms \\(A_1, \\cdots, A_M\\) in Figure 31.4 are called the activations of the hidden layer, \\(M\\) is the number of nodes (=number of neurons) in that layer.\nExpressing neural networks mathematically is straightforward but gets messy quickly because of the many nodes and possibly layers. Here it goes.\n\nThe input variables are \\(x_1, \\cdots, x_p\\). Unstructured data have been converted to a numerical representation through encoding. \\(\\textbf{x}\\) denotes the vector of inputs for a record, \\(\\textbf{x}= [x_1,\\cdots,x_p]^\\prime\\).\n\\(A_j\\), the \\(j\\)th neuron in the hidden layer, is modeled as \\[\nA_j = \\sigma(b_j + \\textbf{w}_j^\\prime\\textbf{x}) = \\sigma(b_j + w_{j1} x_1 + \\cdots + w_{jp} x_p) \\qquad j=1,\\cdots, M\n\\] for some activation function \\(\\sigma()\\).\nThe \\(M\\) activations are collected into a vector \\(\\textbf{a}= [A_1, \\cdots, A_M]^\\prime\\)\nThe output layer is modeled as \\[\nT(\\textbf{a}) = b^o+ \\sum_{j=1}^M w_j^oA_j = b^o + \\textbf{a}^\\prime\\textbf{w}^o\n\\]\n\\(T(\\textbf{a})\\) is transformed with an output function \\(g(T(\\textbf{a}))\\) to produce the desired analytical result.\n\nIn summary, the activations \\(A_j\\) are linear functions of the inputs that go through a nonlinear activation function \\(\\sigma()\\). The transformed activations are modeled as another linear model, the output function \\(g(T(\\textbf{a}))\\) transforms \\(T\\) for prediction or classification. You can think of the activations \\(A_1, \\cdots, A_M\\) as deriving \\(M\\) new inputs from the \\(p\\) inputs \\(x_1, \\cdots, x_p\\). There is no connection between \\(M\\) and \\(p\\), you can choose \\(M\\) to be larger or smaller.\nThere are several linear functions in the ANN, in the activations and in the output. \\(\\sigma()\\) and \\(g()\\) introduce nonlinearity, but the ANN is nonlinear in the parameters (weights and biases) beyond that. Suppose that the output function \\(g()\\) is the identity \\(g(T(\\textbf{a})) = T(\\textbf{a})\\) (This is also called a “linear” output function although identity function is technically more accurate). In this situation \\[\n\\begin{align*}\ng(T(\\textbf{a})) &= b^o + \\textbf{a}^\\prime\\textbf{w}^o \\\\\n&= b^o + \\sum_{j=1}^M w_j^o \\sigma\\left(b_j + \\sum_{k=1}^pw_{jk} x_k \\right)\n\\end{align*}\n\\] While \\(g(T(\\textbf{a}))\\) appears like a linear predictor, the function is highly nonlinear in the parameters since \\(\\textbf{a}\\) contains quantities estimated from the data. Even if \\(\\sigma()\\) is the identity function, \\(g(T(\\textbf{a}))\\) is nonlinear in the weights and biases. \\(\\sigma()\\) and \\(g()\\) add nonlinearity beyond that.\nWhy are the neurons \\(A_1, \\cdots, A_M\\) called a hidden layer? The layer is not directly specified by the training data. It is also not directly observed. We observe the \\(x\\)s but connect the inputs only indirectly to the output of the model. The connection to the output is made through the neurons that hide between the two. The learning algorithm can decide how to best use the hidden layer—that is, assign biases and weights.\n\nActivation Functions\nThe activation function \\(\\sigma()\\) introduces nonlinearity into a linear structure, not unlike the kernel trick encountered with support vector machines (Chapter 15).\nPrior to the advent of deep learning and the discovery of ReLU, the rectified linear unit, the sigmoid (logistic) activation function was the most important in training neural networks. ReLU has since dominated. Some of the more important activation functions are shown in Table 31.1.\n\n\n\nTable 31.1: Activation functions \\(\\sigma(\\nu)\\)\n\n\n\n\n\nName\nFunction\n\n\n\n\nSigmoid (logistic)\n\\(\\frac{1}{1+\\exp\\{-\\nu\\}}\\)\n\n\nHyperbolic tangent\n\\(\\tanh(\\nu)\\)\n\n\nRectified linear unit (ReLU)\n\\(\\max\\{0,\\nu\\}\\)\n\n\nRadial basis function\n\\(\\exp\\{-\\gamma\\nu^2\\}\\)\n\n\nSoftplus\n\\(\\log\\{1+e^\\nu\\}\\)\n\n\n\n\n\n\nYou will recognize the sigmoid activation function as the inverse logit link from generalized linear models. The sigmoid (and hyperbolic tangent) function map from \\((-\\infty, \\infty)\\) to \\((0,1)\\). The sigmoid function has a limited band of activation, its behavior is nearly linear around \\(\\nu = 0\\).\nThe ReLU function is deceptively simple. It takes on the value 0 whenever \\(\\nu &lt; 0\\) and returns the value if \\(\\nu &gt; 0\\) (Figure 31.5). It is somewhat surprising that the ReLU function, which is almost linear, allows the approximation of arbitrary nonlinear functions. The ReLU in Figure 31.5 was scaled by factor \\(1/4\\) to make it comparable to the sigmoid function. ReLU is not bounded for positive values and is not useful as an output function for probabilities if the goal is to predict on the \\((0,1)\\) interval.\n\n\n\n\n\n\n\n\nFigure 31.5: ReLU (scaled) and sigmoid activation function.\n\n\n\n\n\nThe popularity of ReLU stems from its close-to-linear behavior. The function is easy to optimize with gradient-based algorithms (although it is not differentiable at zero), generalizes well, and is just sufficiently complex to be useful. The minimal component principle in computer science says that complicated system can be built from simpler components. As Goodfellow, Bengio, and Courville (2016, 175) put it\n\nMuch as a Turing machine’s memory needs only to be able to store 0 or 1 states, we can build a universal function approximator from rectified linear functions.\n\n\n\n\n\n\n\n\n\nFigure 31.6: Hyperbolic tangent and sigmoid activation function.\n\n\n\n\n\nThe hyperbolic tangent and sigmoid functions are related. If \\(\\sigma_1()\\) is sigmoid, then \\[\n\\tanh(v) = 2\\sigma_1(2v) - 1 = \\frac{2}{1+\\exp\\{-2\\nu\\}} - 1\n\\]\n\n\nOutput Functions\nThe output function \\(g()\\) transforms the result of the output layer, \\(T(\\textbf{a})\\) into a prediction. In a regression context, \\(g()\\) is often the identity function, \\[\ng(T(\\textbf{a})) = T(\\textbf{a})\n\\] that means no further transformation.\nFor classification models \\(g()\\) is typically the softmax function. If we model \\(k\\) output categories with a neural network, then each category will have a separate function \\[\nT_j(\\textbf{a}) = b^o_j + \\textbf{a}^\\prime\\textbf{w}^0_j\n\\] and the softmax function transforms them into values in \\((0,1)\\), \\[\ng(T_j(\\textbf{a})) = \\frac{\\exp\\{T_j(\\textbf{a})\\}}{\\sum_{l=1}^k \\exp\\{T_l(\\textbf{a})\\}}\n\\] In other words, the output layer has \\(k\\) neurons and their “probabilities” sum to 1: \\(\\sum_{j=1}^k g(T_j(\\textbf{a})) = 1\\). The classified value of the output is the category \\(j\\) for which \\(g(T_j(\\textbf{a}))\\) is largest.\nThe softmax output function should look familiar, we encountered a similar function in multinomial logistic regression models. The probability to observe category \\(j\\) of \\(k\\) is modeled in a multinomial logistic model as \\[\n\\Pr(Y=j | \\textbf{x}) = \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}}{\\sum_{l=1}^k \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}}\n\\tag{31.1}\\]\nBecause the probabilities sum to 1 across all categories, it is sufficient to model \\(k-1\\) category probabilities. In multinomial regression models it is common to set the linear predictor for one of the categories to 0. Suppose we choose the last category, so that \\(\\boldsymbol{\\beta}_k = \\textbf{0}\\). Equation 31.1 then becomes \\[\n\\begin{align*}\n\\Pr(Y=j | \\textbf{x}) &= \\frac{\\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_j\\}} {1 +\\sum_{l=1}^{k-1} \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}} \\qquad j &lt; k\\\\\n\\Pr(Y=k | \\textbf{x}) &= \\frac{1}{1 +\\sum_{l=1}^{k-1} \\exp\\{\\textbf{x}^\\prime\\boldsymbol{\\beta}_l\\}} \\\\\n\\end{align*}\n\\]\nStatisticians are familiar with this form than the softmax version that allows for a predictor in all categories—neural networks are often over-parameterized.\n\n\nANN by Hand\nIt is instructive to work through a simple neural network by hand to see how inputs, weights, biases, hidden neurons, activation and output functions work together to map inputs into output. Figure 31.7 shows a simple single layer network with 3 inputs, 4 neurons in the hidden layer, and a single output for prediction. ReLU is used for the activation and the output function.\nThis is essentially a regression context where we build a shallow neural network to predict \\(Y\\) from \\(X_1, X_2, X_3\\).\n\n\n\n\n\n\nFigure 31.7: Single layer ANN with 3 inputs and 4 hidden neurons.\n\n\n\nSuppose we have a single input vector (\\(n=1\\)) \\(\\textbf{x}= [2, 1, 3]^\\prime\\) and the parameters for the neural network are given as (Figure 31.8) \\[\n\\begin{align*}\n  A_1: b_1 &= -5 \\quad \\textbf{w}_1 = [1, -1, 1]^\\prime \\\\\n  A_2: b_2 &=  0 \\quad \\textbf{w}_2 = [1,  1, 0]^\\prime \\\\\n  A_3: b_3 &=  1 \\quad \\textbf{w}_3 = [0,  1, 1]^\\prime \\\\\n  A_4: b_4 &= -2 \\quad \\textbf{w}_4 = [1,  0, 1]^\\prime \\\\\n  T: b^0 &= 1 \\quad \\textbf{w}^o = [1, 2, -1, 0]^\\prime\n\\end{align*}\n\\]\n\n\n\n\n\n\nFigure 31.8: Parameters of the single layer ANN.\n\n\n\nFigure 31.9 applies the parameter values to compute the activations \\(A_1, \\cdots, A_4\\), applies the ReLU function and passes the results to the output layer.\n\n\n\n\n\n\nFigure 31.9: Computing the solution for activations and output.\n\n\n\nThe activations from the hidden layer, after the ReLU is applied, are \\(A_1 = 0\\), \\(A_2 = 3\\), \\(A_3 = 5\\) and \\(A_4 = 3\\). The value in the output layer is \\(T(\\textbf{a}) = b^0 + w_1^0 A_1 + \\cdots + w_4^0 A_4\\) = 2. Because it is positive, the ReLU output function \\(g(T(\\textbf{a})) = \\max\\{0,T(\\textbf{a})\\}\\) returns 2 as the predicted value.\n\n\nThe XOR Gate\nNeural networks have many parameters, frequently more parameters than observations. How is that possible, does that not create a singular situation where solutions are not unique and the model is saturated (perfect fit)? That is true for linear models, a linear regression with \\(p\\) inputs and \\(n= p+1\\) observations is a saturated model with zero residual error. Add another input or take away an observation and a least-squares solution cannot be found.\nThis is not so with nonlinear models, you can find a minimum of the objective function for an over-specified nonlinear model; and it does not necessarily produce a perfect fit at the solution.\nSuppose we train a single layer network with \\(p=10\\) inputs, a single output (regression), and a hidden layer with \\(M=100\\) nodes. How many parameters are in this model?\n\nThe input layer fully connects 10 inputs to 100 hidden units. Each of the \\(A_1, \\cdots, A_M\\) has 11 parameters, a weight for each of the input variables and a bias. So there are 1,100 parameters in the hidden layer.\nThe hidden layer passes 100 activations to the output layer, which adds a weight for each activation and a bias, another 101 parameters.\n\nThe total number of quantities estimated in this fairly simple neural network is 1,100 + 101 = 1,201. Note that this number is independent of the number of observations in the training data. Even with \\(n=20\\), the ANN (attempts to) estimates 1,201 quantities.\nAs an example of an over-specified neural network that can be solved with a more parsimonious modeling alternative, we consider the famous XOR example.\nThe exclusive OR (XOR) function is a logic gate that can be modeled as follows. Take two binary inputs \\(X_1 \\in \\{0,1\\}\\) and \\(X_2 \\in \\{0,1\\}\\). The XOR gate takes on the value \\(Y=1\\) if exactly one of the \\(X\\)s is 1, otherwise the gate is \\(Y=0\\). The following table shows the four possible states of the gate.\n\n\n\nTable 31.2: XOR gate data.\n\n\n\n\n\nObs #\n\\(X_1\\)\n\\(X_2\\)\n\\(Y\\)\n\n\n\n\n1\n0\n0\n0\n\n\n2\n0\n1\n1\n\n\n3\n1\n0\n1\n\n\n4\n1\n1\n0\n\n\n\n\n\n\nThe goal is to construct a neural network with one hidden layer with \\(M=2\\) units, ReLU activation and identity (“linear”) output function (Figure 31.10). The network should model the XOR gate correctly—that is, provide a perfect fit to the data in Table 31.2.\n\n\n\n\n\n\nFigure 31.10: Single layer ANN for XOR problem.\n\n\n\nThe network has nine parameters and the following structure \\[\n\\begin{align*}\n        A_1 &= \\max \\{ 0, b_{1} + w_{11}x_1 + w_{21}x_2 \\} \\\\\n        A_2 &= \\max \\{ 0, b_{2} + w_{21}x_1 + w_{22}x_2 \\} \\\\\n        T &= b^o + w^o_1 A_1 + w^o_2 A_2 \\\\\n        g(T) &= T\n\\end{align*}\n\\]\nYou can easily verify that the following values are a solution (Table 31.3) \\[\n\\begin{align*}\n    b_1 &=  0, \\textbf{w}_1 = [1,1] \\\\\n    b_2 &= -1, \\textbf{w}_2 = [1,1] \\\\\n    b^o &= 0, \\textbf{w}^o = [1, -2]\n\\end{align*}\n\\]\n\n\n\nTable 31.3: XOR gate data and solution.\n\n\n\n\n\nObs #\n\\(X_1\\)\n\\(X_2\\)\n\\(A_1\\)\n\\(A_2\\)\n\\(Y\\)\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n1\n1\n0\n1\n\n\n3\n1\n0\n1\n0\n1\n\n\n4\n1\n1\n2\n1\n0\n\n\n\n\n\n\nThis is great. A neural network with 9 parameters was able to predict the exclusive OR gate. Is there a more parsimonious option?\nGoodfellow, Bengio, and Courville (2016, 173) state that a linear model is not capable of fitting the XOR gate. This is correct if one considers a main-effects only model as in the following code\n\ndat &lt;- data.frame(x1=c(0,0,1,1), x2=c(0,1,0,1), y=c(0,1,1,0))\ndat\n\n  x1 x2 y\n1  0  0 0\n2  0  1 1\n3  1  0 1\n4  1  1 0\n\nreg_main &lt;- lm(y ~ x1 + x2, data=dat)\nsummary(reg_main)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dat)\n\nResiduals:\n   1    2    3    4 \n-0.5  0.5  0.5 -0.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 5.00e-01   8.66e-01   0.577    0.667\nx1          1.11e-16   1.00e+00   0.000    1.000\nx2          0.00e+00   1.00e+00   0.000    1.000\n\nResidual standard error: 1 on 1 degrees of freedom\nMultiple R-squared:  3.698e-32, Adjusted R-squared:     -2 \nF-statistic: 1.849e-32 on 2 and 1 DF,  p-value: 1\n\n\nThe linear regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) does not provide a solution, otherwise it would result in a perfect fit. Instead, it has non-zero residuals. However, if you allow \\(X_1\\) and \\(X_2\\) to interact, then the linear model fits the XOR gate perfectly. In fact, you can also remove the intercept and model the gate with only three parameters:\n\nreg_ia &lt;- lm(y ~ x1 + x2 + x1*x2 -1, data=dat)\nsummary(reg_ia)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x1 * x2 - 1, data = dat)\n\nResiduals:\n         1          2          3          4 \n-1.178e-16 -1.424e-32  1.298e-32  6.588e-34 \n\nCoefficients:\n        Estimate Std. Error    t value Pr(&gt;|t|)    \nx1     1.000e+00  1.178e-16  8.489e+15   &lt;2e-16 ***\nx2     1.000e+00  1.178e-16  8.489e+15   &lt;2e-16 ***\nx1:x2 -2.000e+00  2.040e-16 -9.802e+15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.178e-16 on 1 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.804e+31 on 3 and 1 DF,  p-value: &lt; 2.2e-16\n\nround(predict(reg_ia),3)\n\n1 2 3 4 \n0 1 1 0 \n\n\nNotice that the model \\[\nY = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2\n\\] has zero residual error (all residuals are zero) but still has one degree of freedom left for the error term. It fits perfectly but not by saturating the model with parameters. The parameter estimates are \\(\\widehat{\\beta}_1 = 1\\), \\(\\widehat{\\beta}_2 = 1\\), and \\(\\widehat{\\beta}_3 = -2\\). The fitted equation is \\[\n\\widehat{Y} = x_1 + x_2 - 2 x_1 x_2\n\\]\n\n\n\nTable 31.4: XOR gate data and regression solution.\n\n\n\n\n\nObs #\n\\(X_1\\)\n\\(X_2\\)\n\\(X_1X_2\\)\n\\(Y\\)\n\\(\\widehat{Y}\\)\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n1\n0\n1\n1\n\n\n3\n1\n0\n0\n1\n1\n\n\n4\n1\n1\n1\n0\n0\n\n\n\\(\\beta\\)\n1\n1\n-2\n\n\n\n\n\n\n\n\nI leave it up to you to decide which model is preferable (Figure 31.11):\n\\[\nY = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2\n\\]\nor\n\\[\n\\begin{align*}\n        A_1 &= \\max \\{ 0, b_{1} + w_{11}x_1 + w_{21}x_2 \\} \\\\\n        A_2 &= \\max \\{ 0, b_{2} + w_{21}x_1 + w_{22}x_2 \\} \\\\\n        T &= b^o + w^o_1 A_1 + w^o_2 A_2 \\\\\n        g(T) &= T\n\\end{align*}\n\\]\nThe linear model has a closed-form solution and no tuning parameters. The ANN is a nonlinear model and you have to choose \\(M\\), the number of hidden units as well as the activation function \\(\\sigma()\\) and the output function \\(g()\\).\n\n\n\n\n\n\nFigure 31.11: Two solutions for the XOR gate problem.\n\n\n\nFinally, the neural network solution is not unique, other combinations of weights and biases lead to a solution. For example, \\[\n\\begin{align*}\n    b_1 &=  0, \\textbf{w}_1 = [1,-1] \\\\\n    b_2 &= -1, \\textbf{w}_2 = [-1,2] \\\\\n    b^o &= 0, \\textbf{w}^o = [1, 1]\n\\end{align*}\n\\] is also a solution (Table 31.5).\n\n\n\nTable 31.5: XOR gate data and another solution.\n\n\n\n\n\nObs #\n\\(X_1\\)\n\\(X_2\\)\n\\(A_1\\)\n\\(A_2\\)\n\\(T=A_1+A_2\\)\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n1\n0\n1\n1\n\n\n3\n1\n0\n1\n0\n1\n\n\n4\n1\n1\n0\n0\n0\n\n\n\n\n\n\nThat is a common problem in over-parameterized neural networks. The objective function surface has mutiple local extrema. Training algorithms can get stuck in one of the many local solutions and might never see the global solution. This phenomenon is so common with neural networks that many data scientists simply accept any solution to the estimation problem.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "ann.html#multi-layer-networks",
    "href": "ann.html#multi-layer-networks",
    "title": "31  Artificial Neural Networks",
    "section": "31.3 Multi Layer Networks",
    "text": "31.3 Multi Layer Networks\nA single layer network can in theory approximate most functions, provided the hidden layer has enough neurons (=units). But networks with more than a single layer are often able to use fewer units per layer and can generalize well with fewer parameters overall. Increasing the depth of the network tends to be more effective than increasing the width of the network, although counter examples are easy to find.\nModern neural networks have more than one hidden layer, we call them multi layer networks. In general, when the number of layers is greater than two, the networks are referred to as deep neural networks (DNN). This is not an established definition of DNNs, in part because it is sometime not clear what is being described as a layer. For example, in convolutional neural networks three processing stages alternate—convolution, activation, and pooling. The number of layers varies greatly depending on whether the stages are seen as separate layers or not.\nBesides pooling and convolving, the assumption in convolutional neural networks is that deeper layers learn increasingly higher-level abstractions of the data. The layers represent a nested hierarchy of concepts—edges, shapes, etc—with more abstract representations computed in terms of simpler ones. This idea of abstraction into hierarchy of concepts favors deeper multi layer networks over wider networks with fewer layers.\nIn the chapter on deep learning we will encounter layers with many different purposes (Chapter 34). Examples are\n\nconvolutional layers\npooling layers\ndropout layers\nflattening layers\nfully connected layers\nregularization layers\nnormalization layers\nattention layers\nrecurrent layers\nlong short-term memory layers\nembedding layers\n\nIn this section, multiple layers are simple fully-connected hidden layers. The first hidden layer with \\(M_1\\) neurons passes its output to a second hidden layer with \\(M_2\\) neurons and so forth. Figure 31.12 shows a fully connected ANN with two hidden layers with \\(M_1=4\\) and \\(M_2=3\\) neurons, respectively.\n\n\n\n\n\n\nFigure 31.12\n\n\n\nMathematically we can present multi layer networks as chaining transformations. Let’s use superscripts in parentheses to denote the activations in the hidden layers. For example, \\(A^{(1)}_j\\) is the activation of neuron \\(j\\) in the first hidden layer, \\(A^{(3)}_{M_3}\\) is the activation of the last neuron in the third hidden layer.\nThe full network with \\(h\\) hidden layers can be written as follows: \\[\n\\begin{align*}\nA^{(1)}_j &= \\sigma^{(1)}\\left( b^{(1)}_j + \\textbf{w}^{(1)}_j\\textbf{x}\\right) \\qquad j=1,\\cdots,M_1 \\\\\n\\textbf{a}^{(1)} &= [A^{(1)}_1,\\cdots, A^{(1)}_{M_1}] \\\\\nA^{(2)}_j &= \\sigma^{(2)}\\left( b^{(2)}_j + \\textbf{w}^{(2)}_j\\textbf{a}^{(1)} \\right) \\qquad j=1,\\cdots,M_2 \\\\\n\\textbf{a}^{(2)} &= [A^{(2)}_1,\\cdots, A^{(2)}_{M_2}] \\\\\n\\vdots \\\\\nA^{(h)}_j &= \\sigma^{(h)}\\left( b^{(h)}_j + \\textbf{w}^{(h)}_j\\textbf{a}^{(h-1)} \\right) \\qquad j=1,\\cdots,M_h \\\\\n\\textbf{a}^{(h)} &= [A^{(h)}_1,\\cdots, A^{(h)}_{M_h}] \\\\\ng(T(\\textbf{a}^{h})) &= g(b^o+\\textbf{w}^o\\textbf{a}^{(h)})  \n\\end{align*}\n\\]\nThis looks messier than it is:\n\nEach hidden layer has its own set of biases and weights.\nEach layer can have a different activation function \\(\\sigma^{(1)}, \\cdots, \\sigma^{(h)}\\). Frequently, they are the same function throughout the hidden layers.\nOnly the first hidden layer consumes the inputs \\(\\textbf{x}\\). Subsequent hidden layers consume as input the output of the previous hidden layer.\nOnly the final hidden layer feeds directly into the output layer. The other hidden layers contribute indirectly to the output through their impact on the final hidden layer.\nEach hidden layer can have a different number of neurons.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "ann.html#sec-mnist-first-look",
    "href": "ann.html#sec-mnist-first-look",
    "title": "31  Artificial Neural Networks",
    "section": "31.4 A First Look at MNIST Image Classification",
    "text": "31.4 A First Look at MNIST Image Classification\nThe MNIST image classification data is a famous data set of hand-written digits from 0–9, captured as 28 x 28 grayscale pixel images. The data set comprises 60,000 training observations and 10,000 test observations and is the “Hello World” example for computer vision algorithms. The MNIST database was constructed from NIST’s Special Database 3 (SD-3) and Special Database 1 (SD-1), 30,000/5,000 test/train images were chosen from SD-3, collected from Census Bureau employees, and 30,000/5,000 test/train images were chosen from SD-1, collected from high-school students.\nEvery computer vision algorithm is tested against the MNIST data set and another famous benchmark, the CIFAR data sets. MNIST data is available from multiple sites, e.g., here, and is also distributed with many deep learning frameworks.\n\n\n\n\n\n\nFigure 31.13: Example of hand-written digits in MNIST data.\n\n\n\nFigure 31.13 displays a sampling of the images in the data set for each of the ten digits. Many classification methods have been applied to the MNIST data, including various artificial neural networks—check the table on Yann LeCun’s web site for approaches and their error rates. Many approaches ignore the spatial arrangement of the 28 x 28 pixels on each image and treats each record as 28 x 28 = 784 grayscale values. They still have admirable error rates.\nA neural network with two hidden layers, comparing 128 and 64 hidden units, respectively, is shown in Figure 31.14.\n\n\n\n\n\n\nFigure 31.14: A two layer network for MNIST data. Source.\n\n\n\nThere are 784 input variables, \\(x_1, \\cdots, x_{784}\\), corresponding to the 784 pixel locations on each image. The first hidden layer has 128 neurons and ReLU activation. Consequently, there are 128 x 785 = 100,480 parameters in that layer (784 weights plus a bias for each neuron). The second hidden layer has 64 neurons and ReLU activation, adding 64 * 129 = 8,256 parameters. The output layer has a softmax output function because each image needs to be classified into one of 10 categories, adding 10 * 65 = 650 parameters. The total number of parameters in this two layer ANN is 100,480 + 8,256 + 650 = 109,386. The final layer in Figure 31.14 is shown as a cross-entropy layer. This simply means that the values (probabilities) associated with the softmax criterion in the output layer is used to choose as the classified value the digit for which the probability is largest.\nIn the next chapter we demonstrate that this model has a test accuracy of more than 96%, which is quite remarkable, considering that no spatial information about the pixels is explicitly taken into account. The 28 x 28 grayscale values are simply stretched into a vector of size 784 and processed without considering a value’s neighborhood of grayscale values.\nBefore we crunch the numbers for the MNIST data set, a few words about the process of training neural networks.\n\n\n\nFigure 31.1: Supervised learning in a nutshell.\nFigure 31.2: Supervised learning with single layer ANN.\nFigure 31.3: Perceptron and single layer network.\nFigure 31.4: Single layer ANN.\nFigure 31.7: Single layer ANN with 3 inputs and 4 hidden neurons.\nFigure 31.8: Parameters of the single layer ANN.\nFigure 31.9: Computing the solution for activations and output.\nFigure 31.10: Single layer ANN for XOR problem.\nFigure 31.11: Two solutions for the XOR gate problem.\nFigure 31.12: \nFigure 31.13: Example of hand-written digits in MNIST data.\nFigure 31.14: A two layer network for MNIST data. Source.\n\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” Bulletin of Mathematical Biophysics 5: 115–33.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "training_ann.html",
    "href": "training_ann.html",
    "title": "32  Training Neural Networks",
    "section": "",
    "text": "32.1 Nonlinear Function Optimization",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Training Neural Networks</span>"
    ]
  },
  {
    "objectID": "training_ann.html#nonlinear-function-optimization",
    "href": "training_ann.html#nonlinear-function-optimization",
    "title": "32  Training Neural Networks",
    "section": "",
    "text": "Objective and Loss Functions\nSuppose we have a function \\(\\ell(\\boldsymbol{\\theta};\\textbf{y})\\) of parameters and data and we wish to find estimates \\(\\widehat{\\boldsymbol{\\theta}}\\) that minimize \\(\\ell(\\boldsymbol{\\theta};\\textbf{y})\\). We call \\(\\ell(\\boldsymbol{\\theta};\\textbf{y})\\) the objective function of the estimation problem. In many statistical learning and machine learning applications, \\(\\ell(\\boldsymbol{\\theta};\\textbf{y})\\) takes on the form of a sum over the observations, \\[\n\\ell(\\boldsymbol{\\theta}; \\textbf{y}) = \\sum_{i=1}^n C_i(\\boldsymbol{\\theta};\\textbf{y})\n\\] or an average, \\[\n\\ell(\\boldsymbol{\\theta}; \\textbf{y}) = \\frac{1}{n} \\sum_{i=1}^n C_i(\\boldsymbol{\\theta};\\textbf{y})\n\\] where \\(C_i\\) is a measure of loss associated with observation \\(i\\).\nIn least-squares estimation, for example, \\(C_i(\\boldsymbol{\\theta};\\textbf{y})\\) measures the squared-error loss between the observed data and the model \\(f(\\textbf{x};\\boldsymbol{\\theta})\\): \\[\nC_i(\\boldsymbol{\\theta};\\textbf{y}) = \\left(y_i - f(\\textbf{x}_i;\\boldsymbol{\\theta}) \\right)^2\n\\] In linear least squares, this loss simplifies to \\[\nC_i(\\boldsymbol{\\theta};\\textbf{y}) = \\left(y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\theta}\\right)^2\n\\] In maximum likelihood estimation, \\(C_i(\\boldsymbol{\\theta}; \\textbf{y})\\) is the negative log likelihood function of the \\(i\\)th observation.\nIn statistical learning, loss functions are typically not divided by the number of observations. In machine learning (and in training neural networks) it is common to add the \\(1/n\\) divisor in the objective function. This is the difference between optimizing squared error loss or mean squared error loss, for example. It has no effect on the parameter estimates.\nFor training neural networks the most important loss functions when predicting a continuous target are\n\nSquared error loss: \\(C_i = (y_i - g(T(\\textbf{a}^{(h)}))^2\\).\nAbsolute error loss: \\(C_i = |y_i - g(T(\\textbf{a}^{(h)}))|\\)\n\nIn classification models the most important loss functions are\n\nbinary cross-entropy loss (also called log-loss): \\(C_i = - \\left(y_i\\log(\\pi) + (1-y_i)\\log(1-\\pi)\\right)\\)\ncategorical cross-entropy loss: \\(C_i = - \\sum_{j=1}^k y_{ij} \\log(\\pi_j)\\)\n\nYou will recognize the binary cross-entropy loss as the negative log likelihood of a Bernoulli(\\(\\pi\\)) random variable and the categorical cross-entropy loss as the (kernel of) the negative log likelihood of a Multinomial(\\(\\pi_1,\\cdots,\\pi_k\\)) random variable with \\(k\\) categories.\n\n\nThe Gradient\nHow do we go about finding the values of \\(\\boldsymbol{\\theta}\\) that minimize the objective function given a set of data? When the system has a closed-form solution, like the XOR problem in Section 31.2.4, we can compute the estimates in a single step. Most neural networks are not of that ilk and require an iterative, numeric solution: beginning from a set of starting values, the estimates are updated iteratively, with a general tendency toward improving (lowering) the objective function. When the objective function does not improve—or only negligibly so—the iterations stop.\nIn Section 9.3 we spent considerable energy on finding good starting values for nonlinear regression models. The closer the starting values are to a solution, the more reliably will any iterative algorithm improve on the starting values and converge. Recall the two-layer ANN for the MNIST data from the last chapter. The model has 109,386 parameters. How do you find starting values for all these? In the nonlinear regression models of Chapter 9 we relied on the intrinsic interpretability of the models to find starting values. Neural networks are not interpretable and any attempt to find “meaningful” starting values is futile. Usually, the starting values of neural networks are chosen at random.\nOnce we have starting values \\(\\boldsymbol{\\theta}^{[0]}\\), the objective function \\(\\ell(\\boldsymbol{\\theta}^{[0]};\\textbf{y})\\) can be computed.\n\n\n\n\n\n\nCaution\n\n\n\nThe notation is a bit messy. We used superscripts with parentheses to identify layers in a neural network and now are using superscripts with square brackets to denote iterates of a parameter vector.\n\n\nTo improve on the starting values and get an updated value \\(\\boldsymbol{\\theta}^{[1]}\\), many optimization methods rely on the gradient of the objective function, the set of partial derivatives with respect to the parameter estimates. The gradient of the objective function is thus a vector, the size equals the number of parameters, and the typical element in position \\(j\\) is \\[\n\\delta(\\theta_j; \\textbf{y}) = \\frac{\\partial \\ell(\\boldsymbol{\\theta};\\textbf{y})}{\\partial \\theta_j}\n\\]\nThe overall gradient is the vector \\(\\boldsymbol{\\delta}(\\boldsymbol{\\theta};\\textbf{y}) = [\\delta(\\theta_1;\\textbf{y}), \\cdots, \\delta(\\theta_p;\\textbf{y})]^\\prime\\).\nThe gradient measures the change in the objective function in the direction of \\(\\boldsymbol{\\theta}_j\\).\nUsing terminology and notation familiar from calculus, Figure 32.1 shows a function \\(f(x)\\) in one parameter (\\(x\\)). The function has two minima and two maxima. These occur at values of \\(x\\) where the derivative \\(f^\\prime(x)\\) is zero. Once we have located a minimum or maximum, the second derivative \\(f^{\\prime\\prime}(x)\\) tells us whether we have found a minimum (\\(f^{\\prime\\prime}(x) &gt; 0\\)) or a maximum (\\(f^{\\prime\\prime}(x) &lt; 0\\)). Optimally we locate the overall minimum (or maximum), known as the global minimum (or maximum), not just a local minimum (maximum).\nSuppose we started the search for a minimum at the value indicated with a green arrow in Figure 32.1. To find a smaller objective function value we should move to the right. Similarly, in Figure 32.2, we should move to the left.\n\n\n\n\n\n\nFigure 32.1: Minimizing a function \\(f(x)\\) in one parameter \\(x\\).\n\n\n\n\n\n\n\n\n\nFigure 32.2: Minimizing a function \\(f(x)\\) in one parameter \\(x\\).\n\n\n\nThe upshot of the figures is that from any point in the parameter space, you should follow the negative value of the gradient to find a point in the parameter space where the objective function is lower. If we wanted to maximize the objective function, we would follow the direction of the positive gradient to climb up the objective function (Figure 32.3).\n\n\n\n\n\n\nFigure 32.3: Find solution that minimizes a function in one parameter.\n\n\n\n\nFirst-order methods\nAn optimization technique is called a first-order method if it relies only on gradient information to update the parameter estimates between iterations. The general expression for the update formula at iteration \\(t\\) is \\[\n\\begin{align*}\n\\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[t]}; \\textbf{y}) &= \\frac{\\partial{\\ell(\\boldsymbol{\\theta};\\textbf{y})}}{\\boldsymbol{\\theta}} \\biggr\\vert_{\\boldsymbol{\\theta}^{[t]}} \\\\\n\\boldsymbol{\\theta}^{[t+1]} &= \\boldsymbol{\\theta}^{[t]} - \\epsilon \\times \\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[t]}; \\textbf{y})\n\\end{align*}\n\\tag{32.1}\\]\n\\(\\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[t]}; \\textbf{y})\\) is the vector of derivatives of the objective function with respect to all parameters evaluated at \\(\\boldsymbol{\\theta}^{[t]}\\). The \\((t+1)\\)st value of the parameter estimates is obtained by moving \\(\\boldsymbol{\\theta}^{[t]}\\) in the direction of the negative gradient. The quantity \\(\\epsilon\\) is known as the step size or the learning rate. It determines how big a step in the direction of the negative gradient we should take.\nExamples of first-order optimization methods are\n\nGradient descent\nStochastic gradient descent\nConjugate gradient algorithm\nQuasi-Newton algorithm\nDouble dogleg algorithm\n\n\n\n\n\n\n\nNote\n\n\n\nThe gradient tells us in which direction we need to apply a correction of the parameter estimates, it does not tell us how far we should go. Suppose you stand blindfolded in the vicinity of a cliff. You want to get closer to the cliff but not fall off it. Someone orients you toward the cliff. The learning rate determines the size of your next step. If you know that you are far away from the cliff you might as well take a full step—maybe even jump toward it. If you are very close to the cliff you only take a baby step.\n\n\nThe learning rate is an important hyperparameter of nonlinear optimization. Choosing a rate too small means making only tiny improvements and the procedure will require many iterations. Taking steps too large can make you miss a minimum and step from one valley of the objective function into another, giving the optimization fits. In nonlinear regression estimation, the best step size between iterations is often computed with a separate algorithm such as a line search. Machine learning applications range considerably in how they determine the learning rate. The classical stochastic gradient descent (SGD) algorithm holds \\(\\epsilon\\) fixed throughout. The popular Adam optimizer, on the other hand, has a parameter-specific learning rate that is determined from the exponential moving average of the gradient and the squared gradient.\n\n\nSecond-order methods\nSecond-order optimization methods use information about the second derivative of the objective function in computing the updates. The second derivative contains information about the curvature of the objective function. The gradient tells us how much the objective function changes in a parameter, the curvature describes how much the function bends. Second-order information is captured by the Hessian matrix \\(\\textbf{H}\\) of the objective function. The Hessian is simply the matrix of second derivatives of \\(\\ell(\\boldsymbol{\\theta};by)\\) with respect to the parameters (the Jacobian of the gradient). The \\((p \\times p)\\) Hessian matrix has typical element \\[\n\\textbf{H}= [h_{ij}] = \\frac{\\partial^2{\\ell(\\boldsymbol{\\theta};\\textbf{y})}}{\\partial \\theta_i \\partial \\theta_j}\n\\]\nExamples of second-order optimization methods are the following:\n\nFisher scoring\nNewton-Raphson algorithm\nTrust region algorithm\nLevenberg-Marquardt algorithm\n\nShould you choose a first-order or second-order algorithm? All things being equal, second-order algorithms are superior to first-order algorithms and converge in fewer iterations. On the other hand, the computational effort for each iteration is much greater with a second-order algorithm. The parameter update between iterations is computed as \\[\n\\boldsymbol{\\theta}^{[t+1]} = \\boldsymbol{\\theta}^{[t]} - {\\textbf{H}^{[t]}}^{-1}\\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[t]};\\textbf{y})\n\\] and requires the computation and inversion of the \\((p \\times p)\\) matrix \\(\\textbf{H}\\). Recall the two layer ANN for the MNIST example from the last chapter. The model has 109,386 parameters. When \\(p &gt; 1000\\), computing and inverting the Hessian at each iteration is prohibitive. Approximate methods such as limited memory BFGS (L-BFGS) exist. However, because of the generally large number of parameters, first-order methods dominate in training artificial neural networks; in particular variations of gradient descent.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Training Neural Networks</span>"
    ]
  },
  {
    "objectID": "training_ann.html#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
    "href": "training_ann.html#gradient-descent-gd-and-stochastic-gradient-descent-sgd",
    "title": "32  Training Neural Networks",
    "section": "32.2 Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "text": "32.2 Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\nThe basic gradient descent algorithm was described above: to find the parameter values that minimize the objective function, take a step in the direction of the negative gradient: \\[\n\\boldsymbol{\\theta}^{[t+1]} = \\boldsymbol{\\theta}^{[t]} - \\epsilon \\times \\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[t]}; \\textbf{y})\n\\]\nThe rationale is that the objective function decreases fastest in the direction of the opposite gradient. However, it is not guaranteed that \\(\\ell(\\boldsymbol{\\theta}^{[t+1]};\\textbf{y}) \\le \\ell(\\boldsymbol{\\theta}^{[t]}; \\textbf{y})\\) if one takes a full step in the direction of the negative gradient. But the inequality should be true for some value \\(\\epsilon\\), called the learning rate or step size.\nAn analogy to explain gradient descent is finding one’s way down from a mountain at night. You cannot see the path because of darkness, yet you need to descent from the mountain. The best way is to measure the slope in the vicinity of your location (compute the gradient), and to take a step in the direction of the greatest downhill slope. If you repeat this procedure you will either get off the mountain (find the global minimum) or get stuck in a depression (find a local minimum) such as a mountain lake.\nIn addition to changing the step length with the learning rate, we can also modify the direction in which we move downhill. The idea is that by taking a more shallow path the direction can be sustained for a longer period of time without measuring the slope again. This would reduce the number of gradient calculations, which can be expensive.\nThe classical gradient descent, also called batch GD, computes the gradient across all \\(n\\) observations in the training data set. When the objective function takes the form of a sum, \\[\n\\ell(\\boldsymbol{\\theta}; \\textbf{y}) = \\frac{1}{n} \\sum_{i=1}^n C_i(\\boldsymbol{\\theta};\\textbf{y})\n\\] the gradient is the sum of the individual gradients \\[\n\\delta(\\theta_j; \\textbf{y}) = \\frac{\\partial \\ell(\\boldsymbol{\\theta};\\textbf{y})}{\\partial \\theta_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial C_i(\\boldsymbol{\\theta};\\textbf{y})}{\\partial \\theta_j}\n\\] The parameters are updated after the gradients have been computed for all the parameters according to Equation 32.1. When \\(n\\) is large, computing the gradient can be time consuming. First-order methods tend to require more iterations until convergence and computing the gradient is a primary bottleneck of the algorithms. The stochastic GD version is an online algorithm that computes gradients and updates one observation at a time: \\[\n\\boldsymbol{\\theta}^{[i+1]} = \\boldsymbol{\\theta}^{[i]} - \\epsilon \\boldsymbol{\\delta}(\\boldsymbol{\\theta}^{[i]};y_i) \\qquad i=1,\\cdots,n-1\n\\] The true gradient is approximated as the gradient of the \\(i\\)th sample, and an update is calculated for each sample. This process of passing over the data and updating the parameters \\(n\\) times repeats until convergence, often with a random shuffling of the data between passes to avoid cycling of the algorithm.\nClassical GD and SGD as presented here present the two extremes of calculating gradients between parameter updates: once for the entire sample, or for each observation. While the gradients computed in GD are very stable, they require the most computation between updates. The gradients in SGD on the other hand can be erratic and a poor approximation of the overall gradient. A compromise is minibatch gradient descent, where gradients are calculated for a small batch of observations, usually a few hundred.\nSGD with minibatches and backpropagation has become a standard in training neural networks. The original SGD has a fixed learning rate, and an obvious extension is to modify the learning rate, for example, decreasing \\(\\epsilon\\) with the iteration. The SGD algorithm then learns quickly early on and learns slower as the algorithm reaches convergence. Adaptive versions with a per-parameter learning rate are AdaGrad (Adaptive Gradient) and RMSProp (Root Mean Square Propagation). Since 2014, Adam-based optimizers (Adaptive Moment Estimation), an extension of RMSProp, have been used extensively in training neural networks due to their strong performance in practice.\n\nBackpropagation\nWorking with neural networks you will hear and read about backpropagation. This is not a separate optimization algorithm, but an efficient method of calculating the gradients in a multi layer neural network. Because these networks are based on chaining transformations, you can express the gradient with respect to a parameter by the chain rule of calculus.\nBackpropagation computes the gradient one layer at a time, going backward from the last layer to avoid duplicate computations, and using the chain rule.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Training Neural Networks</span>"
    ]
  },
  {
    "objectID": "training_ann.html#tuning-a-network",
    "href": "training_ann.html#tuning-a-network",
    "title": "32  Training Neural Networks",
    "section": "32.3 Tuning a Network",
    "text": "32.3 Tuning a Network\n-The performance of a neural network in training and scoring (inference) is affected by many choices. Training a neural network means tuning a sensitive and over-parameterized nonlinear problem; it is both art and science. Among the choices the model builder has to make are the following:\n\nHidden layers: number of layers, number of neurons in each layer, activation functions\nOutput function\nRegularization parameters such as the dropout rate or the penalty for lasso or ridge regularization (see below)\nDetails of optimization: minibatch size, early stopping rule, starting values, number of epochs, learning rate, etc.\n\nAll of these should be considered hyperparameters of the network. With other statistical learning algorithms it is customary to determine values for some (or all) of the hyperparameters by estimation or by a form of cross-validation. With neural networks that is rarely the case. Training a single configuration of a network is time consuming and one often settles on a solution if the convergence behavior seems reasonable.\n\nRegularization\nNeural networks are over-parameterized. When we encountered high-dimensional problems previously in statistical learning, we turn to regularization and shrinkage estimation to limit the variability of the fitted function. That was the approach in ridge or lasso regression (Section 8.2) and in spline smoothing (Section 11.3.4).\nRegularization in neural networks to reduce overfitting can take three different forms:\n\n\\(L_1\\) penalty (lasso-style) on a particular layer\n\\(L_2\\) penalty (ridge-style) on a particular layer\nDropout regularization\n\nNote that the regularization is applied on a per-layer basis, and not just as one big penalty term on the weights and biases of the entire network. Also, frameworks like TensorFlow or Keras allow you to add both regularization penalties separately for the weights, the biases, and/or the output of a layer. In Keras these are called the kernel_regularizer, bias_regularizer, and activity_regularizer, respectively. Each of those can apply an \\(L_1\\), an \\(L_2\\), or both penalties. As you can see, there are many choices and possible configurations. Typically, regularization penalties, whether \\(L_1\\) or \\(L_2\\) are applied to the weights of a layer, shrinking them toward zero. The bias terms, which act like intercepts are typically not regularized as that shrinks the model toward one that passes through the origin.\nThe new regularization method in the context of neural networks is dropout regularization, performed by adding a dropout layer to the network architecture. This is a parameter-free layer that randomly removes units from an input layer or a hidden layer by setting its activation to zero. Dropout learning can be applied to any layer, Figure 32.4 shows depicts a singe layer network with one input unit (\\(x_3\\)) and one hidden unit (\\(A_2\\)) being dropped.\n\n\n\n\n\n\nFigure 32.4: Dropout in input and hidden layer.\n\n\n\nThe dropped units do not receive connections from the preceding layer and do not emit output to the following layer.\nWhy does dropout learning work and reduce the chance of overfitting a neural network? Each neuron of a layer to which dropout is applied is removed with some probability \\(\\phi\\). During training, the network cannot rely on any one neuron because it might disappear. As a result, assigning large weights to neurons is avoided in favor of spreading weights across the remaining nodes, making them smaller. As with \\(L_1\\) or \\(L_2\\) regularization, the effect of randomly dropping neurons from layers is to shrink the remaining weights to zero. Note that dropout can be applied to any layer and that the dropout rate \\(\\phi\\) can vary among the dropout layers. Dropout rates range from \\(\\phi=0.1\\) to \\(\\phi = 0.5\\); yet another hyperparameter one has to think about.\nWhile not regularizing a network can lead to overfitting, choosing a dropout rate or regularization penalty that is too high can lead to under-training the network that struggles to learn the patterns in the data.\n\n\nVanishing Gradients and Dying ReLU\nTraining neural networks with backpropagation computes the objective function on a forward pass through the network—from input layer to output layer—and the gradient on the backward pass—from output to input. The goal is to find values for weights and biases where the objective function has a minimum, a zero gradient. In nonlinear optimizations, the gradient thus naturally approaches zero as the iterations converge to a solution. When the gradient is near zero, the model “stops learning” and the process stops.\n\nVanishing gradients\nThe vanishing gradient problem describes the issue where the gradient values become very small and learning of the network slows down (or stops), simply because the gradients are small, not because we have found a minimum of the objective function. This affects deep networks more than shallow networks and the early layers suffer more from vanishing gradients than deep layers since the gradient is accumulated by moving backwards through the network.\nThe intuition for this phenomenon is that the overall gradient has a certain value, distributed across all the layers. As layers are chaining transformations, the overall derivative is essentially a long chain rule of products. If the values multiplied in this operation are small, the overall product can be tiny—the gradient is numerically vanishing.\nThe vanishing gradient problem was more serious before the discovery of ReLU activation. The use of the sigmoid activation function contributed to vanishing gradients and limited the ability to train networks with many layers. To see why, consider the sigmoid activation \\(\\sigma(x) = 1/(1+\\exp\\{-x\\})\\) and its derivative. Since \\(\\sigma(x)\\) is the c.d.f. of the standard logistic distribution, the derivative is the standard logistic density function \\[\nf(x) = \\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{\\exp^{-x}}{\\left(1+\\exp^{-x} \\right)^2}\n\\]\n\n\n\n\n\n\n\n\nFigure 32.5: Density function of the standard logistic distribution.\n\n\n\n\n\nThe density is symmetric about zero with a max value at zero of \\(1/(1+1)^2 = 1/4\\) (Figure 32.5). Because the gradient of the sigmoid activation function is \\(\\le 0.25\\), repeated multiplication can produce very small numbers.\n\n\nDying ReLU\nWhy does ReLU help with the vanishing gradient problem? It partially helps because the function \\(\\sigma(x) = \\max\\{0,x\\}\\) has derivative \\[\n\\frac{\\partial \\sigma(x)}{\\partial x} = \\left \\{\n\\begin{array}{ll} x & x &gt; 0 \\\\ 0 & x &lt;  0 \\end{array}\n\\right .\n\\] When \\(x &gt; 1\\), multiplying with the gradient increases the product. However, for \\(x \\le 0\\), the gradient is exactly zero and vanishes completely. The situation where many activations are negative, ReLU sets them to zero and essentially drops out the neuron, is known as the dying ReLU problem. Once the linear combinations \\(b^{(t)}_j + \\textbf{w}^{(t)}_j\\textbf{a}^{(t-1)}\\) are mostly in the negative range, the ReLU network cannot recover and dies. A large value for the learning rate will exacerbate this problem as it adjusts the parameter estimates downwards.\nTo address the dying ReLU problem and to help with the vanishing gradient issue, the leaky ReLU activation function has been proposed: \\[\n\\sigma(x) = \\left \\{ \\begin{array}{ll} x & x &gt; 0 \\\\ \\alpha x & x \\le 0\\end{array}\\right .\n\\] Leaky ReLU returns a small negative value when \\(x &lt; 0\\), leading to a non-zero activation and a non-zero (albeit constant, \\(\\alpha\\)) gradient. Values for \\(\\alpha\\) in the range of 0.01 to 0.1 are common.\nRelated activation functions that avoid the dying ReLU issue are the exponential linear unit (ELU) function \\[\n\\sigma(x) = \\left \\{ \\begin{array}{ll} x & x &gt; 0 \\\\ \\alpha (e^x-1) & x \\le 0 \\end{array}\\right .\n\\] and the Gaussian ELU \\[\n\\sigma(x) = x \\,\\Phi(x)\n\\] where \\(\\Phi(x)\\) is the standard Gaussian cumulative distribution function (Figure 32.6).\n\n\n\n\n\n\n\n\nFigure 32.6: Acrivation functions related to rectified linear units (ReLU).\n\n\n\n\n\nAnother approach to minimize the odds of network training dying because of negative values, is to initialize the weights using positive values. Choosing random weights as starting points from a standard Gaussian or other distribution that is symmetric about zero, and a ReLU activation function, can cause many zero activations in early stages of training. Choosing random starting weights from distributions of positive random variables avoids this issue—we are assuming here that the inputs are positive-valued.\n\n\n\nScaling\n\nScaling the input variables is typically done for statistical learning methods that depend on measures of distance (clustering) or where the scale of inputs affects the distribution of variability (principal component analysis). In linear regression scaling the inputs by standardizing or normalizing is not really necessary unless the differences in scales across the inputs create numerical instability. Methods that regularize such as ridge or lasso regression often apply scaling internally to make sure that a common adjustment factor (the regularization penalty) applies equally to all coefficients.\nWhere do artificial neural networks fit in this? Should you consider scaling the input variables when training ANNs?\nThe answer is “Yes, usually you should” for the following reasons:\n\nNeural networks are over-parameterized and very sensitive numerically. Numerical instabilities can throw them off and input variables with different scales is one source of instability that can be avoided.\nThe initial weights and biases are chosen at random, not taking into account the scale of the inputs. In order to get the optimization off well with random starting values, it is highly recommended that the inputs are on a similar scale.\nThe training epochs (iterations) are behaving no worse when the data are scaled. The optimization behavior is frequently better with scaled data.\nOne objection to scaling inputs in regression models is the changing interpretation of the coefficient. You need to know how the data were scaled in order to predict new values, for example. Neural networks are non-interpretable models, the actual values of the weights and biases is not of interest.\n\nThe recommended outcome of scaling inputs for neural networks is that all variables have a common range, and their values should be small, between 0 and 1. This suggests two approaches to scaling, standardizing and normalizing.\nA variable is standardized by subtracting its mean and dividing by its standard deviation: \\[\nx_s = \\frac{x-\\overline{x}}{s_x}\n\\] The resulting variable has arithmetic mean 0 and standard deviation 1.\nA variable is normalized by shifting its range and scaling it to fall between 0 and 1: \\[\nx_n = \\frac{x - \\min\\{x\\}}{\\max\\{x\\} - \\min\\{x\\}}\n\\]\nShould you also scale the output variable? Some recommend it, but I do not. More important than scaling is making sure that the output activation function is chosen properly. For example, if the target variable \\(Y\\) is continuous and takes values \\(-\\infty &lt; Y &lt; \\infty\\), then you want an identity (“linear”) output function, and definitely not a ReLU function which would replace all negative values with 0. If, however, you choose a sigmoid or hyperbolic tangent output function, then scaling the output variable to range from 0–1 prior to training is necessary. To interpret the prediction from the neural network you would have to undo any scaling or normalization after the prediction.\n\n\n\nFigure 32.1: Minimizing a function \\(f(x)\\) in one parameter \\(x\\).\nFigure 32.2: Minimizing a function \\(f(x)\\) in one parameter \\(x\\).\nFigure 32.3: Find solution that minimizes a function in one parameter.\nFigure 32.4: Dropout in input and hidden layer.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Training Neural Networks</span>"
    ]
  },
  {
    "objectID": "ann_R.html",
    "href": "ann_R.html",
    "title": "33  Neural Networks in R (with Keras)",
    "section": "",
    "text": "33.1 Introduction\nWorking with neural networks in R can be a bit challenging. For one, there are many packages available that can train ANNs, see Table 33.1 for some examples. The packages vary greatly in capabilities and syntax.\nSeveral frameworks for ANNs and deep learning exist. TensorFlow, Microsoft CNTK, PyTorch, and Theano are among the most important ones.\nKeras has emerged as an important API (Application Programming Interface) for deep learning. It provides a consistent interface on top of JAX, TensorFlow, or PyTorch. While TensorFlow is very powerful, the learning curve can be steep and you tend to write a lot of code. On the other hand, you have complete control over the types of models you build and train with TensorFlow. That makes Keras so relevant: you can tap into the capabilities of TensorFlow with a simpler API.\nThe drawback of using Keras and other deep learning frameworks in R is that they are written in Python. Tools from the modern machine learning toolbox tend to be written in Python. The keras package in R is not an implementation of Keras in R, it is an R-based API that calls into the Keras Python code. And that code calls into Tensorflow, or whatever deep learning framework Keras is running on.\nTo use keras in R, you thus need to manage a Python distribution, manage Python packages, and deal with the idiosyncrasies of function interfaces between programming languages. For example, you will have to deal with Python error messages bubbling up to the R session. Fortunately, some of the headaches of running Python from R are mitigated by the reticulate package which provides the R interface to Python.\nWe will be using the keras package in R. It uses the TensorFlow framework under the cover by default.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Neural Networks in `R` (with Keras)</span>"
    ]
  },
  {
    "objectID": "ann_R.html#introduction",
    "href": "ann_R.html#introduction",
    "title": "33  Neural Networks in R (with Keras)",
    "section": "",
    "text": "Table 33.1: Some R packages for neural network analysis.\n\n\n\n\n\n\n\n\n\nPackage\nNotes\n\n\n\n\nnnet\nFeed-forward neural networks with a single hidden layer, and for multinomial log-linear models\n\n\nneuralnet\nTraining of neural networks using backpropagation\n\n\ntensorflow\nInterface to TensorFlow, a free and open-source software library for machine learning and artificial intelligence\n\n\ndarch\nDeep architectures and Restricted Boltzmann Machines\n\n\ndeepnet\nDeep learning toolkit\n\n\ndeepr\nStreamlines training, tuning, and predicting for deep learning based on darch and deepnet\n\n\nrnn\nRecurrent Neural Networks (RNN)\n\n\ntorch\nTensors and neural networks with GPU acceleration; similar to Pytorch\n\n\nkeras\nInterface to the Python deep learning library Keras\n\n\nkerasR\nInterface to the Python deep learning library Keras\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe KerasR package is not the same as the keras package in R. Both packages provide an API for Keras and the API of KerasR is closer to the Python syntax. That makes switching between R and Python for deep learning easier. However, the keras package supports piping of operations similar to the dplyr package. I find working with keras simple because neural networks can be build by piping layer definitions. After all, that is how neural networks work: the output of one layer is input to the next layer.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Neural Networks in `R` (with Keras)</span>"
    ]
  },
  {
    "objectID": "ann_R.html#running-keras-in-r",
    "href": "ann_R.html#running-keras-in-r",
    "title": "33  Neural Networks in R (with Keras)",
    "section": "33.2 Running Keras in R",
    "text": "33.2 Running Keras in R\n\nInstallation\nAs mentioned earlier, running keras requires a Python distribution. In addition, you need to install the Keras and TensorFlow Python libraries. The preferred Python installation in this case is conda-based. Good instructions for installing TensorFlow, Keras, and the Python runtime at once—depending on whether you have a prior conda installation—can be found here.\nIn the situation without prior conda installation, these commands will install everything you need (do this once in your environment):\n\ninstall.packages(\"keras\")\nreticulate::install_miniconda()\nkeras::install_keras(method=\"conda\", python_version=\"3.11\")\n\nThen, in an R session that runs keras do the following:\n\nlibrary(keras)\nreticulate::use_condaenv(condaenv = \"r-tensorflow\")\n\nThe \"r-tensorflow\" conda environment was installed during the previous step.\n\n\nKeras Basics\nTraining a neural network with keras involves three steps:\n\nDefining the network\nSetting up the optimization\nFitting the model\n\nNot until the third step does the algorithm get in contact with actual data. However, we need to know some things about the data in order to define the network in step 1: the dimensions of the input and output.\n\nDefining the network\nThe most convenient way of specifying a multi layer neural network is by adding layers sequentially, from the input layer to the output layer. These starts with a call to keras_model_sequential(). Suppose we want to predict a continuous response (regression application) based on inputs \\(x_1, \\cdots, x_{20}\\) with one hidden layer and dropout regularization.\nThe following statements define the model sequentially:\n\nfirstANN &lt;- keras_model_sequential() %&gt;%\n    layer_dense(units      =50, \n                activation =\"relu\",\n                input_shape=20\n                ) %&gt;%\n    layer_dropout(rate=0.4) %&gt;%\n    layer_dense(units=1,\n                name =\"Output\")             \n\nlayer_dense() adds a fully connected layer to the networks, the units= option specifies the number of neurons in the layer. The input_shape= option is specified only for the first layer in the network. In summary, the hidden layer receives 20 inputs and has 50 output units (neurons) and ReLU activation. The output from the hidden layer is passed on (piped) to a dropout layer with a dropout rate of \\(\\phi = 0.4\\). The result of the dropout layer is passed on to another fully connected layer with a single neuron. This is the output layer of the network. In other words, the last layer in the sequence is automatically the output layer. Since we are in a regression context to predict a numeric target variable, there is only one output unit in the final layer. If this was a classification problem with \\(5\\) categories, the last layer would have 5 units.\nYou can assign a name to each layer with the name= option, this makes it easier to identify the layers in output. If you do not specify a name, Keras will assign a name that combines a description of the layer type with a numerical index (not always). The numeric indices can be confusing because they depend on counters internal to the Python code. Assigning an explicit name is recommended practice.\nThe activation= option specifies the activation function \\(\\sigma()\\) for the hidden layers and the output function \\(g()\\) for the output layer. The default is the identity (“linear”) activation, \\(\\sigma(x) = x\\). This default is appropriate for the output layer in a regression application. For the hidden layer we choose the ReLU activation.\nTo see the list of activation functions supported by keras (Keras), type\n\n?keras::acti\n\nat the console prompt.\nThe basic neural network is now defined and we can find out how many parameters it entails.\n\nsummary(firstANN)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 50)                      1050        \n dropout (Dropout)                  (None, 50)                      0           \n Output (Dense)                     (None, 1)                       51          \n================================================================================\nTotal params: 1101 (4.30 KB)\nTrainable params: 1101 (4.30 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nWith 20 inputs and 50 neurons, the first layer has 50 x 21 = 1050 parameters (20 slopes and an intercept for each output neuron). The dropout layer does not add any parameters to the estimation, it chooses output neurons of the previous layer at random and sets their activation to zero. The 50 neurons (some with activation set randomly to zero) are the input to the final layer, adding fifty weights (slopes) and one bias (intercept). The total number of parameters of this neural network is 1,101.\n\n\nSetting up the optimization\nThe second step in training a model in Keras is to specify the particulars of the optimization with the keras::compile() function (which actually calls keras::compile.keras.engine.training.Model). Typical specifications include the loss functions, the type of optimization algorithm, and the metrics evaluated by the model during training.\nThe following function call uses the RMSProp algorithm with mean-squared error loss function to estimate the parameters of the network. During training, the mean absolute error is also monitored in addition to the mean squared error.\n\nfirstANN %&gt;% compile(loss=\"mse\",                         # see keras$losses$\n                     optimizer=optimizer_rmsprop(),      # see keras$optimizers$\n                     metrics=list(\"mean_absolute_error\") # see keras$metrics$\n   )\n\nDepending on your environment, not all optimization algorithms are supported.\n\n\nFitting the model\nThe last step in training the network is to connect the defined and compiled model with training—and possibly test—data.\nFor this example we use the Hitters data from the ISLR2 package. This is a data set with 322 observations of major league baseball players from the 1986 and 1987 seasons. The following code removes observations with missing values from the data frame, defines a vector of ids for the test data (1/3 of the observations) and computes a scaled and centered model matrix using all 20 input variables.\n\nlibrary(ISLR2)\n\nGitters &lt;- na.omit(Hitters)\nn &lt;- nrow(Gitters)\nset.seed(13)\nntest &lt;- trunc(n / 3)\ntestid &lt;- sample(1:n, ntest)\n\nx &lt;- scale(model.matrix(Salary ~ . - 1, data = Gitters))\ny &lt;- Gitters$Salary\n\nNote that the model contains several factors (League, Division, NewLeague) whose levels are encoded as binary variables in the model matrix. One could exclude those from scaling and centering as they already are in the proper range. In a regression model you would not want to scale these variables to preserve the interpretation of their coefficients. In a neural network interpretation of the model coefficients is not important and we include all columns of the model matrix in the scaling operation.\nThe following code fits the model to the training data (-testid) using 20 epochs and a minibatch size of 32. That means the gradient is computed based on 32 randomly chosen observations in each step of the stochastic gradient descent algorithm. Since there are 176 training observations it takes \\(176/32=5.5\\) SGD steps to process all \\(n\\) observations. This is known as an epoch and is akin to the concept of an iteration in numerical optimization: a full pass through the data. The fundamental difference between an epoch and an iteration lies in the fact that updates of the parameters occur after each gradient computation. In a full iteration, there is one update after the pass through the entire data. In SGD with minibatch, there are multiple updates of the parameters, one for each minibatch.\nRunning 200 epochs with a batch size of 32 and a training set size of 176 results in 200 * 5.5 = 1,100 gradient evaluations.\nThe validation_data= option lists the test data for the training. The objective function and metrics specified in the compile command earlier are computed at each epoch for the training and the test data if the latter is specified. If you do not have a validation data set, you can specify validation_split= and request that a fraction of the training data is held back for validation.\n\nhistory &lt;- firstANN %&gt;% \n    fit(x[-testid, ], \n        y[-testid  ], \n        epochs=200, \n        batch_size=32,\n        validation_data=list(x[testid, ], y[testid])\n  )\n\nEpoch 1/200\n6/6 - 0s - loss: 456644.3125 - mean_absolute_error: 533.6446 - val_loss: 555469.6250 - val_mean_absolute_error: 539.4579 - 263ms/epoch - 44ms/step\nEpoch 2/200\n6/6 - 0s - loss: 456274.2812 - mean_absolute_error: 533.2629 - val_loss: 555093.5000 - val_mean_absolute_error: 539.2081 - 20ms/epoch - 3ms/step\nEpoch 3/200\n6/6 - 0s - loss: 455988.0000 - mean_absolute_error: 533.0720 - val_loss: 554726.8750 - val_mean_absolute_error: 538.9792 - 11ms/epoch - 2ms/step\nEpoch 4/200\n6/6 - 0s - loss: 455634.0312 - mean_absolute_error: 532.8795 - val_loss: 554368.1875 - val_mean_absolute_error: 538.7687 - 11ms/epoch - 2ms/step\nEpoch 5/200\n6/6 - 0s - loss: 455208.2188 - mean_absolute_error: 532.6113 - val_loss: 554045.4375 - val_mean_absolute_error: 538.5624 - 11ms/epoch - 2ms/step\nEpoch 6/200\n6/6 - 0s - loss: 455046.8750 - mean_absolute_error: 532.3682 - val_loss: 553678.5000 - val_mean_absolute_error: 538.3367 - 11ms/epoch - 2ms/step\nEpoch 7/200\n6/6 - 0s - loss: 454556.2188 - mean_absolute_error: 532.1656 - val_loss: 553284.1875 - val_mean_absolute_error: 538.0997 - 10ms/epoch - 2ms/step\nEpoch 8/200\n6/6 - 0s - loss: 454383.5312 - mean_absolute_error: 531.9534 - val_loss: 552913.5625 - val_mean_absolute_error: 537.8807 - 10ms/epoch - 2ms/step\nEpoch 9/200\n6/6 - 0s - loss: 453955.6875 - mean_absolute_error: 531.7089 - val_loss: 552529.8125 - val_mean_absolute_error: 537.6392 - 11ms/epoch - 2ms/step\nEpoch 10/200\n6/6 - 0s - loss: 453795.3125 - mean_absolute_error: 531.5660 - val_loss: 552128.8125 - val_mean_absolute_error: 537.3878 - 10ms/epoch - 2ms/step\nEpoch 11/200\n6/6 - 0s - loss: 453459.3750 - mean_absolute_error: 531.2780 - val_loss: 551685.2500 - val_mean_absolute_error: 537.1358 - 11ms/epoch - 2ms/step\nEpoch 12/200\n6/6 - 0s - loss: 452782.4062 - mean_absolute_error: 530.8950 - val_loss: 551258.4375 - val_mean_absolute_error: 536.8865 - 10ms/epoch - 2ms/step\nEpoch 13/200\n6/6 - 0s - loss: 452211.9062 - mean_absolute_error: 530.5004 - val_loss: 550771.5000 - val_mean_absolute_error: 536.6090 - 10ms/epoch - 2ms/step\nEpoch 14/200\n6/6 - 0s - loss: 452086.9688 - mean_absolute_error: 530.3961 - val_loss: 550325.3125 - val_mean_absolute_error: 536.3457 - 11ms/epoch - 2ms/step\nEpoch 15/200\n6/6 - 0s - loss: 451780.9688 - mean_absolute_error: 530.2096 - val_loss: 549882.6875 - val_mean_absolute_error: 536.0836 - 11ms/epoch - 2ms/step\nEpoch 16/200\n6/6 - 0s - loss: 451434.9688 - mean_absolute_error: 529.9330 - val_loss: 549423.6250 - val_mean_absolute_error: 535.8117 - 10ms/epoch - 2ms/step\nEpoch 17/200\n6/6 - 0s - loss: 451137.6250 - mean_absolute_error: 529.7032 - val_loss: 548948.1250 - val_mean_absolute_error: 535.5374 - 11ms/epoch - 2ms/step\nEpoch 18/200\n6/6 - 0s - loss: 450326.1875 - mean_absolute_error: 529.1047 - val_loss: 548407.7500 - val_mean_absolute_error: 535.2188 - 11ms/epoch - 2ms/step\nEpoch 19/200\n6/6 - 0s - loss: 450154.8125 - mean_absolute_error: 528.9894 - val_loss: 547916.4375 - val_mean_absolute_error: 534.9429 - 16ms/epoch - 3ms/step\nEpoch 20/200\n6/6 - 0s - loss: 449784.0312 - mean_absolute_error: 528.7288 - val_loss: 547401.0000 - val_mean_absolute_error: 534.6498 - 11ms/epoch - 2ms/step\nEpoch 21/200\n6/6 - 0s - loss: 449432.6875 - mean_absolute_error: 528.5400 - val_loss: 546862.8750 - val_mean_absolute_error: 534.3491 - 12ms/epoch - 2ms/step\nEpoch 22/200\n6/6 - 0s - loss: 448948.9062 - mean_absolute_error: 528.1223 - val_loss: 546280.4375 - val_mean_absolute_error: 534.0266 - 11ms/epoch - 2ms/step\nEpoch 23/200\n6/6 - 0s - loss: 448688.6875 - mean_absolute_error: 527.9073 - val_loss: 545705.7500 - val_mean_absolute_error: 533.7067 - 10ms/epoch - 2ms/step\nEpoch 24/200\n6/6 - 0s - loss: 447713.2812 - mean_absolute_error: 527.3818 - val_loss: 545081.3125 - val_mean_absolute_error: 533.3549 - 10ms/epoch - 2ms/step\nEpoch 25/200\n6/6 - 0s - loss: 447450.4688 - mean_absolute_error: 527.0002 - val_loss: 544509.8125 - val_mean_absolute_error: 533.0328 - 10ms/epoch - 2ms/step\nEpoch 26/200\n6/6 - 0s - loss: 447047.0938 - mean_absolute_error: 526.9117 - val_loss: 543881.6250 - val_mean_absolute_error: 532.6888 - 10ms/epoch - 2ms/step\nEpoch 27/200\n6/6 - 0s - loss: 446562.5312 - mean_absolute_error: 526.4583 - val_loss: 543245.2500 - val_mean_absolute_error: 532.3515 - 10ms/epoch - 2ms/step\nEpoch 28/200\n6/6 - 0s - loss: 446060.0000 - mean_absolute_error: 526.2692 - val_loss: 542563.5625 - val_mean_absolute_error: 531.9918 - 10ms/epoch - 2ms/step\nEpoch 29/200\n6/6 - 0s - loss: 445532.0938 - mean_absolute_error: 525.9401 - val_loss: 541930.6250 - val_mean_absolute_error: 531.6383 - 10ms/epoch - 2ms/step\nEpoch 30/200\n6/6 - 0s - loss: 444698.9062 - mean_absolute_error: 525.2236 - val_loss: 541256.9375 - val_mean_absolute_error: 531.2643 - 10ms/epoch - 2ms/step\nEpoch 31/200\n6/6 - 0s - loss: 444136.6875 - mean_absolute_error: 524.9214 - val_loss: 540526.0625 - val_mean_absolute_error: 530.8729 - 10ms/epoch - 2ms/step\nEpoch 32/200\n6/6 - 0s - loss: 444270.8125 - mean_absolute_error: 524.8728 - val_loss: 539844.1250 - val_mean_absolute_error: 530.5034 - 10ms/epoch - 2ms/step\nEpoch 33/200\n6/6 - 0s - loss: 443296.9062 - mean_absolute_error: 524.3030 - val_loss: 539086.7500 - val_mean_absolute_error: 530.0966 - 10ms/epoch - 2ms/step\nEpoch 34/200\n6/6 - 0s - loss: 442325.0938 - mean_absolute_error: 523.7197 - val_loss: 538365.1250 - val_mean_absolute_error: 529.7025 - 10ms/epoch - 2ms/step\nEpoch 35/200\n6/6 - 0s - loss: 441541.7188 - mean_absolute_error: 523.2784 - val_loss: 537569.1875 - val_mean_absolute_error: 529.2838 - 10ms/epoch - 2ms/step\nEpoch 36/200\n6/6 - 0s - loss: 440672.6250 - mean_absolute_error: 522.8299 - val_loss: 536784.2500 - val_mean_absolute_error: 528.8569 - 10ms/epoch - 2ms/step\nEpoch 37/200\n6/6 - 0s - loss: 441222.6250 - mean_absolute_error: 522.7372 - val_loss: 535995.7500 - val_mean_absolute_error: 528.4432 - 10ms/epoch - 2ms/step\nEpoch 38/200\n6/6 - 0s - loss: 440262.2812 - mean_absolute_error: 522.2713 - val_loss: 535223.4375 - val_mean_absolute_error: 528.0310 - 10ms/epoch - 2ms/step\nEpoch 39/200\n6/6 - 0s - loss: 439796.3750 - mean_absolute_error: 522.1567 - val_loss: 534396.2500 - val_mean_absolute_error: 527.5944 - 10ms/epoch - 2ms/step\nEpoch 40/200\n6/6 - 0s - loss: 439178.4062 - mean_absolute_error: 521.5406 - val_loss: 533610.6875 - val_mean_absolute_error: 527.1760 - 10ms/epoch - 2ms/step\nEpoch 41/200\n6/6 - 0s - loss: 437975.5000 - mean_absolute_error: 521.0745 - val_loss: 532743.8125 - val_mean_absolute_error: 526.7158 - 10ms/epoch - 2ms/step\nEpoch 42/200\n6/6 - 0s - loss: 436836.3750 - mean_absolute_error: 520.1055 - val_loss: 531803.8750 - val_mean_absolute_error: 526.2131 - 10ms/epoch - 2ms/step\nEpoch 43/200\n6/6 - 0s - loss: 436418.0000 - mean_absolute_error: 519.7546 - val_loss: 530965.0000 - val_mean_absolute_error: 525.7685 - 10ms/epoch - 2ms/step\nEpoch 44/200\n6/6 - 0s - loss: 437076.6250 - mean_absolute_error: 519.8438 - val_loss: 530090.1875 - val_mean_absolute_error: 525.3110 - 10ms/epoch - 2ms/step\nEpoch 45/200\n6/6 - 0s - loss: 434892.5312 - mean_absolute_error: 518.8295 - val_loss: 529201.4375 - val_mean_absolute_error: 524.8325 - 10ms/epoch - 2ms/step\nEpoch 46/200\n6/6 - 0s - loss: 434493.4688 - mean_absolute_error: 518.3510 - val_loss: 528259.0000 - val_mean_absolute_error: 524.3165 - 9ms/epoch - 2ms/step\nEpoch 47/200\n6/6 - 0s - loss: 433817.7188 - mean_absolute_error: 518.3179 - val_loss: 527280.3125 - val_mean_absolute_error: 523.7927 - 10ms/epoch - 2ms/step\nEpoch 48/200\n6/6 - 0s - loss: 432596.0312 - mean_absolute_error: 517.2834 - val_loss: 526338.8125 - val_mean_absolute_error: 523.2893 - 10ms/epoch - 2ms/step\nEpoch 49/200\n6/6 - 0s - loss: 433136.7812 - mean_absolute_error: 517.4589 - val_loss: 525377.0000 - val_mean_absolute_error: 522.7854 - 10ms/epoch - 2ms/step\nEpoch 50/200\n6/6 - 0s - loss: 430943.4688 - mean_absolute_error: 516.1446 - val_loss: 524365.3125 - val_mean_absolute_error: 522.2357 - 10ms/epoch - 2ms/step\nEpoch 51/200\n6/6 - 0s - loss: 429647.9688 - mean_absolute_error: 515.5338 - val_loss: 523343.4062 - val_mean_absolute_error: 521.6976 - 10ms/epoch - 2ms/step\nEpoch 52/200\n6/6 - 0s - loss: 428765.0000 - mean_absolute_error: 515.0578 - val_loss: 522298.0312 - val_mean_absolute_error: 521.1383 - 9ms/epoch - 2ms/step\nEpoch 53/200\n6/6 - 0s - loss: 428803.5312 - mean_absolute_error: 514.4788 - val_loss: 521230.1562 - val_mean_absolute_error: 520.5703 - 9ms/epoch - 2ms/step\nEpoch 54/200\n6/6 - 0s - loss: 427674.2812 - mean_absolute_error: 514.3080 - val_loss: 520108.2812 - val_mean_absolute_error: 519.9930 - 10ms/epoch - 2ms/step\nEpoch 55/200\n6/6 - 0s - loss: 428169.4062 - mean_absolute_error: 514.2058 - val_loss: 519070.8125 - val_mean_absolute_error: 519.4458 - 64ms/epoch - 11ms/step\nEpoch 56/200\n6/6 - 0s - loss: 425872.2812 - mean_absolute_error: 512.4800 - val_loss: 517896.4688 - val_mean_absolute_error: 518.8331 - 10ms/epoch - 2ms/step\nEpoch 57/200\n6/6 - 0s - loss: 425690.3750 - mean_absolute_error: 512.1982 - val_loss: 516716.1875 - val_mean_absolute_error: 518.2101 - 10ms/epoch - 2ms/step\nEpoch 58/200\n6/6 - 0s - loss: 423711.7812 - mean_absolute_error: 511.0369 - val_loss: 515623.0312 - val_mean_absolute_error: 517.6277 - 10ms/epoch - 2ms/step\nEpoch 59/200\n6/6 - 0s - loss: 424647.8125 - mean_absolute_error: 511.3729 - val_loss: 514487.2500 - val_mean_absolute_error: 517.0150 - 9ms/epoch - 2ms/step\nEpoch 60/200\n6/6 - 0s - loss: 424362.2812 - mean_absolute_error: 511.1488 - val_loss: 513286.1562 - val_mean_absolute_error: 516.3828 - 10ms/epoch - 2ms/step\nEpoch 61/200\n6/6 - 0s - loss: 421936.0000 - mean_absolute_error: 510.3900 - val_loss: 511984.0000 - val_mean_absolute_error: 515.7080 - 10ms/epoch - 2ms/step\nEpoch 62/200\n6/6 - 0s - loss: 420956.7812 - mean_absolute_error: 509.2412 - val_loss: 510848.7812 - val_mean_absolute_error: 515.0955 - 10ms/epoch - 2ms/step\nEpoch 63/200\n6/6 - 0s - loss: 418952.9062 - mean_absolute_error: 508.1946 - val_loss: 509515.5312 - val_mean_absolute_error: 514.4024 - 10ms/epoch - 2ms/step\nEpoch 64/200\n6/6 - 0s - loss: 418977.1875 - mean_absolute_error: 508.0709 - val_loss: 508330.6562 - val_mean_absolute_error: 513.7665 - 10ms/epoch - 2ms/step\nEpoch 65/200\n6/6 - 0s - loss: 415651.3750 - mean_absolute_error: 506.1685 - val_loss: 507006.3125 - val_mean_absolute_error: 513.0660 - 10ms/epoch - 2ms/step\nEpoch 66/200\n6/6 - 0s - loss: 416594.0312 - mean_absolute_error: 506.5369 - val_loss: 505734.5312 - val_mean_absolute_error: 512.3857 - 9ms/epoch - 2ms/step\nEpoch 67/200\n6/6 - 0s - loss: 417022.1250 - mean_absolute_error: 506.5833 - val_loss: 504509.5938 - val_mean_absolute_error: 511.7219 - 9ms/epoch - 2ms/step\nEpoch 68/200\n6/6 - 0s - loss: 414754.8750 - mean_absolute_error: 504.6824 - val_loss: 503255.5938 - val_mean_absolute_error: 511.0450 - 10ms/epoch - 2ms/step\nEpoch 69/200\n6/6 - 0s - loss: 415340.9062 - mean_absolute_error: 504.4189 - val_loss: 501874.1562 - val_mean_absolute_error: 510.3019 - 10ms/epoch - 2ms/step\nEpoch 70/200\n6/6 - 0s - loss: 412581.3750 - mean_absolute_error: 503.5916 - val_loss: 500563.8750 - val_mean_absolute_error: 509.6007 - 9ms/epoch - 2ms/step\nEpoch 71/200\n6/6 - 0s - loss: 410940.7812 - mean_absolute_error: 502.6407 - val_loss: 499051.7812 - val_mean_absolute_error: 508.8033 - 10ms/epoch - 2ms/step\nEpoch 72/200\n6/6 - 0s - loss: 409193.2812 - mean_absolute_error: 500.7315 - val_loss: 497690.7500 - val_mean_absolute_error: 508.0681 - 11ms/epoch - 2ms/step\nEpoch 73/200\n6/6 - 0s - loss: 410159.7812 - mean_absolute_error: 501.5289 - val_loss: 496370.2188 - val_mean_absolute_error: 507.3464 - 10ms/epoch - 2ms/step\nEpoch 74/200\n6/6 - 0s - loss: 407610.6250 - mean_absolute_error: 500.2912 - val_loss: 494987.0000 - val_mean_absolute_error: 506.5854 - 10ms/epoch - 2ms/step\nEpoch 75/200\n6/6 - 0s - loss: 406913.8750 - mean_absolute_error: 499.4990 - val_loss: 493696.3125 - val_mean_absolute_error: 505.8684 - 10ms/epoch - 2ms/step\nEpoch 76/200\n6/6 - 0s - loss: 404522.8750 - mean_absolute_error: 498.3459 - val_loss: 492275.9688 - val_mean_absolute_error: 505.0986 - 10ms/epoch - 2ms/step\nEpoch 77/200\n6/6 - 0s - loss: 403959.8125 - mean_absolute_error: 497.4801 - val_loss: 490779.1250 - val_mean_absolute_error: 504.2982 - 10ms/epoch - 2ms/step\nEpoch 78/200\n6/6 - 0s - loss: 404353.0000 - mean_absolute_error: 498.2560 - val_loss: 489325.8750 - val_mean_absolute_error: 503.5041 - 10ms/epoch - 2ms/step\nEpoch 79/200\n6/6 - 0s - loss: 402789.0938 - mean_absolute_error: 496.6668 - val_loss: 487815.4062 - val_mean_absolute_error: 502.6926 - 10ms/epoch - 2ms/step\nEpoch 80/200\n6/6 - 0s - loss: 401743.3125 - mean_absolute_error: 495.6089 - val_loss: 486197.5312 - val_mean_absolute_error: 501.8144 - 10ms/epoch - 2ms/step\nEpoch 81/200\n6/6 - 0s - loss: 400386.0000 - mean_absolute_error: 495.0092 - val_loss: 484757.7500 - val_mean_absolute_error: 501.0257 - 10ms/epoch - 2ms/step\nEpoch 82/200\n6/6 - 0s - loss: 398416.8750 - mean_absolute_error: 494.0239 - val_loss: 483213.6875 - val_mean_absolute_error: 500.1781 - 9ms/epoch - 2ms/step\nEpoch 83/200\n6/6 - 0s - loss: 400260.4688 - mean_absolute_error: 494.4417 - val_loss: 481686.8438 - val_mean_absolute_error: 499.3454 - 10ms/epoch - 2ms/step\nEpoch 84/200\n6/6 - 0s - loss: 396913.7188 - mean_absolute_error: 493.0079 - val_loss: 479968.7500 - val_mean_absolute_error: 498.4182 - 10ms/epoch - 2ms/step\nEpoch 85/200\n6/6 - 0s - loss: 395587.4062 - mean_absolute_error: 491.3870 - val_loss: 478480.2188 - val_mean_absolute_error: 497.5660 - 10ms/epoch - 2ms/step\nEpoch 86/200\n6/6 - 0s - loss: 392744.7188 - mean_absolute_error: 490.4919 - val_loss: 476964.8750 - val_mean_absolute_error: 496.7025 - 10ms/epoch - 2ms/step\nEpoch 87/200\n6/6 - 0s - loss: 392498.8125 - mean_absolute_error: 490.1336 - val_loss: 475318.3438 - val_mean_absolute_error: 495.7944 - 10ms/epoch - 2ms/step\nEpoch 88/200\n6/6 - 0s - loss: 392017.7188 - mean_absolute_error: 489.3461 - val_loss: 473643.5938 - val_mean_absolute_error: 494.8808 - 9ms/epoch - 2ms/step\nEpoch 89/200\n6/6 - 0s - loss: 387784.6875 - mean_absolute_error: 486.8877 - val_loss: 472091.4375 - val_mean_absolute_error: 494.0017 - 10ms/epoch - 2ms/step\nEpoch 90/200\n6/6 - 0s - loss: 390133.9062 - mean_absolute_error: 486.7715 - val_loss: 470505.9688 - val_mean_absolute_error: 493.1135 - 10ms/epoch - 2ms/step\nEpoch 91/200\n6/6 - 0s - loss: 386896.3125 - mean_absolute_error: 485.6911 - val_loss: 468860.3125 - val_mean_absolute_error: 492.1971 - 9ms/epoch - 2ms/step\nEpoch 92/200\n6/6 - 0s - loss: 387669.5312 - mean_absolute_error: 485.9102 - val_loss: 467151.0312 - val_mean_absolute_error: 491.2397 - 9ms/epoch - 2ms/step\nEpoch 93/200\n6/6 - 0s - loss: 387649.3750 - mean_absolute_error: 484.8438 - val_loss: 465411.6875 - val_mean_absolute_error: 490.2650 - 10ms/epoch - 2ms/step\nEpoch 94/200\n6/6 - 0s - loss: 382191.8750 - mean_absolute_error: 482.7047 - val_loss: 463663.1875 - val_mean_absolute_error: 489.2798 - 10ms/epoch - 2ms/step\nEpoch 95/200\n6/6 - 0s - loss: 382860.5312 - mean_absolute_error: 481.1545 - val_loss: 461929.9688 - val_mean_absolute_error: 488.2949 - 10ms/epoch - 2ms/step\nEpoch 96/200\n6/6 - 0s - loss: 381303.9062 - mean_absolute_error: 480.8574 - val_loss: 460257.6562 - val_mean_absolute_error: 487.3340 - 9ms/epoch - 2ms/step\nEpoch 97/200\n6/6 - 0s - loss: 381713.0000 - mean_absolute_error: 480.9691 - val_loss: 458482.9375 - val_mean_absolute_error: 486.3263 - 9ms/epoch - 2ms/step\nEpoch 98/200\n6/6 - 0s - loss: 378838.9688 - mean_absolute_error: 479.6269 - val_loss: 456783.9062 - val_mean_absolute_error: 485.3534 - 9ms/epoch - 2ms/step\nEpoch 99/200\n6/6 - 0s - loss: 376422.6250 - mean_absolute_error: 477.5639 - val_loss: 454927.5312 - val_mean_absolute_error: 484.2967 - 9ms/epoch - 2ms/step\nEpoch 100/200\n6/6 - 0s - loss: 378605.8125 - mean_absolute_error: 478.2555 - val_loss: 453063.5000 - val_mean_absolute_error: 483.2385 - 10ms/epoch - 2ms/step\nEpoch 101/200\n6/6 - 0s - loss: 374899.7500 - mean_absolute_error: 475.6619 - val_loss: 451339.6875 - val_mean_absolute_error: 482.2365 - 9ms/epoch - 2ms/step\nEpoch 102/200\n6/6 - 0s - loss: 373776.5312 - mean_absolute_error: 475.2396 - val_loss: 449642.3125 - val_mean_absolute_error: 481.2584 - 10ms/epoch - 2ms/step\nEpoch 103/200\n6/6 - 0s - loss: 372836.2812 - mean_absolute_error: 474.2115 - val_loss: 447913.9375 - val_mean_absolute_error: 480.2423 - 9ms/epoch - 2ms/step\nEpoch 104/200\n6/6 - 0s - loss: 366000.3750 - mean_absolute_error: 470.0787 - val_loss: 445930.9375 - val_mean_absolute_error: 479.1044 - 10ms/epoch - 2ms/step\nEpoch 105/200\n6/6 - 0s - loss: 370793.4062 - mean_absolute_error: 472.7267 - val_loss: 444182.0625 - val_mean_absolute_error: 478.0664 - 9ms/epoch - 2ms/step\nEpoch 106/200\n6/6 - 0s - loss: 365146.8750 - mean_absolute_error: 470.1920 - val_loss: 442342.6250 - val_mean_absolute_error: 476.9896 - 9ms/epoch - 2ms/step\nEpoch 107/200\n6/6 - 0s - loss: 368017.9375 - mean_absolute_error: 471.6598 - val_loss: 440457.2500 - val_mean_absolute_error: 475.8891 - 9ms/epoch - 2ms/step\nEpoch 108/200\n6/6 - 0s - loss: 361826.0938 - mean_absolute_error: 467.7759 - val_loss: 438519.0938 - val_mean_absolute_error: 474.7636 - 9ms/epoch - 2ms/step\nEpoch 109/200\n6/6 - 0s - loss: 363906.5938 - mean_absolute_error: 468.1679 - val_loss: 436697.5938 - val_mean_absolute_error: 473.6639 - 9ms/epoch - 2ms/step\nEpoch 110/200\n6/6 - 0s - loss: 359955.9062 - mean_absolute_error: 464.7954 - val_loss: 434768.9062 - val_mean_absolute_error: 472.5248 - 10ms/epoch - 2ms/step\nEpoch 111/200\n6/6 - 0s - loss: 360684.8438 - mean_absolute_error: 465.4165 - val_loss: 432919.4375 - val_mean_absolute_error: 471.4290 - 9ms/epoch - 2ms/step\nEpoch 112/200\n6/6 - 0s - loss: 358331.5938 - mean_absolute_error: 464.6687 - val_loss: 430994.5625 - val_mean_absolute_error: 470.2858 - 10ms/epoch - 2ms/step\nEpoch 113/200\n6/6 - 0s - loss: 358840.0938 - mean_absolute_error: 464.1562 - val_loss: 428903.0938 - val_mean_absolute_error: 469.0525 - 10ms/epoch - 2ms/step\nEpoch 114/200\n6/6 - 0s - loss: 356875.8750 - mean_absolute_error: 462.7935 - val_loss: 427019.9688 - val_mean_absolute_error: 467.9274 - 9ms/epoch - 2ms/step\nEpoch 115/200\n6/6 - 0s - loss: 356757.6250 - mean_absolute_error: 462.2954 - val_loss: 425031.7812 - val_mean_absolute_error: 466.7268 - 9ms/epoch - 2ms/step\nEpoch 116/200\n6/6 - 0s - loss: 348880.6562 - mean_absolute_error: 458.7281 - val_loss: 423059.3438 - val_mean_absolute_error: 465.5396 - 9ms/epoch - 2ms/step\nEpoch 117/200\n6/6 - 0s - loss: 349502.8438 - mean_absolute_error: 458.5204 - val_loss: 421071.1875 - val_mean_absolute_error: 464.3333 - 9ms/epoch - 2ms/step\nEpoch 118/200\n6/6 - 0s - loss: 349405.7812 - mean_absolute_error: 457.3466 - val_loss: 419230.1562 - val_mean_absolute_error: 463.1929 - 9ms/epoch - 2ms/step\nEpoch 119/200\n6/6 - 0s - loss: 348566.9688 - mean_absolute_error: 455.3513 - val_loss: 417374.1562 - val_mean_absolute_error: 462.0520 - 9ms/epoch - 2ms/step\nEpoch 120/200\n6/6 - 0s - loss: 347633.7188 - mean_absolute_error: 456.4008 - val_loss: 415386.0625 - val_mean_absolute_error: 460.8297 - 9ms/epoch - 2ms/step\nEpoch 121/200\n6/6 - 0s - loss: 343844.3750 - mean_absolute_error: 452.6172 - val_loss: 413391.1250 - val_mean_absolute_error: 459.5986 - 10ms/epoch - 2ms/step\nEpoch 122/200\n6/6 - 0s - loss: 342819.1875 - mean_absolute_error: 453.1965 - val_loss: 411323.5938 - val_mean_absolute_error: 458.3265 - 9ms/epoch - 2ms/step\nEpoch 123/200\n6/6 - 0s - loss: 343585.2812 - mean_absolute_error: 453.6905 - val_loss: 409303.6250 - val_mean_absolute_error: 457.0757 - 9ms/epoch - 2ms/step\nEpoch 124/200\n6/6 - 0s - loss: 341864.0938 - mean_absolute_error: 451.3810 - val_loss: 407241.3750 - val_mean_absolute_error: 455.7898 - 10ms/epoch - 2ms/step\nEpoch 125/200\n6/6 - 0s - loss: 334973.1250 - mean_absolute_error: 448.6094 - val_loss: 405171.1250 - val_mean_absolute_error: 454.5045 - 11ms/epoch - 2ms/step\nEpoch 126/200\n6/6 - 0s - loss: 335985.6562 - mean_absolute_error: 447.1190 - val_loss: 403038.8438 - val_mean_absolute_error: 453.1833 - 10ms/epoch - 2ms/step\nEpoch 127/200\n6/6 - 0s - loss: 339201.1250 - mean_absolute_error: 448.4805 - val_loss: 401184.8125 - val_mean_absolute_error: 452.0023 - 9ms/epoch - 2ms/step\nEpoch 128/200\n6/6 - 0s - loss: 332001.9688 - mean_absolute_error: 445.9047 - val_loss: 399020.8125 - val_mean_absolute_error: 450.6295 - 9ms/epoch - 2ms/step\nEpoch 129/200\n6/6 - 0s - loss: 328798.6875 - mean_absolute_error: 442.4619 - val_loss: 396960.7500 - val_mean_absolute_error: 449.3177 - 9ms/epoch - 2ms/step\nEpoch 130/200\n6/6 - 0s - loss: 331367.4062 - mean_absolute_error: 443.6797 - val_loss: 394891.6875 - val_mean_absolute_error: 447.9893 - 9ms/epoch - 2ms/step\nEpoch 131/200\n6/6 - 0s - loss: 331565.2500 - mean_absolute_error: 443.3186 - val_loss: 392863.0000 - val_mean_absolute_error: 446.7002 - 9ms/epoch - 2ms/step\nEpoch 132/200\n6/6 - 0s - loss: 325860.7500 - mean_absolute_error: 441.0396 - val_loss: 390750.9062 - val_mean_absolute_error: 445.3518 - 9ms/epoch - 2ms/step\nEpoch 133/200\n6/6 - 0s - loss: 320349.6250 - mean_absolute_error: 436.7061 - val_loss: 388453.5938 - val_mean_absolute_error: 443.8797 - 10ms/epoch - 2ms/step\nEpoch 134/200\n6/6 - 0s - loss: 328350.3750 - mean_absolute_error: 440.4363 - val_loss: 386503.0000 - val_mean_absolute_error: 442.5907 - 10ms/epoch - 2ms/step\nEpoch 135/200\n6/6 - 0s - loss: 319764.4062 - mean_absolute_error: 435.7116 - val_loss: 384296.9062 - val_mean_absolute_error: 441.1268 - 10ms/epoch - 2ms/step\nEpoch 136/200\n6/6 - 0s - loss: 326027.1250 - mean_absolute_error: 436.6479 - val_loss: 382365.6562 - val_mean_absolute_error: 440.0406 - 10ms/epoch - 2ms/step\nEpoch 137/200\n6/6 - 0s - loss: 322509.3750 - mean_absolute_error: 436.4805 - val_loss: 380191.2188 - val_mean_absolute_error: 438.7986 - 10ms/epoch - 2ms/step\nEpoch 138/200\n6/6 - 0s - loss: 319210.7188 - mean_absolute_error: 435.2621 - val_loss: 378283.0938 - val_mean_absolute_error: 437.6879 - 9ms/epoch - 2ms/step\nEpoch 139/200\n6/6 - 0s - loss: 317143.3750 - mean_absolute_error: 434.3148 - val_loss: 376271.4375 - val_mean_absolute_error: 436.5038 - 9ms/epoch - 2ms/step\nEpoch 140/200\n6/6 - 0s - loss: 316929.8750 - mean_absolute_error: 432.5716 - val_loss: 374119.3125 - val_mean_absolute_error: 435.2233 - 9ms/epoch - 2ms/step\nEpoch 141/200\n6/6 - 0s - loss: 311882.1250 - mean_absolute_error: 428.2838 - val_loss: 371897.3438 - val_mean_absolute_error: 433.9341 - 10ms/epoch - 2ms/step\nEpoch 142/200\n6/6 - 0s - loss: 309038.9062 - mean_absolute_error: 427.7454 - val_loss: 369870.6562 - val_mean_absolute_error: 432.7551 - 10ms/epoch - 2ms/step\nEpoch 143/200\n6/6 - 0s - loss: 309652.2188 - mean_absolute_error: 426.7080 - val_loss: 367828.5625 - val_mean_absolute_error: 431.5538 - 10ms/epoch - 2ms/step\nEpoch 144/200\n6/6 - 0s - loss: 310778.8750 - mean_absolute_error: 426.5231 - val_loss: 365748.3750 - val_mean_absolute_error: 430.3279 - 10ms/epoch - 2ms/step\nEpoch 145/200\n6/6 - 0s - loss: 304751.3125 - mean_absolute_error: 422.4782 - val_loss: 363634.5625 - val_mean_absolute_error: 429.0683 - 10ms/epoch - 2ms/step\nEpoch 146/200\n6/6 - 0s - loss: 303244.8125 - mean_absolute_error: 423.4715 - val_loss: 361576.2500 - val_mean_absolute_error: 427.8094 - 10ms/epoch - 2ms/step\nEpoch 147/200\n6/6 - 0s - loss: 306080.0938 - mean_absolute_error: 422.6278 - val_loss: 359597.9375 - val_mean_absolute_error: 426.6024 - 10ms/epoch - 2ms/step\nEpoch 148/200\n6/6 - 0s - loss: 302660.9688 - mean_absolute_error: 420.5752 - val_loss: 357387.2500 - val_mean_absolute_error: 425.2550 - 10ms/epoch - 2ms/step\nEpoch 149/200\n6/6 - 0s - loss: 295895.8125 - mean_absolute_error: 418.4938 - val_loss: 355263.5000 - val_mean_absolute_error: 423.9625 - 10ms/epoch - 2ms/step\nEpoch 150/200\n6/6 - 0s - loss: 296861.7812 - mean_absolute_error: 416.0020 - val_loss: 353230.3125 - val_mean_absolute_error: 422.6977 - 10ms/epoch - 2ms/step\nEpoch 151/200\n6/6 - 0s - loss: 297236.4375 - mean_absolute_error: 416.7293 - val_loss: 350989.6875 - val_mean_absolute_error: 421.3322 - 10ms/epoch - 2ms/step\nEpoch 152/200\n6/6 - 0s - loss: 301399.8125 - mean_absolute_error: 419.4023 - val_loss: 348852.6562 - val_mean_absolute_error: 419.9969 - 10ms/epoch - 2ms/step\nEpoch 153/200\n6/6 - 0s - loss: 293748.0312 - mean_absolute_error: 412.3866 - val_loss: 346809.9688 - val_mean_absolute_error: 418.7117 - 10ms/epoch - 2ms/step\nEpoch 154/200\n6/6 - 0s - loss: 289263.0938 - mean_absolute_error: 409.7946 - val_loss: 344646.0000 - val_mean_absolute_error: 417.3722 - 11ms/epoch - 2ms/step\nEpoch 155/200\n6/6 - 0s - loss: 293611.9688 - mean_absolute_error: 412.7209 - val_loss: 342497.7812 - val_mean_absolute_error: 416.0128 - 10ms/epoch - 2ms/step\nEpoch 156/200\n6/6 - 0s - loss: 289486.7188 - mean_absolute_error: 410.0717 - val_loss: 340417.2500 - val_mean_absolute_error: 414.6617 - 10ms/epoch - 2ms/step\nEpoch 157/200\n6/6 - 0s - loss: 288352.2188 - mean_absolute_error: 406.5334 - val_loss: 338330.3438 - val_mean_absolute_error: 413.3247 - 10ms/epoch - 2ms/step\nEpoch 158/200\n6/6 - 0s - loss: 289687.1250 - mean_absolute_error: 412.4028 - val_loss: 336187.2188 - val_mean_absolute_error: 411.9449 - 10ms/epoch - 2ms/step\nEpoch 159/200\n6/6 - 0s - loss: 285013.2500 - mean_absolute_error: 407.3428 - val_loss: 333998.4062 - val_mean_absolute_error: 410.5549 - 10ms/epoch - 2ms/step\nEpoch 160/200\n6/6 - 0s - loss: 282929.3438 - mean_absolute_error: 404.3928 - val_loss: 331800.0000 - val_mean_absolute_error: 409.1444 - 10ms/epoch - 2ms/step\nEpoch 161/200\n6/6 - 0s - loss: 281475.2812 - mean_absolute_error: 402.7773 - val_loss: 329743.0938 - val_mean_absolute_error: 407.7829 - 10ms/epoch - 2ms/step\nEpoch 162/200\n6/6 - 0s - loss: 276525.9062 - mean_absolute_error: 398.6386 - val_loss: 327572.3438 - val_mean_absolute_error: 406.3618 - 10ms/epoch - 2ms/step\nEpoch 163/200\n6/6 - 0s - loss: 281000.4062 - mean_absolute_error: 399.4973 - val_loss: 325275.8750 - val_mean_absolute_error: 404.8635 - 10ms/epoch - 2ms/step\nEpoch 164/200\n6/6 - 0s - loss: 272824.2812 - mean_absolute_error: 399.0277 - val_loss: 323102.1562 - val_mean_absolute_error: 403.4183 - 10ms/epoch - 2ms/step\nEpoch 165/200\n6/6 - 0s - loss: 270419.0938 - mean_absolute_error: 391.8975 - val_loss: 321030.3125 - val_mean_absolute_error: 401.9980 - 10ms/epoch - 2ms/step\nEpoch 166/200\n6/6 - 0s - loss: 272433.0938 - mean_absolute_error: 396.0275 - val_loss: 318798.5312 - val_mean_absolute_error: 400.5170 - 9ms/epoch - 2ms/step\nEpoch 167/200\n6/6 - 0s - loss: 271191.6875 - mean_absolute_error: 393.4953 - val_loss: 316720.7812 - val_mean_absolute_error: 399.1055 - 10ms/epoch - 2ms/step\nEpoch 168/200\n6/6 - 0s - loss: 269069.5938 - mean_absolute_error: 390.1188 - val_loss: 314434.0312 - val_mean_absolute_error: 397.5743 - 10ms/epoch - 2ms/step\nEpoch 169/200\n6/6 - 0s - loss: 268713.6562 - mean_absolute_error: 393.0327 - val_loss: 312445.5312 - val_mean_absolute_error: 396.1721 - 9ms/epoch - 2ms/step\nEpoch 170/200\n6/6 - 0s - loss: 271394.2188 - mean_absolute_error: 389.2978 - val_loss: 310457.2812 - val_mean_absolute_error: 394.8211 - 10ms/epoch - 2ms/step\nEpoch 171/200\n6/6 - 0s - loss: 264396.3438 - mean_absolute_error: 385.7815 - val_loss: 308426.3438 - val_mean_absolute_error: 393.3835 - 10ms/epoch - 2ms/step\nEpoch 172/200\n6/6 - 0s - loss: 266503.7188 - mean_absolute_error: 386.3963 - val_loss: 306288.1250 - val_mean_absolute_error: 391.8739 - 10ms/epoch - 2ms/step\nEpoch 173/200\n6/6 - 0s - loss: 265888.1562 - mean_absolute_error: 388.9788 - val_loss: 304277.8438 - val_mean_absolute_error: 390.4519 - 10ms/epoch - 2ms/step\nEpoch 174/200\n6/6 - 0s - loss: 259203.0938 - mean_absolute_error: 384.2166 - val_loss: 302195.0312 - val_mean_absolute_error: 388.9444 - 10ms/epoch - 2ms/step\nEpoch 175/200\n6/6 - 0s - loss: 263938.0312 - mean_absolute_error: 386.1097 - val_loss: 300257.5000 - val_mean_absolute_error: 387.5395 - 10ms/epoch - 2ms/step\nEpoch 176/200\n6/6 - 0s - loss: 258036.3906 - mean_absolute_error: 381.9988 - val_loss: 298309.3438 - val_mean_absolute_error: 386.1551 - 9ms/epoch - 2ms/step\nEpoch 177/200\n6/6 - 0s - loss: 267218.2500 - mean_absolute_error: 390.9753 - val_loss: 296384.8750 - val_mean_absolute_error: 384.8245 - 10ms/epoch - 2ms/step\nEpoch 178/200\n6/6 - 0s - loss: 255699.3594 - mean_absolute_error: 381.9271 - val_loss: 294110.8750 - val_mean_absolute_error: 383.2989 - 10ms/epoch - 2ms/step\nEpoch 179/200\n6/6 - 0s - loss: 253812.8438 - mean_absolute_error: 377.2140 - val_loss: 292017.7500 - val_mean_absolute_error: 381.8839 - 11ms/epoch - 2ms/step\nEpoch 180/200\n6/6 - 0s - loss: 251900.5000 - mean_absolute_error: 372.0742 - val_loss: 290057.5625 - val_mean_absolute_error: 380.5223 - 9ms/epoch - 2ms/step\nEpoch 181/200\n6/6 - 0s - loss: 251491.3438 - mean_absolute_error: 374.4917 - val_loss: 288065.3125 - val_mean_absolute_error: 379.1251 - 9ms/epoch - 2ms/step\nEpoch 182/200\n6/6 - 0s - loss: 252318.9062 - mean_absolute_error: 373.0292 - val_loss: 286045.9688 - val_mean_absolute_error: 377.7904 - 10ms/epoch - 2ms/step\nEpoch 183/200\n6/6 - 0s - loss: 248036.2656 - mean_absolute_error: 370.3348 - val_loss: 283937.5938 - val_mean_absolute_error: 376.3823 - 9ms/epoch - 2ms/step\nEpoch 184/200\n6/6 - 0s - loss: 245005.9531 - mean_absolute_error: 368.1648 - val_loss: 281818.7500 - val_mean_absolute_error: 374.9890 - 10ms/epoch - 2ms/step\nEpoch 185/200\n6/6 - 0s - loss: 251364.2969 - mean_absolute_error: 372.7625 - val_loss: 279926.8438 - val_mean_absolute_error: 373.7157 - 10ms/epoch - 2ms/step\nEpoch 186/200\n6/6 - 0s - loss: 245838.0469 - mean_absolute_error: 367.4299 - val_loss: 277838.8438 - val_mean_absolute_error: 372.3391 - 9ms/epoch - 2ms/step\nEpoch 187/200\n6/6 - 0s - loss: 243601.7656 - mean_absolute_error: 368.8007 - val_loss: 275813.4375 - val_mean_absolute_error: 370.9956 - 10ms/epoch - 2ms/step\nEpoch 188/200\n6/6 - 0s - loss: 243130.5000 - mean_absolute_error: 365.1373 - val_loss: 273885.8438 - val_mean_absolute_error: 369.6850 - 9ms/epoch - 2ms/step\nEpoch 189/200\n6/6 - 0s - loss: 248506.9844 - mean_absolute_error: 368.6686 - val_loss: 271949.6875 - val_mean_absolute_error: 368.3928 - 10ms/epoch - 2ms/step\nEpoch 190/200\n6/6 - 0s - loss: 239085.1094 - mean_absolute_error: 362.7067 - val_loss: 270036.5000 - val_mean_absolute_error: 367.0942 - 9ms/epoch - 2ms/step\nEpoch 191/200\n6/6 - 0s - loss: 238062.1406 - mean_absolute_error: 361.2962 - val_loss: 268143.1875 - val_mean_absolute_error: 365.7937 - 10ms/epoch - 2ms/step\nEpoch 192/200\n6/6 - 0s - loss: 235484.5469 - mean_absolute_error: 357.1794 - val_loss: 266324.7812 - val_mean_absolute_error: 364.5018 - 10ms/epoch - 2ms/step\nEpoch 193/200\n6/6 - 0s - loss: 237992.7344 - mean_absolute_error: 364.6215 - val_loss: 264405.5938 - val_mean_absolute_error: 363.1797 - 9ms/epoch - 2ms/step\nEpoch 194/200\n6/6 - 0s - loss: 240541.7969 - mean_absolute_error: 362.7332 - val_loss: 262533.0312 - val_mean_absolute_error: 361.8672 - 10ms/epoch - 2ms/step\nEpoch 195/200\n6/6 - 0s - loss: 231132.0000 - mean_absolute_error: 356.2292 - val_loss: 260902.0156 - val_mean_absolute_error: 360.6966 - 9ms/epoch - 2ms/step\nEpoch 196/200\n6/6 - 0s - loss: 226158.3594 - mean_absolute_error: 352.0297 - val_loss: 258906.3438 - val_mean_absolute_error: 359.3184 - 10ms/epoch - 2ms/step\nEpoch 197/200\n6/6 - 0s - loss: 229560.0156 - mean_absolute_error: 353.4359 - val_loss: 257067.3594 - val_mean_absolute_error: 357.9825 - 9ms/epoch - 2ms/step\nEpoch 198/200\n6/6 - 0s - loss: 230282.7969 - mean_absolute_error: 354.1779 - val_loss: 255074.9844 - val_mean_absolute_error: 356.5592 - 10ms/epoch - 2ms/step\nEpoch 199/200\n6/6 - 0s - loss: 227201.6875 - mean_absolute_error: 352.9511 - val_loss: 253384.3906 - val_mean_absolute_error: 355.3925 - 11ms/epoch - 2ms/step\nEpoch 200/200\n6/6 - 0s - loss: 234115.7344 - mean_absolute_error: 358.6491 - val_loss: 251767.6719 - val_mean_absolute_error: 354.3768 - 10ms/epoch - 2ms/step\n\n\nKeras reports for each epoch the value of the loss metric (mean squared error) for the training and validation data and the monitored metrics (mean absolute error) for the validation data. As you can see from the lengthy output, all criteria are still decreasing after 200 epochs. It is helpful to view the epoch history graphically. If you run the code in an interactive environment (e.g., RStudio), the epoch history is displayed and updated live. You can always plot the epoch history with the plot command:\n\nplot(history, smooth=FALSE)  # see ?plot.keras_training_history for doc\n\n\n\n\n\n\n\nFigure 33.1: Epoch history for the first 200 epochs.\n\n\n\n\n\nAll criteria are steadily declining and have not leveled out after 200 epochs (Figure 33.1). As expected, the mean squared error and mean absolute error are higher in the validation data than in the training data. This is not always the case when training neural networks. Maybe surprisingly, after about 75 epochs the metrics are showing more variability from epoch to epoch in the training data than in the validation data. Also, there is no guarantee that criteria decrease monotonically, the mean squared error of epoch \\(t\\) can be higher than that of epoch \\(t-1\\). We are looking for the results to settle down and stabilize before calling the optimization completed. More epochs need to be run in this example. Fortunately, you can continue where the previous run has left off. The following code trains the network for another 100 epochs:\n\nfirstANN %&gt;% \n    fit(x[-testid, ], \n        y[-testid], \n        epochs=100, \n        batch_size=32,\n        validation_data= list(x[testid, ], y[testid])\n  )\n\nEpoch 1/100\n6/6 - 0s - loss: 235162.2344 - mean_absolute_error: 356.8281 - val_loss: 249973.6562 - val_mean_absolute_error: 353.2513 - 28ms/epoch - 5ms/step\nEpoch 2/100\n6/6 - 0s - loss: 227124.6875 - mean_absolute_error: 349.9795 - val_loss: 248359.4062 - val_mean_absolute_error: 352.1725 - 11ms/epoch - 2ms/step\nEpoch 3/100\n6/6 - 0s - loss: 223675.6094 - mean_absolute_error: 348.1922 - val_loss: 246577.4688 - val_mean_absolute_error: 351.0129 - 10ms/epoch - 2ms/step\nEpoch 4/100\n6/6 - 0s - loss: 220536.4062 - mean_absolute_error: 346.3802 - val_loss: 244721.6719 - val_mean_absolute_error: 349.7575 - 10ms/epoch - 2ms/step\nEpoch 5/100\n6/6 - 0s - loss: 223971.4844 - mean_absolute_error: 347.6629 - val_loss: 243012.2031 - val_mean_absolute_error: 348.6621 - 10ms/epoch - 2ms/step\nEpoch 6/100\n6/6 - 0s - loss: 213548.0469 - mean_absolute_error: 341.7778 - val_loss: 241261.1562 - val_mean_absolute_error: 347.4973 - 10ms/epoch - 2ms/step\nEpoch 7/100\n6/6 - 0s - loss: 217622.2344 - mean_absolute_error: 340.4199 - val_loss: 239542.3281 - val_mean_absolute_error: 346.3548 - 10ms/epoch - 2ms/step\nEpoch 8/100\n6/6 - 0s - loss: 221623.9844 - mean_absolute_error: 344.7711 - val_loss: 238105.8906 - val_mean_absolute_error: 345.3802 - 10ms/epoch - 2ms/step\nEpoch 9/100\n6/6 - 0s - loss: 215652.6875 - mean_absolute_error: 340.3169 - val_loss: 236484.1406 - val_mean_absolute_error: 344.2688 - 10ms/epoch - 2ms/step\nEpoch 10/100\n6/6 - 0s - loss: 215888.8594 - mean_absolute_error: 341.6668 - val_loss: 234606.5469 - val_mean_absolute_error: 343.0414 - 10ms/epoch - 2ms/step\nEpoch 11/100\n6/6 - 0s - loss: 219297.5625 - mean_absolute_error: 347.1562 - val_loss: 232948.7812 - val_mean_absolute_error: 341.9417 - 10ms/epoch - 2ms/step\nEpoch 12/100\n6/6 - 0s - loss: 214289.0469 - mean_absolute_error: 343.1263 - val_loss: 231407.5000 - val_mean_absolute_error: 340.9212 - 10ms/epoch - 2ms/step\nEpoch 13/100\n6/6 - 0s - loss: 202545.7344 - mean_absolute_error: 333.3247 - val_loss: 229981.4062 - val_mean_absolute_error: 340.0316 - 9ms/epoch - 2ms/step\nEpoch 14/100\n6/6 - 0s - loss: 212926.0000 - mean_absolute_error: 339.6272 - val_loss: 228310.0625 - val_mean_absolute_error: 339.0390 - 10ms/epoch - 2ms/step\nEpoch 15/100\n6/6 - 0s - loss: 215009.5000 - mean_absolute_error: 342.5746 - val_loss: 226904.5469 - val_mean_absolute_error: 338.1363 - 10ms/epoch - 2ms/step\nEpoch 16/100\n6/6 - 0s - loss: 210791.7969 - mean_absolute_error: 341.3379 - val_loss: 225564.1562 - val_mean_absolute_error: 337.3196 - 10ms/epoch - 2ms/step\nEpoch 17/100\n6/6 - 0s - loss: 208513.4844 - mean_absolute_error: 334.8676 - val_loss: 224295.9062 - val_mean_absolute_error: 336.4972 - 10ms/epoch - 2ms/step\nEpoch 18/100\n6/6 - 0s - loss: 211304.8438 - mean_absolute_error: 339.3423 - val_loss: 222982.7188 - val_mean_absolute_error: 335.6900 - 10ms/epoch - 2ms/step\nEpoch 19/100\n6/6 - 0s - loss: 207620.3125 - mean_absolute_error: 329.1488 - val_loss: 221474.2500 - val_mean_absolute_error: 334.7701 - 10ms/epoch - 2ms/step\nEpoch 20/100\n6/6 - 0s - loss: 199470.3594 - mean_absolute_error: 328.8928 - val_loss: 219830.4844 - val_mean_absolute_error: 333.7775 - 10ms/epoch - 2ms/step\nEpoch 21/100\n6/6 - 0s - loss: 209208.9844 - mean_absolute_error: 335.9739 - val_loss: 218498.9219 - val_mean_absolute_error: 332.9214 - 10ms/epoch - 2ms/step\nEpoch 22/100\n6/6 - 0s - loss: 203499.9844 - mean_absolute_error: 330.5531 - val_loss: 217171.2188 - val_mean_absolute_error: 332.0294 - 10ms/epoch - 2ms/step\nEpoch 23/100\n6/6 - 0s - loss: 205696.0938 - mean_absolute_error: 330.9353 - val_loss: 216009.3750 - val_mean_absolute_error: 331.2743 - 10ms/epoch - 2ms/step\nEpoch 24/100\n6/6 - 0s - loss: 202579.2656 - mean_absolute_error: 331.1476 - val_loss: 214955.4531 - val_mean_absolute_error: 330.5721 - 10ms/epoch - 2ms/step\nEpoch 25/100\n6/6 - 0s - loss: 200309.2344 - mean_absolute_error: 328.4398 - val_loss: 213502.6250 - val_mean_absolute_error: 329.6791 - 10ms/epoch - 2ms/step\nEpoch 26/100\n6/6 - 0s - loss: 202274.1562 - mean_absolute_error: 332.5124 - val_loss: 212192.1094 - val_mean_absolute_error: 328.8698 - 10ms/epoch - 2ms/step\nEpoch 27/100\n6/6 - 0s - loss: 197080.7656 - mean_absolute_error: 324.0207 - val_loss: 210682.3750 - val_mean_absolute_error: 327.9472 - 10ms/epoch - 2ms/step\nEpoch 28/100\n6/6 - 0s - loss: 199090.5000 - mean_absolute_error: 327.2231 - val_loss: 209386.7656 - val_mean_absolute_error: 327.1157 - 10ms/epoch - 2ms/step\nEpoch 29/100\n6/6 - 0s - loss: 199999.3125 - mean_absolute_error: 332.1605 - val_loss: 208245.9844 - val_mean_absolute_error: 326.3528 - 10ms/epoch - 2ms/step\nEpoch 30/100\n6/6 - 0s - loss: 199387.0938 - mean_absolute_error: 325.9272 - val_loss: 206946.7188 - val_mean_absolute_error: 325.4943 - 10ms/epoch - 2ms/step\nEpoch 31/100\n6/6 - 0s - loss: 197919.3125 - mean_absolute_error: 329.9712 - val_loss: 205548.9219 - val_mean_absolute_error: 324.6052 - 10ms/epoch - 2ms/step\nEpoch 32/100\n6/6 - 0s - loss: 202632.3438 - mean_absolute_error: 330.1127 - val_loss: 204415.8594 - val_mean_absolute_error: 323.8447 - 10ms/epoch - 2ms/step\nEpoch 33/100\n6/6 - 0s - loss: 187659.7188 - mean_absolute_error: 321.9861 - val_loss: 203234.7188 - val_mean_absolute_error: 323.0219 - 10ms/epoch - 2ms/step\nEpoch 34/100\n6/6 - 0s - loss: 189089.5781 - mean_absolute_error: 314.7178 - val_loss: 201932.6250 - val_mean_absolute_error: 322.1450 - 10ms/epoch - 2ms/step\nEpoch 35/100\n6/6 - 0s - loss: 183974.0938 - mean_absolute_error: 315.4939 - val_loss: 200627.5625 - val_mean_absolute_error: 321.2622 - 10ms/epoch - 2ms/step\nEpoch 36/100\n6/6 - 0s - loss: 185044.6250 - mean_absolute_error: 317.5464 - val_loss: 199190.1406 - val_mean_absolute_error: 320.2658 - 9ms/epoch - 2ms/step\nEpoch 37/100\n6/6 - 0s - loss: 189883.3125 - mean_absolute_error: 317.6797 - val_loss: 198038.7656 - val_mean_absolute_error: 319.4498 - 9ms/epoch - 2ms/step\nEpoch 38/100\n6/6 - 0s - loss: 185542.8594 - mean_absolute_error: 311.1573 - val_loss: 196856.3750 - val_mean_absolute_error: 318.6121 - 10ms/epoch - 2ms/step\nEpoch 39/100\n6/6 - 0s - loss: 184584.6406 - mean_absolute_error: 316.1197 - val_loss: 195878.9688 - val_mean_absolute_error: 317.8906 - 10ms/epoch - 2ms/step\nEpoch 40/100\n6/6 - 0s - loss: 187881.7031 - mean_absolute_error: 316.2698 - val_loss: 194844.0156 - val_mean_absolute_error: 317.1588 - 10ms/epoch - 2ms/step\nEpoch 41/100\n6/6 - 0s - loss: 194517.6875 - mean_absolute_error: 321.9532 - val_loss: 193817.4688 - val_mean_absolute_error: 316.4131 - 9ms/epoch - 2ms/step\nEpoch 42/100\n6/6 - 0s - loss: 185463.0156 - mean_absolute_error: 317.6747 - val_loss: 192903.5000 - val_mean_absolute_error: 315.7482 - 10ms/epoch - 2ms/step\nEpoch 43/100\n6/6 - 0s - loss: 180585.6562 - mean_absolute_error: 311.6922 - val_loss: 191928.0625 - val_mean_absolute_error: 315.0240 - 10ms/epoch - 2ms/step\nEpoch 44/100\n6/6 - 0s - loss: 193349.9375 - mean_absolute_error: 319.6145 - val_loss: 190984.0938 - val_mean_absolute_error: 314.3444 - 10ms/epoch - 2ms/step\nEpoch 45/100\n6/6 - 0s - loss: 187489.7500 - mean_absolute_error: 319.5717 - val_loss: 189964.8281 - val_mean_absolute_error: 313.6478 - 10ms/epoch - 2ms/step\nEpoch 46/100\n6/6 - 0s - loss: 191184.8125 - mean_absolute_error: 318.4938 - val_loss: 189063.0000 - val_mean_absolute_error: 312.9803 - 10ms/epoch - 2ms/step\nEpoch 47/100\n6/6 - 0s - loss: 184643.1875 - mean_absolute_error: 316.4543 - val_loss: 188178.7656 - val_mean_absolute_error: 312.3470 - 10ms/epoch - 2ms/step\nEpoch 48/100\n6/6 - 0s - loss: 179654.1094 - mean_absolute_error: 310.9713 - val_loss: 187370.3438 - val_mean_absolute_error: 311.7758 - 10ms/epoch - 2ms/step\nEpoch 49/100\n6/6 - 0s - loss: 169689.2812 - mean_absolute_error: 301.8134 - val_loss: 186302.6250 - val_mean_absolute_error: 311.0377 - 10ms/epoch - 2ms/step\nEpoch 50/100\n6/6 - 0s - loss: 180011.9375 - mean_absolute_error: 311.0054 - val_loss: 185132.6250 - val_mean_absolute_error: 310.2269 - 10ms/epoch - 2ms/step\nEpoch 51/100\n6/6 - 0s - loss: 176865.9219 - mean_absolute_error: 309.2646 - val_loss: 184171.0156 - val_mean_absolute_error: 309.5695 - 10ms/epoch - 2ms/step\nEpoch 52/100\n6/6 - 0s - loss: 179995.6562 - mean_absolute_error: 308.4983 - val_loss: 183198.5156 - val_mean_absolute_error: 308.8884 - 10ms/epoch - 2ms/step\nEpoch 53/100\n6/6 - 0s - loss: 190259.1406 - mean_absolute_error: 316.7602 - val_loss: 182308.0156 - val_mean_absolute_error: 308.2335 - 10ms/epoch - 2ms/step\nEpoch 54/100\n6/6 - 0s - loss: 182877.3906 - mean_absolute_error: 310.6611 - val_loss: 181375.4219 - val_mean_absolute_error: 307.5412 - 10ms/epoch - 2ms/step\nEpoch 55/100\n6/6 - 0s - loss: 185388.2812 - mean_absolute_error: 317.3799 - val_loss: 180373.8125 - val_mean_absolute_error: 306.8027 - 10ms/epoch - 2ms/step\nEpoch 56/100\n6/6 - 0s - loss: 179605.8125 - mean_absolute_error: 309.9001 - val_loss: 179491.7656 - val_mean_absolute_error: 306.1248 - 10ms/epoch - 2ms/step\nEpoch 57/100\n6/6 - 0s - loss: 171920.7812 - mean_absolute_error: 303.3571 - val_loss: 178605.1250 - val_mean_absolute_error: 305.4509 - 10ms/epoch - 2ms/step\nEpoch 58/100\n6/6 - 0s - loss: 177706.8906 - mean_absolute_error: 302.1384 - val_loss: 177617.5938 - val_mean_absolute_error: 304.7087 - 10ms/epoch - 2ms/step\nEpoch 59/100\n6/6 - 0s - loss: 168861.4219 - mean_absolute_error: 302.5320 - val_loss: 176771.2188 - val_mean_absolute_error: 304.0304 - 10ms/epoch - 2ms/step\nEpoch 60/100\n6/6 - 0s - loss: 170427.4062 - mean_absolute_error: 299.2004 - val_loss: 175900.0156 - val_mean_absolute_error: 303.3622 - 10ms/epoch - 2ms/step\nEpoch 61/100\n6/6 - 0s - loss: 182797.0625 - mean_absolute_error: 313.7403 - val_loss: 175120.6250 - val_mean_absolute_error: 302.7082 - 10ms/epoch - 2ms/step\nEpoch 62/100\n6/6 - 0s - loss: 174055.9375 - mean_absolute_error: 310.0703 - val_loss: 174297.2188 - val_mean_absolute_error: 302.0762 - 10ms/epoch - 2ms/step\nEpoch 63/100\n6/6 - 0s - loss: 178073.9062 - mean_absolute_error: 307.6194 - val_loss: 173541.3750 - val_mean_absolute_error: 301.5027 - 10ms/epoch - 2ms/step\nEpoch 64/100\n6/6 - 0s - loss: 186641.2656 - mean_absolute_error: 320.5426 - val_loss: 173049.3281 - val_mean_absolute_error: 301.1250 - 10ms/epoch - 2ms/step\nEpoch 65/100\n6/6 - 0s - loss: 175205.2188 - mean_absolute_error: 304.8622 - val_loss: 172220.8750 - val_mean_absolute_error: 300.5187 - 10ms/epoch - 2ms/step\nEpoch 66/100\n6/6 - 0s - loss: 173134.9062 - mean_absolute_error: 303.8184 - val_loss: 171633.4688 - val_mean_absolute_error: 300.0750 - 10ms/epoch - 2ms/step\nEpoch 67/100\n6/6 - 0s - loss: 187828.6719 - mean_absolute_error: 316.3444 - val_loss: 171247.4219 - val_mean_absolute_error: 299.7914 - 10ms/epoch - 2ms/step\nEpoch 68/100\n6/6 - 0s - loss: 173913.2031 - mean_absolute_error: 305.9345 - val_loss: 170639.0312 - val_mean_absolute_error: 299.3336 - 10ms/epoch - 2ms/step\nEpoch 69/100\n6/6 - 0s - loss: 167519.9375 - mean_absolute_error: 300.5960 - val_loss: 169894.1094 - val_mean_absolute_error: 298.8028 - 10ms/epoch - 2ms/step\nEpoch 70/100\n6/6 - 0s - loss: 162824.4844 - mean_absolute_error: 294.1468 - val_loss: 169084.9219 - val_mean_absolute_error: 298.2397 - 10ms/epoch - 2ms/step\nEpoch 71/100\n6/6 - 0s - loss: 170298.4688 - mean_absolute_error: 299.1925 - val_loss: 168513.7656 - val_mean_absolute_error: 297.8411 - 10ms/epoch - 2ms/step\nEpoch 72/100\n6/6 - 0s - loss: 169915.1562 - mean_absolute_error: 302.0935 - val_loss: 167779.5625 - val_mean_absolute_error: 297.3293 - 10ms/epoch - 2ms/step\nEpoch 73/100\n6/6 - 0s - loss: 172608.3281 - mean_absolute_error: 304.5370 - val_loss: 167249.0000 - val_mean_absolute_error: 296.9433 - 10ms/epoch - 2ms/step\nEpoch 74/100\n6/6 - 0s - loss: 167785.8906 - mean_absolute_error: 301.6566 - val_loss: 166698.9844 - val_mean_absolute_error: 296.5391 - 10ms/epoch - 2ms/step\nEpoch 75/100\n6/6 - 0s - loss: 173998.1875 - mean_absolute_error: 304.4858 - val_loss: 166011.5938 - val_mean_absolute_error: 296.0414 - 10ms/epoch - 2ms/step\nEpoch 76/100\n6/6 - 0s - loss: 177637.6406 - mean_absolute_error: 306.1377 - val_loss: 165447.1719 - val_mean_absolute_error: 295.6394 - 10ms/epoch - 2ms/step\nEpoch 77/100\n6/6 - 0s - loss: 165226.3125 - mean_absolute_error: 302.1341 - val_loss: 164693.0938 - val_mean_absolute_error: 295.0961 - 10ms/epoch - 2ms/step\nEpoch 78/100\n6/6 - 0s - loss: 167706.8594 - mean_absolute_error: 304.1009 - val_loss: 164206.8281 - val_mean_absolute_error: 294.7349 - 10ms/epoch - 2ms/step\nEpoch 79/100\n6/6 - 0s - loss: 166010.4531 - mean_absolute_error: 299.4156 - val_loss: 163515.3594 - val_mean_absolute_error: 294.2280 - 10ms/epoch - 2ms/step\nEpoch 80/100\n6/6 - 0s - loss: 164898.8594 - mean_absolute_error: 296.1396 - val_loss: 162875.5000 - val_mean_absolute_error: 293.7555 - 10ms/epoch - 2ms/step\nEpoch 81/100\n6/6 - 0s - loss: 172376.3125 - mean_absolute_error: 304.5506 - val_loss: 162445.2812 - val_mean_absolute_error: 293.4335 - 10ms/epoch - 2ms/step\nEpoch 82/100\n6/6 - 0s - loss: 173918.1094 - mean_absolute_error: 303.5177 - val_loss: 161970.7344 - val_mean_absolute_error: 293.0901 - 10ms/epoch - 2ms/step\nEpoch 83/100\n6/6 - 0s - loss: 173138.3125 - mean_absolute_error: 302.5192 - val_loss: 161523.6094 - val_mean_absolute_error: 292.7641 - 10ms/epoch - 2ms/step\nEpoch 84/100\n6/6 - 0s - loss: 155196.7344 - mean_absolute_error: 290.0139 - val_loss: 160810.5625 - val_mean_absolute_error: 292.2314 - 10ms/epoch - 2ms/step\nEpoch 85/100\n6/6 - 0s - loss: 169151.8125 - mean_absolute_error: 299.2198 - val_loss: 160332.0469 - val_mean_absolute_error: 291.8710 - 10ms/epoch - 2ms/step\nEpoch 86/100\n6/6 - 0s - loss: 156046.8125 - mean_absolute_error: 291.1232 - val_loss: 159621.2344 - val_mean_absolute_error: 291.3150 - 10ms/epoch - 2ms/step\nEpoch 87/100\n6/6 - 0s - loss: 171491.6250 - mean_absolute_error: 299.8004 - val_loss: 159068.4531 - val_mean_absolute_error: 290.8917 - 10ms/epoch - 2ms/step\nEpoch 88/100\n6/6 - 0s - loss: 164938.4844 - mean_absolute_error: 293.3096 - val_loss: 158599.5469 - val_mean_absolute_error: 290.5317 - 10ms/epoch - 2ms/step\nEpoch 89/100\n6/6 - 0s - loss: 172227.1406 - mean_absolute_error: 291.8225 - val_loss: 158017.9062 - val_mean_absolute_error: 290.0740 - 10ms/epoch - 2ms/step\nEpoch 90/100\n6/6 - 0s - loss: 161008.2969 - mean_absolute_error: 293.7607 - val_loss: 157355.3750 - val_mean_absolute_error: 289.5539 - 10ms/epoch - 2ms/step\nEpoch 91/100\n6/6 - 0s - loss: 155111.1250 - mean_absolute_error: 287.0963 - val_loss: 156748.5469 - val_mean_absolute_error: 289.0711 - 9ms/epoch - 2ms/step\nEpoch 92/100\n6/6 - 0s - loss: 161928.5156 - mean_absolute_error: 290.5968 - val_loss: 156280.0938 - val_mean_absolute_error: 288.6997 - 9ms/epoch - 2ms/step\nEpoch 93/100\n6/6 - 0s - loss: 157645.6406 - mean_absolute_error: 290.0891 - val_loss: 155851.4844 - val_mean_absolute_error: 288.3556 - 10ms/epoch - 2ms/step\nEpoch 94/100\n6/6 - 0s - loss: 157648.2188 - mean_absolute_error: 293.2608 - val_loss: 155384.4219 - val_mean_absolute_error: 287.9782 - 10ms/epoch - 2ms/step\nEpoch 95/100\n6/6 - 0s - loss: 160378.0312 - mean_absolute_error: 297.3802 - val_loss: 154893.1562 - val_mean_absolute_error: 287.6026 - 10ms/epoch - 2ms/step\nEpoch 96/100\n6/6 - 0s - loss: 160288.8906 - mean_absolute_error: 298.0783 - val_loss: 154448.5000 - val_mean_absolute_error: 287.2488 - 10ms/epoch - 2ms/step\nEpoch 97/100\n6/6 - 0s - loss: 156110.7031 - mean_absolute_error: 292.2749 - val_loss: 154109.9375 - val_mean_absolute_error: 286.9748 - 9ms/epoch - 2ms/step\nEpoch 98/100\n6/6 - 0s - loss: 157591.6562 - mean_absolute_error: 294.3321 - val_loss: 153678.4531 - val_mean_absolute_error: 286.6404 - 10ms/epoch - 2ms/step\nEpoch 99/100\n6/6 - 0s - loss: 167888.2500 - mean_absolute_error: 298.4736 - val_loss: 153234.0156 - val_mean_absolute_error: 286.3086 - 10ms/epoch - 2ms/step\nEpoch 100/100\n6/6 - 0s - loss: 150076.5625 - mean_absolute_error: 285.2715 - val_loss: 152811.1719 - val_mean_absolute_error: 285.9948 - 9ms/epoch - 2ms/step\n\n\n\nWhen training models this way you keep your eyes on the epoch history to study the behavior of the loss function and other metrics on training and test data sets. You have to make a judgement call as to when the optimization has stabilized and further progress is minimal. Alternatively, you can install a function that stops the optimization when certain conditions are met.\nThis is done in the following code with the callback_early_stopping callback function (not run here). The options of the early stopping function ask it to monitor the loss function on the validation data and stop the optimization when the criterion fails to decrease (mode=\"min\") over 10 epochs (patience=10). Any change of the monitored metric has to be at least 0.1 in magnitude to qualify as an improvement (min_delta=.1).\n\nearly_stopping &lt;- callback_early_stopping(monitor = 'val_loss', \n                                          patience = 10,\n                                          min_delta = .1,\n                                          mode=\"min\")\nfirstANN %&gt;% \n    fit(x[-testid, ], \n        y[-testid], \n        epochs=400, \n        batch_size=32,\n        validation_data= list(x[testid, ], y[testid])\n        callbacks=c(early_stopping)\n  )\n\nTo see a list of all Keras callback functions type\n\n?keras::call\n\nat the console prompt.\n\nFinally, we predict from the final model, and evaluate its performance on the test data. Due to the use of random elements in the fit (stochastic gradient descent, random dropout, …), the results vary slightly with each fit. Unfortunately the set.seed() function does not ensure identical results (since the fitting is done in python), so your results will differ slightly.\n\npredvals &lt;- predict(firstANN, x[testid, ])\n\n3/3 - 0s - 27ms/epoch - 9ms/step\n\nmean(abs(y[testid] - predvals))\n\n[1] 285.9949\n\n\n\n\nRandom numbers\nAn aspect of Keras that can be befuddling to R users is lack of control over the random mechanisms during training. Neural networks rely on random numbers for picking starting values, selecting observations into mini batches, selecting neurons in dropout layers, etc.\nSince the code executes in Python, the set.seed() operation does not have the intended effect of fixing the sequence of generated random numbers. The underlying Python code relies on the NumPy random number generator. TensorFlow has its own random number generator on top of that. Python code that uses Keras with the TensorFlow backend needs to set the seed for the NumPy and the TensorFlow generator to obtain reproducible results:\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\nThe R user is unfortunately out of luck.\nIf it is any consolation, running Keras in Python might still generate non-reproducible results. You might also need to set the seed in the random Python library. Multi-threading operations on CPUs—and GPUs in particular—can produce a non-deterministic order of operations.\nOne recommendation to deal with non-deterministic results is training the model several times and averaging the results, essentially ensembling them. When a single training run takes several hours, doing it thirty times is not practical.\n\n\n\nMNIST Image Classification\nWe now return to the MNIST image classification data introduced in Section 31.4. Recall that the data comprise 60,000 training images and 10,000 test images of handwritten digits (0–9). Each image has 28 x 28 pixels recording a grayscale value.\nThe MNIST data is provided by Keras:\n\nSetup the data\n\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x\ng_train &lt;- mnist$train$y\n\nx_test &lt;- mnist$test$x\ng_test &lt;- mnist$test$y\ndim(x_train)\n\n[1] 60000    28    28\n\ndim(x_test)\n\n[1] 10000    28    28\n\n\nThe images are stored as a three-dimensional array, and need to be reshaped into a matrix. For classification tasks with \\(k\\) categories, Keras expects as the target values a matrix of \\(k\\) columns. Column \\(k\\) contains ones in the rows for observations where the observed category is \\(k\\), and zeros otherwise. This is called one-hot encoding of the target variable. Luckily, keras has built-in functions that handle both tasks for us.\n\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))\nx_test  &lt;- array_reshape(x_test, c(nrow(x_test), 784))\n\ny_train &lt;- to_categorical(g_train, 10)\ny_test  &lt;- to_categorical(g_test, 10)\n\nLet’s look at the one-hot encoding of the target data. g_test contains the value of the digit from 0–9. y_test is a matrix with 10 columns, each column corresponds to one digit. If observation \\(i\\) represents digit \\(j\\) then there is a 1 in row \\(i\\), column \\(j+1\\) of the encoded matrix. For example, for the first twenty images:\n\ng_test[1:20]\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4\n\ny_test[1:20,1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    0    0    0    0    0    1    0     0\n [2,]    0    0    1    0    0    0    0    0    0     0\n [3,]    0    1    0    0    0    0    0    0    0     0\n [4,]    1    0    0    0    0    0    0    0    0     0\n [5,]    0    0    0    0    1    0    0    0    0     0\n [6,]    0    1    0    0    0    0    0    0    0     0\n [7,]    0    0    0    0    1    0    0    0    0     0\n [8,]    0    0    0    0    0    0    0    0    0     1\n [9,]    0    0    0    0    0    1    0    0    0     0\n[10,]    0    0    0    0    0    0    0    0    0     1\n[11,]    1    0    0    0    0    0    0    0    0     0\n[12,]    0    0    0    0    0    0    1    0    0     0\n[13,]    0    0    0    0    0    0    0    0    0     1\n[14,]    1    0    0    0    0    0    0    0    0     0\n[15,]    0    1    0    0    0    0    0    0    0     0\n[16,]    0    0    0    0    0    1    0    0    0     0\n[17,]    0    0    0    0    0    0    0    0    0     1\n[18,]    0    0    0    0    0    0    0    1    0     0\n[19,]    0    0    0    1    0    0    0    0    0     0\n[20,]    0    0    0    0    1    0    0    0    0     0\n\n\nLet’s look at the matrix of inputs. The next array shows the 28 x 28 - 784 input columns for the third image. The values are grayscale values between 0 and 255.\n\nx_test[3,]\n\n  [1]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [19]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [37]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [55]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [73]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [91]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[109]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[127]   0   0  38 254 109   0   0   0   0   0   0   0   0   0   0   0   0   0\n[145]   0   0   0   0   0   0   0   0   0   0   0   0  87 252  82   0   0   0\n[163]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[181]   0   0   0   0 135 241   0   0   0   0   0   0   0   0   0   0   0   0\n[199]   0   0   0   0   0   0   0   0   0   0   0   0   0  45 244 150   0   0\n[217]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[235]   0   0   0   0   0  84 254  63   0   0   0   0   0   0   0   0   0   0\n[253]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 202 223  11\n[271]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[289]   0   0   0   0   0   0  32 254 216   0   0   0   0   0   0   0   0   0\n[307]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  95 254\n[325] 195   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[343]   0   0   0   0   0   0   0   0 140 254  77   0   0   0   0   0   0   0\n[361]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  57\n[379] 237 205   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[397]   0   0   0   0   0   0   0   0   0 124 255 165   0   0   0   0   0   0\n[415]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[433]   0 171 254  81   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[451]   0   0   0   0   0   0   0   0   0   0  24 232 215   0   0   0   0   0\n[469]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[487]   0   0 120 254 159   0   0   0   0   0   0   0   0   0   0   0   0   0\n[505]   0   0   0   0   0   0   0   0   0   0   0   0 151 254 142   0   0   0\n[523]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[541]   0   0   0   0 228 254  66   0   0   0   0   0   0   0   0   0   0   0\n[559]   0   0   0   0   0   0   0   0   0   0   0   0   0  61 251 254  66   0\n[577]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[595]   0   0   0   0   0 141 254 205   3   0   0   0   0   0   0   0   0   0\n[613]   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10 215 254 121\n[631]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[649]   0   0   0   0   0   0   5 198 176  10   0   0   0   0   0   0   0   0\n[667]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[685]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[703]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[721]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[739]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[757]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[775]   0   0   0   0   0   0   0   0   0   0\n\n\nFinally, prior to training the network, we scale the input values to lie between 0–1.\n\nx_train &lt;- x_train / 255\nx_test  &lt;- x_test / 255\n\nThe target variable does not need to be scaled, the one-hot encoding together with the use of a softmax output function ensures that the output for each category is a value between 0 and 1, and that they sum to 1 across the 10 categories. We will interpret them as predicted probabilities that an observed image is assigned to a particular digit.\nTo classify the MNIST images we consider two types of neural networks in the remainder of this chapter: a multi layer ANN and a network without a hidden layer. The latter is a multi category perceptron and very similar to a multinomial logistic regression model.\n\n\nMulti layer neural network\nWe now train the network shown in Figure 31.14, an ANN with two hidden layers. We also add dropout regularization layers after each fully connected hidden layer. The first layer specifies the input shape of 28 x 28 = 784. It has 128 neurons and ReLU activation. Why? Because. This is followed by a first dropout layer with rate \\(\\phi_1 = 0.3\\), another fully connected hidden layer with 64 nodes and hyperbolic tangent activation function, a second dropout layer with rate \\(\\phi_2 = 0.2\\), and a final softmax output layer. Why? Because.\n\nSetup the network\nThe following statements set up the network in keras:\n\nmodelnn &lt;- keras_model_sequential() %&gt;%\n    layer_dense(units=128,\n                activation=\"relu\",\n                input_shape=784,\n                name=\"FirstHidden\") %&gt;%\n    layer_dropout(rate=0.3,\n                  name=\"FirstDropOut\") %&gt;%\n    layer_dense(units=64,\n                activation=\"tanh\",\n                name=\"SecondHidden\") %&gt;% \n    layer_dropout(rate=0.2,\n                  name=\"SecondDropOut\") %&gt;% \n    layer_dense(units=10, \n                activation=\"softmax\",\n                name=\"Output\")\n\nThe summary() function let’s us inspect whether we got it all right.\n\nsummary(modelnn)\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n FirstHidden (Dense)                (None, 128)                     100480      \n FirstDropOut (Dropout)             (None, 128)                     0           \n SecondHidden (Dense)               (None, 64)                      8256        \n SecondDropOut (Dropout)            (None, 64)                      0           \n Output (Dense)                     (None, 10)                      650         \n================================================================================\nTotal params: 109386 (427.29 KB)\nTrainable params: 109386 (427.29 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nThe total number of parameters in this network is 109,386, a sizeable network but not a huge network.\n\n\nSet up the optimization\nNext, we add details to the model to specify the fitting algorithm. We fit the model by minimizing the categorical cross-entropy function and monitor the classification accuracy during the iterations.\n\nmodelnn %&gt;% compile(loss=\"categorical_crossentropy\",\n                    optimizer=optimizer_rmsprop(), \n                    metrics=c(\"accuracy\")\n                    )\n\n\n\nFit the model\nWe are ready to go. The final step is to supply training data, and fit the model. With a batch size of 128 observations, each epoch corresponds to 60,000 / 128 = 469 gradient evaluations.\n\nhistory &lt;- modelnn %&gt;%\n      fit(x_train, \n          y_train, \n          epochs=20, \n          batch_size=128,\n          validation_data= list(x_test, y_test),\n          )\n\nEpoch 1/20\n469/469 - 1s - loss: 0.3888 - accuracy: 0.8876 - val_loss: 0.1818 - val_accuracy: 0.9426 - 819ms/epoch - 2ms/step\nEpoch 2/20\n469/469 - 1s - loss: 0.1906 - accuracy: 0.9422 - val_loss: 0.1206 - val_accuracy: 0.9614 - 542ms/epoch - 1ms/step\nEpoch 3/20\n469/469 - 1s - loss: 0.1487 - accuracy: 0.9552 - val_loss: 0.1027 - val_accuracy: 0.9686 - 566ms/epoch - 1ms/step\nEpoch 4/20\n469/469 - 1s - loss: 0.1271 - accuracy: 0.9615 - val_loss: 0.0944 - val_accuracy: 0.9712 - 545ms/epoch - 1ms/step\nEpoch 5/20\n469/469 - 1s - loss: 0.1126 - accuracy: 0.9655 - val_loss: 0.0857 - val_accuracy: 0.9751 - 534ms/epoch - 1ms/step\nEpoch 6/20\n469/469 - 1s - loss: 0.1050 - accuracy: 0.9682 - val_loss: 0.0844 - val_accuracy: 0.9739 - 539ms/epoch - 1ms/step\nEpoch 7/20\n469/469 - 1s - loss: 0.0938 - accuracy: 0.9718 - val_loss: 0.0794 - val_accuracy: 0.9765 - 533ms/epoch - 1ms/step\nEpoch 8/20\n469/469 - 1s - loss: 0.0894 - accuracy: 0.9720 - val_loss: 0.0767 - val_accuracy: 0.9771 - 541ms/epoch - 1ms/step\nEpoch 9/20\n469/469 - 1s - loss: 0.0827 - accuracy: 0.9744 - val_loss: 0.0806 - val_accuracy: 0.9759 - 537ms/epoch - 1ms/step\nEpoch 10/20\n469/469 - 1s - loss: 0.0798 - accuracy: 0.9750 - val_loss: 0.0735 - val_accuracy: 0.9776 - 536ms/epoch - 1ms/step\nEpoch 11/20\n469/469 - 1s - loss: 0.0771 - accuracy: 0.9762 - val_loss: 0.0713 - val_accuracy: 0.9785 - 529ms/epoch - 1ms/step\nEpoch 12/20\n469/469 - 1s - loss: 0.0731 - accuracy: 0.9773 - val_loss: 0.0750 - val_accuracy: 0.9786 - 534ms/epoch - 1ms/step\nEpoch 13/20\n469/469 - 1s - loss: 0.0698 - accuracy: 0.9776 - val_loss: 0.0777 - val_accuracy: 0.9779 - 536ms/epoch - 1ms/step\nEpoch 14/20\n469/469 - 1s - loss: 0.0661 - accuracy: 0.9795 - val_loss: 0.0720 - val_accuracy: 0.9798 - 536ms/epoch - 1ms/step\nEpoch 15/20\n469/469 - 1s - loss: 0.0614 - accuracy: 0.9806 - val_loss: 0.0738 - val_accuracy: 0.9789 - 545ms/epoch - 1ms/step\nEpoch 16/20\n469/469 - 1s - loss: 0.0630 - accuracy: 0.9804 - val_loss: 0.0715 - val_accuracy: 0.9789 - 530ms/epoch - 1ms/step\nEpoch 17/20\n469/469 - 1s - loss: 0.0581 - accuracy: 0.9817 - val_loss: 0.0735 - val_accuracy: 0.9792 - 538ms/epoch - 1ms/step\nEpoch 18/20\n469/469 - 1s - loss: 0.0583 - accuracy: 0.9819 - val_loss: 0.0709 - val_accuracy: 0.9795 - 557ms/epoch - 1ms/step\nEpoch 19/20\n469/469 - 1s - loss: 0.0545 - accuracy: 0.9825 - val_loss: 0.0704 - val_accuracy: 0.9810 - 557ms/epoch - 1ms/step\nEpoch 20/20\n469/469 - 1s - loss: 0.0549 - accuracy: 0.9834 - val_loss: 0.0710 - val_accuracy: 0.9793 - 613ms/epoch - 1ms/step\n\nplot(history, smooth = FALSE)\n\n\n\n\n\n\n\n\nAfter about 10 epochs the training and validation accuracy are stabilizing although the loss continues to decrease. Interestingly, the accuracy and loss in the 10,000 image validation set is better than in the 60,000 image training data set. Considering that the grayscale values are entered into this neural network as 784 numeric input variables without taking into account any spatial arrangement of the pixels on the image, a classification accuracy of 96% on unseen images is quite good. Whether that is sufficient depends on the application.\nAs we will see in Chapter 34, neural networks that specialize in the processing of grid-like data such as images easily improve on this performance.\n\n\nCalculate predicted categories\nTo calculate the predicted categories for the images in the test data set, we use the predict function. The result of that operation is a vector of 10 predicted probabilities for each observation.\n\npredvals &lt;- modelnn %&gt;% predict(x_test)\n\n313/313 - 0s - 108ms/epoch - 346us/step\n\n\nFor the first image, the probabilities that its digit belongs to any of the 10 classes is given by this vector\n\nround(predvals[1,],4)\n\n [1] 0 0 0 0 0 0 0 1 0 0\n\nwhich.max(predvals[1,])\n\n[1] 8\n\n\nThe maximum probability is 1 in position 8. The image is classified as a “7” (the digits are 0-based).\nkeras provides the convenience function k_argmax() to perform this operation; it returns the index of the maximum value:\n\npredcl &lt;- modelnn %&gt;% predict(x_test) %&gt;% k_argmax() \n\n313/313 - 0s - 88ms/epoch - 281us/step\n\nas.numeric(predcl[1:36])\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\ng_test[1:36]\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\n\nOnly one of the images among the first 20 has been misclassified. The ninth image was labeled as a “5” but has been predicted as a “4”. Looking at the softmax probabilities for image #34, two categories have a sizable probability: the algorithm is not quite sure whether the image depicts a “5” or a “6”.\n\nround(predvals[34,],4)\n\n [1] 0.0001 0.0000 0.0001 0.0000 0.9994 0.0001 0.0003 0.0000 0.0000 0.0000\n\n\nWe can visualize the data with the image function. The next code segment does this for the first and ninth observations.\n\n# visualize the digits\nplotIt &lt;- function(id=1) {\n    im &lt;- mnist$test$x[id,,]\n    im &lt;- t(apply(im, 2, rev)) \n    image(1:28, 1:28, \n          im, \n          col=gray((0:255)/255), \n          xaxt='n', \n          main=paste(\"Image label: \",\n                     g_test[id], \n                     \" Predicted: \", \n                     as.numeric(predcl[id])))\n}\n\nplotIt(1)\n\n\n\n\n\n\n\nplotIt(9)\n\n\n\n\n\n\n\nplotIt(34)\n\n\n\n\n\n\n\n\nWe see why predicting image #34 correctly could be challenging. That is one ugly “4”.\n\n\n\nMultinomial logistic regression\nA 96% accuracy is impressive, but maybe it is not good enough. In applications where the consequences of errors are high, this accuracy might be insufficient. Suppose we are using the trained network to recognize written digits on personal checks. Getting 400 out of 10,000 digits wrong would be unacceptable. Banks would deposit incorrect amounts all the time.\nIf that is the application for the trained algorithm, we should consider other models for these data. This raises an interesting question: how much did we gain by adding the layers of the network? If this is an effective strategy to increase accuracy then we could consider adding more layers. If not, then maybe we need to research an entirely different network architecture.\nBefore trying deeper alternatives we can establish one performance benchmark by removing the hidden layers and training what essentially is a single layer perceptron (Section 31.1). This model has an input layer and an output layer. In terms of the keras syntax it is specified with a single layer:\n\nmodellr &lt;- keras_model_sequential() %&gt;%\n    layer_dense(input_shape=784, \n                units=10,\n                activation=\"softmax\")\nsummary(modellr)\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 10)                      7850        \n================================================================================\nTotal params: 7850 (30.66 KB)\nTrainable params: 7850 (30.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nThis is essentially a multinomial logistic regression model with a 10-category target variable and 784 input variables. The model is much smaller than the previous network (it has only 7,850 parameters) but is huge if we think of it as a multinomial logistic regression model. Many software packages for multinomial regression would struggle to fit a model of this size. When articulated as a neural network, training such a model is actually a breeze.\nWe proceed just as before.\n\nmodellr %&gt;% compile(loss = \"categorical_crossentropy\",\n     optimizer = optimizer_rmsprop(), \n     metrics = c(\"accuracy\"))\n\nhistory &lt;- modellr %&gt;% fit(x_train, \n                y_train, \n                epochs=20,\n                batch_size=128,\n                validation_data=list(x_test, y_test))\n\nEpoch 1/20\n469/469 - 0s - loss: 0.6051 - accuracy: 0.8498 - val_loss: 0.3478 - val_accuracy: 0.9065 - 406ms/epoch - 867us/step\nEpoch 2/20\n469/469 - 0s - loss: 0.3339 - accuracy: 0.9078 - val_loss: 0.3005 - val_accuracy: 0.9160 - 231ms/epoch - 492us/step\nEpoch 3/20\n469/469 - 0s - loss: 0.3048 - accuracy: 0.9151 - val_loss: 0.2886 - val_accuracy: 0.9200 - 227ms/epoch - 485us/step\nEpoch 4/20\n469/469 - 0s - loss: 0.2913 - accuracy: 0.9186 - val_loss: 0.2795 - val_accuracy: 0.9218 - 220ms/epoch - 470us/step\nEpoch 5/20\n469/469 - 0s - loss: 0.2834 - accuracy: 0.9212 - val_loss: 0.2750 - val_accuracy: 0.9234 - 220ms/epoch - 468us/step\nEpoch 6/20\n469/469 - 0s - loss: 0.2777 - accuracy: 0.9226 - val_loss: 0.2723 - val_accuracy: 0.9251 - 222ms/epoch - 473us/step\nEpoch 7/20\n469/469 - 0s - loss: 0.2735 - accuracy: 0.9243 - val_loss: 0.2726 - val_accuracy: 0.9232 - 220ms/epoch - 469us/step\nEpoch 8/20\n469/469 - 0s - loss: 0.2699 - accuracy: 0.9253 - val_loss: 0.2687 - val_accuracy: 0.9254 - 220ms/epoch - 469us/step\nEpoch 9/20\n469/469 - 0s - loss: 0.2674 - accuracy: 0.9265 - val_loss: 0.2688 - val_accuracy: 0.9250 - 223ms/epoch - 475us/step\nEpoch 10/20\n469/469 - 0s - loss: 0.2651 - accuracy: 0.9271 - val_loss: 0.2665 - val_accuracy: 0.9267 - 221ms/epoch - 472us/step\nEpoch 11/20\n469/469 - 0s - loss: 0.2629 - accuracy: 0.9283 - val_loss: 0.2687 - val_accuracy: 0.9256 - 220ms/epoch - 468us/step\nEpoch 12/20\n469/469 - 0s - loss: 0.2614 - accuracy: 0.9282 - val_loss: 0.2655 - val_accuracy: 0.9262 - 224ms/epoch - 478us/step\nEpoch 13/20\n469/469 - 0s - loss: 0.2598 - accuracy: 0.9295 - val_loss: 0.2650 - val_accuracy: 0.9269 - 222ms/epoch - 473us/step\nEpoch 14/20\n469/469 - 0s - loss: 0.2585 - accuracy: 0.9292 - val_loss: 0.2644 - val_accuracy: 0.9273 - 222ms/epoch - 473us/step\nEpoch 15/20\n469/469 - 0s - loss: 0.2573 - accuracy: 0.9298 - val_loss: 0.2636 - val_accuracy: 0.9275 - 228ms/epoch - 486us/step\nEpoch 16/20\n469/469 - 0s - loss: 0.2564 - accuracy: 0.9302 - val_loss: 0.2650 - val_accuracy: 0.9273 - 225ms/epoch - 480us/step\nEpoch 17/20\n469/469 - 0s - loss: 0.2551 - accuracy: 0.9313 - val_loss: 0.2657 - val_accuracy: 0.9274 - 223ms/epoch - 475us/step\nEpoch 18/20\n469/469 - 0s - loss: 0.2542 - accuracy: 0.9312 - val_loss: 0.2644 - val_accuracy: 0.9282 - 223ms/epoch - 475us/step\nEpoch 19/20\n469/469 - 0s - loss: 0.2535 - accuracy: 0.9316 - val_loss: 0.2664 - val_accuracy: 0.9266 - 221ms/epoch - 472us/step\nEpoch 20/20\n469/469 - 0s - loss: 0.2529 - accuracy: 0.9315 - val_loss: 0.2642 - val_accuracy: 0.9283 - 221ms/epoch - 472us/step\n\nplot(history, smooth = FALSE)\n\n\n\n\n\n\n\n\nEven with just a single layer, the model performs quite well, its accuracy is around 92%. Adding the additional layer in the previous ANN did improve the accuracy. On the other hand, it took more than 100,000 extra parameters to move from 92% to 96% accuracy.\n\n\n\nFigure 33.1: Epoch history for the first 200 epochs.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Neural Networks in `R` (with Keras)</span>"
    ]
  },
  {
    "objectID": "deeplearning.html",
    "href": "deeplearning.html",
    "title": "34  Deep Learning",
    "section": "",
    "text": "34.1 Introduction\nWe can at best provide an introduction into the vast topic of deep learning in this material. Our goal is to continue the discussion of neural networks and move from the generic ANN to specialized neural networks that are used for specific types of data and to specialized forms of learning from data. Figure 34.1 categorizes important approaches in deep learning.\nConvolutional neural networks process data that appears on a grid (Section 34.2). The most frequent case is two-dimensional data in images that are arranged into rows and columns of pixels. These networks take advantage of the spatial layout of the pixels and train neural networks for tasks that are relevant in computer vision: object detection, object classification, edge detection, face detection and recognition, optical character recognition (OCR) and so forth.\nRecurrent neural networks process data that are sequential in nature such as text or time series (Section 34.3).\nDeep neural networks are one manifestation of deep learning.\nSince 2017, when the architecture was described in Vaswani et al. (2017), and in particular since 2022, when Chat GPT 3.5 was released, transformer architectures have revolutionized the approach to natural language modeling and to artificial intelligence in general. We give a brief introduction into transformer architectures as an evolution of recurrent encoder/decoder networks in Section 34.4.\nReinforcement learning also belongs to the deep learning technologies (Chapter 35). It is a very different form of learning that does not fall neatly into the delineation of supervised and unsupervised methods of learning. In reinforcement learning, an agent reacts to feedback from the environment and chooses actions to optimize future positive feedback.\nDeep learning flipped the narrative about artificial intelligence applying to tasks that are easy for computers but difficult for humans. Automating things that are intuitive for us, such as sensing the world, reading, writing, playing games, understanding language, were long thought to be out of reach for computers. That changed with the rise of deep learning in the early 2000s and was marked by watershed moments such as a reinforcement-trained system beating the world’s best Go player 4:1.\nThe idea of deep learning is to represent solutions as a hierarchy of related concepts. The hierarchy looks like a layered graph, hence the name deep learning. Instead of engineering features and trying to predict or classify based on carefully chosen features, deep learning starts with the obvious inputs (pixels, audio, text) and learns concepts about them from the data. Instead of providing to the system representation of the concepts through features, we let the system discover representations. The definition of deep learning according to Goodfellow, Bengio, and Courville (2016) reflects this view:\nFigure 34.2, adapted from Goodfellow, Bengio, and Courville (2016), depicts this concept for the case of object classification with images. To build a model that can classify the content of the image directly from the pixel information is difficult at best. However, we can train a model to learn different representations about the image content, first colors, then edges, and so forth. The combination of the abstract representations then allows the algorithm to assign “probabilities” with the possible object categories. Based on the probabilities we choose a possible explanation.\nA fundamental issue with artificial intelligence for tasks that are intuitive for humans is how to get the required knowledge about the world into the machine? A computer does not look at a photo the same way we do. It sees pixels, we see concepts. Once we know what a tomato looks like we can conceptualize tomatoness and recognize a different tomato variety immediately. How do we teach computers about tomatoes?\nThe deep learning solution is to extract knowledge from data through machine learning. Several implicit assumptions are at work here:\nThis raises many fundamental questions. For example, by scanning all the text on the internet, does a machine really understand language? Is language not more—much more—than just data? Is a computer vision algorithm that has 95% accuracy in object classification not just a pixel pattern-matching system and does not really understand the concepts it is classifying? The answer to both questions is an emphatic “Yes!”\nThe quintessential tool for deep learning is the deep neural network.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#introduction",
    "href": "deeplearning.html#introduction",
    "title": "34  Deep Learning",
    "section": "",
    "text": "Figure 34.1: Deep learning mind map.\n\n\n\n\n\n\n\n\n\n\n\n\nDeep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe put probabilities in quotation marks because the last step of the\n\n\n\n\n\n\n\n\nFigure 34.2: Deep learning as a hierarchy of concepts. Adapted from Goodfellow, Bengio, and Courville (2016)\n\n\n\n\n\n\nthe knowledge required to understand a system is actually in the data\nthe knowledge can be extracted\nthere is enough data to make sure the machine can learn and learn the right thing",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-cnn",
    "href": "deeplearning.html#sec-dl-cnn",
    "title": "34  Deep Learning",
    "section": "34.2 Convolutional Neural Networks",
    "text": "34.2 Convolutional Neural Networks\nConvolutional neural networks (CNN) have become the dominant architecture in computer vision applications—until transformer architectures, that is. A CNN is a neural network for data with a grid-like topology, such as the array of pixels on an image. In contrast to a generic ANN with fully connected layers, a CNN takes advantage of the spatial arrangement of the data. CNNs can also be applied to types of time series data such as samples at regular time intervals. It turns out that another class of deep learning networks, the recurrent neural networks (RNNs), are better suited to analyze sequential data.\nCNNs take advantage of the rich structure of image data to\n\nreduce the number of parameters\nincrease opportunities for parallel processing\nanswer questions that are pertinent to this type of data, such as object detection and classification (Figure 34.3 and Figure 34.4), facial recognition (Figure 34.5), etc.\n\n\n\n\n\n\n\nFigure 34.3: Object detection and classification\n\n\n\n\n\n\n\n\n\nFigure 34.4: Object detection and classification.\n\n\n\n\n\n\n\n\n\nFigure 34.5: Facial detection and recognition\n\n\n\nIn Section 31.4 we took a first look at the MNIST Image classification data. Recall that each image is a 3-dimensional tensor that consists of a 28 x 28 array of pixels and a grayscale value. The two-layer fully connected AN with 128 and 64 neurons in the hidden layers had 109,386 parameters. This approach does not scale to larger images. A 1-megapixel color image would have \\(3 \\times 10^6\\) features if we analyze it like the MNIST data in Section 31.4 and Section 33.2.3. The classification accuracy of the fully connected ANN of about 95% looks good at first on paper, but might not be sufficient in practical applications. An optical character recognition (OCR) system that reads numbers from hand-written checks cannot get 5 out of every 100 digits wrong.\nSomehow, images need to be processed differently than with a fully connected neural network. Also, the network architectures for image processing should have other desirable properties:\n\nTranslation invariance: In early layers the network should respond similarly to the same pattern, regardless of where it appears. What Waldo looks like does not depend on where Waldo is located (Figure 34.6). Shifting the input should shift the output.\nLocality: early layers of the network should concentrated on local features. Local representations can be aggregated to make predictions at the level of the entire image. Deeper layers should capture longer-ranging features.\n\n\n\n\n\n\n\nFigure 34.6: Where is Waldo?\n\n\n\n\nParameter Sharing: A fully connected network uses each element of the weight matrix exactly once when computing the output of the layer. To reduce the number of parameters, weights should be reused in some form.\n\nConvolutional networks accomplish these goals through a special layer architecture, using convolutions and pooling.\n\nConvolutions\n\nDefinition\nMathematically, convolutions of continuous or discrete functions are defined as follows.\n\n\nDefinition: Convolution\n\n\nThe convolution \\((f\\ast g)(t)\\) of functions \\(f\\) and \\(g\\) describes the process—as well as the result—of computing the integral of one function after the other function is shifted and reflected about the \\(y\\)-axis \\[\n    (f \\ast g)(t) = \\int_{-\\infty}^\\infty f(x)\\,g(t-x)\\,dx\n\\] We can interpret this as averaging \\(f\\) with the weighting (kernel) function \\(g\\) or as measuring the overlap between \\(f\\) and \\(g\\) when one function is “flipped” and shifted by \\(t\\).\nIf \\(f\\) and \\(g\\) are defined on the integers, the discrete convolution is \\[\n(f \\ast g)(i) = \\sum_{m=-\\infty}^\\infty f(m)g(i-m)\n\\]\nIn two dimensions, the discrete convolution is \\[\n    (f \\ast g)(i,j) = \\sum_{m}\\sum_{n} f(m,n) \\, g(i-m,j-n)\n\\]\nBecause convolution is commutative, you can equivalently write \\[\n\\begin{align*}\n    (f \\ast g)(t) &= \\int_{-\\infty}^\\infty f(t-x)\\,g(x)\\,dx \\\\\n    (f \\ast g)(i) &= \\sum_{m=-\\infty}^\\infty f(i-m)g(m)\\\\\n    (f \\ast g)(i,j) &= \\sum_{m}\\sum_{n} f(i-m, j-n) \\, g(m,n)\n\\end{align*}\n\\]\n\n\n\n\nHow it works\nIn processing images in deep learning, what is called a convolution is mathematically more akin to the concept of the cross-correlation between two functions: \\[\n(f \\ast g)(i,j) = \\sum_m \\sum_n f(i+m,j+n)g(m,n)\n\\] From a practical perspective it does not matter whether it is a genuine convolution or a cross-correlation. What matters is that we are taking weighted combinations of the cells of an array with the elements of some discrete kernel functions. Figure 34.7 and Figure 34.8 show this process for the convolution of a 3 x 4 image with a 2 x 2 kernel with stride 1.\n\n\n\n\n\n\nFigure 34.7: Convolving the first row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\n\n\n\n\n\n\n\n\n\nFigure 34.8: Convolving the second row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\n\n\n\nThe operations are not matrix multiplications, they are a form of dot product between the values in the array and the values of the kernel. Once a group of cells has been convolved, the kernel shifts to a new location and processes the next \\(2 \\times 2\\) set of cells. The kernel can shift by one cell, creating overlap with the previous convolution or by two cells, avoiding overlap. This shift is known as the stride of the convolution. The figures above show the process for a stride of 1.\nThe result of the convolution is an array of size \\(2 \\times 3\\). The size of the array has decreased and the values in the output cells are weighted combinations of the values in the input cells.\nRather than having separate weights (and biases) for all cells of the array, the parameters of the convolution are the weights in the kernel array. The same kernel is applied to all sub-arrays as it slides across the input array. This is a form of parameter sharing. Instead of estimating parameters associated with all input cells, only the weights in the kernel need to be estimated—a much smaller number. On the other hand, we have introduced a number of new decisions that affect the architecture of the convolution layer:\n\nthe size of the kernel\nthe stride of the kernel\nthe handling of the boundary values\n\n\n\nSparse connectivity\nThe effect of the convolution is sparse connectivity between input and output layers. An input unit does not affect all output units and an output unit is not affected by all input units. In a fully connected network all input units are connected to all output units. The receptive field of a unit are the neurons in the previous layer that contribute to it. In a CNN, only the neurons covered by the kernel window fall into the receptive field, providing locality. This allows the layer to focus on pixels in the vicinity of the input rather than receiving contributions from all pixels across the image. Similarly, when viewed from the input layer (viewed from below), a neuron affects only those output units that include it in their receptive fields. Figure 34.9, adapted from Goodfellow, Bengio, and Courville (2016), displays the concepts of sparse and full connectivity.\n\n\n\n\n\n\nFigure 34.9: Sparse and full connectivity, adapted from Goodfellow, Bengio, and Courville (2016, 336–37)\n\n\n\n\n\nHandling boundary values\nConsider again the convolution example in Figure 34.8. If we had chosen a stride of 2, how could we have processed the second row of the \\(3 \\times 4\\) array, there is no fourth row to apply the kernel. Handling the boundary values of grids can be done with several methods:\n\nPadding: assume constant values (zeros or other values) beyond the border.\nExtending: assume that the values in boundary cells extend beyond the border cells.\nMirroring: to read at \\(m\\) pixels outside image use the value from \\(-m\\) pixels inside image.\nWrapping: the image is wrapped on edge or corner.\nCropping: limit convolutions to interior cells that fully overlay the kernel. This means no padding and is the keras method padding=\"valid\".\n\nHow far you have to pad, extend, mirror, or wrap depends on the image size, the kernel size and the stride. Different amounts of padding in the horizontal and vertical direction makes sense for rectangular-shaped input arrays.\n\n\nFiltering example\nTo see how kernels can extract features of arrays, we consider a special case where the kernel weights are predetermined to create a desired effect. Suppose you are dealing with grayscale images and you subtract from a cell the value of the neighboring pixel to the right (or left). This operation will reveal vertical edges. A slightly more sophisticated application is the Sobel filter for finding edges. The filter consists of two kernels, \\(G_x\\) for edges in the \\(x\\)-direction (horizontal) and its transpose, \\(G_y\\) for the detection of edges in the \\(y\\)-direction: \\[\nG_x = \\left [ \\begin{array}{rrr} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{array} \\right ]\n    \\qquad\nG_y = \\left [ \\begin{array}{rrr} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{array} \\right ]\n\\]\nFigure 34.10 depicts the application of \\(G_x\\) to the first cell of the array after extending the bounday by one cell. Figure 34.11 shows the result of the convolution for all cells.\n\n\n\n\n\n\nFigure 34.10: Edge detection with Sobel filter \\(G_x\\).\n\n\n\n\n\n\n\n\n\nFigure 34.11: Edge detection with Sobel filter \\(G_x\\).\n\n\n\nFilters such as the Sobel filter are applied in classical image processing where known transformations are applied to the pixels. When fitting convolutional neural networks we do not specify which filters to apply. Instead, we let the algorithm learn the best filters for a layer by estimating the weights of the kernel.\nAfter convolving the cells, the values pass through an activation function as in other neural networks—also called the detector stage. ReLU activation is common these days, sigmoid functions were popular prior to the discovery of the rectified linear unit. The convolution and activation are often combined into a single step, sometimes they are shown as separate processing stages (separate layers).\n\n\n\nPooling\nThe pooling stage of a convolutional layer—often shown as its own layer—takes the output of a convolved and activated layer and replaces its values with summaries of nearby values. Similar to applying a kernel, a pooling layer has a window of cells to consider and a stride by which it shifts across the array. Unlike the convolution layer, there are no parameters associated with pooling. The effect of pooling is a reduction in the dimension of the output; the goal is to perform this reduction with minimal loss of information. The pooling layer is also called a downsample layer for this reason.\nAdding a pooling layer serves two purposes: to mitigate the sensitivity of the convolution to location and to downsample the representation, reducing the number of parameters for subsequent layers.\nFigure 34.12 shows a full convolutional layer with all three phases: convolution, activation (here, ReLU), and pooling. Two pooling operations are shown for \\(2 \\times 2\\) pooling with stride 2: using the average of the values and using the maximum value in the pooling window.\n\n\n\n\n\n\nFigure 34.12: Convolutional layer with pooling.\n\n\n\nThe most common pooling operations are taking the max value (max-pooling) and taking the average, with a preference of the former because it confers more invariance to the output.\n\n\nConsiderations\n\nChannels and feature maps\nWhen working with images and CNNs you encounter the terms channels and feature maps. The terms are used interchangeably and refer to the number of input or output features. When processing color images, channels at the input layer refer to the RGB color values, the input layer has three channels or feature maps. In the case of the MNIST digit data set, there is a single channel, the grayscale value.\nAfter the input layer, the number of channels or feature maps is determined by the number of filters. A convolutional layer with 16 filters has 16 output channels, one per filter. Each channel has a different set of weights, but the weights within a channel that convolve the image are the same (parameter sharing).\nWe think of the first layer of the CNN as consuming the color information and other layers creating new channels of information about the image.\nEach layer of a CNN has input and output features, known as feature maps.\nAfter the input layer, the number of feature maps is determined by the number of filters. After the input layer, these are also referred to as the channels. At the input layer, the channels for RGB images refer to the three color values.\n\n\nOther considerations\nThere are many architectural choices that define a convolutional neural network. The number and arrangement of convolutional and pooling layers. Strides of kernels and pooling windows. The handling of boundary values. The choice of activation functions.\nHere are some general recommendations:\n\nHaving multiple convolutional layers before a pooling layer effectively increases the dimension of the filter.\nMax poling is used more frequently than average pooling.\nReLU is the most important activation function in CNNs today.\nThe number of filters increases for the deeper layers of the network as feature maps get smaller due to previous pooling.\nWhen the feature maps are sufficiently reduced the network is flattened, treating the pixels as separate units. This is followed by one or more fully connected layer(s) and an output layer.\nYou can have multiple convolutional layers before a pooling layer. In fact, the pooling effect of reducing the output size can also be achieved with a convolutional layer without padding and stride greater than 1.\nYou can add dropout layers in CNNs just like in ANNs to reduce the number of neurons and to avoid overfitting. However, dropout layers are less common in CNNs because the number of parameters can be managed through pooling layers and padding/stride of convolutional layers. In the fully connected parts near the end of CNNs dropout can make sense.\n\n\n\n\nLeNet\nIt is common in image processing tasks to apply a previously defined network. Famous network architectures are AlexNet, VGG, GoogLeNet, and others. These can be seen as improvements over the first CNN that solved an important practical problem and found commercial application: LeNet\nLeNet is a famous example of a CNN, developed by Yann LeCun, a founding father of modern deep learning, between 1989 and 1995. Various iterations of LeNet exist, we describe and train LeNet-5 in this chapter. This network outperformed other approaches for hand-written digit classification and started the deep learning revolution.\nIt is remarkable that LeNet was developed and used for character recognition as early as 1989. Computing resources were extremely limited compared to today. In fact, reducing the number of parameters that needed to be estimated while increasing accuracy was part of the motivation of developing specialized networks for specialized applications.\nHere is a video of Yann LeCun demonstrating the first LeNet for character recognition using a 486 PC with a DSP (digital signal processing card).\n\n\nNetwork structure\nThe network structure of LeNet-5 for the MNIST data starts with a single channel of 28 x 28 pixel images (Figure 34.13).\n\n\n\n\n\n\nFigure 34.13: LetNet-5 architecture. Source\n\n\n\nThe first convolution layer (C1) has 6 filters (kernels) of size 5 x 5. Its result is passed to a 2 x 2 pooling layer with stride 2. This reduces the dimension of the output to 14 x 14. The next convolutional layer C2 applies 16 filters of size 5 x 5, leading to output channels of size 10 x 10. These pass through another pooling layer to reduce the output to size 5 x 5. After flattening, two fully connected layers follow with 120 and 84 units, respectively. The final dense output layer has 10 units corresponding to the digits “0”–“9”. The convolutional and fully connected layers used sigmoid activation functions. The original LeNets used a Gaussian decoder rather than the softmax output function.\nSetting up LeNet-5 in keras is pretty straightforward. You build up the layers sequentially as just described.\n\nlibrary(keras)\nreticulate::use_condaenv(condaenv = \"r-tensorflow\")\n\nlenet5 &lt;- keras_model_sequential() %&gt;%\n    layer_conv_2d(kernel_size=c(5,5),\n                  filters    =6,\n                  padding    =\"same\",\n                  activation =\"sigmoid\",\n                  input_shape=c(28,28,1),\n                  name       =\"Conv_1\"\n                  ) %&gt;%\n    layer_average_pooling_2d(pool_size=c(2,2),\n                             name     =\"Pool_1\") %&gt;%\n    layer_conv_2d(kernel_size=c(5,5),\n                  filters    =16,\n                  activation =\"sigmoid\",\n                  name       =\"Conv_2\") %&gt;%\n    layer_average_pooling_2d(pool_size=c(2,2),\n                             name=\"Pool_2\") %&gt;%\n    layer_flatten(name=\"Flatten\") %&gt;%\n    layer_dense(units     =120,\n                activation=\"sigmoid\",\n                name      =\"FC_1\") %&gt;%\n    layer_dense(units     =84,\n                activation=\"sigmoid\",\n                name      =\"FC_2\") %&gt;%\n    layer_dense(units     =10,\n                activation=\"softmax\",\n                name      =\"Output\")\n\nThe only difference to the original LeNets is that we use softmax in the output layer.\nThe new layer types compared to Section 33.2.3 are\n\nlayer_conv_2d: a 2-d convolutional layer\nlayer_average_pooling_2d: a pooling layer for 2-d data\nlayer_flatten: a flattening layer\n\nLeNet-5 is much deeper than the 2-hidden layer ANN in Section 33.2.3. Recall that ANN had 109,386 parameters. How many parameters are in LeNet-5?\n\nsummary(lenet5)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n Conv_1 (Conv2D)                    (None, 28, 28, 6)               156         \n Pool_1 (AveragePooling2D)          (None, 14, 14, 6)               0           \n Conv_2 (Conv2D)                    (None, 10, 10, 16)              2416        \n Pool_2 (AveragePooling2D)          (None, 5, 5, 16)                0           \n Flatten (Flatten)                  (None, 400)                     0           \n FC_1 (Dense)                       (None, 120)                     48120       \n FC_2 (Dense)                       (None, 84)                      10164       \n Output (Dense)                     (None, 10)                      850         \n================================================================================\nTotal params: 61706 (241.04 KB)\nTrainable params: 61706 (241.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nNotice that pooling and flattening are parameter-free operations, like the dropout layer.\nLeNet-5 has only 56% of the parameters of the 2-layer ANN. Does it perform better than the larger ANN on this data?\n\n\nTraining MNIST data with keras\nAll the data pre-processing steps are the same as in Section 33.2.3. We also use the same optimization setup.\n\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x\ng_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x\ng_test &lt;- mnist$test$y\n\ny_train &lt;- to_categorical(g_train, 10)\ny_test &lt;- to_categorical(g_test, 10)\n\n\nlenet5 %&gt;% compile(loss=\"categorical_crossentropy\",\n                   optimizer=optimizer_rmsprop(), \n                   metrics=c(\"accuracy\")\n                   )\n\nNow we are ready to go. The final step is to supply training data, and fit the model.\n\nhistory &lt;- lenet5 %&gt;%\n      fit(x_train, \n          y_train, \n          epochs=15, \n          batch_size=128,\n          validation_data= list(x_test, y_test)\n          )\n\nEpoch 1/15\n469/469 - 3s - loss: 1.0360 - accuracy: 0.7042 - val_loss: 0.3174 - val_accuracy: 0.9099 - 3s/epoch - 7ms/step\nEpoch 2/15\n469/469 - 3s - loss: 0.2536 - accuracy: 0.9253 - val_loss: 0.1946 - val_accuracy: 0.9434 - 3s/epoch - 7ms/step\nEpoch 3/15\n469/469 - 3s - loss: 0.1699 - accuracy: 0.9484 - val_loss: 0.1369 - val_accuracy: 0.9567 - 3s/epoch - 7ms/step\nEpoch 4/15\n469/469 - 3s - loss: 0.1301 - accuracy: 0.9599 - val_loss: 0.1267 - val_accuracy: 0.9620 - 3s/epoch - 7ms/step\nEpoch 5/15\n469/469 - 3s - loss: 0.1078 - accuracy: 0.9667 - val_loss: 0.1095 - val_accuracy: 0.9645 - 3s/epoch - 7ms/step\nEpoch 6/15\n469/469 - 3s - loss: 0.0925 - accuracy: 0.9717 - val_loss: 0.0754 - val_accuracy: 0.9763 - 3s/epoch - 7ms/step\nEpoch 7/15\n469/469 - 3s - loss: 0.0813 - accuracy: 0.9751 - val_loss: 0.0826 - val_accuracy: 0.9730 - 3s/epoch - 7ms/step\nEpoch 8/15\n469/469 - 3s - loss: 0.0731 - accuracy: 0.9770 - val_loss: 0.0601 - val_accuracy: 0.9811 - 3s/epoch - 7ms/step\nEpoch 9/15\n469/469 - 3s - loss: 0.0675 - accuracy: 0.9789 - val_loss: 0.0672 - val_accuracy: 0.9780 - 3s/epoch - 7ms/step\nEpoch 10/15\n469/469 - 3s - loss: 0.0622 - accuracy: 0.9809 - val_loss: 0.0592 - val_accuracy: 0.9802 - 3s/epoch - 7ms/step\nEpoch 11/15\n469/469 - 3s - loss: 0.0569 - accuracy: 0.9822 - val_loss: 0.0530 - val_accuracy: 0.9818 - 3s/epoch - 7ms/step\nEpoch 12/15\n469/469 - 3s - loss: 0.0541 - accuracy: 0.9831 - val_loss: 0.0507 - val_accuracy: 0.9838 - 3s/epoch - 7ms/step\nEpoch 13/15\n469/469 - 3s - loss: 0.0500 - accuracy: 0.9843 - val_loss: 0.0488 - val_accuracy: 0.9849 - 3s/epoch - 7ms/step\nEpoch 14/15\n469/469 - 3s - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0458 - val_accuracy: 0.9845 - 3s/epoch - 7ms/step\nEpoch 15/15\n469/469 - 3s - loss: 0.0451 - accuracy: 0.9857 - val_loss: 0.0449 - val_accuracy: 0.9844 - 3s/epoch - 7ms/step\n\nplot(history, smooth=FALSE)\n\n\n\n\n\n\n\n\nAfter only 15 epochs the LeNet model has achieved a classification accuracy of 98.64%, considerably higher than the fully connected network, and with fewer parameters. You can improve the model to 99% accuracy by continuing to train. Also, I encourage you to play with different activation functions (try the hyperbolic tangent), batch sizes, layer structure, etc.\nHere are some of the predicted values. Remember that the first “5” in position 9 of the test data set was notoriously difficult to classify for the ANN.\n\npredcl &lt;- lenet5 %&gt;% predict(x_test) %&gt;% k_argmax() \n\n313/313 - 0s - 364ms/epoch - 1ms/step\n\ng_test[1:36]\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\nas.numeric(predcl[1:36])\n\n [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 8 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2\n\n\n\nplotIt &lt;- function(id=1) {\n    im &lt;- mnist$test$x[id,,]\n    im &lt;- t(apply(im, 2, rev)) \n    image(1:28, 1:28, \n          im, \n          col=gray((0:255)/255), \n          xaxt='n', \n          main=paste(\"Image label: \",\n                     g_test[id], \n                     \" Predicted: \", \n                     as.numeric(predcl[id])))\n}\n\nplotIt(1)\n\n\n\n\n\n\n\nplotIt(9)\n\n\n\n\n\n\n\n\n\n\n\nMerlin Bird ID\nIf you love technology and the outdoors, like me, then you might get a kick out of the Merlin Bird ID app from Cornell University. It can identify birds based on sound captured on your device. Simply stand outside, use Sound ID in the app, and Merlin will recognize the birds within “hearing” distance.\nFigure 34.14 shows part of my “Life List” of birds identified by sound through the app at our home in Blacksburg, VA.\n\n\n\n\n\n\nFigure 34.14: Part of my Life List in Merlin Bird ID.\n\n\n\nWhy bring this up here? Convolutional neural networks are not just for image data. As your phone records sounds, Sound ID converts it into a spectral representation of frequencies over time. The spectogram is the input to a convolutional neural network that maps spectograms to bird species. In addition to the spectral signature, Sound ID also uses spatial and temporal information to improve the accuracy of the bird species prediction. Over 1,300 bird species are supported, that is a pretty big softmax output layer.\nFigure 34.15 shows the spectogram of a sound signature identified as that of a Carolina Wren.\n\n\n\n\n\n\nFigure 34.15: Spectogram in Merlin Bird ID.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-rnn",
    "href": "deeplearning.html#sec-dl-rnn",
    "title": "34  Deep Learning",
    "section": "34.3 Recurrent Neural Networks",
    "text": "34.3 Recurrent Neural Networks\nRecurrent neural networks (RNNs) process sequential data, data in which the order of the information is important. Examples of such data types are\n\nWritten documents: summarize, capture sentiment, translate language\nRecordings: summarize, transcribe, assess quality, classify speech, music, and other sounds\nTime series: forecast future values, predict financial indices\nHandwriting: digitization, optical character recognition\n\nUntil generative transformers appeared on the scene, RNNs were the go-to architecture in natural language processing.\nWhile CNNs take advantage of the spatial arrangement of data, RNNs take advantage of the serial nature of data. Like CNNs, they use parameter sharing to reduce the number of parameters, to generalize models, to require fewer input samples and to gain statistical strength across sequences of different lengths. The parameter sharing in CNNs happens in the convolutional layers by using kernels with the same weights across all subarrays of an image. RNNs share parameters in a different way: Each member of the output\n\nis a function of the previous members of the output\nis produced using the same update rules applied previously\n\nThe challenge of processing order-dependent data is to imbue the system with some form of “memory”. When processing a word in a sentence, the system must have some notion of how the word relates to previous words. Locating the year in the sentence “I was born in 1983” and “In 1983, I was born” should not require different sets of parameters. Recurrent neural networks accomplish that by replacing the hidden layer in ANN with a special layer that maintains the internal state of past layer inputs—this state is called the hidden state of the network.\nThe input is processed sequentially—one word or token at a time—and the hidden state is updated at each step. Output is generated based on the internal state and the new input value.\nTo make these ideas more concrete, suppose that we are processing some text. First, we use an encoding mechanism to transform the text in a series of input vectors. For example, using one-hot encoding with a dictionary of 1,000 words, the input is a sequence \\(\\textbf{x}^{(1)}, \\cdots, \\textbf{x}^{(n)}\\) of \\(n\\) vectors of size \\(1000 \\times 1\\). Each vector has a 1 in the dictionary position where the word is found and 999 zeros in all other positions. One can also encode the input with a word embedding, where each word is represented by a \\(m\\)-dimensional vector of real numbers, called an embedding. The embeddings can be learned as separate layers or you can use pre-trained embeddings such as word2vec. GloVe, fastText, ELMo, and others.\nLet \\(\\textbf{h}^{(t)}\\) denote the state of the system at “time” \\(t%\\). Time is simply an indication of the position in the system, not necessarily wall clock time. A dynamic (recurrent) system is defined as \\[\n\\textbf{h}^{(t)} = f(\\textbf{h}^{(t-1)},\\textbf{x}^{(t)};\\boldsymbol{\\theta})\n\\]\nA recurrent neural network learns \\(\\textbf{h}^{(t)}\\) from \\([\\textbf{x}^{(t)},\\textbf{x}^{(t-1)},\\textbf{x}^{(t-2)},\\cdots,\\textbf{x}^{(2)},\\textbf{x}^{(1)}]\\) by using at each step the same transition function \\(f\\) and the same parameters \\(\\boldsymbol{\\theta}\\).\nThe traditional way of depicting neural networks as connected neurons with activations does not work well for RNNs. Instead, two ways of representing recurrence in networks are popular, known as the folded and unfolded views (Figure 34.16)\n\n\n\n\n\n\nFigure 34.16: Folded and unfolded RNN representation.\n\n\n\nFigure 34.17 shows the full view of an RNN in both representation, adding target, loss functions and weight matrices \\(\\textbf{U}\\), \\(\\textbf{W}\\), and \\(\\textbf{V}\\). These matrices represent the weights for the input-to-hidden state connections, the hidden state-to-hidden state connections, and the hidden state-to-output connections. These weight matrices \\(\\textbf{U}\\), \\(\\textbf{W}\\), and \\(\\textbf{V}\\) are the parameters of the RNN that need to be estimated.\n\n\n\n\n\n\nFigure 34.17: Complete representation of an RNN.\n\n\n\nNotice that a recurrent neural network can generate output at every step of the recurrence. This is relevant in language translation, for example. In other applications, such as sentiment analysis, only the output at the final stage of the sequence matters.\nThe same weights are used at each step, this is where the parameter sharing comes in: \\[\n\\textbf{h}^{(t)} = \\sigma\\left( \\bU\\textbf{x}^{(t)} + \\textbf{W}\\textbf{h}^{(t-1)} + \\textbf{b}\\right)\n\\]\nThe hidden layers of the network, represented by the hidden state at each step, are fed by activations from the previous layer and by the input data for this step.\nAs in other neural network architectures, the size of the layers is a hyperparameter that needs to be tuned. In ANNs and RNNs it corresponds to the number of units in a hidden layer, in CNNs it is a function of the kernel size and the number of channels (feature maps). If the hidden state of an RNN is chosen too small the model will underfit and the network lacks expressive power. If the hidden state is chosen too big the model will overfit and not generalize well to new input sequences.\n\nVanishing Gradients\nThe vanishing gradient problem, first discussed in Section 32.3.2.1, is particular punishing for recurrent neural networks, because the same weight matrices are used at all steps of the sequence. Exploding gradients, the opposite problem, when repeated multiplication leads to very large values, is also an issue with RNNs.\nSuppose that we repeatedly multiply the square symmetric matrix \\(\\textbf{A}\\) with itself. Apply the eigenvalue decomposition \\[\n\\textbf{A}= \\textbf{Q}\\boldsymbol{\\lambda}\\textbf{Q}^\\prime\n\\] where \\(\\boldsymbol{\\lambda}\\) is the diagonal matrix of eigenvalues and \\(\\textbf{Q}\\) is an orthogonal matrix. You can now write the \\(k\\)-fold product in terms of the decomposition: \\[\n\\textbf{A}\\times \\textbf{A}\\times \\cdots \\times \\textbf{A}= \\textbf{Q}\\boldsymbol{\\lambda}^k\\textbf{Q}^\\prime\n\\] The \\(k\\)-fold product involves the \\(k\\)th power of the eigenvalues. If \\(\\lambda_i &lt; 0\\), then \\(\\lambda_i^k\\) will vanish. If \\(\\lambda_i &gt; 1\\), then \\(\\lambda_i^k\\) will explode for large values of \\(k\\).\nBecause of this problem, training RNNs can be difficult. Vanishing gradients slow down the learning of the network, making it difficult to learn long-term dependencies. Signals that occur early in the sequence get washed out by the time they have propagated through the chain. Short-term dependencies will receive more weight. Exploding gradients can make the training numerically unstable.\nAs a result, RNNs are limited to learning relatively short sequences (&lt; 20). To overcome these issues, related architectures have been proposed, for example, Gated RNNs and Long Short-Term Memory (LSTM) models.\n\n\nLong Short-Term Memory Models (LSTM)\nThe name long short-term memory appears strange at first. What is long short-term memory? The basic idea of the LSTM becomes clear when its goals are considered: the idea is to create paths through time that do not vanish. Rather than trying to hold on to all past information through the common weights, an LSTM model adds components to the recurrence that allow the network to decide which states to remember and which states to forget.\nAn LSTM provides short-term memory for RNNs that can last many steps in the sequence—hence the name. How it accomplishes this is by way of gates, units that control other elements of the network. Each gate is itself a network with weights, biases, and an activation function. A memory cell of an LSTM (Figure 34.18) consists of\n\nInternal state \\(s^{(t)}\\)\nHidden state \\(h^{(t)}\\)\nInput node \\(C_i^{(t)}\\)\nInput gate \\(i_i{(t)}\\): determines whether an input should affect the internal state\nForget gate \\(f_i^{(t)}\\): determines whether the internal state should be flushed to zero\nOutput gate \\(o_i^{(t)}\\): determines whether the internal state should impact the cell’s output\n\nThe gates are modeled with learned weights and a sigmoid activation function. \\[\n\\begin{align*}\n    f_i^{(t)} &= \\sigma\\left(b_i^f + \\sum_j U_{ij}^f \\,x_j^{(t)} + \\sum_j W_{ij}^f \\,h_j^{(t-1)} \\right ) \\\\\n    i_i^{(t)} &= \\sigma\\left(b_i^i + \\sum_j U_{ij}^i \\,x_j^{(t)} + \\sum_j W_{ij}^i \\,h_j^{(t-1)} \\right ) \\\\\n    o_i^{(t)} &= \\sigma\\left(b_i^o + \\sum_j U_{ij}^o \\,x_j^{(t)} + \\sum_j W_{ij}^o \\,h_j^{(t-1)} \\right )\n\\end{align*}\n\\]\nInput node \\[\n    C_i^{(t)} = \\sigma \\left(b_i + \\sum_j U_{ij} \\,x_j^{(t)} + \\sum_j W_{ij} \\, h_j^{(t-1)} \\right )\n\\]\nInternal state update \\[\n    s_i{(t)} = f_i^{(t)} \\, s_i^{(t-1)} + i_i^{(t)} \\, C_i^{(t)}\n\\] Hidden state update \\[\n    h_{(t)} = \\tanh\\left( s_i^{(t)}\\right) o_i^{(t)}\n\\]\nIf the forget gate \\(f_i^{(t)} = 1\\) and the input gate \\(i_i^{(t)} = 0\\), the cell’s internal state \\(s _i^{(t)}\\) will not change. If the forget gate \\(f_i^{(t)} &lt; 1\\) and the input gate \\(i_i^{(t)} &gt; 0\\), the cell’s internal state \\(s _i^{(t)}\\) will be perturbed by the inputs. If the output gate \\(o_i^{(t)} \\approx 1\\), then the cell’s internal state impacts the subsequent layers fully. If the output gate \\(o_i^{(t)} \\approx 0\\), then the memory is prevented from impacting other layers at the current time step.\n\n\n\n\n\n\nFigure 34.18: LSTM Memory Cell",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "deeplearning.html#sec-dl-transformers",
    "href": "deeplearning.html#sec-dl-transformers",
    "title": "34  Deep Learning",
    "section": "34.4 Transformers",
    "text": "34.4 Transformers\n\nIntroduction\nFigure 34.19 displays the major neural network types discussed so far.\n\n\n\n\n\n\nFigure 34.19: Basic neural network types.\n\n\n\nFor three decades the basic neural network architecture remained the same, despite many advances. CNNs became the de-factor standard in computer vision and LSTMs became the standard in natural language processing. Rather than fundamental architectural breakthroughs, the progress in deep learning was due to greater computational resources and the availability of more data. Until transformers, that is.\nIn 2017, Vaswani et al. (2017) proposed in the paper “Attention is all you need” a new network architecture for sequence-to-sequence data where the input and output is sequential (as in a RNN). The goal was to address issues with long-range dependencies and contextual understanding in RNN-style models. All recurrent layers were replaced with a new type of network layer, the self-attention layer. Besides improved performance in language translation tasks, a key difference between recurrent and attention architectures was the opportunities for parallel training in the latter. Because recurrent networks process data in a sequential order, opportunities for parallel execution are limited to within-layer operations. Transformer architecture introduced an additional encoding—positional encoding—to capture positional information.\nThe paper by Vaswani et al. (2017) has been cited more than 120,000 times (by July 2024). For statisticians used to read methodological papers, it is noteworthy that the paper does not contain a single theorem or proof. It introduces self-attention mechanism as an extension of the attention mechanism introduced in Bahdanau, Cho, and Bengio (2014) and makes recommendations how to build self-attention-based encoder-decoder models. This work fundamentally changed work on neural network architectures and led to the foundation large-language models such as GPT, Bert, LLama, and others. The revolution was on.\nTo summarize what makes transformer architectures special, here are a few points:\n\nUsing self-attention mechanism instead of recurrence allows the model to consider the entire sequence simultaneously. This allows processing of longer sequences compared to RNN-style models.\nEliminates the need for recurrence or hidden states. Instead, positional encoding maintains information about the sequential nature of the data.\nTraining in parallel allows building of larger networks\nSelf-supervised learning: unsupervised learning is followed by supervised fine-tuning. A good example are GPT and Chat GPT. GPT is the foundation model that provides language understanding. Chat GPT is the question-answer application trained on top of the GPT foundation model.\nTransformers excel not only in natural language processing, but also computer vision, audio processing, etc. Vision transformers, for example, convert images into a sequential format and apply a transformer architecture. Transformer models is the new hammer deep learning had been waiting for.\n\n\n\nSelf Attention\nConsider the two sentences in Figure 34.20. It seems obvious to us that “it” in the first sentence refers to the cup and in the second sentence refers to the pitcher. How would a computer algorithm figure that out?\n\n\n\n\n\n\nFigure 34.20\n\n\n\nAttention mechanisms use weights to focus the algorithm on the elements of the sequence that matter most (Figure 34.21).\n\n\n\n\n\n\nFigure 34.21\n\n\n\n\n\n\nFigure 34.1: Deep learning mind map.\nFigure 34.2: Deep learning as a hierarchy of concepts. Adapted from Goodfellow, Bengio, and Courville (2016)\nFigure 34.3: Object detection and classification\nFigure 34.6: Where is Waldo?\nFigure 34.7: Convolving the first row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\nFigure 34.8: Convolving the second row of a \\(3\\times 4\\) image with a \\(2 \\times 2\\) kernel.\nFigure 34.9: Sparse and full connectivity, adapted from Goodfellow, Bengio, and Courville (2016, 336–37)\nFigure 34.12: Convolutional layer with pooling.\nFigure 34.13: LetNet-5 architecture. Source\nFigure 34.14: Part of my Life List in Merlin Bird ID.\nFigure 34.15: Spectogram in Merlin Bird ID.\nFigure 34.16: Folded and unfolded RNN representation.\nFigure 34.17: Complete representation of an RNN.\nFigure 34.18: LSTM Memory Cell\nFigure 34.19: Basic neural network types.\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. https://api.semanticscholar.org/CorpusID:11212020.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "reinforcement.html",
    "href": "reinforcement.html",
    "title": "35  Reinforcement Learning",
    "section": "",
    "text": "35.1 Introduction\nReinforcement learning (RL) is a form of computerized learning that does not fit neatly in the distinction between supervised and unsupervised learning. It applies in situations of sequential decision making in dynamic environments (Figure 35.1). An agent operates in an environment and chooses from a set of available actions.\nThe action \\(s(t)\\) taken at time \\(t\\) alters the state \\(S(t)\\) of the environment and is associated with a reward \\(R(t)\\). Reinforcement learning trains the agent to take actions that maximize the total expected future rewards. An analogy from game play—to which RL is often applied—is to achieve the best possible game outcome given the situation you find yourself in.\nRather than predicting or classifying an outcome, reinforcement learning is focused on achieving an overall complex goal such as winning a game, driving a car, allocating energy resources in a data center, etc. Training data is not provided ahead of time, it is collected on the fly as the agent interacts with the environment. Rather than from data, in reinforcement learning the system learns from experience. Historical data plays a role in RL. For example, recorded games of chess experts can be used to train a chess AI by having it play against the experts. In zero-shot learning there is no historical data, the system generates the information needed for training as it goes.\nExample applications or reinforcement learning are\nSuccesses in RL put it in the map in the 2010s when it accomplished impressive successes in game play. The approach was fundamentally different from the expert system-based approach used so far to teach computers how to play games. An expert system translates the rules of the game into machine code and adds strategy logic. For example, the Stockfish open-source chess program, released first in 2008, has developed with community support into (one of) the best chess engines in the world. In 2017, Google’s DeepMind released AlphaZero, a chess system trained using reinforcement learning. After only 24 hours of training, using zero-shot reinforcement learning, the AlphaZero algorithm crushed Stockfish, the best chess engine humans have been able to build over 10 years.\nPreviously, Google’s DeepMind had developed AlphaGo, a reinforcement-trained system that beat the best Go player in the world, Lee Sedol, four to one. This was a remarkable achievement as Go had been thought to be so complex and requiring intuition that would escape computerization at the level of expert players.\nUntil recently, a limitation of RL was the need for a good reward function. It is important that actions in the environment are properly judged. In situation where the result of a move is difficult to judge, reinforcement learning was difficult to apply. For example, in natural language processing, where an action produces some prose, how do we rate the quality of the answer?\nThis was the problem faced by systems like Chat-GPT. How do you score the answer produced during training to make sure the algorithm continuously improves? The solution was a form of reinforcement learning modified by human intervention. RLHF, reinforcement learning with human feedback, uses human interpreters to assign scores to the actions (the Chat-GPT answers).\nIn 2017, when AlphaGo beat Lee Sedol, it was thought that reinforcement learning would change the world. Despite its remarkable achievement in gameplay and robotics, the impact of RL fell short of expectations.\nWhy did RL fall short? Developing and training reinforcement learning models is an expensive undertaking. The barrier to entry is very high, limiting RL research and development to large tech-savvy organizations. The main reason is the Sim2Real problem mentioned in the tweet above. Reinforcement learning trains an agent in a simulated, artificial environment. The real world is much more complex and transferring training based on simulation to reality is difficult. The RL agents end up performing poorly in real applications.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "reinforcement.html#introduction",
    "href": "reinforcement.html#introduction",
    "title": "35  Reinforcement Learning",
    "section": "",
    "text": "Figure 35.1: The reinforcement cycle.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is always possible to frame the feedback from the environment as a positive reward. If the environment punishes an action, the reward is negative.\n\n\n\n\n\nAutonomous driving: Learning by driving in simulated environments.\nEnergy: Controlling conditions in a data center based on sensor data to optimize power consumption.\nTraffic control: Optimizing traffic lights based on traffic intensity.\nMedicine: Designing sequential treatment strategies.\nRobotics: Delivery of goods, inventory management, defect detection …\nMarketing: Personalized recommendations; marketing campaign design\nGaming: Game play and testing (bug detection)\nEducation: Personalized learning paths.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.2: Yann LeCun weighs in on the impact of reinforcement learning (RL).",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "reinforcement.html#markov-decision-process",
    "href": "reinforcement.html#markov-decision-process",
    "title": "35  Reinforcement Learning",
    "section": "35.2 Markov Decision Process",
    "text": "35.2 Markov Decision Process\nReinforcement learning is framed in terms of Markov Decision Processes (MDP, Bellman (1957)). An MDP comprises\n\n\\(\\mathcal{S}\\): a set of states the system can be in\n\\(\\mathcal{A}\\): a set of actions that can be taken at each state\n\\(T\\): a transition function of conditional probabilities to reach state \\(s^\\prime\\), given that the system was in state \\(s\\) when action \\(a\\) was taken, \\(\\Pr(s^\\prime | s, a)\\).\nA reward \\(r:\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\). The reward function \\(r(s,a)\\) determines the reward if action \\(a\\) is taken in state \\(s\\).\n\nFigure 35.3 shows a robot on a 4 x 4 grid of cells. The goal is to reach the capacitor in cell \\((4,4)\\). The set of states in this MDP is given by the possible grid locations, \\(S = \\{(1,1), \\cdots, (4,4)\\}\\). The set of actions are the possible moves from one cell to another.\n\n\n\n\n\n\nFigure 35.3: Robot on a grid.\n\n\n\n\nTrajectory and Return\nThe sequence of states, actions, and rewards results in a trajectory \\(\\tau = \\left([s_0,a_0,r_0]), [s_1, a_1, r_1], [s_2, a_2, r_2], \\cdots \\right )\\) The discounted return of a trajectory is the sum of the (discounted) rewards: \\[\nR(\\tau) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\cdots = \\sum_{t=0}^\\infty \\gamma^t r_t = \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)\n\\] The discount factor \\(\\gamma\\) controls the balance between exploration and exploitation as the agent explores the environment. If \\(\\gamma\\) is small, 0.1 say, future rewards are heavily discounted; this encouraged the agent to reach the goal quickly and not to rely on future rewards too much. On the other hand, if \\(\\gamma\\) is large, 0.9 say, exploration is encouraged to find the best possible trajectory. Obviously, \\(\\gamma\\) will be a hyperparameter in reinforcement learning systems we build.\n\n\nPolicies\nA policy is a multinomial probability distribution \\(\\pi(\\textbf{a}|s)\\) to take actions \\(\\textbf{a} = [a_1,\\cdots,a_k]^\\prime\\) given that the system is in state \\(s\\). This distribution can be different for each state. How likely you are to ask the dealer for another card in blackjack depends on the cards in your hand. Note that \\(\\pi(\\textbf{a}|s)\\) is different from the transition probabilities \\(T\\) to reach a state given an action and a previous state, \\(Pr(s^\\prime | s,a)\\).\nA policy is called deterministic if \\(\\pi(\\textbf{a}|s) = 1\\) for a particular action \\(a_j\\) and 0 otherwise. For example, if the robot always starts moving right in cell \\((1,1)\\), then \\(\\pi(\\{\\text{\"right\"}\\} | (1,1)) = 1\\).\nA policy that is not deterministic is a stochastic policy. We then decide with a random draw according to the probabilities in \\(\\pi(\\textbf{a}|s)\\) which action to take. Sometimes the stochastic policy is enhanced with an exploration parameter: with probability \\(\\epsilon\\) an action is chosen uniformly from the set of all possible actions, with probability \\(1-\\epsilon\\) an action is chosen according to the policy.\n\n\nValue Function\nThe value function of policy \\(\\pi(\\textbf{a}|s)\\) is the expected \\(\\gamma\\)-discounted return from state \\(s_0\\) \\[\n    V(s_0) = \\text{E}_\\pi \\left[ R(\\tau) \\right] = \\text{E}_\\pi \\left [ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right ]\n\\] This is the expected return if we start at \\(s_0\\) and follow policy \\(\\pi(\\textbf{a}|s)\\) afterwards. Since the value function depends on the policy we can turn the question around and ask what is the optimal policy \\(\\pi^*(\\textbf{a}|s)\\) that maximizes the value function: \\[ \\pi^*(\\textbf{a}|s) = \\arg\\max_\\pi V(s_0)\\] How do we solve this? The optimal trajectory depends on future paths not yet taken. The solution can be found by dynamic programming. An alternative approach is Q-Learning.\n\n\nQ-Learning\nIn Q-Learning we learn the value function without completely knowing the Markov Decision Process. The basic idea is for the agent to acquire its own data by operating in the environment. The transition function is implicit in the states the agent will go through.\nThe value function \\(V(s_0)\\) answers the question “How good is the state I am in?” \\[\nV(s_0) = \\text{E}_\\pi \\left[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t) | s=s_0\\right]\n\\]\nThe Q-function, on which Q-Learning is based, answers the question “How good is the state-action pair?” \\[\nQ(s_0,a_0) = \\text{E}_\\pi \\left[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t) | s=s0,a=a_0\\right]\n\\] The Q-function fixes the initial state and the initial action. Based on a set of \\(n\\) observed trajectories of \\(T\\) time steps each, the optimal policy can be estimated \\[\n\\begin{align*}\n\\widehat{\\pi}(\\textbf{a}|s) &= \\arg\\max_a \\widehat{Q}(s,a) \\\\\n\\widehat{Q}(s,a) &= \\min_Q \\frac{1}{nT} \\sum_{i=1}^n\\sum_{t=0}^{T-1}\n        \\left ( Q(s_{it}, a_{it}) - r(s_{it},a_{it}) - \\gamma \\max_{a\\prime} Q(s_{i,t+1},a^\\prime) \\right) ^2\n\\end{align*}\n\\] This can be solved by, wait for it, gradient descent. In addition, several enhancements to the basic Q-Learning algorithm are applied:\n\nTerminal state: when the goal is reached the trajectories end.\nExploration: add a random element to the policy to make sure all states are visited \\[\n      \\pi(\\textbf{a}|s) = \\left \\{ \\begin{array}{l l} \\arg\\max_{a^\\prime}\\widehat{Q}(s,a^\\prime) & \\text{with prob. } 1-\\epsilon\\\\\n      \\text{uniform}(\\mathcal{A}) & \\text{with prob. } \\epsilon \\end{array}\\right .\n\\] \\(\\epsilon\\) is the exploration parameter\nLearning rate: parameter \\(\\alpha\\) to limit step length based on gradient",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "reinforcement.html#training-the-robot-in-r",
    "href": "reinforcement.html#training-the-robot-in-r",
    "title": "35  Reinforcement Learning",
    "section": "35.3 Training the Robot in R",
    "text": "35.3 Training the Robot in R\nIn order to train our robot we need a reward function in addition to the rules given in Figure 35.3. Figure 35.4 shows a possible reward function, expressed in terms of penalties for actions. A move from one cell to another without stepping on a trap or finding the capacitor receives 0 reward. Stepping off the grid or onto a cell with a trap receives a negative reward.\n\n\n\n\n\n\nFigure 35.4: Reward function for robot moves.\n\n\n\nTo carry out the training we use the Reinforcementearning package in R. The next code segment loads the library and defines actions and states.\n\nlibrary(ReinforcementLearning)\n\nactions = c(\"left\", \"right\", \"up\", \"down\", \"stand\")\n\nstates = c(\"(1,1)\",\"(1,2)\",\"(1,3)\",\"(1,4)\",\"(2,1)\",\"(2,2)\",\"(2,3)\",\"(2,4)\",\n           \"(3,1)\",\"(3,2)\",\"(3,3)\",\"(3,4)\",\"(4,1)\",\"(4,2)\",\"(4,3)\",\"(4,4)\")\n\nNext, we write a function that defines the environment. The function takes two arguments, the current state and the current action. Based on the (state, action) pair, the function returns a list with NextState and the reward for the action. Note that the function assumes a reward of 0 and no change in state. For the possible movements it then assigns reward and computes the next state.\n\ngridenv &lt;- function(state, action) {\n    next_state &lt;- state\n    reward &lt;- 0\n    trap_reward &lt;- -10\n    wall_reward &lt;- -1\n    # (1,1)\n    if (state==state(\"(1,1)\")) {\n        if (action==\"right\") \n            next_state &lt;- state(\"(1,2)\")\n        else if (action==\"down\") \n            next_state &lt;- state(\"(2,1)\")\n        else \n            reward &lt;- wall_reward\n    }\n    # (1,2)\n    if (state==state(\"(1,2)\")) {\n        if (action==\"right\") \n            next_state &lt;- state(\"(1,3)\")\n        else if (action==\"left\")\n            next_state &lt;- state(\"(1,1)\")\n        else if (action==\"down\") {\n            next_state &lt;- state(\"(2,2)\")\n            reward &lt;- trap_reward  # found a trap\n        } else\n            reward &lt;- wall_reward\n    }\n    # (1,3)\n    if (state==state(\"(1,3)\")) {\n        if (action==\"right\") \n            next_state &lt;- state(\"(1,4)\")\n        else if (action==\"down\") \n            next_state &lt;- state(\"(2,3)\")\n        else if (action==\"left\")\n            next_state &lt;- state(\"(1,2)\")\n        else \n            reward &lt;- wall_reward\n    }\n    # (1,4)\n    if (state==state(\"(1,4)\")) {\n        if (action==\"left\") \n            next_state &lt;- state(\"(1,3)\")\n        else if (action==\"down\") {\n            next_state &lt;- state(\"(2,4)\")\n            reward &lt;- trap_reward\n        } else \n            reward &lt;- wall_reward\n    }\n    # (2,1)\n    if (state==state(\"(2,1)\")) {\n        if (action==\"right\") { \n            next_state &lt;- state(\"(2,2)\")\n            reward &lt;- trap_reward\n        } else if (action==\"down\") \n            next_state &lt;- state(\"(3,1)\")\n        else if (action==\"up\")\n            next_state &lt;- state(\"(1,1)\")\n        else \n            reward &lt;- wall_reward\n    }\n    # (2,3)\n    if (state==state(\"(2,3)\")) {\n        if (action==\"right\") { \n            next_state &lt;- state(\"(2,4)\")\n            reward &lt;- trap_reward\n        } else if (action==\"left\") { \n            next_state &lt;- state(\"(2,2)\")\n            reward &lt;- trap_reward        \n        } else if (action==\"up\")\n            next_state &lt;- state(\"(1,3)\")\n        else if (action==\"down\")\n            next_state &lt;- state(\"(3,3)\")\n    }\n    # (3,1)\n    if (state==state(\"(3,1)\")) {\n        if (action==\"down\") { \n            next_state &lt;- state(\"(4,1)\")\n            reward &lt;- trap_reward\n        } else if (action==\"right\") \n            next_state &lt;- state(\"(3,2)\")\n        else if (action==\"up\")\n            next_state &lt;- state(\"(2,1)\")\n        else \n            reward &lt;- wall_reward\n    }\n    # (3,2)\n    if (state==state(\"(3,2)\")) {\n        if (action==\"left\")  \n            next_state &lt;- state(\"(3,1)\")\n         else if (action==\"right\") \n            next_state &lt;- state(\"(3,3)\")\n         else if (action==\"down\")\n             next_state &lt;- state(\"(4,2)\")\n         else if (action==\"up\") {\n             next_state &lt;- state(\"(2,2)\")\n             reward &lt;- trap_reward\n         }\n    }\n    # (3,3)\n    if (state==state(\"(3,3)\")) {\n        if (action==\"left\")  \n            next_state &lt;- state(\"(3,2)\")\n         else if (action==\"right\") {\n            next_state &lt;- state(\"(3,4)\")\n            reward &lt;- trap_reward\n         } else if (action==\"down\")\n             next_state &lt;- state(\"(4,3)\")\n         else if (action==\"up\")\n             next_state &lt;- state(\"(2,3)\")\n    }\n    # (4,2)\n    if (state==state(\"(4,2)\")) {\n        if (action==\"left\") { \n            next_state &lt;- state(\"(4,1)\")\n            reward &lt;- trap_reward\n         } else if (action==\"right\") \n            next_state &lt;- state(\"(4,3)\")\n         else if (action==\"up\")\n             next_state &lt;- state(\"(3,2)\")\n         else \n            reward &lt;- wall_reward\n    }\n    # (4,3)\n    if (state==state(\"(4,3)\")) {\n        if (action==\"right\")  \n            next_state &lt;- state(\"(4,4)\")\n         else if (action==\"left\") \n            next_state &lt;- state(\"(4,2)\")\n         else if (action==\"up\")\n             next_state &lt;- state(\"(3,3)\")\n         else \n            reward &lt;- wall_reward\n    }\n    # Did we move into the super capacitor?\n    if (next_state==state(\"(4,4)\") && (state != next_state)) {\n         reward &lt;- 10\n    }\n\n    return(list(NextState=next_state, Reward=reward))\n}\n\nWith the environment defined, the next step in Q-Learning is to sample the environment:\n\nset.seed(12)\nrobot_data &lt;- sampleExperience(N      =1000,\n                               env    =gridenv,\n                               states =states,\n                               actions=actions)\nhead(robot_data,10)\n\n   State Action Reward NextState\n1  (1,2)  stand     -1     (1,2)\n2  (3,2)   left      0     (3,1)\n3  (4,4)  stand      0     (4,4)\n4  (2,3)     up      0     (1,3)\n5  (3,3)     up      0     (2,3)\n6  (4,2)  stand     -1     (4,2)\n7  (2,1)     up      0     (1,1)\n8  (2,1)  right    -10     (2,2)\n9  (3,4)  right      0     (3,4)\n10 (1,2)  stand     -1     (1,2)\n\n\nLet’s make sure we drew enough samples of the environment, we want to make sure all rewards and states have been visited.\n\nlibrary(dplyr)\n\nrobot_data %&gt;% group_by(Reward) %&gt;% summarize(count=n())\n\n# A tibble: 4 × 2\n  Reward count\n   &lt;dbl&gt; &lt;int&gt;\n1    -10   124\n2     -1   217\n3      0   644\n4     10    15\n\nrobot_data %&gt;% group_by(State) %&gt;% summarize(count=n())\n\n# A tibble: 16 × 2\n   State count\n   &lt;chr&gt; &lt;int&gt;\n 1 (1,1)    51\n 2 (1,2)    57\n 3 (1,3)    68\n 4 (1,4)    64\n 5 (2,1)    64\n 6 (2,2)    45\n 7 (2,3)    76\n 8 (2,4)    66\n 9 (3,1)    65\n10 (3,2)    53\n11 (3,3)    60\n12 (3,4)    59\n13 (4,1)    71\n14 (4,2)    61\n15 (4,3)    74\n16 (4,4)    66\n\n\nGiven this sampled version of the environment we can now train the model. The parameters in the control structure are the learning rate for gradient descent (\\(\\alpha\\)), the discounted return for the reward function (\\(\\gamma\\)), and the random exploration (\\(\\epsilon\\)). The input data to the ReinforcementLearning function is the data sampled from the environment in the previous step.\n\ncontrol &lt;- list(alpha=0.1, gamma=0.9, epsilon=0.1)\n\nrl &lt;- ReinforcementLearning(data   =robot_data,\n                            s      =\"State\", \n                            a      =\"Action\", \n                            r      =\"Reward\",\n                            s_new  =\"NextState\",\n                            control=control)\n\nrl\n\nState-Action function Q\n             right            up         down      stand          left\n(4,3)  7.941088679  1.0904210863  3.363969714  3.9165548  0.9976119134\n(2,4)  0.000000000  0.0000000000  0.000000000  0.0000000  0.0000000000\n(1,1)  0.002757743 -0.7172174847  0.001530369 -0.6860346 -0.4680874259\n(3,2)  0.459193835 -7.1757046352  0.499863832  0.1891378  0.0146491218\n(1,3)  0.012611843 -0.6557163193  0.317197237 -0.6548928  0.0085491200\n(3,4)  0.000000000  0.0000000000  0.000000000  0.0000000  0.0000000000\n(2,1) -7.712320755  0.0009734679  0.027714524 -0.7150762 -0.7406978249\n(4,2)  3.050116230  0.1896432524 -0.022589185  0.2631646 -7.1757046352\n(2,3) -5.695327900  0.0991057878  0.623888464  0.2525663 -9.0152290978\n(4,4)  0.000000000  0.0000000000  0.000000000  0.0000000  0.0000000000\n(3,1)  0.256321285  0.0083965753 -7.458134172 -0.6522334 -0.7330142645\n(1,2)  0.048677494 -0.6096084110 -8.332281830 -0.6836687  0.0003337767\n(3,3) -6.125795110  0.1578961518  2.271327820  0.7387191  0.1639584361\n(1,4) -0.673367570 -0.6773193387 -8.332281830 -0.7260261  0.0567553857\n(4,1)  0.000000000  0.0000000000  0.000000000  0.0000000  0.0000000000\n(2,2)  0.000000000  0.0000000000  0.000000000  0.0000000  0.0000000000\n\nPolicy\n  (4,3)   (2,4)   (1,1)   (3,2)   (1,3)   (3,4)   (2,1)   (4,2)   (2,3)   (4,4) \n\"right\" \"right\" \"right\"  \"down\"  \"down\" \"right\"  \"down\" \"right\"  \"down\" \"right\" \n  (3,1)   (1,2)   (3,3)   (1,4)   (4,1)   (2,2) \n\"right\" \"right\"  \"down\"  \"left\" \"right\" \"right\" \n\nReward (last iteration)\n[1] -1307\n\n\nThe result of ReinforcementLearning include the Q-function of (state, action) pairs and the learned optimal policy. It helps to rearrange those and to map the policy onto the grid environment.\n\n# The Q matrix of state-action pairs\nround(rl$Q[order(rownames(rl$Q)),],4)\n\n        right      up    down   stand    left\n(1,1)  0.0028 -0.7172  0.0015 -0.6860 -0.4681\n(1,2)  0.0487 -0.6096 -8.3323 -0.6837  0.0003\n(1,3)  0.0126 -0.6557  0.3172 -0.6549  0.0085\n(1,4) -0.6734 -0.6773 -8.3323 -0.7260  0.0568\n(2,1) -7.7123  0.0010  0.0277 -0.7151 -0.7407\n(2,2)  0.0000  0.0000  0.0000  0.0000  0.0000\n(2,3) -5.6953  0.0991  0.6239  0.2526 -9.0152\n(2,4)  0.0000  0.0000  0.0000  0.0000  0.0000\n(3,1)  0.2563  0.0084 -7.4581 -0.6522 -0.7330\n(3,2)  0.4592 -7.1757  0.4999  0.1891  0.0146\n(3,3) -6.1258  0.1579  2.2713  0.7387  0.1640\n(3,4)  0.0000  0.0000  0.0000  0.0000  0.0000\n(4,1)  0.0000  0.0000  0.0000  0.0000  0.0000\n(4,2)  3.0501  0.1896 -0.0226  0.2632 -7.1757\n(4,3)  7.9411  1.0904  3.3640  3.9166  0.9976\n(4,4)  0.0000  0.0000  0.0000  0.0000  0.0000\n\n# Map the moves to the 4x4 grid\np &lt;- rl$Policy[order(names(rl$Policy))]\nmatrix(p,nrow=4,ncol=4,byrow=TRUE)\n\n     [,1]    [,2]    [,3]    [,4]   \n[1,] \"right\" \"right\" \"down\"  \"left\" \n[2,] \"down\"  \"right\" \"down\"  \"right\"\n[3,] \"right\" \"down\"  \"down\"  \"right\"\n[4,] \"right\" \"right\" \"right\" \"right\"\n\n\nHow do we read these results? Suppose the robot starts in cell \\((1,1)\\). The optimal policy is then to take action “right” which moves the robot to cell \\((1,2)\\). Now the best policy is again “right”, leading to cell \\((1,3)\\). At this point we take action “down” followed by “down” and “down”. We now stand on cell \\((4,3)\\) and take a step “right” to reach the goal (Figure 35.5).\n\n\n\n\n\n\nFigure 35.5: Optimal policy (learned)\n\n\n\nThe optimal policy is not necessarily unique. Other paths lead to the goal, we can now impose additional rules such as reducing rewards for long paths to narrow the solution(s).\n\n\n\n\n\n\nFigure 35.6: Other optimal policies (learned).\n\n\n\nFigure 35.7 shows a solution when cell \\((2,3)\\) is made less desirable than other cells. The robot now starts the journey with a downward move because moving right initiallu would lead to outcomes with smaller total rewards.\n\n\n\n\n\n\nFigure 35.7: Policy when penalizing cell \\((2,3)\\).\n\n\n\n\n\n\nFigure 35.3: Robot on a grid.\nFigure 35.4: Reward function for robot moves.\nFigure 35.5: Optimal policy (learned)\nFigure 35.6: Other optimal policies (learned).\nFigure 35.7: Policy when penalizing cell \\((2,3)\\).\n\n\n\nBellman, R. 1957. “A Markovian Decision Process.” Journal of Mathematics and Mechanics 6 (5). http://www.jstor.org/stable/24900506.",
    "crumbs": [
      "Part VIII. Neural Networks and Deep Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "explainability.html",
    "href": "explainability.html",
    "title": "36  Interpretability and Explainability",
    "section": "",
    "text": "Figure 36.1: Model explainability mind map\n\n\n\nTo answer the question “What makes a model interpretable?”, Sankaran (2024) draw parallels between model interpretability and data visualization, asking “What makes a visualization effective?” A good visualization\n\nstreamlines a taxing cognitive operation into a perceptual one\nuses representations that are familiar or easily learnable\nuses graphical elements that are legible and well-annotated\nprevents oversummarization and highlights details for follow-up\npushes readers past passive condsumption, inspiring deeper exploration\nshows data provenance and lineage\n\nSimilarly, an interpretable model can be broken down into relevant components, each of which can be assigned meaning. Instead of information density (showing more of the data), interpretability relies on faithfulness (showing more of the model). The parallels run deeper. As in a good visualization, the data provenance of a trustworthy model can be traced back to the original measurement mechanism — beautiful design and high-accuracy have little value otherwise. Moreover, like visualization, interpretability must be tailored to an audience and the problems they need solved (Lipton, 2018). There are different levels of data literacy, and visual representations may be familiar to some audiences but not others. Similarly, AI models are employed across a range of problem domains, necessitating validation in realistic settings (see Section 3). Finally, effective visualizations push readers beyond passive consumption — they inspire deeper exploration of complexity. Likewise, interpretability can support the transition from automated to augmented decision-making (Heer, 2019), enhancing rather than substituting human reason. Model interpretability can be approached\nIntrinsically interpretable models are also called glass boxes to distinguish them from black boxes that are inherently non-interpretable (Sankaran 2024).\n\n\n\nFigure 36.1: Model explainability mind map\n\n\n\nSankaran, Kris. 2024. “Data Science Principles for Interpretable and Explainable AI.” Journal of Data Science, 1–27. https://doi.org/10.6339/24-JDS1150.",
    "crumbs": [
      "Part IX. Explainability",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Interpretability and Explainability</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agrawal, Rakesh, Tomasz Imieliński, and Arun Swami. 1993. “Mining\nAssociation Rules Between Sets of Items in Large Databases.” In\nProceedings of the 1993 ACM SIGMOD International Conference on\nManagement of Data, 207–16. SIGMOD ’93. New York, NY, USA:\nAssociation for Computing Machinery. https://doi.org/10.1145/170035.170072.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural\nMachine Translation by Jointly Learning to Align and Translate.”\nCoRR abs/1409.0473. https://api.semanticscholar.org/CorpusID:11212020.\n\n\nBelenky, Gregory, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas,\nHelen C. Sing, Daniel P. Redmond, Michael B. Russo, and Thomas J.\nBalkin. 2003. “Patterns of Performance Degradation and Restoration\nDuring Sleep Restriction and Subsequent Recovery: A Sleep Dose-Response\nStudy.” Journal of Sleep Research 12: 1--12.\n\n\nBellman, R. 1957. “A Markovian Decision Process.”\nJournal of Mathematics and Mechanics 6 (5). http://www.jstor.org/stable/24900506.\n\n\nBoeckmann, A. J., L. B. Sheiner, and S. L. Beal. 1992. NONMEM Users\nGuide: Part v, Introductory Guide. NONMEM Project Group, University\nof California, San Francisco.\n\n\nBorne, Kirk. 2021. “Association Rule Mining — Not Your Typical ML\nAlgorithm.” Medium. https://medium.com/@kirk.borne/association-rule-mining-not-your-typical-ml-algorithm-97acda6b86c2.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99.\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine\nLearning 24: 123–40.\n\n\n———. 2001a. “Random Forests.” Machine Learning 45:\n5–32.\n\n\n———. 2001b. “Statistical Modeling: The Two Cultures.”\nStatistical Science 16 (3): 199–231.\n\n\nBreiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984.\nClassification and Regression Trees. Wadsworth, Pacific Grove,\nCA.\n\n\nCleveland, William S. 1979. “Robust Locally Weighted Regression\nand Smoothing Scatterplots.” Journal of the American\nStatistical Association 74 (368): 829–36. https://doi.org/10.1080/01621459.1979.10481038.\n\n\nDavidian, M., and D. M. Giltinan. 1995. Nonlinear Models for\nRepeated Measurement Data. Chapman & Hall, London.\n\n\nFerrari, S. L. P., and F. Cribari-Neto. 2004. “Beta Regression for\nModeling Rates and Proportions.” Journal of Applied\nStatistics 31 (7): 799–815.\n\n\nFraley, Chris, and Adrian E Raftery. 2002. “Model-Based\nClustering, Discriminant Analysis, and Density Estimation.”\nJournal of the American Statistical Association 97 (458):\n611–31. https://doi.org/10.1198/016214502760047131.\n\n\nFurnival, George M., and Robert W. Wilson. 1974. “Regression by\nLeaps and Bounds.” Technometrics 16 (4): 499–511.\n\n\nGarcía-Portugués, E. 2024. Notes for Predictive Modeling. https://bookdown.org/egarpor/PM-UC3M/.\n\n\nGilliland, D., and O. Schabenberger. 2001. “Limits on Pairwise\nAssociation for Equi-Correlated Binary Variables.” Journal of\nApplied Statistical Sciences 10: 279--285.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nGower, J. C. n.d. “A General Coefficient of Similarity and Some of\nIts Properties.” Biometrics 27 (4): 857–71.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of\nNormality–Reflections on the Work of Quetelet and Galton.”\nScandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nHartigan, J. A., and M. A. Wong. 1979. “Algorithm AS 136: A\nk-Means Clustering Algorithm.” Journal of the Royal\nStatistical Society. Series C (Applied Statistics) 28 (1): 100–108.\n\n\nHarville, D. A. 1976. “Extension of the Gauss-Markov Theorem to\nInclude the Estimation of Random Effects.” The Annals of\nStatistics 4: 384–95.\n\n\nHastie, T. J., and R. Tibshirani. 1990. Generalized Additive\nModels. Chapman & Hall, London.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The\nElements of Statistical Learning. Springer Series in Statistics.\nNew York, NY, USA: Springer New York Inc.\n\n\nHenderson, C. R. 1950. “The Estimation of Genetic\nParameters.” The Annals of Mathematical Statistics 21\n(2): 309–10.\n\n\n———. 1984. Applications of Linear Models in Animal Breeding.\nUniversity of Guelph.\n\n\nHinne, Max, Quentin F. Gronau, Don van den Bergh, and Eric-Jan\nWagenmakers. 2020. “A Conceptual Introduction to Bayesian Model\nAveraging.” Advances in Methods and Practices in\nPsychological Science 3 (2): 200–215. https://doi.org/10.1177/2515245919898657.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nKleinbaum, David G., Lawrence L. Kupper, A. Nizam, and Eli S. Rosenberg.\n2013. Applied Regression Analysis and Other Multivariable Methods, 5\nEd. Cengage Learning.\n\n\nLittle, R., and D. Rubin. 1987. Statistical Analysis with Missing\nData. Wiley, New York.\n\n\nMallows, C. L. 1973. “Some Comments on Cp.”\nTechnometrics 15 (4): 661–75.\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear\nModels, 2nd Ed. Chapman & Hall, New York.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus\nof the Ideas Immanent in Nervous Activity.” Bulletin of\nMathematical Biophysics 5: 115–33.\n\n\nMead, R., R. N. Curnow, and A. M. Hasted. 1993. Statistical Methods\nin Agriculture and Experimental Biology. CRC Press, New York; Boca\nRaton, FL.\n\n\nMiller, Alan J. 1984. “Selection of Subsets of Regression\nVariables.” Journal of the Royal Statistical Society, Series\nA. 147 (3): 389–425.\n\n\nPinheiro, J. C., and D. M. Bates. 1995. “Approximations to the\nLog-Likelihood Function in the Nonlinear Mixed-Effects Model.”\nJournal of Computational and Graphical Statistics 4: 12–35.\n\n\nPrater, N. H. 1956. “Estimate Gasoline Yields from Crudes.”\nPetroleum Refiner 35 (3).\n\n\nRaftery, Adrian E. 1995. “Bayesian Model Selection in Social\nResearch.” Sociological Methodology 25: 111–63.\n\n\nRatkowsky, D. A. 1983. Nonlinear Regression Modeling. Marcel\nDekker, New York.\n\n\n———. 1990. Handbook of Nonlinear Regression Models. Marcel\nDekker, New York.\n\n\nSankaran, Kris. 2024. “Data Science Principles for Interpretable\nand Explainable AI.” Journal of Data Science, 1–27. https://doi.org/10.6339/24-JDS1150.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary\nStatistical Models for the Plant and Soil Sciences. CRC Press, Boca\nRaton.\n\n\nSchabenberger, O., B. E. Tharp, Kells J. J., and D. Penner. 1999.\n“Statistical Tests for Hormesis and Effective Dosages in Herbicide\nDose Response.” Agronomy Journal 91: 713–21.\n\n\nSutton, Clifton D. 2005. “Classification and Regression Trees,\nBagging, and Boosting.” Handbook of Statistics 24:\n303–29.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nZhang, Grace. 2018. “What Is the Kernel Trick? Why Is It\nImportant?” Medium. https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d.",
    "crumbs": [
      "References"
    ]
  }
]