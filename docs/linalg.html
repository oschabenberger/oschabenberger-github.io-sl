<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 4&nbsp; Linear Algebra Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./estimation.html" rel="next">
<link href="./biasvariance.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./statmodels.html">Foundation</a></li><li class="breadcrumb-item"><a href="./linalg.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discriminant.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Discriminant Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./naivebayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Naive Bayes Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Decision Trees</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Ensemble Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Introduction to Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#basics" id="toc-basics" class="nav-link active" data-scroll-target="#basics"><span class="header-section-number">4.1</span> Basics</a></li>
  <li><a href="#special-matrices" id="toc-special-matrices" class="nav-link" data-scroll-target="#special-matrices"><span class="header-section-number">4.2</span> Special Matrices</a></li>
  <li><a href="#basic-operations-on-matrices-and-vectors" id="toc-basic-operations-on-matrices-and-vectors" class="nav-link" data-scroll-target="#basic-operations-on-matrices-and-vectors"><span class="header-section-number">4.3</span> Basic Operations on Matrices and Vectors</a>
  <ul>
  <li><a href="#transpose" id="toc-transpose" class="nav-link" data-scroll-target="#transpose">Transpose</a></li>
  <li><a href="#addition-and-subtraction" id="toc-addition-and-subtraction" class="nav-link" data-scroll-target="#addition-and-subtraction">Addition and Subtraction</a></li>
  <li><a href="#multiplication" id="toc-multiplication" class="nav-link" data-scroll-target="#multiplication">Multiplication</a></li>
  <li><a href="#inversion-and-rank" id="toc-inversion-and-rank" class="nav-link" data-scroll-target="#inversion-and-rank">Inversion and Rank</a></li>
  <li><a href="#determinant" id="toc-determinant" class="nav-link" data-scroll-target="#determinant">Determinant</a></li>
  <li><a href="#trace" id="toc-trace" class="nav-link" data-scroll-target="#trace">Trace</a></li>
  </ul></li>
  <li><a href="#random-vectors" id="toc-random-vectors" class="nav-link" data-scroll-target="#random-vectors"><span class="header-section-number">4.4</span> Random Vectors</a>
  <ul>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value">Expected Value</a></li>
  <li><a href="#covariance-matrix" id="toc-covariance-matrix" class="nav-link" data-scroll-target="#covariance-matrix">Covariance Matrix</a></li>
  <li><a href="#variance-covariance-matrix" id="toc-variance-covariance-matrix" class="nav-link" data-scroll-target="#variance-covariance-matrix">Variance-covariance Matrix</a></li>
  </ul></li>
  <li><a href="#sec-matrix-differentiation" id="toc-sec-matrix-differentiation" class="nav-link" data-scroll-target="#sec-matrix-differentiation"><span class="header-section-number">4.5</span> Matrix Differentiation</a></li>
  <li><a href="#sec-idempotent" id="toc-sec-idempotent" class="nav-link" data-scroll-target="#sec-idempotent"><span class="header-section-number">4.6</span> Idempotent Matrices</a>
  <ul>
  <li><a href="#projections" id="toc-projections" class="nav-link" data-scroll-target="#projections">Projections</a></li>
  <li><a href="#sec-hat-matrix" id="toc-sec-hat-matrix" class="nav-link" data-scroll-target="#sec-hat-matrix">The “Hat” Matrix</a></li>
  <li><a href="#a-special-case" id="toc-a-special-case" class="nav-link" data-scroll-target="#a-special-case">A Special Case</a></li>
  </ul></li>
  <li><a href="#sec-multi-gaussian" id="toc-sec-multi-gaussian" class="nav-link" data-scroll-target="#sec-multi-gaussian"><span class="header-section-number">4.7</span> Multivariate Gaussian Distribution</a>
  <ul>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a>
  <ul class="collapse">
  <li><a href="#linear-combinations-are-gaussian" id="toc-linear-combinations-are-gaussian" class="nav-link" data-scroll-target="#linear-combinations-are-gaussian">Linear combinations are Gaussian</a></li>
  <li><a href="#zero-covariance-implies-independence" id="toc-zero-covariance-implies-independence" class="nav-link" data-scroll-target="#zero-covariance-implies-independence">Zero covariance implies independence</a></li>
  <li><a href="#conditionals-are-gaussian" id="toc-conditionals-are-gaussian" class="nav-link" data-scroll-target="#conditionals-are-gaussian">Conditionals are Gaussian}</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./statmodels.html">Foundation</a></li><li class="breadcrumb-item"><a href="./linalg.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-linear-algebra" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="basics" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="basics"><span class="header-section-number">4.1</span> Basics</h2>
<p>Command of linear algebra is essential in data science, models and estimators are often expressed in terms of tensors, matrices, and vectors. Using scalar-based arithmetic becomes tedious very quickly as models become more complex. For example, the simple linear regression model and a straight line through the intercept model can be written as</p>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}\]</span></p>
<p><span class="math display">\[Y_{i} = \beta_{1}x_{i} + \epsilon_{i}\]</span></p>
<p>Using scalar algebra, the estimates of the slope are quite different:</p>
<p><span class="math display">\[{\widehat{\beta}}_{1} = \frac{\left( \sum_{i = 1}^{n}{\left( y_{i} - \overline{y} \right)\left( x_{i} - \overline{x} \right)} \right)}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[{\widehat{\beta}}_{1} = \frac{\left( \sum_{i = 1}^{n}{y_{i}x_{i}} \right)}{\sum_{i = 1}^{n}x_{i}^{2}}\]</span></p>
<p>The formulas get messier as we add another input variable to the model. Using matrix—vector notation, the estimator of all the regression coefficients takes the same form, regardless of the size of the model:</p>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}} = \left( \textbf{X}^{\prime}\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}\]</span></p>
<p>A scalar is a single real number, a vector is an array of scalars arranged in a single column (a column vector) or a row (a row vector). A matrix is a two-dimensional array of scalars, a tensor is a multi-dimensional array.</p>
<p>The <strong>order</strong> of a vector or matrix is specified as (rows x columns) and is sometimes used as a subscript for clarity. For example,<span class="math inline">\(\textbf{A}_{(3 \times 5)}\)</span> denotes a matrix with 3 rows and 5 columns. It can be viewed as a concatenation} of five <span class="math inline">\((3 \times 1)\)</span> column vectors:</p>
<p><span class="math display">\[\textbf{A}_{(3 \times 5)}=\begin{bmatrix}
\begin{matrix}
1 \\
1 \\
1
\end{matrix} &amp; \begin{matrix}
9.0 \\
3.2 \\
4.1
\end{matrix} &amp; \begin{matrix}
\begin{matrix}
6.2 \\
1.4 \\
- 0.6
\end{matrix} &amp; \begin{matrix}
1 \\
0 \\
0
\end{matrix} &amp; \begin{matrix}
0 \\
1 \\
0
\end{matrix}
\end{matrix}
\end{bmatrix}\]</span></p>
<p><span class="math inline">\(\textbf{a}_{1} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}\ \ \ \ \ \textbf{a}_{2} = \begin{bmatrix} 9.0 \\ 3.2 \\ 4.1 \end{bmatrix}\ \ \ \ \textbf{a}_{3} = \begin{bmatrix} 6.2 \\ 1.4 \\ - 0.6 \end{bmatrix}\ \ \ \ \ \textbf{a}_{4} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}\ \ \ \ \ \textbf{a}_{5} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}\)</span>.</p>
<p>A matrix with as many rows as columns is called a <strong>square</strong> matrix.</p>
<p>Bold symbols are common, lowercase for vectors and uppercase for matrices, but there are some exceptions. When dealing with vectors of random variables, bold uppercase notation is used for a vector of random variables and bold lowercase notation is used for a vector of the realized values. For example, if <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> is a random sample of size <span class="math inline">\(n\)</span>, the vector of random variables is</p>
<p><span class="math display">\[\textbf{Y}_{(n \times 1)} = \begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}\]</span></p>
<p>and the vector of realized values is</p>
<p><span class="math display">\[\textbf{y}_{(n \times 1)} = \begin{bmatrix}
y_{1} \\
\vdots \\
y_{n}
\end{bmatrix}\]</span></p>
<p>The difference is significant because <span class="math inline">\(\textbf{Y}\)</span> is a random variable and <span class="math inline">\(\textbf{y}\)</span> is a vector of constants. <span class="math inline">\(\textbf{Y}\)</span> has a multi-variate distribution with mean and variance, <span class="math inline">\(\textbf{y}\)</span> is just a vector of numbers.</p>
<p>We follow the convention that all vectors are column vectors, so that <span class="math inline">\(\textbf{y}_{(n)}\)</span> serves as a shorthand for <span class="math inline">\(\textbf{y}_{(n \times 1)}\)</span>.</p>
<p>The typical element of a matrix is written as a scalar with subscripts that refer to rows and columns. For example, the statement</p>
<p><span class="math display">\[\textbf{A}= \left\lbrack a_{ij} \right\rbrack\]</span></p>
<p>says that matrix <span class="math inline">\(\textbf{A}\)</span> consists of the scalars <span class="math inline">\(a_{ij}\)</span>; for example, <span class="math inline">\(a_{23}\)</span> is the scalar in row 2, column 3 of the matrix.</p>
</section>
<section id="special-matrices" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="special-matrices"><span class="header-section-number">4.2</span> Special Matrices</h2>
<p>A few special matrices, common in statistics and machine learning are</p>
<ul>
<li><p><span class="math inline">\(\textbf{1}_{n} = \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}\)</span>, the <strong>unit</strong> vector of size <span class="math inline">\(n\)</span>; all its elements are 1.</p></li>
<li><p><span class="math inline">\(\textbf{0}_{n} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}\)</span>, the <strong>zero</strong> vector of size <span class="math inline">\(n\)</span>; all its elements are 0.</p></li>
<li><p><span class="math inline">\(\textbf{0}_{(n \times k)} = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ \vdots &amp; \cdots &amp; \vdots \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>, the <strong>zero</strong> matrix of order <span class="math inline">\((n \times k)\)</span>. All its elements are 0.</p></li>
<li><p><span class="math inline">\(\textbf{J}_{(n \times k)} = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ \vdots &amp; \cdots &amp; \vdots \\ 1 &amp; 1 &amp; 1 \end{bmatrix}\)</span>, the <strong>unit</strong> matrix of size <span class="math inline">\((n \times k)\)</span>. All its elements are 1.</p></li>
<li><p><span class="math inline">\(\textbf{I}_{n} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>, the <strong>identity</strong> matrix of size <span class="math inline">\((n \times n)\)</span> with 1s on the diagonal and 0s elsewhere.</p></li>
</ul>
<p>If the order of these matrices is obvious from the context, the subscripts tend to be omitted.</p>
</section>
<section id="basic-operations-on-matrices-and-vectors" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="basic-operations-on-matrices-and-vectors"><span class="header-section-number">4.3</span> Basic Operations on Matrices and Vectors</h2>
<p>The basic operations on matrices and vectors are addition, subtraction, multiplication, transposition, and inversion. These are standard operations in manipulating matrix and vector equations. Decompositions such as Cholesky roots, eigenvalue and singular value decompositions are more advanced operations that are important in solving estimation problems in statistics.</p>
<section id="transpose" class="level3">
<h3 class="anchored" data-anchor-id="transpose">Transpose</h3>
<p>The <strong>transpose</strong> of a matrix is obtained by exchanging rows and columns. If <span class="math inline">\(a_{ij}\)</span> is the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> of matrix <span class="math inline">\(\textbf{A}\)</span>, the transpose of <span class="math inline">\(\textbf{A}\)</span>, denoted <span class="math inline">\(\textbf{A}^\prime\)</span>, has typical element <span class="math inline">\(a_{ji}\)</span>. In case of the <span class="math inline">\((3\  \times 5)\)</span> matrix shown previously, its transpose is</p>
<p><span class="math display">\[\textbf{A}^\prime_{(5 \times 3)} = \begin{bmatrix}
\begin{matrix}
1 \\
9.0 \\
\begin{matrix}
6.2 \\
1 \\
0
\end{matrix}
\end{matrix} &amp; \begin{matrix}
1 \\
3.2 \\
\begin{matrix}
1.4 \\
0 \\
1
\end{matrix}
\end{matrix} &amp; \begin{matrix}
1 \\
4.1 \\
\begin{matrix}
- 0.6 \\
0 \\
0
\end{matrix}
\end{matrix}
\end{bmatrix}\]</span></p>
<p>The transpose of a column vector is a row vector:</p>
<p><span class="math display">\[\textbf{a}^{\prime} = \begin{bmatrix}
a_{1} \\
\vdots \\
a_{n}
\end{bmatrix}^\prime = \left\lbrack a_{1},\cdots,a_{n} \right\rbrack\]</span></p>
<p>Transposing a transpose produces the original matrix, <span class="math inline">\(\left( \textbf{A}^{\prime} \right)^{\prime}\ = \textbf{A}\)</span>.</p>
<p>A matrix is <strong>symmetric</strong> if it is equal to its transpose, <span class="math inline">\(\textbf{A}^\prime = \textbf{A}\)</span>. Symmetric matrices are square matrices (same numbers of rows and columns). The matrices <span class="math inline">\(\textbf{A}^\prime\textbf{A}\)</span> and <span class="math inline">\(\textbf{A}\textbf{A}^\prime\)</span> are always symmetric. A symmetric matrix whose off-diagonal elements are zero is called a <strong>diagonal</strong> matrix.</p>
</section>
<section id="addition-and-subtraction" class="level3">
<h3 class="anchored" data-anchor-id="addition-and-subtraction">Addition and Subtraction</h3>
<p>The <strong>sum</strong> (<strong>difference</strong>) of two matrices is the matrix of the elementwise sums (differences) of their elements. These operations require that the matrices being summed or subtracted have the same order:</p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} + \textbf{B}_{(n \times k)} = \left\lbrack a_{ij} + b_{ij} \right\rbrack\]</span></p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} - \textbf{B}_{(n \times k)} = \left\lbrack a_{ij} - b_{ij} \right\rbrack\]</span></p>
<p>Suppose, for example, that <span class="math inline">\(\textbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}\)</span> and <span class="math inline">\(\textbf{B}=\begin{bmatrix} - 1 &amp; - 2 &amp; - 3 \\ - 4 &amp; - 5 &amp; - 6 \end{bmatrix}\)</span>. Then,</p>
<p><span class="math display">\[\textbf{A}+ \textbf{B}= \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}\]</span></p>
<p><span class="math display">\[\textbf{A}- \textbf{B}= \begin{bmatrix}
2 &amp; 4 &amp; 6 \\
8 &amp; 10 &amp; 12
\end{bmatrix}\]</span></p>
<p>Since addition (subtraction) are elementwise operations, they can be combined with transposition:</p>
<p><span class="math display">\[\left( \textbf{A}+ \textbf{B}\right)^\prime = \textbf{A}^{\prime} + \textbf{B}^{\prime}\]</span></p>
</section>
<section id="multiplication" class="level3">
<h3 class="anchored" data-anchor-id="multiplication">Multiplication</h3>
<p>Two matrices conform for addition (subtraction) if they have the same order, that is, the same number of rows and columns. Multiplication of matrices requires a different type of conformity; two matrices <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> can be multiplied as <span class="math inline">\(\text{AB}\)</span> (or <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span>), if the number of columns in <span class="math inline">\(\textbf{A}\)</span> equals the number of columns in <span class="math inline">\(\textbf{B}\)</span>. We say that in the product <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span>, <span class="math inline">\(\textbf{A}\)</span> is post-multiplied by <span class="math inline">\(\textbf{B}\)</span> or that <span class="math inline">\(\textbf{A}\)</span> is multiplied into <span class="math inline">\(\textbf{B}\)</span>. The result of multiplying a <span class="math inline">\((n \times k)\)</span> matrix into a <span class="math inline">\((k \times p)\)</span> matrix is a <span class="math inline">\((n \times p)\)</span> matrix.</p>
<p>Before examining the typical elements in the result of multiplication, let’s look at a special case, the <strong>inner product</strong> of two <span class="math inline">\((k \times 1)\)</span> vectors <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span>, also called the <strong>dot product</strong> or the <strong>scalar product</strong>, is the result of multiplying the transpose of <span class="math inline">\(\textbf{A}\)</span> into <span class="math inline">\(\textbf{B}\)</span>, a scalar value</p>
<p><span class="math display">\[\textbf{A}^\prime\textbf{B}= \left\lbrack a_{1}, \cdots,a_{k} \right\rbrack\begin{bmatrix}
b_{1} \\
\vdots \\
b_{k}
\end{bmatrix} = a_{1}b_{1} + \cdots a_{k}b_{k} = \sum_{i = 1}^{k}{a_{i}b_{i}}\]</span></p>
<p>The square root of the dot product of a vector with itself is the Euclidean <span class="math inline">\({(L}_{2})\)</span> <strong>norm</strong> of the vector,</p>
<p><span class="math display">\[\left| \left| \textbf{a}\right| \right| = \sqrt{\textbf{a}^\prime\textbf{a}} = \sum_{i = 1}^{k}a_{i}^{2}\]</span></p>
<p>The <span class="math inline">\(L_{2}\)</span> norm plays an important role as a loss function in statistical models. The vector for which the norm is calculated is then often a vector of model errors.</p>
<p>Now let’s return to the problem of multiplying the <span class="math inline">\((n \times k)\)</span> matrix <span class="math inline">\(\textbf{A}\)</span> into the <span class="math inline">\((k \times p)\)</span> matrix <span class="math inline">\(\textbf{B}\)</span> and introduce one more piece of notation: the <span class="math inline">\(i\)</span><sup>th</sup> row of <span class="math inline">\(\textbf{A}\)</span> is denoted <span class="math inline">\(\mathbf{\alpha}_{i}\)</span> and the <span class="math inline">\(j\)</span><sup>th</sup> column of <span class="math inline">\(\textbf{B}\)</span> is denoted <span class="math inline">\(\textbf{B}_{j}\)</span>. Now we can finally write the product <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span> as a matrix whose typical element is the inner product of <span class="math inline">\(\mathbf{\alpha}_{i}\)</span> and <span class="math inline">\(\textbf{B}_{j}\)</span>:</p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} \times \textbf{B}_{(k \times p)} = \left\lbrack \boldsymbol{\alpha}_{i}^\prime\ \textbf{b}_{j} \right\rbrack_{(n \times p)}\ \]</span></p>
<p>As an example, let <span class="math inline">\(\textbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 0 \\ 3 &amp; 1 &amp; - 3 \\ 4 &amp; 1 &amp; 2 \end{bmatrix}\)</span> and <span class="math inline">\(\textbf{B}= \begin{bmatrix} 1 &amp; 0 \\ 2 &amp; 3 \\ 2 &amp; 1 \end{bmatrix}\)</span>. The product <span class="math inline">\(\textbf{A}\times\textbf{B}\)</span> is a <span class="math inline">\((3 \times 2)\)</span> matrix with elements</p>
<p><span class="math display">\[\textbf{A}\times\textbf{B}= \begin{bmatrix}
1 \times 1 + 2 \times 2 + 0 \times 2 &amp; 1 \times 0 + 2 \times 3 + 0 \times 1 \\
3 \times 1 + 1 \times 2 - 3 \times 2 &amp; 3 \times 0 + 1 \times 3 - 3 \times 1 \\
4 \times 1 + 1 \times 2 + 2 \times 2 &amp; 4 \times 0 + 1 \times 3 + 2 \times 1
\end{bmatrix} = \begin{bmatrix}
5 &amp; 6 \\
- 1 &amp; 0 \\
10 &amp; 5
\end{bmatrix}\]</span></p>
<p>Here are a few helpful rules for matrix multiplication:</p>
<ol type="1">
<li><p><span class="math inline">\(c\textbf{A}= \left\lbrack ca_{ij} \right\rbrack\)</span></p></li>
<li><p><span class="math inline">\(c\left( \textbf{A}+ \textbf{B}\right) = c\textbf{A}+ c\textbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\textbf{C}\left( \textbf{A}+ \textbf{B}\right) = \textbf{C}\textbf{A}+ \textbf{C}\textbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)\textbf{C}= \textbf{A}(\textbf{B}\textbf{C})\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}+ \textbf{B}\right)\left( \textbf{C}+ \mathbf{D} \right) = \textbf{A}\textbf{C}+ \textbf{A}\mathbf{D} + \mathbf{BC} + \mathbf{BD}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)^\prime = \textbf{B}^\prime\textbf{A}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\left( c\textbf{A}\right)^\prime = c\textbf{A}^\prime\)</span></p></li>
</ol>
</section>
<section id="inversion-and-rank" class="level3">
<h3 class="anchored" data-anchor-id="inversion-and-rank">Inversion and Rank</h3>
<p>In scalar algebra, division and multiplication are inverse operations, dividing a non-zero scalar by itself yields the multiplicative identity: <span class="math inline">\(\frac{a}{a} = 1\)</span>. What is the equivalent of this operation for matrices? First, inversion of a matrix does not reduce it to a scalar, the multiplicative identity for matrices is the identity matrix <span class="math inline">\(\textbf{I}\)</span>, a diagonal matrix with 1s on the diagonal. Second, the inversion is only defined for square matrices. If <span class="math inline">\(\textbf{A}\)</span> is an <span class="math inline">\((n \times n)\)</span> matrix, the matrix <span class="math inline">\(\textbf{B}\)</span> for which</p>
<p><span class="math display">\[\textbf{A}\textbf{B}= \textbf{I}\]</span></p>
<p>is called the <strong>inverse</strong> of <span class="math inline">\(\textbf{A}\)</span>, denoted as <span class="math inline">\(\textbf{A}^{- 1}\)</span>.</p>
<p>Inverse matrices do not have to exist, even for square matrices. If <span class="math inline">\(\textbf{A}\)</span> has an inverse matrix, then <span class="math inline">\(\textbf{A}\)</span> is called a <strong>non-singular</strong> matrix. In that case, <span class="math inline">\(\textbf{A}^{- 1}\textbf{A}= \textbf{A}\textbf{A}^{- 1} = \text{I}\)</span>.</p>
<p>For the inverse of a square matrix to exist, for the matrix to be non-singular, the matrix must be of full <strong>rank</strong>. The rank of a matrix, denoted <span class="math inline">\(r(\textbf{A})\)</span>, is the number of its linearly independent columns. What does that mean? Suppose we are dealing with a <span class="math inline">\((n \times k)\)</span> matrix <span class="math inline">\(\textbf{B}\)</span> and its column vectors are <span class="math inline">\(\textbf{B}_{1},\cdots,\textbf{B}_{k}\)</span>. A linear combination of the columns of <span class="math inline">\(\textbf{B}\)</span> is</p>
<p><span class="math display">\[c_{1}\textbf{b}_{1} + c_{2}\textbf{b}_{2} + \cdots + c_{k}\textbf{b}_{k} = q\]</span></p>
<p>If you can find a set of scalars <span class="math inline">\(c_{1},\cdots,c_{k}\)</span> such that <span class="math inline">\(q = 0\)</span>, then the columns of <span class="math inline">\(\textbf{B}\)</span> are linearly dependent. If the only set of scalars that yields <span class="math inline">\(q = 0\)</span> is</p>
<p><span class="math display">\[c_{1} = c_{2} = \cdots = c_{k} = 0\]</span></p>
<p>then the columns of <span class="math inline">\(\textbf{B}\)</span> are not linearly dependent and the rank of <span class="math inline">\(\textbf{B}\)</span> is <span class="math inline">\(k\)</span>.</p>
<p>Here are a few more useful results about the rank of a matrix:</p>
<ol type="1">
<li><p><span class="math inline">\(r\left( \textbf{A}\right) = r\left( \textbf{A}^\prime \right) = r\left( \textbf{A}^\prime\textbf{A}\right) = r\left( \textbf{A}\textbf{A}^{\prime} \right)\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}\textbf{B}\right) \leq \min\left\{ r\left( \textbf{A}\right),r\left( \textbf{B}\right) \right\}\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}+ \textbf{B}\right) \leq r\left( \textbf{A}\right) + r(\textbf{B})\)</span></p></li>
</ol>
<p>The first two results are particularly important in statistical models. In models with linear structures, it is common to collect the <span class="math inline">\(p\)</span> input variables in a linear model, including the intercept as a column of ones, into a matrix <span class="math inline">\(\textbf{X}_{(n\  \times p + 1)}\)</span>:</p>
<p><span class="math display">\[\textbf{X}_{(n\  \times p + 1)} = \begin{bmatrix}
1 &amp; x_{11} &amp; \begin{matrix}
\cdots &amp; x_{1p}
\end{matrix} \\
\vdots &amp; \vdots &amp; \begin{matrix}
\ddots &amp; \vdots
\end{matrix} \\
1 &amp; x_{n1} &amp; \begin{matrix}
\cdots &amp; x_{np}
\end{matrix}
\end{bmatrix}\]</span></p>
<p>Suppose we want to solve the linear system <span class="math inline">\(\textbf{Y}= \textbf{X}\textbf{c}\)</span> for <span class="math inline">\(\textbf{c}\)</span>. Start by pre-multiplying both sides of the equation with the transpose of <span class="math inline">\(\textbf{X}\)</span>:</p>
<p><span class="math display">\[\textbf{X}^{\prime}\textbf{Y}= \textbf{X}^{\prime}\textbf{X}\textbf{c}\]</span></p>
<p>If we had an inverse of <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span>, then we can now pre-multiply both sides with that inverse and isolate <span class="math inline">\(\text{c}\)</span>:</p>
<p><span class="math display">\[{\left( \textbf{X}^\prime\textbf{X}\right)^{\mathbf{- 1}}\textbf{X}}^{\prime}\textbf{Y}= \left( \textbf{X}^\prime\textbf{X}\right)^{-1}\textbf{X}^\prime\textbf{X}\textbf{c}= \textbf{I}\textbf{c}= \textbf{c}\]</span></p>
<p>We have a solution to the system, namely <span class="math inline">\({\textbf{c}=\left( \textbf{X}^\prime\textbf{X}\right)}^{- 1}\textbf{X}^{\prime}\textbf{Y}\)</span>, only if the inverse <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\)</span> exists. And that requires this <span class="math inline">\((p + 1) \times (p + 1)\)</span> matrix is of full rank <span class="math inline">\(r\left( \left( \textbf{X}^\prime\textbf{X}\right)^{- 1} \right) = p + 1\)</span>. This, in turn is equivalent to saying that <span class="math inline">\(\textbf{X}\)</span> has full rank <span class="math inline">\(p + 1\)</span> because of property (i).</p>
<p>Here are some useful results about inverse matrices:</p>
<ol type="1">
<li><p><span class="math inline">\(\left( \textbf{A}^{- 1} \right)^\prime = \left( \textbf{A}^\prime \right)^{- 1}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}^{- 1} \right)^{- 1} = \textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)^{- 1} = \textbf{B}^{-1}\textbf{A}^{-1}\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}^{- 1} \right) = r(\textbf{A})\)</span></p></li>
</ol>
<p>If the matrix <span class="math inline">\(\textbf{X}\)</span> is of less than full rank, it is called a <strong>rank-deficient</strong> matrix. Can we still solve the linear system <span class="math inline">\(\textbf{Y}= \textbf{X}\textbf{c}\)</span>? Not by using a (regular) inverse matrix, but there is a way out, by using a <strong>generalized inverse</strong> matrix. If a matrix <span class="math inline">\(\textbf{A}^{-}\)</span> can be found that satisfies</p>
<p><span class="math display">\[\textbf{A}\textbf{A}^{-}\textbf{A}= \textbf{A}\]</span></p>
<p>then it is called the generalized inverse (or pseudo-inverse or g-inverse) of <span class="math inline">\(\textbf{A}\)</span>. Suppose we can find such a generalized inverse <span class="math inline">\(\left( \textbf{X}^{\prime}\textbf{X}\right)^{-}\)</span>f or <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span>. What if we use that in the solution of the linear system,</p>
<p><span class="math display">\[\textbf{c}= {\left( \textbf{X}^{\prime}\textbf{X}\right)^{-}\textbf{X}}^{\prime}\textbf{Y}\]</span></p>
<p>Unfortunately, whereas regular inverses are unique, there are (infinitely) many generalized inverses that satisfy the condition <span class="math inline">\((\textbf{X}^\prime\textbf{X})\left( \textbf{X}^\prime\textbf{X}\right)^{-}\textbf{X}^\prime\textbf{X}= \textbf{X}^\prime\textbf{X}\)</span>. So, there will be infinitely many possible solutions to the linear system. Fortunately, it turns out that generalized inverses have some nice properties, for example, <span class="math inline">\(\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{-}\textbf{X}\)</span> is invariant to the choice of the generalized inverse. Even if the solution <span class="math inline">\(\textbf{c}\)</span> is not unique, <span class="math inline">\(\textbf{X}\textbf{c}\)</span> is unique. This result is important in linear models with rank-deficient design matrices, a condition that is common when the model contains classification variables. While the parameter estimates in such a model are not unique, because we need to use a generalized inverse to derive the estimates, the predicted values are the same, no matter which generalized inverse we choose.</p>
</section>
<section id="determinant" class="level3">
<h3 class="anchored" data-anchor-id="determinant">Determinant</h3>
<p>The rank reduces a matrix to a single scalar value, the number of linearly independent columns of the matrix. Another value that reduces a square matrix to a single scalar is the determinant, written as <span class="math inline">\(det(\textbf{A})\)</span> or <span class="math inline">\(|\textbf{A}|\)</span>. The determinant has a geometric interpretation which is not that relevant for our discussion. What matters more is that the determinant appears frequently in expressions of multivariate probability distributions and knowing how to manipulate the determinants.</p>
<ol type="1">
<li><p><span class="math inline">\(|\textbf{A}| = |\textbf{A}^\prime|\)</span></p></li>
<li><p><span class="math inline">\(|\textbf{I}| = 1\)</span></p></li>
<li><p><span class="math inline">\(\left| c\textbf{A}\right| = c^{n}\mathbf{|A}\mathbf{|}\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is singular, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p>If each element of a row (column) of <span class="math inline">\(\textbf{A}\)</span> is zero, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p>If two rows (column) of <span class="math inline">\(\textbf{A}\)</span> are identical, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p><span class="math inline">\(\left| \textbf{A}\textbf{B}\right| = \left| \textbf{A}\right|\ \left| \textbf{B}\right|\)</span></p></li>
<li><p><span class="math inline">\(\left| \textbf{A}^{- 1} \right| = 1/|\textbf{A}|\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is a triangular matrix, then <span class="math inline">\(|\textbf{A}| = \prod_{i = 1}^{n}a_{ii}\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is a diagonal matrix, then <span class="math inline">\(|\textbf{A}| = \prod_{i = 1}^{n}a_{ii}\)</span></p></li>
</ol>
</section>
<section id="trace" class="level3">
<h3 class="anchored" data-anchor-id="trace">Trace</h3>
<p>The trace operator, <span class="math inline">\(tr(\textbf{A})\)</span>, applies only to square matrices. The trace of an <span class="math inline">\(\textbf{A}_{(n \times n)}\)</span> matrix is the sum of its diagonal elements:</p>
<p><span class="math display">\[tr\left( \textbf{A}\right) = \sum_{i = 1}^{n}a_{ii}\]</span></p>
<p>The trace plays an important role in statistics in determining expected values of quadratic forms of random variables, for example, sums of squares in linear models. An important property of the trace is its invariance under cyclic permutations,</p>
<p><span class="math display">\[tr\left( \mathbf{ABC} \right) = tr\left( \mathbf{BCA} \right) = tr(\mathbf{CAB})\]</span></p>
<p>provided the matrices conform to multiplication.</p>
<p>Some other useful properties of the trace are</p>
<ol type="1">
<li><p><span class="math inline">\(tr\left( \textbf{A}+ \textbf{B}\right) = tr\left( \textbf{A}\right) + tr\left( \textbf{B}\right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( \textbf{A}\right) = tr\left( \textbf{A}^\prime \right)\)</span></p></li>
<li><p><span class="math inline">\(\textbf{Y}^\prime\text{Ay} = tr\left( \textbf{Y}^\prime\text{Ay} \right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( c\textbf{A}\right) = c \times tr\left( \textbf{A}\right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( \textbf{A}\right) = r(\textbf{A})\)</span> if <span class="math inline">\(\textbf{A}\)</span> is symmetric and idempotent (<span class="math inline">\(\textbf{A}\textbf{A}= \textbf{A}\)</span> and <span class="math inline">\(\textbf{A}= \textbf{A}^\prime\)</span>)</p></li>
</ol>
</section>
</section>
<section id="random-vectors" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="random-vectors"><span class="header-section-number">4.4</span> Random Vectors</h2>
<p>If the elements of a vector are random variables, the vector object itself is a random variable. You can think of random vectors as a convenient mechanism to collect random variables. Suppose we draw a random sample <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span>, then we can collect the <span class="math inline">\(n\)</span> random variables in a single random vector</p>
<p><span class="math display">\[\textbf{Y}= \begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}\]</span></p>
<section id="expected-value" class="level3">
<h3 class="anchored" data-anchor-id="expected-value">Expected Value</h3>
<p>Since each <span class="math inline">\(Y_{i}\)</span> has a probability distribution, a mean (expected value) <span class="math inline">\(\text{E}\left\lbrack Y_{i} \right\rbrack\)</span>, a variance <span class="math inline">\(\text{Var}\left\lbrack Y_{i} \right\rbrack\)</span>, and so forth, the same applies to their collection. The expected value (mean) of a random vector is the vector of the expected values of its elements:</p>
<p><span class="math display">\[\text{E}\left\lbrack \textbf{Y}\right\rbrack = \begin{bmatrix}
\text{E}\left\lbrack Y_{1} \right\rbrack \\
\vdots \\
\text{E}\left\lbrack Y_{n} \right\rbrack
\end{bmatrix}\]</span></p>
<p>Suppose that <span class="math inline">\(\textbf{A},\ \textbf{B},\ \textbf{c}\)</span> are matrices and vectors of constants, respectively, and that <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are random vectors. The following are useful expectation operations in this situations:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{E}\left\lbrack \textbf{A}\right\rbrack = \textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AYB} + \mathbf{C} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack\textbf{B}+ \textbf{C}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AY} + \mathbf{c} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack + \textbf{c}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AY} + \mathbf{BU} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack + \textbf{B}\ \text{E}\lbrack\mathbf{U}\rbrack\)</span></p></li>
</ol>
</section>
<section id="covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="covariance-matrix">Covariance Matrix</h3>
<p>While the distribution of <span class="math inline">\(Y_{i}\)</span> is univariate, <span class="math inline">\(\textbf{Y}\)</span> has a multivariate (<span class="math inline">\(n\)</span>-variate) distribution. The mean of the distribution is represented by a vector. The variance of the distribution is represented by a matrix, the <strong>variance-covariance matrix</strong>, a special case of a <strong>covariance</strong> matrix.</p>
<p>The covariance matrix between random vectors <span class="math inline">\(\textbf{Y}_{(k \times 1)}\)</span> and <span class="math inline">\(\mathbf{U}_{(p \times 1)}\)</span> is a <span class="math inline">\((k \times p)\)</span> matrix whose typical elements are the covariances between the elements of <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span>:</p>
<p><span class="math display">\[\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack = \left\lbrack \text{Cov}(Y_{i},U_{j}) \right\rbrack\]</span></p>
<p>The covariance matrix can be written in terms of expected values of <span class="math inline">\(\textbf{Y}\)</span>, <span class="math inline">\(\mathbf{U}\)</span>, and <span class="math inline">\(\textbf{Y}\mathbf{U}^\prime\)</span></p>
<p><span class="math display">\[\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack = \text{E}\left\lbrack \left( \textbf{Y}- \text{E}\lbrack\textbf{Y}\rbrack \right)\left( \mathbf{U} - \text{E}\left\lbrack \mathbf{U} \right\rbrack \right)^\prime \right\rbrack = \text{E}\left\lbrack \textbf{Y}\mathbf{U}^\prime \right\rbrack - \text{E}\left\lbrack \textbf{Y}\right\rbrack \text{E}\left\lbrack \mathbf{U} \right\rbrack^{\prime}\]</span></p>
<p>Some useful rules to manipulate covariance matrices are:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \mathbf{AY},\mathbf{U} \right\rbrack = \textbf{A}\text{Cov}\lbrack\textbf{Y},\mathbf{U}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \textbf{Y},\mathbf{BU} \right\rbrack = \text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack\textbf{B}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \mathbf{AY},\mathbf{BU} \right\rbrack = \textbf{A}\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack\ \textbf{B}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack a\textbf{Y}+ b\mathbf{U},c\mathbf{W} + d\textbf{V}\right\rbrack = ac\text{Cov}\left\lbrack \textbf{Y},\mathbf{W} \right\rbrack + bc\text{Cov}\left\lbrack \mathbf{U},\mathbf{W} \right\rbrack + ad\text{Cov}\left\lbrack \textbf{Y},\textbf{V}\right\rbrack + bd\text{Cov}\lbrack\mathbf{U},\textbf{V}\rbrack\)</span></p></li>
</ol>
</section>
<section id="variance-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="variance-covariance-matrix">Variance-covariance Matrix</h3>
<p>The variance-covariance matrix (or <strong>variance matrix</strong> for short) of a random vector <span class="math inline">\(\textbf{Y}\)</span> is the covariance matrix of <span class="math inline">\(\textbf{Y}\)</span> with itself.</p>
<p><span class="math display">\[\text{Var}\left\lbrack \textbf{Y}\right\rbrack = \text{Cov}\left\lbrack \textbf{Y},\textbf{Y}\right\rbrack = \text{E}\left\lbrack \left( \textbf{Y}- \text{E}\lbrack\textbf{Y}\rbrack \right)\left( \textbf{Y}-\text{E}\left\lbrack \textbf{Y}\right\rbrack \right)^\prime \right\rbrack = \text{E}\left\lbrack \textbf{Y}\textbf{Y}^\prime \right\rbrack - \text{E}\left\lbrack \textbf{Y}\right\rbrack \text{E}\left\lbrack \textbf{Y}\right\rbrack^{\prime}\]</span></p>
<p>The diagonal entries of the variance-covariance matrix contain the variances of the <span class="math inline">\(Y_{i}\)</span>. The off-diagonal cells contain the covariances <span class="math inline">\(\text{Cov}\left\lbrack Y_{i},Y_{j} \right\rbrack\)</span>. If the variance matrix is diagonal, the elements of random vector <span class="math inline">\(\textbf{Y}\)</span> are uncorrelated. Two random vectors <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are uncorrelated if their variance matrix is block-diagonal:</p>
<p><span class="math display">\[\text{Var}\begin{bmatrix}
\textbf{Y}_{1} \\
\textbf{Y}_{2}
\end{bmatrix} = \begin{bmatrix}
\text{Var}\lbrack\textbf{Y}_{2}\rbrack &amp; \textbf{0}\\
\textbf{0}&amp; \text{Var}\lbrack\textbf{Y}_{1}\rbrack
\end{bmatrix}\]</span></p>
<p>A very special variance-covariance matrix in statistical models is the scaled identity matrix, <span class="math inline">\(\sigma^{2}\textbf{I}\)</span>. This is the variance matrix of uncorrelated observations drawn from the same distribution—a common assumption for the error terms in models.</p>
<p>The rules for working with covariances extend to working with variance matrices:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{Var}\left\lbrack \mathbf{AY} \right\rbrack = \textbf{A}\ \text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{A}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack \textbf{Y}+ \textbf{A}\right\rbrack = \text{Var}\lbrack\textbf{Y}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack \textbf{A}^\prime\textbf{Y}\right\rbrack = \textbf{A}^\prime\text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack a\textbf{Y}\right\rbrack = a^{2}\text{Var}\lbrack\textbf{Y}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack a\textbf{Y}+ b\mathbf{U} \right\rbrack = a^{2}\text{Var}\left\lbrack \textbf{Y}\right\rbrack + b^{2}\text{Var}\left\lbrack \mathbf{U} \right\rbrack + 2ab\ \text{Cov}\lbrack\textbf{Y},\mathbf{U}\rbrack\)</span></p></li>
</ol>
<p>Finally, an important result about expected values of quadratic forms, heavily used to in decomposing variability is</p>
<p><span class="math display">\[\text{E}\left\lbrack \textbf{Y}^\prime\mathbf{AY} \right\rbrack = tr\left( \textbf{A}\text{Var}\left\lbrack \textbf{Y}\right\rbrack \right) + \text{E}\left\lbrack \textbf{Y}\right\rbrack^\prime\textbf{A}\ \text{E}\lbrack\textbf{Y}\rbrack\]</span></p>
</section>
</section>
<section id="sec-matrix-differentiation" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-matrix-differentiation"><span class="header-section-number">4.5</span> Matrix Differentiation</h2>
<p>Estimation of parameters in statistical models often requires minimization or maximization of an objective function. For example, the ordinary least squares (OLS) principle finds the OLS estimator as the function of the data that minimizes the error sum of squares of the model. Maximum likelihood finds estimators of the parameters as the functions of the data that maximizes the joint likelihood (the joint distribution function) of the data.</p>
<p>The parameters of the models appear as elements of vectors and matrices. Finding estimators of the parameters thus requires calculus on vectors and matrices. Consider matrix <span class="math inline">\(\textbf{A}\)</span>, whose elements depend on a scalar parameter <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\textbf{A}= \left\lbrack a_{ij}(\theta) \right\rbrack\)</span>. The derivative of <span class="math inline">\(\textbf{A}\)</span> with respect to <span class="math inline">\(\theta\)</span> is the matrix of the derivatives of the typical elements <span class="math inline">\(a_{ij}(\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span>. We write this formally as</p>
<p><span class="math display">\[\frac{\partial\textbf{A}}{\partial\theta} = \left\lbrack \frac{\partial a_{ij}(\theta)}{\partial\theta} \right\rbrack\]</span></p>
<p>The derivative of a function <span class="math inline">\(f(\boldsymbol{\theta})\)</span> with respect to the vector <span class="math inline">\(\boldsymbol{\theta}_{(p \times 1)}\)</span> is the vector of the partial derivatives of the function</p>
<p><span class="math display">\[\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}} = \begin{bmatrix}
\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\theta_{1}} \\
\vdots \\
\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\theta_{p}}
\end{bmatrix}\]</span></p>
<p>Here are some useful results from vector and matrix calculus where <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are functions of <span class="math inline">\(\theta\)</span> and vector <span class="math inline">\(\textbf{X}\)</span> does not depend on <span class="math inline">\(\theta\)</span>:</p>
<ol type="1">
<li><p><span class="math inline">\(\frac{{\partial ln}\left| \textbf{A}\right|}{\partial\theta} = \frac{1}{\left| \textbf{A}\right|}\frac{\partial\left| \textbf{A}\right|}{\partial\theta} = tr\left( \textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta} \right)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{A}^{- 1}}{\partial\theta} = - \textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta\ }\textbf{A}^{- 1}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial tr\left( \mathbf{AB} \right)}{\partial\theta} = tr\left( \frac{\mathbf{\partial}\textbf{A}}{\partial\theta}\textbf{B}\right) + tr\left( \textbf{A}\frac{\mathbf{\partial}\textbf{B}}{\partial\theta} \right)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^\prime\textbf{A}^{- 1}\textbf{X}}{\partial\theta} = - \textbf{X}^\prime\textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta}\textbf{A}^{- 1}\textbf{X}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^{\prime}\mathbf{Ax}}{\partial\textbf{X}} = 2\mathbf{Ax}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^\prime\textbf{A}}{\partial\textbf{X}} = \frac{\partial\textbf{A}^\prime\textbf{X}}{\partial\textbf{X}} = \textbf{A}\)</span></p></li>
</ol>
</section>
<section id="sec-idempotent" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="sec-idempotent"><span class="header-section-number">4.6</span> Idempotent Matrices</h2>
<p>A matrix is called <strong>idempotent</strong> if multiplying the matrix by itself yields the matrix–<span class="math inline">\(\textbf{A}\textbf{A}= \textbf{A}\)</span>. Because <span class="math inline">\(\textbf{A}\)</span> must conform to itself for multiplication, idempotent matrices are square matrices. Idempotent matrices that are also symmetric play an important role in statistical estimation. Idempotent matrices are <strong>projection</strong> matrices, that means they map a vector from a space to a sub-space. Symmetric idempotent matrices are orthogonal projection matrices.</p>
<p>Among some of the interesting properties of idempotent matrices are the following:</p>
<ol type="1">
<li><p>The trace of an idempotent matrix is equal to its rank.</p></li>
<li><p>The trace of an idempotent matrix is an integer.</p></li>
<li><p>The eigenvalues of an idempotent matrix are either 0 or 1.</p></li>
</ol>
<section id="projections" class="level3">
<h3 class="anchored" data-anchor-id="projections">Projections</h3>
<p>For example, suppose we want to find a solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> in the linear model <span class="math inline">\(\textbf{Y}_{(n \times 1)} = \textbf{X}\boldsymbol{\beta}_{(p + 1 \times 1)} + \mathbf{\epsilon}\)</span>. The vector <span class="math inline">\(\textbf{Y}\)</span> is a vector in <span class="math inline">\(n\)</span>-dimensional space <span class="math inline">\(\mathbb{R}^{n}\)</span> and the model places a restriction on the predicted values <span class="math inline">\(\widehat{\textbf{Y}} = \textbf{X}\widehat{\boldsymbol{\beta}}\)</span>: the predicted values are confined to a <span class="math inline">\((p + 1)\)</span>-dimensional sub-space of <span class="math inline">\(\mathbb{R}^{n}\)</span>. Regardless of how we choose the estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, we are dealing with projecting vector <span class="math inline">\(\textbf{Y}\)</span> onto a sub-space of <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<p>We can thus think of the problem of finding the best estimator in this model as the problem of finding the best projection onto the space generated by the columns of <span class="math inline">\(\textbf{X}\)</span>. In that case, why not choose the projection that minimizes the distance between the observed values <span class="math inline">\(\textbf{Y}\)</span> and the predicted values <span class="math inline">\(\widehat{\textbf{Y}}\)</span>. This is achieved by projecting <span class="math inline">\(\textbf{Y}\)</span> perpendicular (orthogonal) onto the sub-space generated by <span class="math inline">\(\textbf{X}\)</span>. In other words, our solution is the vector <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> that satisfies</p>
<p><span class="math display">\[\left( \textbf{Y}- \textbf{X}\widehat{\boldsymbol{\beta}} \right)^\prime\textbf{X}\widehat{\boldsymbol{\beta}} = 0\]</span></p>
<p>Multiplying out and rearranging terms yields</p>
<p><span class="math display">\[{\widehat{\boldsymbol{\beta}}}^{\prime}\textbf{X}^{\prime}\textbf{Y}= {\widehat{\boldsymbol{\beta}}}^{\prime}\textbf{X}^\prime\textbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[\textbf{X}^{\prime}\textbf{Y}= \textbf{X}^\prime\textbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<p>If <span class="math inline">\(\textbf{X}\)</span> is of full column rank, then <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\)</span> exists and we can solve:</p>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}}=\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}\]</span></p>
</section>
<section id="sec-hat-matrix" class="level3">
<h3 class="anchored" data-anchor-id="sec-hat-matrix">The “Hat” Matrix</h3>
<p>The ordinary least squares estimator is the orthogonal projection of <span class="math inline">\(\textbf{Y}\)</span> onto the sub-space created by the columns of <span class="math inline">\(\textbf{X}\)</span>. To see the projection matrix at work, compute the predicted values, <span class="math inline">\(\textbf{X}\widehat{\boldsymbol{\beta}}\)</span>:</p>
<p><span class="math display">\[\textbf{X}\widehat{\boldsymbol{\beta}}=\textbf{X}\left( \textbf{X}^{\prime}\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}= \widehat{\textbf{Y}}\]</span></p>
<p>The matrix <span class="math inline">\(\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\)</span> has a very special role in regression analysis, it is often called the “hat” matrix and denoted <span class="math inline">\(\textbf{H}\)</span>, because pre-multiplying <span class="math inline">\(\textbf{Y}\)</span> with <span class="math inline">\(\textbf{H}\)</span> puts the hats on <span class="math inline">\(\textbf{Y}\)</span>:</p>
<p><span class="math display">\[\textbf{H}\textbf{Y}= \widehat{\textbf{Y}}\]</span></p>
<p>Let’s verify that <span class="math inline">\(\textbf{H}\)</span> is indeed a projection matrix, which requires that <span class="math inline">\(\textbf{H}\textbf{H}= \textbf{H}\)</span>:</p>
<p><span class="math display">\[\textbf{H}\textbf{H}= \textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime} \times \textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\textbf{X}^\prime=\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime} = \textbf{H}\]</span></p>
<p>Matrices with the property that <span class="math inline">\(\textbf{H}\textbf{H}= \textbf{H}\)</span> are called <strong>idempotent</strong> matrices, these are projection matrices. If, in addition, <span class="math inline">\(\textbf{H}\)</span> is symmetric, <span class="math inline">\(\textbf{H}^\prime = \textbf{H}\)</span>, the matrix is called symmetric idempotent—these are orthogonal projection matrices. (An idempotent matrix that is not symmetric is called an oblique projector.)</p>
<p>The hat matrix in the regression model is a symmetric idempotent matrix.</p>
<p>Here are some results about (symmetric) idempotent matrices that come in handy when working out the properties of estimators in regression-type models:</p>
<ol type="1">
<li><p>Projection matrices are typically not of full rank. If an <span class="math inline">\((n \times n)\)</span> idempotent matrix is of rank <span class="math inline">\(n\)</span>, then it is the identity matrix <span class="math inline">\(\textbf{I}\)</span>.</p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is (symmetric) idempotent, then <span class="math inline">\(\textbf{I}- \textbf{A}\)</span> is (symmetric) idempotent.</p></li>
<li><p>If <span class="math inline">\(\mathbf{P}\)</span> is non-singular, then <span class="math inline">\(\mathbf{PA}\mathbf{P}^{-1}\)</span> is an idempotent matrix.</p></li>
</ol>
<p>You can use these properties to show that in the linear regression model with uncorrelated errors and equal variance the variance matrix of the model residuals <span class="math inline">\(\textbf{Y}- \widehat{\textbf{Y}}\)</span> is</p>
<p><span class="math display">\[\text{Var}\left\lbrack \textbf{Y}- \widehat{\textbf{Y}} \right\rbrack = \sigma^{2}(\textbf{I}- \textbf{H})\]</span></p>
</section>
<section id="a-special-case" class="level3">
<h3 class="anchored" data-anchor-id="a-special-case">A Special Case</h3>
<p>Consider the special case where <span class="math inline">\(\textbf{X}= \textbf{1}_{n}\)</span>, a column vector of ones. The corresponding linear model is <span class="math inline">\(\textbf{Y}= \textbf{1}\beta + \boldsymbol{\epsilon}\)</span>, an intercept-only model. The hat matrix for this model is</p>
<p><span class="math display">\[\textbf{1}\left( \textbf{1}^{\prime}\textbf{1}\right)^{- 1}\textbf{1}^\prime = \frac{1}{n}\textbf{1}\textbf{1}^\prime = \frac{1}{n}\textbf{J}= \begin{bmatrix}
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}
\end{bmatrix}
\]</span>a matrix filled with <span class="math inline">\(\frac{1}{n}\)</span>. The projection of <span class="math inline">\(\textbf{Y}\)</span> onto the space generated by <span class="math inline">\(\textbf{1}_{n}\)</span> is</p>
<p><span class="math display">\[\frac{1}{n}\mathbf{JY} = \begin{bmatrix}
\overline{Y} \\
\vdots \\
\overline{Y}
\end{bmatrix}\]</span></p>
<p>The predicted values are all the same, the sample mean. In other words, <span class="math inline">\(\beta = \overline{Y}\)</span>. Since the projector is idempotent, deriving the variance of the predicted value in the <em>iid</em> case is simple:</p>
<p><span class="math display">\[\text{Var}\left\lbrack \textbf{H}\textbf{Y}\right\rbrack = \textbf{H}\text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{H}^\prime = \sigma^{2}\textbf{H}\textbf{H}^{\prime} = \sigma^{2}\textbf{H}= \sigma^{2}\frac{1}{n}\textbf{J}=\begin{bmatrix}
\frac{\sigma^{2}}{n} &amp; \cdots &amp; \frac{\sigma^{2}}{n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\sigma^{2}}{n} &amp; \cdots &amp; \frac{\sigma^{2}}{n}
\end{bmatrix}\]</span></p>
</section>
</section>
<section id="sec-multi-gaussian" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="sec-multi-gaussian"><span class="header-section-number">4.7</span> Multivariate Gaussian Distribution</h2>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition</h3>
<p>A scalar random variable <span class="math inline">\(Y\)</span> has a <strong>Gaussian</strong> distribution function with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> if its probability density function is given by</p>
<p><span class="math display">\[f(y) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ - \frac{1}{2\sigma^{2}}(y - \mu)^{2} \right\}\]</span></p>
<p>We also say that <span class="math inline">\(Y\)</span> is <strong>normally</strong> distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>. The shorthand expressions <span class="math inline">\(Y \sim G(\mu,\sigma^{2})\)</span> or <span class="math inline">\(Y \sim N(\mu,\sigma^{2})\)</span> are common.</p>
<p>The generalization from a scalar random variable <span class="math inline">\(Y\)</span> to a random vector <span class="math inline">\(\textbf{Y}_{(n \times 1)}\)</span> with a <strong>multivariate Gaussian</strong> distribution is as follows. <span class="math inline">\(\textbf{Y}_{(n \times 1)}\)</span> has a multivariate Gaussian (normal) distribution with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and variance matrix <span class="math inline">\(\textbf{V}\)</span>, if its density is given by</p>
<p><span class="math display">\[f\left( \textbf{Y}\right)=\frac{\left| \textbf{V}\right|^{- 1/2}}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\left( \textbf{Y}- \boldsymbol{\mu}\right)^\prime\textbf{V}^{- 1}\left( \textbf{Y}- \boldsymbol{\mu}\right) \right\}\]</span></p>
<p>This is denoted with the shorthand <span class="math inline">\(\textbf{Y}\sim G_{n}(\boldsymbol{\mu},\textbf{V})\)</span> or <span class="math inline">\(\textbf{Y}\sim N_{n}(\boldsymbol{\mu},\textbf{V}\)\)</span>. If the dimension of the distribution is clear from context, the subscript <span class="math inline">\(n\)</span> can be omitted. A special case is the <strong>standard</strong> multivariate Gaussian distribution with mean <span class="math inline">\(\textbf{0}\)</span> and variance matrix <span class="math inline">\(\textbf{I}\)</span> :</p>
<p><span class="math display">\[f\left( \textbf{Y}\right)=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\textbf{Y}^{\prime}\textbf{Y}\right\} = \frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\sum_{i}^{n}y_{i}^{2} \right\}\]</span></p>
<p>But this is just the product of the <span class="math inline">\(n\)</span> univariate densities of <span class="math inline">\(N(0,1)\)</span> random variables:</p>
<p><span class="math display">\[f\left( \textbf{Y}\right) = f\left( y_{1} \right) \times \cdots \times f\left( y_{n} \right)\]</span></p>
<p>where</p>
<p><span class="math display">\[f\left( y_{i} \right) = \frac{1}{(2\pi)^{1/2}}\exp\left\{ - \frac{1}{2}y^{2} \right\}\]</span></p>
<p>If the variance matrix is diagonal—that is, the <span class="math inline">\(Y_{i}\)</span> are uncorrelated—the multivariate normal distribution is the product of the univariate distributions. The random variables are independent.</p>
</section>
<section id="properties" class="level3">
<h3 class="anchored" data-anchor-id="properties">Properties</h3>
<p>Gaussian distributions have amazing (magical) properties.</p>
<section id="linear-combinations-are-gaussian" class="level4">
<h4 class="anchored" data-anchor-id="linear-combinations-are-gaussian">Linear combinations are Gaussian</h4>
<p>For example, a linear combination of Gaussian random variables also follows a Gaussian distribution. Formally, this can be expressed as follows: if <span class="math inline">\(\textbf{Y}\sim G_{n}\left( \boldsymbol{\mu},\textbf{V}\right)\)</span> and <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are a matrix and vector of constants (not random variables), respectively, then <span class="math inline">\(\mathbf{AY} + \textbf{B}\)</span> follows a <span class="math inline">\(G(\textbf{A}\boldsymbol{\mu},\mathbf{AVA})\)</span> distribution.</p>
<p>A special case of this result is that if <span class="math inline">\(\textbf{Y}\sim G_{n}\left( \boldsymbol{\mu},\textbf{V}\right)\)</span>, <span class="math inline">\(\textbf{Y}- \boldsymbol{\mu}\)</span> has a <span class="math inline">\(G\left( 0,\textbf{V}\right)\)</span> distribution.</p>
<p>Because a linear function of a Gaussian random variable is Gaussian distributed, you can define all multivariate Gaussian distributions as linear transformations of the standard multivariate Gaussian distribution. If <span class="math inline">\(\textbf{Z}\sim G_{n}(\textbf{0},\textbf{I})\)</span>, and <span class="math inline">\(\textbf{V}= \textbf{C}^\prime\textbf{C}\)</span>, then <span class="math inline">\(\textbf{Y}= \mathbf{C}^\prime\textbf{Z}+ \boldsymbol{\mu}\)</span> has a <span class="math inline">\(G(\boldsymbol{\mu},\textbf{V})\)</span> distribution.</p>
</section>
<section id="zero-covariance-implies-independence" class="level4">
<h4 class="anchored" data-anchor-id="zero-covariance-implies-independence">Zero covariance implies independence</h4>
<p>Another unusual property of Gaussian random variables is that if they are uncorrelated, they are also stochastically independent. We derived this above for the special case of <span class="math inline">\(\textbf{Y}\sim G(\textbf{0},\sigma^{2}\textbf{I})\)</span>.</p>
<p>You cannot in general conclude that random variables are independent based on their lack of correlation. For Gaussian random variables you can. This result can be extended to Gaussian random vectors. Suppose <span class="math inline">\(\textbf{Y}_{(n \times 1)} \sim G(\boldsymbol{\mu},\ \textbf{V})\)</span> is partitioned into two sub-vectors of size <span class="math inline">\(s\)</span> and <span class="math inline">\(k\)</span>, where <span class="math inline">\(n = s + k\)</span>. Then we can similarly partition the mean vector and variance matrix:</p>
<p><span class="math display">\[\textbf{Y}_{(n \times 1)} = \begin{bmatrix}
\textbf{Y}_{1(s \times 1)} \\
\textbf{Y}_{2(k \times 1)}
\end{bmatrix},\ \ \boldsymbol{\mu}= \begin{bmatrix}
\boldsymbol{\mu}_{1} \\
\boldsymbol{\mu}_{2}
\end{bmatrix},\ \ \ \ \ \ \textbf{V}= \begin{bmatrix}
\textbf{V}_{11} &amp; \textbf{V}_{12} \\
\textbf{V}_{21} &amp; \textbf{V}_{22}
\end{bmatrix}\]</span></p>
<p>If <span class="math inline">\(\textbf{V}_{12} = \textbf{0}\)</span>, then <span class="math inline">\(\textbf{Y}_{1}\)</span> and <span class="math inline">\(\textbf{Y}_{2}\)</span> are independent. Also, each partition is Gaussian distributed, for example, <span class="math inline">\(\textbf{Y}_{1} \sim G(\boldsymbol{\mu}_{1},\ \textbf{V}_{11})\)</span>. We call the distribution of <span class="math inline">\(\textbf{Y}_{1}\)</span> the marginal distribution.</p>
<p>It follows immediately that each element of <span class="math inline">\(\textbf{Y}\)</span> follows a (univariate) Gaussian distribution, <span class="math inline">\(Y_{i} \sim G(\mu_{i},V_{ii})\)</span>—all marginal univariate distributions are Gaussian.</p>
</section>
<section id="conditionals-are-gaussian" class="level4">
<h4 class="anchored" data-anchor-id="conditionals-are-gaussian">Conditionals are Gaussian}</h4>
<p>The conditional distribution of <span class="math inline">\(\textbf{Y}_{1}\)</span> given <span class="math inline">\(\textbf{Y}_{2}\)</span> is also a Gaussian distribution, specifically:</p>
<p><span class="math display">\[\textbf{Y}_{1}|\textbf{Y}_{2} \sim G\left( \boldsymbol{\mu}_{1}\mathbf{+}\textbf{V}_{12}\textbf{V}_{22}^{- 1}\left( \textbf{Y}_{2} - \boldsymbol{\mu}_{2} \right),\ \textbf{V}_{11} - \textbf{V}_{12}\textbf{V}_{22}^{- 1}\textbf{V}_{12}^\prime \right)\]</span></p>
<p>This result plays an important role when predicting Gaussian random variables, for example in mixed models.</p>
<p>Notice that the variance matrix of the conditional distribution does not depend on the particular value <span class="math inline">\(\textbf{Y}_{2} = \textbf{Y}_{2}\)</span> on which the distribution is conditioned. However, the mean of the conditional distribution does depend on <span class="math inline">\(\textbf{Y}_{2}\)</span> unless <span class="math inline">\(\textbf{V}_{12} = \textbf{0}\)</span>, a condition established earlier for independence of <span class="math inline">\(\textbf{Y}_{1}\)</span> and <span class="math inline">\(\textbf{Y}_{2}\)</span>.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./biasvariance.html" class="pagination-link" aria-label="Bias Variance Tradeoff">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./estimation.html" class="pagination-link" aria-label="Parameter Estimation">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>