<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 3&nbsp; Linear Algebra Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./estimation.html" rel="next">
<link href="./biasvariance.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./statmodels.html">Part I. Foundation</a></li><li class="breadcrumb-item"><a href="./linalg.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification with Random Inputs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Decision Trees</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesinR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesInPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Trees in Python</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Part VI. Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Part VII. Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Part VIII. Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Part IX. Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#basics" id="toc-basics" class="nav-link active" data-scroll-target="#basics"><span class="header-section-number">3.1</span> Basics</a></li>
  <li><a href="#special-matrices" id="toc-special-matrices" class="nav-link" data-scroll-target="#special-matrices"><span class="header-section-number">3.2</span> Special Matrices</a>
  <ul>
  <li><a href="#special-values" id="toc-special-values" class="nav-link" data-scroll-target="#special-values">Special Values</a></li>
  <li><a href="#special-patterns" id="toc-special-patterns" class="nav-link" data-scroll-target="#special-patterns">Special Patterns</a></li>
  </ul></li>
  <li><a href="#basic-operations-on-matrices-and-vectors" id="toc-basic-operations-on-matrices-and-vectors" class="nav-link" data-scroll-target="#basic-operations-on-matrices-and-vectors"><span class="header-section-number">3.3</span> Basic Operations on Matrices and Vectors</a>
  <ul>
  <li><a href="#transpose" id="toc-transpose" class="nav-link" data-scroll-target="#transpose">Transpose</a></li>
  <li><a href="#addition-and-subtraction" id="toc-addition-and-subtraction" class="nav-link" data-scroll-target="#addition-and-subtraction">Addition and Subtraction</a></li>
  <li><a href="#multiplication" id="toc-multiplication" class="nav-link" data-scroll-target="#multiplication">Multiplication</a></li>
  <li><a href="#inversion-and-rank" id="toc-inversion-and-rank" class="nav-link" data-scroll-target="#inversion-and-rank">Inversion and Rank</a>
  <ul class="collapse">
  <li><a href="#full-rank-matrices" id="toc-full-rank-matrices" class="nav-link" data-scroll-target="#full-rank-matrices">Full rank matrices</a></li>
  <li><a href="#inverse-of-partitioned-matrices" id="toc-inverse-of-partitioned-matrices" class="nav-link" data-scroll-target="#inverse-of-partitioned-matrices">Inverse of partitioned matrices</a></li>
  <li><a href="#rank-deficient-matrices" id="toc-rank-deficient-matrices" class="nav-link" data-scroll-target="#rank-deficient-matrices">Rank-deficient matrices</a></li>
  </ul></li>
  <li><a href="#determinant" id="toc-determinant" class="nav-link" data-scroll-target="#determinant">Determinant</a></li>
  <li><a href="#trace" id="toc-trace" class="nav-link" data-scroll-target="#trace">Trace</a></li>
  </ul></li>
  <li><a href="#random-vectors" id="toc-random-vectors" class="nav-link" data-scroll-target="#random-vectors"><span class="header-section-number">3.4</span> Random Vectors</a>
  <ul>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value">Expected Value</a></li>
  <li><a href="#covariance-matrix" id="toc-covariance-matrix" class="nav-link" data-scroll-target="#covariance-matrix">Covariance Matrix</a></li>
  <li><a href="#variance-covariance-matrix" id="toc-variance-covariance-matrix" class="nav-link" data-scroll-target="#variance-covariance-matrix">Variance-covariance Matrix</a></li>
  <li><a href="#correlation-matrix" id="toc-correlation-matrix" class="nav-link" data-scroll-target="#correlation-matrix">Correlation Matrix</a></li>
  </ul></li>
  <li><a href="#cross-product-matrix" id="toc-cross-product-matrix" class="nav-link" data-scroll-target="#cross-product-matrix"><span class="header-section-number">3.5</span> Cross-product Matrix</a></li>
  <li><a href="#sec-idempotent" id="toc-sec-idempotent" class="nav-link" data-scroll-target="#sec-idempotent"><span class="header-section-number">3.6</span> Idempotent Matrices</a>
  <ul>
  <li><a href="#projections" id="toc-projections" class="nav-link" data-scroll-target="#projections">Projections</a></li>
  <li><a href="#sec-hat-matrix" id="toc-sec-hat-matrix" class="nav-link" data-scroll-target="#sec-hat-matrix">The “Hat” Matrix</a></li>
  </ul></li>
  <li><a href="#matrix-factorization" id="toc-matrix-factorization" class="nav-link" data-scroll-target="#matrix-factorization"><span class="header-section-number">3.7</span> Matrix Factorization</a>
  <ul>
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link" data-scroll-target="#linear-transformations">Linear Transformations</a></li>
  <li><a href="#eigenvalue-decomposition" id="toc-eigenvalue-decomposition" class="nav-link" data-scroll-target="#eigenvalue-decomposition">Eigenvalue Decomposition</a>
  <ul class="collapse">
  <li><a href="#constructing-an-inverse" id="toc-constructing-an-inverse" class="nav-link" data-scroll-target="#constructing-an-inverse">Constructing an inverse</a></li>
  </ul></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a>
  <ul class="collapse">
  <li><a href="#constructing-an-inverse-1" id="toc-constructing-an-inverse-1" class="nav-link" data-scroll-target="#constructing-an-inverse-1">Constructing an inverse</a></li>
  <li><a href="#constructing-a-generalized-inverse" id="toc-constructing-a-generalized-inverse" class="nav-link" data-scroll-target="#constructing-a-generalized-inverse">Constructing a generalized inverse</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-matrix-differentiation" id="toc-sec-matrix-differentiation" class="nav-link" data-scroll-target="#sec-matrix-differentiation"><span class="header-section-number">3.8</span> Matrix Differentiation</a></li>
  <li><a href="#sec-multi-gaussian" id="toc-sec-multi-gaussian" class="nav-link" data-scroll-target="#sec-multi-gaussian"><span class="header-section-number">3.9</span> Multivariate Gaussian Distribution</a>
  <ul>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a>
  <ul class="collapse">
  <li><a href="#linear-combinations-are-gaussian" id="toc-linear-combinations-are-gaussian" class="nav-link" data-scroll-target="#linear-combinations-are-gaussian">Linear combinations are Gaussian</a></li>
  <li><a href="#zero-covariance-implies-independence" id="toc-zero-covariance-implies-independence" class="nav-link" data-scroll-target="#zero-covariance-implies-independence">Zero covariance implies independence</a></li>
  <li><a href="#conditionals-are-gaussian" id="toc-conditionals-are-gaussian" class="nav-link" data-scroll-target="#conditionals-are-gaussian">Conditionals are Gaussian}</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./statmodels.html">Part I. Foundation</a></li><li class="breadcrumb-item"><a href="./linalg.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-linear-algebra" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="basics" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="basics"><span class="header-section-number">3.1</span> Basics</h2>
<p>Command of linear algebra is essential in data science, models and estimators are often expressed in terms of tensors, matrices, and vectors. Using scalar-based arithmetic becomes tedious very quickly as models become more complex. For example, the simple linear regression model and a straight line through the intercept model can be written as</p>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}\]</span></p>
<p><span class="math display">\[Y_{i} = \beta_{1}x_{i} + \epsilon_{i}\]</span></p>
<p>Using scalar algebra, the estimates of the slope are quite different:</p>
<p><span class="math display">\[{\widehat{\beta}}_{1} = \frac{\left( \sum_{i = 1}^{n}{\left( y_{i} - \overline{y} \right)\left( x_{i} - \overline{x} \right)} \right)}{\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}}\]</span></p>
<p><span class="math display">\[{\widehat{\beta}}_{1} = \frac{\left( \sum_{i = 1}^{n}{y_{i}x_{i}} \right)}{\sum_{i = 1}^{n}x_{i}^{2}}\]</span></p>
<p>The formulas get messier as we add another input variable to the model. Using matrix—vector notation, the estimator of all the regression coefficients takes the same form, regardless of the size of the model:</p>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}} = \left( \textbf{X}^{\prime}\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}\]</span></p>
<p>A scalar is a single real number, a vector is an array of scalars arranged in a single column (a column vector) or a row (a row vector). A matrix is a two-dimensional array of scalars, a tensor is a multi-dimensional array.</p>
<p>The <strong>order</strong> of a vector or matrix is specified as (rows x columns) and is sometimes used as a subscript for clarity. For example,<span class="math inline">\(\textbf{A}_{(3 \times 5)}\)</span> denotes a matrix with 3 rows and 5 columns. It can be viewed as a concatenation} of five <span class="math inline">\((3 \times 1)\)</span> column vectors:</p>
<p><span class="math display">\[\textbf{A}_{(3 \times 5)}=\begin{bmatrix}
\begin{matrix}
1 \\
1 \\
1
\end{matrix} &amp; \begin{matrix}
9.0 \\
3.2 \\
4.1
\end{matrix} &amp; \begin{matrix}
\begin{matrix}
6.2 \\
1.4 \\
- 0.6
\end{matrix} &amp; \begin{matrix}
1 \\
0 \\
0
\end{matrix} &amp; \begin{matrix}
0 \\
1 \\
0
\end{matrix}
\end{matrix}
\end{bmatrix}\]</span></p>
<p><span class="math inline">\(\textbf{a}_{1} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}\ \ \ \ \ \textbf{a}_{2} = \begin{bmatrix} 9.0 \\ 3.2 \\ 4.1 \end{bmatrix}\ \ \ \ \textbf{a}_{3} = \begin{bmatrix} 6.2 \\ 1.4 \\ - 0.6 \end{bmatrix}\ \ \ \ \ \textbf{a}_{4} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}\ \ \ \ \ \textbf{a}_{5} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}\)</span>.</p>
<p>A matrix with as many rows as columns is called a <strong>square</strong> matrix.</p>
<p>Bold symbols are common, lowercase for vectors and uppercase for matrices, but there are some exceptions. When dealing with vectors of random variables, bold uppercase notation is used for a vector of random variables and bold lowercase notation is used for a vector of the realized values. For example, if <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> is a random sample of size <span class="math inline">\(n\)</span>, the vector of random variables is</p>
<p><span class="math display">\[\textbf{Y}_{(n \times 1)} = \begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}\]</span></p>
<p>and the vector of realized values is</p>
<p><span class="math display">\[\textbf{y}_{(n \times 1)} = \begin{bmatrix}
y_{1} \\
\vdots \\
y_{n}
\end{bmatrix}\]</span></p>
<p>The difference is significant because <span class="math inline">\(\textbf{Y}\)</span> is a random variable and <span class="math inline">\(\textbf{y}\)</span> is a vector of constants. <span class="math inline">\(\textbf{Y}\)</span> has a multi-variate distribution with mean and variance, <span class="math inline">\(\textbf{y}\)</span> is just a vector of numbers.</p>
<p>We follow the convention that all vectors are column vectors, so that <span class="math inline">\(\textbf{y}_{(n)}\)</span> serves as a shorthand for <span class="math inline">\(\textbf{y}_{(n \times 1)}\)</span>.</p>
<p>The typical element of a matrix is written as a scalar with subscripts that refer to rows and columns. For example, the statement</p>
<p><span class="math display">\[\textbf{A}= \left\lbrack a_{ij} \right\rbrack\]</span></p>
<p>says that matrix <span class="math inline">\(\textbf{A}\)</span> consists of the scalars <span class="math inline">\(a_{ij}\)</span>; for example, <span class="math inline">\(a_{23}\)</span> is the scalar in row 2, column 3 of the matrix.</p>
</section>
<section id="special-matrices" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="special-matrices"><span class="header-section-number">3.2</span> Special Matrices</h2>
<section id="special-values" class="level3">
<h3 class="anchored" data-anchor-id="special-values">Special Values</h3>
<p>A few special matrices, common in statistics and machine learning are</p>
<ul>
<li><p><span class="math inline">\(\textbf{1}_{n} = \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}\)</span>, the <strong>unit</strong> vector of size <span class="math inline">\(n\)</span>; all its elements are 1.</p></li>
<li><p><span class="math inline">\(\textbf{0}_{n} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}\)</span>, the <strong>zero</strong> vector of size <span class="math inline">\(n\)</span>; all its elements are 0.</p></li>
<li><p><span class="math inline">\(\textbf{0}_{(n \times k)} = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ \vdots &amp; \cdots &amp; \vdots \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>, the <strong>zero</strong> matrix of order <span class="math inline">\((n \times k)\)</span>. All its elements are 0.</p></li>
<li><p><span class="math inline">\(\textbf{J}_{(n \times k)} = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ \vdots &amp; \cdots &amp; \vdots \\ 1 &amp; 1 &amp; 1 \end{bmatrix}\)</span>, the <strong>unit</strong> matrix of size <span class="math inline">\((n \times k)\)</span>. All its elements are 1.</p></li>
<li><p><span class="math inline">\(\textbf{I}_{n} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>, the <strong>identity</strong> matrix of size <span class="math inline">\((n \times n)\)</span> with 1s on the diagonal and 0s elsewhere.</p></li>
</ul>
<p>If the order of these matrices is obvious from the context, the subscripts can be omitted.</p>
</section>
<section id="special-patterns" class="level3">
<h3 class="anchored" data-anchor-id="special-patterns">Special Patterns</h3>
<p>Patterned matrices play an important role in statistical operations. Recognizing a particular structure of a matrix we can reduce the amount of work required to find inverses, eigenvalues, determinants, etc. For example, if <span class="math inline">\(\textbf{D}\)</span> is a diagonal matrix with <span class="math inline">\(d_{ii} \ne 0\)</span>, then an inverse <span class="math inline">\(\textbf{D}^{-1}\)</span> can be found by simply replacing <span class="math inline">\(d_{ii}\)</span> with <span class="math inline">\(1/d_{ii}\)</span>. Finding the inverse of a matrix in general is a much more elaborate task.</p>
<p>Performing operations on patterned matrices can lead to patterns in the result of the operations. An example is the multiplication of partitioned matrices.</p>
<p>A matrix with the same number of rows and columns is a <strong>square</strong> matrix.</p>
<p>Square matrices for which the typical element <span class="math inline">\(a_{ij} = a_{ji}\)</span> are <strong>symmetric</strong> matrices. For example, the matrix <span class="math display">\[
\begin{bmatrix}
54 &amp; 37 &amp; 5 \\
37 &amp; 27 &amp; 4 \\
5 &amp;   4 &amp; 21
\end{bmatrix}
\]</span> is a symmetric matrix, The values in the off-diagonal cells are mirrored across the diagonal. Symmetric matrices play an important role in statistics; cross-product, variance-covariance, “hat”, and correlation matrices are examples.</p>
<p>A <strong>diagonal</strong> matrix is a symmetric matrix where all off-diagonal entries are zero. The identity matrix <span class="math inline">\(\textbf{I}_{n}\)</span> is diagonal, as is the following <span class="math display">\[
\begin{bmatrix}
54 &amp; 0 &amp; 0 \\
0 &amp; 27 &amp; 0 \\
0 &amp; 0 &amp; 21
\end{bmatrix}
\]</span></p>
<p>A <strong>triangular</strong> matrix is a square matrix with zeros above (lower triangular) or below (upper triangular) the diagonal. For example, <span class="math display">\[
\textbf{A}_u = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
0      &amp; a_{22} &amp; a_{23} \\
0      &amp;   0    &amp; a_{33}
\end{bmatrix}
\]</span> is upper-triangular and <span class="math display">\[
\textbf{A}_l = \begin{bmatrix}
a_{11} &amp;   0    &amp;    0 \\
a_{21} &amp; a_{22} &amp;    0 \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix}
\]</span> is lower-triangular. Lower and upper triangular matrices play an important role in solving linear systems by LU decomposition. The procedure derives its name from factorizing a matrix into a lower (L) and upper (U) triangular form.</p>
<hr>
<p>A matrix is said to be <strong>partitioned</strong>, if it can be arranged in separate blocks of sub-matrices. Most important in statistics and data science are partitions of square matrices: <span class="math display">\[
\textbf{B}_{(n \times n)} = \begin{bmatrix}
\textbf{B}_{11} &amp; \textbf{B}_{12} \\
\textbf{B}_{21} &amp; \textbf{B}_{22}
\end{bmatrix}
\]</span></p>
<p>The sizes of the <span class="math inline">\(B_{ij}\)</span> sub-matrices are <span class="math inline">\((n_i \times n_j)\)</span>. An important case of partitioning occurs in the variance-covariance matrix of two random vectors (more on this below). Briefly, if <span class="math inline">\(\textbf{Y}= [\textbf{Y}_1, \textbf{Y}_2]^\prime\)</span>, then the variance matrix of <span class="math inline">\(\textbf{Y}\)</span> is partitioned as follows: <span class="math display">\[
\text{Var}[\textbf{Y}] = \begin{bmatrix}
\text{Var}[\textbf{Y}_1] &amp; \text{Cov}[\textbf{Y}_1,\textbf{Y}_2] \\
\text{Cov}[\textbf{Y}_2,\textbf{Y}_1] &amp; \text{Var}[\textbf{Y}_2]
\end{bmatrix}
\]</span></p>
<p>A partitioned matrix is <strong>block-diagonal</strong>, if the off-diagonal sub-matrices are zero matrices. This occurs with the variance matrix of <span class="math inline">\(\textbf{Y}= [\textbf{Y}_1, \textbf{Y}_2]^\prime\)</span>, if <span class="math inline">\(\textbf{Y}_1\)</span> and <span class="math inline">\(\textbf{Y}_2\)</span> are uncorrelated: <span class="math display">\[
\text{Var}[\textbf{Y}] = \begin{bmatrix}
\text{Var}[\textbf{Y}_1] &amp; \textbf{0}\\
\textbf{0}&amp; \text{Var}[\textbf{Y}_2]
\end{bmatrix}
\]</span></p>
</section>
</section>
<section id="basic-operations-on-matrices-and-vectors" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="basic-operations-on-matrices-and-vectors"><span class="header-section-number">3.3</span> Basic Operations on Matrices and Vectors</h2>
<p>The basic operations on matrices and vectors are addition, subtraction, multiplication, transposition, and inversion. These are standard operations in manipulating matrix and vector equations. Decompositions such as Cholesky roots, eigenvalue and singular value decompositions are more advanced operations that are important in solving estimation problems in statistics.</p>
<section id="transpose" class="level3">
<h3 class="anchored" data-anchor-id="transpose">Transpose</h3>
<p>The <strong>transpose</strong> of a matrix is obtained by exchanging rows and columns. If <span class="math inline">\(a_{ij}\)</span> is the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> of matrix <span class="math inline">\(\textbf{A}\)</span>, the transpose of <span class="math inline">\(\textbf{A}\)</span>, denoted <span class="math inline">\(\textbf{A}^\prime\)</span>, has typical element <span class="math inline">\(a_{ji}\)</span>. In case of the <span class="math inline">\((3\  \times 5)\)</span> matrix shown previously, its transpose is</p>
<p><span class="math display">\[\textbf{A}^\prime_{(5 \times 3)} = \begin{bmatrix}
\begin{matrix}
1 \\
9.0 \\
\begin{matrix}
6.2 \\
1 \\
0
\end{matrix}
\end{matrix} &amp; \begin{matrix}
1 \\
3.2 \\
\begin{matrix}
1.4 \\
0 \\
1
\end{matrix}
\end{matrix} &amp; \begin{matrix}
1 \\
4.1 \\
\begin{matrix}
- 0.6 \\
0 \\
0
\end{matrix}
\end{matrix}
\end{bmatrix}\]</span></p>
<p>The transpose of a column vector is a row vector:</p>
<p><span class="math display">\[\textbf{a}^{\prime} = \begin{bmatrix}
a_{1} \\
\vdots \\
a_{n}
\end{bmatrix}^\prime = \left\lbrack a_{1},\cdots,a_{n} \right\rbrack\]</span></p>
<p>Transposing a transpose produces the original matrix, <span class="math inline">\(\left( \textbf{A}^{\prime} \right)^{\prime}\ = \textbf{A}\)</span>.</p>
<p>A matrix is <strong>symmetric</strong> if it is equal to its transpose, <span class="math inline">\(\textbf{A}^\prime = \textbf{A}\)</span>. Symmetric matrices are square matrices (same numbers of rows and columns). The matrices <span class="math inline">\(\textbf{A}^\prime\textbf{A}\)</span> and <span class="math inline">\(\textbf{A}\textbf{A}^\prime\)</span> are always symmetric. A symmetric matrix whose off-diagonal elements are zero is called a <strong>diagonal</strong> matrix.</p>
</section>
<section id="addition-and-subtraction" class="level3">
<h3 class="anchored" data-anchor-id="addition-and-subtraction">Addition and Subtraction</h3>
<p>The <strong>sum</strong> (<strong>difference</strong>) of two matrices is the matrix of the elementwise sums (differences) of their elements. These operations require that the matrices being summed or subtracted have the same order:</p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} + \textbf{B}_{(n \times k)} = \left\lbrack a_{ij} + b_{ij} \right\rbrack\]</span></p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} - \textbf{B}_{(n \times k)} = \left\lbrack a_{ij} - b_{ij} \right\rbrack\]</span></p>
<p>Suppose, for example, that <span class="math inline">\(\textbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}\)</span> and <span class="math inline">\(\textbf{B}=\begin{bmatrix} - 1 &amp; - 2 &amp; - 3 \\ - 4 &amp; - 5 &amp; - 6 \end{bmatrix}\)</span>. Then,</p>
<p><span class="math display">\[\textbf{A}+ \textbf{B}= \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}\]</span></p>
<p><span class="math display">\[\textbf{A}- \textbf{B}= \begin{bmatrix}
2 &amp; 4 &amp; 6 \\
8 &amp; 10 &amp; 12
\end{bmatrix}\]</span></p>
<p>Since addition (subtraction) are elementwise operations, they can be combined with transposition:</p>
<p><span class="math display">\[\left( \textbf{A}+ \textbf{B}\right)^\prime = \textbf{A}^{\prime} + \textbf{B}^{\prime}\]</span></p>
</section>
<section id="multiplication" class="level3">
<h3 class="anchored" data-anchor-id="multiplication">Multiplication</h3>
<p>Two matrices conform for addition (subtraction) if they have the same order, that is, the same number of rows and columns. Multiplication of matrices requires a different type of conformity; two matrices <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> can be multiplied as <span class="math inline">\(\text{AB}\)</span> (or <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span>), if the number of columns in <span class="math inline">\(\textbf{A}\)</span> equals the number of columns in <span class="math inline">\(\textbf{B}\)</span>. We say that in the product <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span>, <span class="math inline">\(\textbf{A}\)</span> is post-multiplied by <span class="math inline">\(\textbf{B}\)</span> or that <span class="math inline">\(\textbf{A}\)</span> is multiplied into <span class="math inline">\(\textbf{B}\)</span>. The result of multiplying a <span class="math inline">\((n \times k)\)</span> matrix into a <span class="math inline">\((k \times p)\)</span> matrix is a <span class="math inline">\((n \times p)\)</span> matrix.</p>
<p>Before examining the typical elements in the result of multiplication, let’s look at a special case, the <strong>inner product</strong> of two <span class="math inline">\((k \times 1)\)</span> vectors <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span>, also called the <strong>dot product</strong> or the <strong>scalar product</strong>, is the result of multiplying the transpose of <span class="math inline">\(\textbf{A}\)</span> into <span class="math inline">\(\textbf{B}\)</span>, a scalar value</p>
<p><span class="math display">\[\textbf{A}^\prime\textbf{B}= \left\lbrack a_{1}, \cdots,a_{k} \right\rbrack\begin{bmatrix}
b_{1} \\
\vdots \\
b_{k}
\end{bmatrix} = a_{1}b_{1} + \cdots a_{k}b_{k} = \sum_{i = 1}^{k}{a_{i}b_{i}}\]</span></p>
<p>The square root of the dot product of a vector with itself is the Euclidean <span class="math inline">\({(L}_{2})\)</span> <strong>norm</strong> of the vector,</p>
<p><span class="math display">\[\left| \left| \textbf{a}\right| \right| = \sqrt{\textbf{a}^\prime\textbf{a}} = \sum_{i = 1}^{k}a_{i}^{2}\]</span></p>
<p>The <span class="math inline">\(L_{2}\)</span> norm plays an important role as a loss function in statistical models. The vector for which the norm is calculated is then often a vector of model errors.</p>
<p>Now let’s return to the problem of multiplying the <span class="math inline">\((n \times k)\)</span> matrix <span class="math inline">\(\textbf{A}\)</span> into the <span class="math inline">\((k \times p)\)</span> matrix <span class="math inline">\(\textbf{B}\)</span> and introduce one more piece of notation: the <span class="math inline">\(i\)</span><sup>th</sup> row of <span class="math inline">\(\textbf{A}\)</span> is denoted <span class="math inline">\(\mathbf{\alpha}_{i}\)</span> and the <span class="math inline">\(j\)</span><sup>th</sup> column of <span class="math inline">\(\textbf{B}\)</span> is denoted <span class="math inline">\(\textbf{B}_{j}\)</span>. Now we can finally write the product <span class="math inline">\(\textbf{A}\text{×}\textbf{B}\)</span> as a matrix whose typical element is the inner product of <span class="math inline">\(\mathbf{\alpha}_{i}\)</span> and <span class="math inline">\(\textbf{B}_{j}\)</span>:</p>
<p><span class="math display">\[\textbf{A}_{(n \times k)} \times \textbf{B}_{(k \times p)} = \left\lbrack \boldsymbol{\alpha}_{i}^\prime\ \textbf{b}_{j} \right\rbrack_{(n \times p)}\ \]</span></p>
<p>As an example, let <span class="math inline">\(\textbf{A}= \begin{bmatrix} 1 &amp; 2 &amp; 0 \\ 3 &amp; 1 &amp; - 3 \\ 4 &amp; 1 &amp; 2 \end{bmatrix}\)</span> and <span class="math inline">\(\textbf{B}= \begin{bmatrix} 1 &amp; 0 \\ 2 &amp; 3 \\ 2 &amp; 1 \end{bmatrix}\)</span>. The product <span class="math inline">\(\textbf{A}\times\textbf{B}\)</span> is a <span class="math inline">\((3 \times 2)\)</span> matrix with elements</p>
<p><span class="math display">\[\textbf{A}\times\textbf{B}= \begin{bmatrix}
1 \times 1 + 2 \times 2 + 0 \times 2 &amp; 1 \times 0 + 2 \times 3 + 0 \times 1 \\
3 \times 1 + 1 \times 2 - 3 \times 2 &amp; 3 \times 0 + 1 \times 3 - 3 \times 1 \\
4 \times 1 + 1 \times 2 + 2 \times 2 &amp; 4 \times 0 + 1 \times 3 + 2 \times 1
\end{bmatrix} = \begin{bmatrix}
5 &amp; 6 \\
- 1 &amp; 0 \\
10 &amp; 5
\end{bmatrix}\]</span></p>
<p>Here are a few helpful rules for matrix multiplication:</p>
<ol type="1">
<li><p><span class="math inline">\(c\textbf{A}= \left\lbrack ca_{ij} \right\rbrack\)</span></p></li>
<li><p><span class="math inline">\(c\left( \textbf{A}+ \textbf{B}\right) = c\textbf{A}+ c\textbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\textbf{C}\left( \textbf{A}+ \textbf{B}\right) = \textbf{C}\textbf{A}+ \textbf{C}\textbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)\textbf{C}= \textbf{A}(\textbf{B}\textbf{C})\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}+ \textbf{B}\right)\left( \textbf{C}+ \mathbf{D} \right) = \textbf{A}\textbf{C}+ \textbf{A}\mathbf{D} + \mathbf{BC} + \mathbf{BD}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)^\prime = \textbf{B}^\prime\textbf{A}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\left( c\textbf{A}\right)^\prime = c\textbf{A}^\prime\)</span></p></li>
</ol>
</section>
<section id="inversion-and-rank" class="level3">
<h3 class="anchored" data-anchor-id="inversion-and-rank">Inversion and Rank</h3>
<p>In scalar algebra, division and multiplication are inverse operations, dividing a non-zero scalar by itself yields the multiplicative identity: <span class="math inline">\(\frac{a}{a} = 1\)</span>. What is the equivalent of this operation for matrices? First, inversion of a matrix does not reduce it to a scalar, the multiplicative identity for matrices is the identity matrix <span class="math inline">\(\textbf{I}\)</span>, a diagonal matrix with 1s on the diagonal. Second, the inversion is only defined for square matrices. If <span class="math inline">\(\textbf{A}\)</span> is an <span class="math inline">\((n \times n)\)</span> matrix, the matrix <span class="math inline">\(\textbf{B}\)</span> for which</p>
<p><span class="math display">\[
\textbf{A}\textbf{B}= \textbf{I}
\]</span></p>
<p>is called the <strong>inverse</strong> of <span class="math inline">\(\textbf{A}\)</span>, denoted as <span class="math inline">\(\textbf{A}^{- 1}\)</span>.</p>
<section id="full-rank-matrices" class="level4">
<h4 class="anchored" data-anchor-id="full-rank-matrices">Full rank matrices</h4>
<p>Inverse matrices do not have to exist, even for square matrices. If <span class="math inline">\(\textbf{A}\)</span> has an inverse matrix, then <span class="math inline">\(\textbf{A}\)</span> is called a <strong>nonsingular</strong> matrix. In that case, <span class="math inline">\(\textbf{A}^{- 1}\textbf{A}= \textbf{A}\textbf{A}^{- 1} = \text{I}\)</span>.</p>
<p>For the inverse of a square matrix to exist, for the matrix to be non-singular, the matrix must be of full <strong>rank</strong>. The rank of a matrix, denoted <span class="math inline">\(r(\textbf{A})\)</span>, is the number of its linearly independent columns. What does that mean? Suppose we are dealing with a <span class="math inline">\((n \times k)\)</span> matrix <span class="math inline">\(\textbf{B}\)</span> and its column vectors are <span class="math inline">\(\textbf{B}_{1},\cdots,\textbf{B}_{k}\)</span>. A linear combination of the columns of <span class="math inline">\(\textbf{B}\)</span> is</p>
<p><span class="math display">\[
c_{1}\textbf{b}_{1} + c_{2}\textbf{b}_{2} + \cdots + c_{k}\textbf{b}_{k} = q
\]</span></p>
<p>If you can find a set of scalars <span class="math inline">\(c_{1},\cdots,c_{k}\)</span> such that <span class="math inline">\(q = 0\)</span>, then the columns of <span class="math inline">\(\textbf{B}\)</span> are linearly dependent. If the only set of scalars that yields <span class="math inline">\(q = 0\)</span> is</p>
<p><span class="math display">\[c_{1} = c_{2} = \cdots = c_{k} = 0\]</span></p>
<p>then the columns of <span class="math inline">\(\textbf{B}\)</span> are not linearly dependent and the rank of <span class="math inline">\(\textbf{B}\)</span> is <span class="math inline">\(k\)</span>.</p>
<p>Here are a few more useful results about the rank of a matrix:</p>
<ol type="1">
<li><p><span class="math inline">\(r\left( \textbf{A}\right) = r\left( \textbf{A}^\prime \right) = r\left( \textbf{A}^\prime\textbf{A}\right) = r\left( \textbf{A}\textbf{A}^{\prime} \right)\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}\textbf{B}\right) \leq \min\left\{ r\left( \textbf{A}\right),r\left( \textbf{B}\right) \right\}\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}+ \textbf{B}\right) \leq r\left( \textbf{A}\right) + r(\textbf{B})\)</span></p></li>
</ol>
<p>The first two results are particularly important in statistical models. In models with linear structures, it is common to collect the <span class="math inline">\(p\)</span> input variables in a linear model, including the intercept as a column of ones, into a matrix <span class="math inline">\(\textbf{X}_{(n\  \times p + 1)}\)</span>:</p>
<p><span class="math display">\[
\textbf{X}_{(n\  \times p + 1)} =
\begin{bmatrix}
1 &amp; x_{11} &amp; \begin{matrix}
\cdots &amp; x_{1p}
\end{matrix} \\
\vdots &amp; \vdots &amp;
\begin{matrix}
\ddots &amp; \vdots
\end{matrix} \\
1 &amp; x_{n1} &amp; \begin{matrix}
\cdots &amp; x_{np}
\end{matrix}
\end{bmatrix}
\]</span></p>
<p>Suppose we want to solve the linear system <span class="math inline">\(\textbf{Y}= \textbf{X}\textbf{c}\)</span> for <span class="math inline">\(\textbf{c}\)</span>. Start by pre-multiplying both sides of the equation with the transpose of <span class="math inline">\(\textbf{X}\)</span>:</p>
<p><span class="math display">\[
\textbf{X}^{\prime}\textbf{Y}= \textbf{X}^{\prime}\textbf{X}\textbf{c}
\]</span></p>
<p>If we had an inverse of <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span>, then we can now pre-multiply both sides with that inverse and isolate <span class="math inline">\(\text{c}\)</span>:</p>
<p><span class="math display">\[
\left( \textbf{X}^\prime\textbf{X}\right)^{\mathbf{- 1}}\textbf{X}^{\prime}\textbf{Y}= \left( \textbf{X}^\prime\textbf{X}\right)^{-1}\textbf{X}^\prime\textbf{X}\textbf{c}= \textbf{I}\textbf{c}= \textbf{c}
\]</span></p>
<p>We have a solution to the system, namely <span class="math inline">\({\textbf{c}=\left( \textbf{X}^\prime\textbf{X}\right)}^{- 1}\textbf{X}^{\prime}\textbf{Y}\)</span>, only if the inverse <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\)</span> exists. And that requires this <span class="math inline">\((p + 1) \times (p + 1)\)</span> matrix is of full rank <span class="math inline">\(r\left( \left( \textbf{X}^\prime\textbf{X}\right)^{- 1} \right) = p + 1\)</span>. This, in turn is equivalent to saying that <span class="math inline">\(\textbf{X}\)</span> has full rank <span class="math inline">\(p + 1\)</span> because of property (i).</p>
<p>Here are some useful results about inverse matrices:</p>
<ol type="1">
<li><p><span class="math inline">\(\left( \textbf{A}^{- 1} \right)^\prime = \left( \textbf{A}^\prime \right)^{- 1}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}^{- 1} \right)^{- 1} = \textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\left( \textbf{A}\textbf{B}\right)^{- 1} = \textbf{B}^{-1}\textbf{A}^{-1}\)</span></p></li>
<li><p><span class="math inline">\(r\left( \textbf{A}^{- 1} \right) = r(\textbf{A})\)</span></p></li>
</ol>
</section>
<section id="inverse-of-partitioned-matrices" class="level4">
<h4 class="anchored" data-anchor-id="inverse-of-partitioned-matrices">Inverse of partitioned matrices</h4>
<p>Suppose that <span class="math inline">\(\textbf{B}_{(n \times n)}\)</span> is a partitioned matrix <span class="math display">\[
\textbf{B}_{(n \times n)} = \begin{bmatrix}
\textbf{B}_{11} &amp; \textbf{B}_{12} \\
\textbf{B}_{21} &amp; \textbf{B}_{22}
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(B_{ij}\)</span> is of dimension <span class="math inline">\(n_i \times n_j\)</span>. If <span class="math inline">\(\textbf{A}= \textbf{B}^{-1}\)</span>, then <span class="math inline">\(\textbf{A}\)</span> is also a partitioned matrix with <span class="math display">\[
\begin{align*}
\textbf{A}_{11} &amp;= \left[\textbf{B}_{11} - \textbf{B}_{12}\textbf{B}_{22}^{-1}\textbf{B}_{21} \right]^{-1} \\
\textbf{A}_{12} &amp;= -\textbf{B}_{11}^{-1}\textbf{B}_{12}\left[\textbf{B}_{22} - \textbf{B}_{21}\textbf{B}_{11}^{-1}\textbf{B}_{12}\right]^{-1} = \textbf{B}_{11}^{-1}\textbf{B}_{12}\textbf{A}_{22} \\
\textbf{A}_{22} &amp;= \left[\textbf{B}_{22} - \textbf{B}_{21}\textbf{B}_{11}^{-1}\textbf{B}_{12}\right] \\
\textbf{A}_{21} &amp;= -\textbf{B}_{22}^{-1}\textbf{B}_{21}\left[\textbf{B}_{11} - \textbf{B}_{12}\textbf{B}_{22}^{-1}\textbf{B}_{21}\right]^{-1} -\textbf{B}_{22}^{-1}\textbf{B}_{21}\textbf{A}_{11}
\end{align*}
\]</span></p>
</section>
<section id="rank-deficient-matrices" class="level4">
<h4 class="anchored" data-anchor-id="rank-deficient-matrices">Rank-deficient matrices</h4>
<p>The linear system <span class="math inline">\(\textbf{Y}= \textbf{X}\textbf{c}\)</span> has a unique solution if <span class="math inline">\(\textbf{X}\)</span> is an <span class="math inline">\((n\times n)\)</span> nonsingular matrix. If <span class="math inline">\(\textbf{X}\)</span> is not square or if <span class="math inline">\(\textbf{X}\)</span> is singular, then we can still find a solution to the linear system, but it is not unique. If the matrix <span class="math inline">\(\textbf{X}\)</span> is of less than full rank (singular), it is called a <strong>rank-deficient</strong> matrix. We can then make progress by using a <strong>generalized inverse</strong> matrix instead of the (non-existing) inverse.</p>
<p>Also called <strong>pseudo inverses</strong> or <strong>g-inverses</strong>, the generalized inverse of matrix <span class="math inline">\(\textbf{X}\)</span> is denoted <span class="math inline">\(\textbf{X}^{-}\)</span> and satisfies</p>
<p><span class="math display">\[
\textbf{X}\textbf{X}^{-}\textbf{X}= \textbf{X}
\]</span> Suppose we can find such a generalized inverse <span class="math inline">\(\left( \textbf{X}^{\prime}\textbf{X}\right)^{-}\)</span> of <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span>. What if we use that in the solution of the linear system,</p>
<p><span class="math display">\[
\textbf{c}= {\left( \textbf{X}^{\prime}\textbf{X}\right)^{-}\textbf{X}}^{\prime}\textbf{Y}
\]</span></p>
<p>Unfortunately, whereas regular inverses are unique, there are (infinitely) many generalized inverses that satisfy the condition <span class="math inline">\((\textbf{X}^\prime\textbf{X})\left( \textbf{X}^\prime\textbf{X}\right)^{-}\textbf{X}^\prime\textbf{X}= \textbf{X}^\prime\textbf{X}\)</span>. So, there will be infinitely many possible solutions to the linear system. Fortunately, it turns out that generalized inverses have some nice properties, for example, <span class="math inline">\(\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{-}\textbf{X}\)</span> is invariant to the choice of the generalized inverse. Even if the solution <span class="math inline">\(\textbf{c}\)</span> is not unique, <span class="math inline">\(\textbf{X}\textbf{c}\)</span> is unique. This result is important in linear models with rank-deficient design matrices, a condition that is common when the model contains classification variables. While the parameter estimates in such a model are not unique—because we need to use a generalized inverse to derive the estimates—the predicted values are the same, no matter which generalized inverse we selected.</p>
<p>Here are some properties of generalized inverses, these resemble the properties of the inverse:</p>
<ol type="1">
<li><p>If <span class="math inline">\(c\)</span> is nonzero, then <span class="math inline">\((c\textbf{A})^- = (1/c)\textbf{A}^-\)</span></p></li>
<li><p><span class="math inline">\(\textbf{A}\textbf{A}^bA = \textbf{A}\text{ and } \textbf{A}^-\textbf{A}\textbf{A}^- = \textbf{A}^-\)</span></p></li>
<li><p><span class="math inline">\(\left(\textbf{A}^\prime\right)^- = \left(\textbf{A}^-\right)^\prime\)</span></p></li>
<li><p><span class="math inline">\(\left(\textbf{A}^- \right)^- = \textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(r(\textbf{A}^-) = r(\textbf{A}) = r(\textbf{A}\textbf{A}^-) = r(\textbf{A}\textbf{A}^-\textbf{A})\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is symmetric, so is <span class="math inline">\(\textbf{A}^-\)</span></p></li>
</ol>
<p>A technique for constructing generalized inverses relies on the singular value decomposition and is covered below. For a diagonal matrix <span class="math inline">\(\textbf{D}\)</span>, constructing a generalized inverse is simple: replace the non-zero diagonal values <span class="math inline">\(d_{ii}\)</span> with <span class="math inline">\(1\d_{ii}\)</span>, and leave the zero diagonal values in place.</p>
</section>
</section>
<section id="determinant" class="level3">
<h3 class="anchored" data-anchor-id="determinant">Determinant</h3>
<p>The rank reduces a matrix to a single scalar value, the number of linearly independent columns of the matrix. Another value that reduces a square matrix to a single scalar is the <strong>determinant</strong>, written as <span class="math inline">\(det(\textbf{A})\)</span> or <span class="math inline">\(|\textbf{A}|\)</span>.</p>
<p>If a matrix <span class="math inline">\(\textbf{A}\)</span> has an inverse, then it is square and its determinant is nonzero.</p>
<p>The determinant has a geometric interpretation which is not that relevant for our discussion. What matters is that the determinant appears frequently in expressions of multivariate probability distributions and knowing how to manipulate the determinants.</p>
<ol type="1">
<li><p><span class="math inline">\(|\textbf{A}| = |\textbf{A}^\prime|\)</span></p></li>
<li><p><span class="math inline">\(|\textbf{I}| = 1\)</span></p></li>
<li><p><span class="math inline">\(\left| c\textbf{A}\right| = c^{n}\mathbf{|A}\mathbf{|}\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is singular, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p>If each element of a row (column) of <span class="math inline">\(\textbf{A}\)</span> is zero, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p>If two rows (column) of <span class="math inline">\(\textbf{A}\)</span> are identical, then <span class="math inline">\(\left| \textbf{A}\right| = 0\)</span></p></li>
<li><p><span class="math inline">\(\left| \textbf{A}\textbf{B}\right| = \left| \textbf{A}\right|\ \left| \textbf{B}\right|\)</span></p></li>
<li><p><span class="math inline">\(\left| \textbf{A}^{- 1} \right| = 1/|\textbf{A}|\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is a triangular matrix, then <span class="math inline">\(|\textbf{A}| = \prod_{i = 1}^{n}a_{ii}\)</span></p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is a diagonal matrix, then <span class="math inline">\(|\textbf{A}| = \prod_{i = 1}^{n}a_{ii}\)</span></p></li>
</ol>
</section>
<section id="trace" class="level3">
<h3 class="anchored" data-anchor-id="trace">Trace</h3>
<p>The trace operator, <span class="math inline">\(tr(\textbf{A})\)</span>, applies only to square matrices. The trace of an <span class="math inline">\(\textbf{A}_{(n \times n)}\)</span> matrix is the sum of its diagonal elements:</p>
<p><span class="math display">\[tr\left( \textbf{A}\right) = \sum_{i = 1}^{n}a_{ii}\]</span></p>
<p>The trace plays an important role in statistics in determining expected values of quadratic forms of random variables, for example, sums of squares in linear models. An important property of the trace is its invariance under cyclic permutations,</p>
<p><span class="math display">\[tr\left( \mathbf{ABC} \right) = tr\left( \mathbf{BCA} \right) = tr(\mathbf{CAB})\]</span></p>
<p>provided the matrices conform to multiplication.</p>
<p>Some other useful properties of the trace are</p>
<ol type="1">
<li><p><span class="math inline">\(tr\left( \textbf{A}+ \textbf{B}\right) = tr\left( \textbf{A}\right) + tr\left( \textbf{B}\right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( \textbf{A}\right) = tr\left( \textbf{A}^\prime \right)\)</span></p></li>
<li><p><span class="math inline">\(\textbf{Y}^\prime\text{Ay} = tr\left( \textbf{Y}^\prime\text{Ay} \right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( c\textbf{A}\right) = c \times tr\left( \textbf{A}\right)\)</span></p></li>
<li><p><span class="math inline">\(tr\left( \textbf{A}\right) = r(\textbf{A})\)</span> if <span class="math inline">\(\textbf{A}\)</span> is symmetric and idempotent (<span class="math inline">\(\textbf{A}\textbf{A}= \textbf{A}\)</span> and <span class="math inline">\(\textbf{A}= \textbf{A}^\prime\)</span>)</p></li>
</ol>
</section>
</section>
<section id="random-vectors" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="random-vectors"><span class="header-section-number">3.4</span> Random Vectors</h2>
<p>If the elements of a vector are random variables, the vector object itself is a random variable. You can think of random vectors as a convenient mechanism to collect random variables. Suppose we draw a random sample <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span>, then we can collect the <span class="math inline">\(n\)</span> random variables in a single random vector</p>
<p><span class="math display">\[\textbf{Y}= \begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}\]</span></p>
<section id="expected-value" class="level3">
<h3 class="anchored" data-anchor-id="expected-value">Expected Value</h3>
<p>Since each <span class="math inline">\(Y_{i}\)</span> has a probability distribution, a mean (expected value) <span class="math inline">\(\text{E}\left\lbrack Y_{i} \right\rbrack\)</span>, a variance <span class="math inline">\(\text{Var}\left\lbrack Y_{i} \right\rbrack\)</span>, and so forth, the same applies to their collection. The expected value (mean) of a random vector is the vector of the expected values of its elements:</p>
<p><span class="math display">\[\text{E}\left\lbrack \textbf{Y}\right\rbrack = \begin{bmatrix}
\text{E}\left\lbrack Y_{1} \right\rbrack \\
\vdots \\
\text{E}\left\lbrack Y_{n} \right\rbrack
\end{bmatrix}\]</span></p>
<p>Suppose that <span class="math inline">\(\textbf{A},\ \textbf{B},\ \textbf{c}\)</span> are matrices and vectors of constants, respectively, and that <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are random vectors. The following are useful expectation operations in this situations:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{E}\left\lbrack \textbf{A}\right\rbrack = \textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AYB} + \mathbf{C} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack\textbf{B}+ \textbf{C}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AY} + \mathbf{c} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack + \textbf{c}\)</span></p></li>
<li><p><span class="math inline">\(\text{E}\left\lbrack \mathbf{AY} + \mathbf{BU} \right\rbrack = \textbf{A}\text{E}\left\lbrack \textbf{Y}\right\rbrack + \textbf{B}\ \text{E}\lbrack\mathbf{U}\rbrack\)</span></p></li>
</ol>
</section>
<section id="covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="covariance-matrix">Covariance Matrix</h3>
<p>While the distribution of <span class="math inline">\(Y_{i}\)</span> is univariate, <span class="math inline">\(\textbf{Y}\)</span> has a multivariate (<span class="math inline">\(n\)</span>-variate) distribution. The mean of the distribution is represented by a vector. The variance of the distribution is represented by a matrix, the <strong>variance-covariance matrix</strong>, a special case of a <strong>covariance</strong> matrix.</p>
<p>The covariance matrix between random vectors <span class="math inline">\(\textbf{Y}_{(k \times 1)}\)</span> and <span class="math inline">\(\mathbf{U}_{(p \times 1)}\)</span> is a <span class="math inline">\((k \times p)\)</span> matrix whose typical elements are the covariances between the elements of <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span>:</p>
<p><span class="math display">\[\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack = \left\lbrack \text{Cov}(Y_{i},U_{j}) \right\rbrack\]</span></p>
<p>The covariance matrix can be written in terms of expected values of <span class="math inline">\(\textbf{Y}\)</span>, <span class="math inline">\(\mathbf{U}\)</span>, and <span class="math inline">\(\textbf{Y}\mathbf{U}^\prime\)</span></p>
<p><span class="math display">\[\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack = \text{E}\left\lbrack \left( \textbf{Y}- \text{E}\lbrack\textbf{Y}\rbrack \right)\left( \mathbf{U} - \text{E}\left\lbrack \mathbf{U} \right\rbrack \right)^\prime \right\rbrack = \text{E}\left\lbrack \textbf{Y}\mathbf{U}^\prime \right\rbrack - \text{E}\left\lbrack \textbf{Y}\right\rbrack \text{E}\left\lbrack \mathbf{U} \right\rbrack^{\prime}\]</span></p>
<p>Some useful rules to manipulate covariance matrices are:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \mathbf{AY},\mathbf{U} \right\rbrack = \textbf{A}\text{Cov}\lbrack\textbf{Y},\mathbf{U}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \textbf{Y},\mathbf{BU} \right\rbrack = \text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack\textbf{B}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack \mathbf{AY},\mathbf{BU} \right\rbrack = \textbf{A}\text{Cov}\left\lbrack \textbf{Y},\mathbf{U} \right\rbrack\ \textbf{B}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\left\lbrack a\textbf{Y}+ b\mathbf{U},c\mathbf{W} + d\textbf{V}\right\rbrack = ac\text{Cov}\left\lbrack \textbf{Y},\mathbf{W} \right\rbrack + bc\text{Cov}\left\lbrack \mathbf{U},\mathbf{W} \right\rbrack + ad\text{Cov}\left\lbrack \textbf{Y},\textbf{V}\right\rbrack + bd\text{Cov}\lbrack\mathbf{U},\textbf{V}\rbrack\)</span></p></li>
</ol>
</section>
<section id="variance-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="variance-covariance-matrix">Variance-covariance Matrix</h3>
<p>The variance-covariance matrix (or <strong>variance matrix</strong> for short) of a random vector <span class="math inline">\(\textbf{Y}\)</span> is the covariance matrix of <span class="math inline">\(\textbf{Y}\)</span> with itself.</p>
<p><span class="math display">\[\text{Var}\left\lbrack \textbf{Y}\right\rbrack = \text{Cov}\left\lbrack \textbf{Y},\textbf{Y}\right\rbrack = \text{E}\left\lbrack \left( \textbf{Y}- \text{E}\lbrack\textbf{Y}\rbrack \right)\left( \textbf{Y}-\text{E}\left\lbrack \textbf{Y}\right\rbrack \right)^\prime \right\rbrack = \text{E}\left\lbrack \textbf{Y}\textbf{Y}^\prime \right\rbrack - \text{E}\left\lbrack \textbf{Y}\right\rbrack \text{E}\left\lbrack \textbf{Y}\right\rbrack^{\prime}\]</span></p>
<p>The diagonal entries of the variance-covariance matrix contain the variances of the <span class="math inline">\(Y_{i}\)</span>. The off-diagonal cells contain the covariances <span class="math inline">\(\text{Cov}\left\lbrack Y_{i},Y_{j} \right\rbrack\)</span>. If the variance matrix is diagonal, the elements of random vector <span class="math inline">\(\textbf{Y}\)</span> are uncorrelated. Two random vectors <span class="math inline">\(\textbf{Y}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are uncorrelated if their variance matrix is block-diagonal:</p>
<p><span class="math display">\[\text{Var}\begin{bmatrix}
\textbf{Y}_{1} \\
\textbf{Y}_{2}
\end{bmatrix} = \begin{bmatrix}
\text{Var}\lbrack\textbf{Y}_{2}\rbrack &amp; \textbf{0}\\
\textbf{0}&amp; \text{Var}\lbrack\textbf{Y}_{1}\rbrack
\end{bmatrix}\]</span></p>
<p>A very special variance-covariance matrix in statistical models is the scaled identity matrix, <span class="math inline">\(\sigma^{2}\textbf{I}\)</span>. This is the variance matrix of uncorrelated observations drawn from the same distribution—a common assumption for the error terms in models.</p>
<p>The rules for working with covariances extend to working with variance matrices:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{Var}\left\lbrack \mathbf{AY} \right\rbrack = \textbf{A}\ \text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{A}^\prime\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack \textbf{Y}+ \textbf{A}\right\rbrack = \text{Var}\lbrack\textbf{Y}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack \textbf{A}^\prime\textbf{Y}\right\rbrack = \textbf{A}^\prime\text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack a\textbf{Y}\right\rbrack = a^{2}\text{Var}\lbrack\textbf{Y}\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\left\lbrack a\textbf{Y}+ b\mathbf{U} \right\rbrack = a^{2}\text{Var}\left\lbrack \textbf{Y}\right\rbrack + b^{2}\text{Var}\left\lbrack \mathbf{U} \right\rbrack + 2ab\ \text{Cov}\lbrack\textbf{Y},\mathbf{U}\rbrack\)</span></p></li>
</ol>
<p>Finally, an important result about expected values of quadratic forms, heavily used to in decomposing variability is</p>
<p><span class="math display">\[\text{E}\left\lbrack \textbf{Y}^\prime\mathbf{AY} \right\rbrack = tr\left( \textbf{A}\text{Var}\left\lbrack \textbf{Y}\right\rbrack \right) + \text{E}\left\lbrack \textbf{Y}\right\rbrack^\prime\textbf{A}\ \text{E}\lbrack\textbf{Y}\rbrack\]</span></p>
</section>
<section id="correlation-matrix" class="level3">
<h3 class="anchored" data-anchor-id="correlation-matrix">Correlation Matrix</h3>
<p>Suppose <span class="math inline">\(\textbf{Y}_n\)</span> is a random vector with a positive definite (full rank) variance matrix <span class="math inline">\(\textbf{V}= \text{Var}[\textbf{Y}] = [v_{ij}]\)</span>. The <strong>correlation matrix</strong> is the <span class="math inline">\((n \times n)\)</span> matrix with typical element <span class="math display">\[
\textbf{R}= \left[\text{Corr}[Y_i,Y_j]\right] = \frac{v_{ij}}{\sqrt{v_{ii} \ v_{jj}}}
\]</span></p>
<p>You can express <span class="math inline">\(\textbf{R}\)</span> as a product of matrices, <span class="math display">\[
\textbf{R}= \textbf{D}^{-1/2} \textbf{V}\textbf{D}^{-1/2}
\]</span> where <span class="math inline">\(\textbf{D}\)</span> is a <span class="math inline">\((n \times n)\)</span> diagonal matrix that contains the variances <span class="math inline">\(v_{ii}\)</span> on its diagonal.</p>
</section>
</section>
<section id="cross-product-matrix" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="cross-product-matrix"><span class="header-section-number">3.5</span> Cross-product Matrix</h2>
<p>Statistical models are often expressed in terms of a vector of length <span class="math inline">\(p\)</span> derived from the input variables of the model: <span class="math inline">\(\textbf{x}= [x_1, x_2, \cdots, x_p]^\prime\)</span>. With continuous input variables, <span class="math inline">\(x_j\)</span> refers to the value of the variable itself or a transformation such as the logarithm, square root, etc. The values of <span class="math inline">\(x_j\)</span> can also be constructed from transformations and functions involving other variables, for example <span class="math inline">\(x_3 = x_1 * x_2\)</span>.</p>
<p>In the case of a qualitative factor with <span class="math inline">\(k\)</span> levels, the factor can occupy <span class="math inline">\(k-1\)</span> binary variables in the model vector. Often, a 1 is added at the beginning of the vector to accommodate an intercept term: <span class="math inline">\(\textbf{x}= [1, x_1, x_2, \cdots, x_p]^\prime\)</span>.</p>
<p>The vector of inputs for the <span class="math inline">\(i\)</span><sup>th</sup> observation is then <span class="math inline">\(\textbf{x}_i = [1, x_{i1}, x_{i2}, \cdots, x_{ip}]^\prime\)</span>.</p>
<p>When these input rows are stacked, we obtain the <span class="math inline">\((n \times p+1)\)</span> <strong>model matrix</strong> (sometimes called the <strong>design matrix</strong>) <span id="eq-model-matrix"><span class="math display">\[
\textbf{X}_{(n\times p+1)} = \begin{bmatrix}
   1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
   1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
   \vdots     &amp; \vdots &amp; \ddots &amp; \vdots \\
   1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix}
\tag{3.1}\]</span></span></p>
<p>A <strong>cross-product</strong> matrix is the product of the transpose of a matrix with itself: <span class="math display">\[
\textbf{X}^\prime\textbf{X}
\]</span> A cross-product matrix is square and symmetric. The cross-product of the model matrix plays an important role in estimating model parameters and in determining their properties. For the model matrix in <a href="#eq-model-matrix" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>, the entries of the cross-product matrix take the following form <span class="math display">\[
\textbf{X}^\prime\textbf{X}= \begin{bmatrix}
  n &amp; \sum x_{i1} &amp; \sum x_{i2} &amp; \cdots &amp; \sum x_{ip} \\
  \sum x_{i1}   &amp; \sum x_{i1}^2 &amp; \sum x_{i1}x_{i2} &amp; \cdots &amp; \sum x_{i1}x_{ip} \\
  \sum x_{i2} &amp; \sum x_{i2}x_{i1} &amp; \sum x_{i2}^2 &amp; \cdots &amp; \sum x_{i2}x_{ip}  \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \sum x_{ip} &amp; \cdots &amp; \cdots &amp; \cdots &amp; \sum x_{ip}^2
\end{bmatrix}
\]</span></p>
<p>The sum of squares of the <span class="math inline">\(\textbf{X}\)</span> columns are on the diagonal, the sum of the cross-products are in the off-diagonal cells.</p>
</section>
<section id="sec-idempotent" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-idempotent"><span class="header-section-number">3.6</span> Idempotent Matrices</h2>
<p>A matrix is called <strong>idempotent</strong> if multiplying the matrix by itself yields the matrix–<span class="math inline">\(\textbf{A}\textbf{A}= \textbf{A}\)</span>. Because <span class="math inline">\(\textbf{A}\)</span> must conform to itself for multiplication, idempotent matrices are square matrices. Idempotent matrices that are also symmetric play an important role in statistical estimation. Idempotent matrices are <strong>projection</strong> matrices, that means they map a vector from a space to a sub-space. Symmetric idempotent matrices are orthogonal projection matrices.</p>
<p>Among some of the interesting properties of idempotent matrices are the following:</p>
<ol type="1">
<li><p>The trace of an idempotent matrix is equal to its rank.</p></li>
<li><p>The trace of an idempotent matrix is an integer.</p></li>
<li><p>The eigenvalues of an idempotent matrix are either 0 or 1.</p></li>
</ol>
<section id="projections" class="level3">
<h3 class="anchored" data-anchor-id="projections">Projections</h3>
<p>For example, suppose we want to find a solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> in the linear model <span class="math inline">\(\textbf{Y}_{(n \times 1)} = \textbf{X}\boldsymbol{\beta}_{(p + 1 \times 1)} + \mathbf{\epsilon}\)</span>. The vector <span class="math inline">\(\textbf{Y}\)</span> is a vector in <span class="math inline">\(n\)</span>-dimensional space <span class="math inline">\(\mathbb{R}^{n}\)</span> and the model places a restriction on the predicted values <span class="math inline">\(\widehat{\textbf{Y}} = \textbf{X}\widehat{\boldsymbol{\beta}}\)</span>: the predicted values are confined to a <span class="math inline">\((p + 1)\)</span>-dimensional sub-space of <span class="math inline">\(\mathbb{R}^{n}\)</span>. Regardless of how we choose the estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, we are dealing with projecting vector <span class="math inline">\(\textbf{Y}\)</span> onto a sub-space of <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<p>We can thus think of the problem of finding the best estimator in this model as the problem of finding the best projection onto the space generated by the columns of <span class="math inline">\(\textbf{X}\)</span>. In that case, why not choose the projection that minimizes the distance between the observed values <span class="math inline">\(\textbf{Y}\)</span> and the predicted values <span class="math inline">\(\widehat{\textbf{Y}}\)</span>. This is achieved by projecting <span class="math inline">\(\textbf{Y}\)</span> perpendicular (orthogonal) onto the sub-space generated by <span class="math inline">\(\textbf{X}\)</span>. In other words, our solution is the vector <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> that satisfies</p>
<p><span class="math display">\[\left( \textbf{Y}- \textbf{X}\widehat{\boldsymbol{\beta}} \right)^\prime\textbf{X}\widehat{\boldsymbol{\beta}} = 0\]</span></p>
<p>Multiplying out and rearranging terms yields</p>
<p><span class="math display">\[{\widehat{\boldsymbol{\beta}}}^{\prime}\textbf{X}^{\prime}\textbf{Y}= {\widehat{\boldsymbol{\beta}}}^{\prime}\textbf{X}^\prime\textbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[\textbf{X}^{\prime}\textbf{Y}= \textbf{X}^\prime\textbf{X}\widehat{\boldsymbol{\beta}}\]</span></p>
<p>If <span class="math inline">\(\textbf{X}\)</span> is of full column rank, then <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\)</span> exists and we can solve:</p>
<p><span class="math display">\[\widehat{\boldsymbol{\beta}}=\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}\]</span></p>
</section>
<section id="sec-hat-matrix" class="level3">
<h3 class="anchored" data-anchor-id="sec-hat-matrix">The “Hat” Matrix</h3>
<p>The ordinary least squares estimator is the orthogonal projection of <span class="math inline">\(\textbf{Y}\)</span> onto the sub-space created by the columns of <span class="math inline">\(\textbf{X}\)</span>. To see the projection matrix at work, compute the predicted values, <span class="math inline">\(\textbf{X}\widehat{\boldsymbol{\beta}}\)</span>:</p>
<p><span class="math display">\[\textbf{X}\widehat{\boldsymbol{\beta}}=\textbf{X}\left( \textbf{X}^{\prime}\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\textbf{Y}= \widehat{\textbf{Y}}\]</span></p>
<p>The matrix <span class="math inline">\(\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime}\)</span> has a very special role in regression analysis, it is often called the “hat” matrix and denoted <span class="math inline">\(\textbf{H}\)</span>, because pre-multiplying <span class="math inline">\(\textbf{Y}\)</span> with <span class="math inline">\(\textbf{H}\)</span> puts the hats on <span class="math inline">\(\textbf{Y}\)</span>:</p>
<p><span class="math display">\[\textbf{H}\textbf{Y}= \widehat{\textbf{Y}}\]</span></p>
<p>Let’s verify that <span class="math inline">\(\textbf{H}\)</span> is indeed a projection matrix, which requires that <span class="math inline">\(\textbf{H}\textbf{H}= \textbf{H}\)</span>:</p>
<p><span class="math display">\[\textbf{H}\textbf{H}= \textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime} \times \textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\textbf{X}^\prime=\textbf{X}\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\textbf{X}^{\prime} = \textbf{H}\]</span></p>
<p>Matrices with the property that <span class="math inline">\(\textbf{H}\textbf{H}= \textbf{H}\)</span> are called <strong>idempotent</strong> matrices, these are projection matrices. If, in addition, <span class="math inline">\(\textbf{H}\)</span> is symmetric, <span class="math inline">\(\textbf{H}^\prime = \textbf{H}\)</span>, the matrix is called symmetric idempotent—these are orthogonal projection matrices. (An idempotent matrix that is not symmetric is called an oblique projector.)</p>
<p>The hat matrix in the regression model is a symmetric idempotent matrix.</p>
<p>Here are some results about (symmetric) idempotent matrices that come in handy when working out the properties of estimators in regression-type models:</p>
<ol type="1">
<li><p>Projection matrices are typically not of full rank. If an <span class="math inline">\((n \times n)\)</span> idempotent matrix is of rank <span class="math inline">\(n\)</span>, then it is the identity matrix <span class="math inline">\(\textbf{I}\)</span>.</p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is (symmetric) idempotent, then <span class="math inline">\(\textbf{I}- \textbf{A}\)</span> is (symmetric) idempotent.</p></li>
<li><p>If <span class="math inline">\(\mathbf{P}\)</span> is non-singular, then <span class="math inline">\(\mathbf{PA}\mathbf{P}^{-1}\)</span> is an idempotent matrix.</p></li>
</ol>
<p>You can use these properties to show that in the linear regression model with uncorrelated errors and equal variance the variance matrix of the model residuals <span class="math inline">\(\textbf{Y}- \widehat{\textbf{Y}}\)</span> is</p>
<p><span class="math display">\[\text{Var}\left\lbrack \textbf{Y}- \widehat{\textbf{Y}} \right\rbrack = \sigma^{2}(\textbf{I}- \textbf{H})\]</span></p>
<hr>
<p>Consider the special case where <span class="math inline">\(\textbf{X}= \textbf{1}_{n}\)</span>, a column vector of ones. The corresponding linear model is <span class="math inline">\(\textbf{Y}= \textbf{1}\beta + \boldsymbol{\epsilon}\)</span>, an intercept-only model. The hat matrix for this model is</p>
<p><span class="math display">\[\textbf{1}\left( \textbf{1}^{\prime}\textbf{1}\right)^{- 1}\textbf{1}^\prime = \frac{1}{n}\textbf{1}\textbf{1}^\prime = \frac{1}{n}\textbf{J}= \begin{bmatrix}
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}
\end{bmatrix}
\]</span>a matrix filled with <span class="math inline">\(\frac{1}{n}\)</span>. The projection of <span class="math inline">\(\textbf{Y}\)</span> onto the space generated by <span class="math inline">\(\textbf{1}_{n}\)</span> is</p>
<p><span class="math display">\[\frac{1}{n}\mathbf{JY} = \begin{bmatrix}
\overline{Y} \\
\vdots \\
\overline{Y}
\end{bmatrix}\]</span></p>
<p>The predicted values are all the same, the sample mean. In other words, <span class="math inline">\(\beta = \overline{Y}\)</span>. Since the projector is idempotent, deriving the variance of the predicted value in the <em>iid</em> case is simple:</p>
<p><span class="math display">\[
\text{Var}\left\lbrack \textbf{H}\textbf{Y}\right\rbrack = \textbf{H}\text{Var}\left\lbrack \textbf{Y}\right\rbrack\textbf{H}^\prime = \sigma^{2}\textbf{H}\textbf{H}^{\prime} = \sigma^{2}\textbf{H}= \sigma^{2}\frac{1}{n}\textbf{J}=\begin{bmatrix}
\frac{\sigma^{2}}{n} &amp; \cdots &amp; \frac{\sigma^{2}}{n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\sigma^{2}}{n} &amp; \cdots &amp; \frac{\sigma^{2}}{n}
\end{bmatrix}
\]</span></p>
</section>
</section>
<section id="matrix-factorization" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="matrix-factorization"><span class="header-section-number">3.7</span> Matrix Factorization</h2>
<!---
https://www.cs.bu.edu/fac/snyder/cs132-book/L12MatrixFactorizations.html#
--->
<p>The <strong>factorization</strong> (or <strong>decomposition</strong>) of a matrix <span class="math inline">\(\textbf{A}\)</span> expresses the matrix as a product of two or more matrices, for example, <span class="math inline">\(\textbf{A}= \textbf{B}\textbf{C}\)</span>. Matrix multiplication defines the result of <span class="math inline">\(\textbf{B}\textbf{C}\)</span>, but how can we reverse this and express the result as a product of matrices?</p>
<p>The reasons for matrix factorization are to simplify computation, to make working with the matrix easier, and to reveal important properties of <span class="math inline">\(\textbf{A}\)</span>. Consequently, not any factorization will do, we are interested in very specific forms of writing <span class="math inline">\(\textbf{A}\)</span> as a product. Two important matrix factorizations are the <strong>eigenvalue</strong> decomposition and the <strong>singular value</strong> decomposition.</p>
<p>The word <em>eigen</em> in <strong>eigenvalue</strong> has German roots. It means to own, the German noun <em>Eigentum</em> refers to possession (ownership). It also means an inherent property that is special to a person or entity. <em>Eigen</em> can refer to a peculiar property. To make the concept of an eigenvalue or an eigenvector understandable, the idea of an inherent property of oneself is appropriate. To see the connection we consider first linear transformations from <span class="math inline">\(\mathbb{R}^n\)</span> tp <span class="math inline">\(\mathbb{R}^m\)</span>.</p>
<section id="linear-transformations" class="level3">
<h3 class="anchored" data-anchor-id="linear-transformations">Linear Transformations</h3>
<p>If <span class="math inline">\(\textbf{A}_{(m \times n)}\)</span> is a matrix and <span class="math inline">\(x_n\)</span> is a vector in <span class="math inline">\(\mathbb{R}^n\)</span>, then <span class="math display">\[
\textbf{y}= \textbf{A}\textbf{x}
\]</span> is a vector in <span class="math inline">\(\mathbb{R}^m\)</span>. The matrix <span class="math inline">\(\textbf{A}\)</span> linearly transformed the <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\textbf{x}\)</span> into the <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\textbf{y}\)</span>. You can say that <span class="math inline">\(\textbf{A}\)</span> “moved” <span class="math inline">\(\textbf{x}\)</span> to %<span class="math inline">\(\textbf{y}\)</span>. The transform is called <strong>linear</strong> because the transformation <span class="math inline">\(\textbf{A}\)</span> of a linear combination of two vectors—<span class="math inline">\(c_1\textbf{x}_1 + c_2\textbf{x}_2\)</span>—is identical to the linear combination of the tranformed vectors: <span class="math display">\[
\textbf{A}(c_1\textbf{x}_1 + c_2\textbf{x}_2) = c_1(\textbf{A}\textbf{x}_1) + c_2(\textbf{A}\textbf{x}_2)
\]</span></p>
</section>
<section id="eigenvalue-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalue-decomposition">Eigenvalue Decomposition</h3>
<p>Suppose <span class="math inline">\(\textbf{A}\)</span> is a square matrix, <span class="math inline">\((n \times n)\)</span>. Is there a vector <span class="math inline">\(\textbf{x}\)</span> for which the transformation of <span class="math inline">\(\textbf{x}\)</span> results in a multiple of <strong>itself</strong>? That is, can we find some vector <span class="math inline">\(\textbf{x}\)</span> and some number <span class="math inline">\(\lambda\)</span> for which <span class="math display">\[
\textbf{A}\textbf{x}= \lambda\textbf{x}
\]</span></p>
<p>Rearranging terms, the equation can be written as <span class="math display">\[
(\textbf{A}- \lambda\textbf{I})\textbf{x}= \textbf{0}
\]</span></p>
<p>Clearly, <span class="math inline">\(\textbf{x}=\textbf{0}\)</span> satisfies the equation regardless of the value of <span class="math inline">\(\lambda\)</span>. A nonzero solution exists if and only if <span class="math inline">\(det(\textbf{A}-\lambda\textbf{I}) = 0\)</span>. The values for <span class="math inline">\(\lambda\)</span> for which this <strong>characteristic equation</strong> holds are called the <strong>eigenvalues</strong> of <span class="math inline">\(\textbf{A}\)</span>.</p>
<p>If <span class="math inline">\(\lambda_j\)</span> is such a value, and <span class="math inline">\(\textbf{x}_j\)</span> is a vector for which <span class="math display">\[
\textbf{A}\textbf{x}_j = \lambda_j \textbf{x}_j
\]</span> then <span class="math inline">\(\textbf{x}_j\)</span> is called the <strong>eigenvector</strong> of <span class="math inline">\(\textbf{A}\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda_j\)</span>.</p>
<p>Some interesting results about eigenvalues:</p>
<ol type="1">
<li><p>If <span class="math inline">\(\textbf{A}_{(n\times n)}\)</span> is singular, then it has at least one zero eigenvalue.</p></li>
<li><p>For <span class="math inline">\(\textbf{A}_{(n\times n)}\)</span> and <span class="math inline">\(\textbf{C}_{(n \times n)}\)</span> nonsingular, the following matrices have the same eigenvalues: <span class="math inline">\(\textbf{A}\)</span>, <span class="math inline">\(\textbf{C}^{-1}\textbf{A}\textbf{C}\)</span>, <span class="math inline">\(\textbf{C}\textbf{A}\textbf{C}^{-1}\)</span>.</p></li>
<li><p><span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{A}^\prime\)</span> have the same eigenvalues.</p></li>
<li><p>If <span class="math inline">\(\textbf{A}\)</span> is nonsingular and <span class="math inline">\(\lambda\)</span> is one of its eigenvalues, then <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(\textbf{A}^{-1}\)</span>.</p></li>
</ol>
<hr>
<p>We have finally arrived at the following: the <strong>eigenvalue decomposition</strong> (or <strong>eigendecomposition</strong>) of a square <span class="math inline">\((n \times n)\)</span> matrix <span class="math inline">\(\textbf{A}\)</span> is a factorization in terms of eigenvalues and eigenvectors: <span class="math display">\[
\textbf{A}= \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^{-1}
\]</span> where <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix containing the <span class="math inline">\(n\)</span> eigenvalues <span class="math inline">\(\lambda_1, \cdots, \lambda_n\)</span> and <span class="math inline">\(\textbf{Q}\)</span> is an <span class="math inline">\((n \times n)\)</span> matrix of eigenvectors.</p>
<p>If <span class="math inline">\(\textbf{A}\)</span> is a real symmetric matrix, then <span class="math inline">\(\textbf{Q}\)</span> is an <strong>orthogonal</strong> matrix, which implies that <span class="math inline">\(\textbf{Q}^{-1} = \textbf{Q}^\prime\)</span>. An orthogonal (or orthonormal) matrix is a square matrix of real numbers whose columns are independent (perpendicular) of each other and have unit length. If <span class="math inline">\(\textbf{Q}\)</span> is orthogonal, this property can be expressed as <span class="math inline">\(\textbf{Q}^\prime\textbf{Q}=\textbf{I}\)</span>, which implies that the transpose of the orthogonal matrix is its inverse.</p>
<p>We can then write the eigenvalue decomposition as <span class="math display">\[
\textbf{A}= \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^\prime
\]</span></p>
<p>One interpretation of the eigenvalues is the extent to which they shrink or stretch <span class="math inline">\(\textbf{A}\)</span> in the direction of the eigenvectors. For a real symmetric <span class="math inline">\(\textbf{A}\)</span> we can write <span class="math display">\[
\textbf{A}\textbf{q} = \lambda\textbf{q}
\]</span> where <span class="math inline">\(\textbf{q}\)</span> is one of the orthonormal eigenvectors, <span class="math inline">\(||\textbf{q}||=1\)</span>. Then <span class="math display">\[
||\textbf{A}\textbf{q}|| = ||\lambda\textbf{q}|| =  |\lambda| \, ||\textbf{q}|| = |\lambda|
\]</span></p>
<section id="constructing-an-inverse" class="level4">
<h4 class="anchored" data-anchor-id="constructing-an-inverse">Constructing an inverse</h4>
<p>If <span class="math inline">\(\textbf{A}\)</span> is of full rank, then an inverse can be constructed from the eigenvalue decomposition as <span class="math display">\[
\textbf{A}^{-1} = \textbf{Q}\boldsymbol{\Lambda}^{-1}\textbf{Q}^{-1}
\]</span> and in the case of a real symmetric matrix: <span class="math display">\[
\textbf{A}= \textbf{Q}\boldsymbol{\Lambda}^{-1}\textbf{Q}^\prime
\]</span></p>
<p>The number of nonzero diagonal values of <span class="math inline">\(\boldsymbol{\Lambda}\)</span> equals the rank of the decomposed matrix. For a nonsingular (full-rank) <span class="math inline">\(\textbf{A}\)</span>, all eigenvalues are greater than zero. The <span class="math inline">\(\lambda_j\)</span> can be close to zero, however, so when computing eigenvalues with finite-precision arithmetic (computers!), we apply thresholds to declare a singularity.</p>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3>
<p>The matrix factorized in the eigendecomposition is a square <span class="math inline">\((n \times n)\)</span> matrix. What can we say about arbitrary <span class="math inline">\(\textbf{A}_{(m \times n)}\)</span> matrices? The eigendecomposition cannot be applied directly, because we need a square matrix. What happens if we consider the decomposition of the square symmetric <span class="math inline">\(\textbf{A}^\prime \textbf{A}\)</span> instead? We know that there is an eigenvalue decomposition of the form <span class="math display">\[
\textbf{A}^\prime \textbf{A}= \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^\prime
\]</span> The <strong>singular values</strong> of <span class="math inline">\(\textbf{A}\)</span> are the square roots of the eigenvalues of <span class="math inline">\(\textbf{A}^\prime\textbf{A}\)</span>. They are the lengths of the vectors <span class="math inline">\(\textbf{A}\textbf{q}_1, \cdots, \textbf{A}\textbf{q}_n\)</span>, where <span class="math inline">\(\textbf{q}_j\)</span> is the <span class="math inline">\(j\)</span><sup>th</sup> eigenvector of <span class="math inline">\(\textbf{A}^\prime\textbf{A}\)</span>.</p>
<p>The singular value decomposition (SVD) of the <span class="math inline">\((m \times n)\)</span> matrix <span class="math inline">\(\textbf{A}\)</span> is written as <span class="math display">\[
\textbf{A}= \textbf{U}\boldsymbol{\Sigma}\textbf{V}^\prime
\]</span> where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is a <span class="math inline">\((m \times n)\)</span> diagonal matrix containing the singular values of <span class="math inline">\(\textbf{A}\)</span> (the square roots of the eigenvalues of <span class="math inline">\(\textbf{A}^\prime \textbf{A}\)</span>), <span class="math inline">\(\textbf{U}\)</span> is a <span class="math inline">\((m \times m)\)</span> orthogonal matrix of <strong>left singular vectors</strong>, and <span class="math inline">\(\textbf{V}\)</span> is a <span class="math inline">\((n \times n)\)</span> matrix of <strong>right singular vectors</strong> of <span class="math inline">\(\textbf{A}\)</span>.</p>
<p>The SVD is an important operation on matrices with many applications in statistics and data science:</p>
<ul>
<li>Principal component analysis</li>
<li>Reducing the dimensionality of a high-dimensional problem</li>
<li>Computing a low-rank approximation to a matrix</li>
<li>Imputation of missing values</li>
<li>Spectral analysis</li>
<li>Image processing</li>
<li>Computing inverse and generalized inverse matrices</li>
</ul>
<section id="constructing-an-inverse-1" class="level4">
<h4 class="anchored" data-anchor-id="constructing-an-inverse-1">Constructing an inverse</h4>
<p>If <span class="math inline">\(\textbf{A}\)</span> is square and nonsingular, we can find its inverse from the elements of the singular value decomposition. Since <span class="math display">\[
\textbf{A}= \textbf{U}\boldsymbol{\Sigma}\textbf{V}^\prime
\]</span> <span class="math display">\[
\textbf{A}^{-1} = \left(\textbf{V}^\prime\right)^{-1}\boldsymbol{\Sigma}^{-1}\textbf{U}^{-1} = \textbf{V}\boldsymbol{\Sigma}^{-1}\textbf{U}^\prime
\]</span> The first equality applies the rule for the inverse of a product, the second equality uses the fact that <span class="math inline">\(\textbf{U}\)</span> and <span class="math inline">\(\textbf{V}\)</span> are orthogonal matrices. As in the case of using the eigendecomposition for inversion, <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> is a diagonal matrix with the reciprocals of the singular values on the diagonal.</p>
</section>
<section id="constructing-a-generalized-inverse" class="level4">
<h4 class="anchored" data-anchor-id="constructing-a-generalized-inverse">Constructing a generalized inverse</h4>
<p>If <span class="math inline">\(\textbf{A}\)</span> is nonsingular, we cannot construct an inverse, but the previous approach can be modified to construct a generalized inverse as <span class="math display">\[
\textbf{A}^{-1} = \textbf{V}{\boldsymbol{\Sigma}^*}^{-1}\textbf{U}^\prime
\]</span> where <span class="math inline">\({\boldsymbol{\Sigma}^*}^{-1}\)</span> is a diagonal matrix; its <span class="math inline">\(i\)</span><sup>th</sup> diagonal value is the <span class="math inline">\(i\)</span><sup>th</sup> singular value if <span class="math inline">\(\sigma_i\)</span> is nonzero, and is zero otherwise. Since the singular values are often arranged in order, <span class="math inline">\(\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n\)</span>, this amounts to setting the last <span class="math inline">\(n-r\)</span> singular values to zero, if <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(\textbf{A}\)</span>.</p>
<p>You can show that a matrix so constructed satisfies the conditions for a generalized inverse.</p>
</section>
</section>
</section>
<section id="sec-matrix-differentiation" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="sec-matrix-differentiation"><span class="header-section-number">3.8</span> Matrix Differentiation</h2>
<p>Estimation of parameters in statistical models often requires minimization or maximization of an objective function. For example, the ordinary least squares (OLS) principle finds the OLS estimator as the function of the data that minimizes the error sum of squares of the model. Maximum likelihood finds estimators of the parameters as the functions of the data that maximizes the joint likelihood (the joint distribution function) of the data.</p>
<p>The parameters of the models appear as elements of vectors and matrices. Finding estimators of the parameters thus requires calculus on vectors and matrices. Consider matrix <span class="math inline">\(\textbf{A}\)</span>, whose elements depend on a scalar parameter <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\textbf{A}= \left\lbrack a_{ij}(\theta) \right\rbrack\)</span>. The derivative of <span class="math inline">\(\textbf{A}\)</span> with respect to <span class="math inline">\(\theta\)</span> is the matrix of the derivatives of the typical elements <span class="math inline">\(a_{ij}(\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span>. We write this formally as</p>
<p><span class="math display">\[\frac{\partial\textbf{A}}{\partial\theta} = \left\lbrack \frac{\partial a_{ij}(\theta)}{\partial\theta} \right\rbrack\]</span></p>
<p>The derivative of a function <span class="math inline">\(f(\boldsymbol{\theta})\)</span> with respect to the vector <span class="math inline">\(\boldsymbol{\theta}_{(p \times 1)}\)</span> is the vector of the partial derivatives of the function</p>
<p><span class="math display">\[\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}} = \begin{bmatrix}
\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\theta_{1}} \\
\vdots \\
\frac{\partial f\left( \boldsymbol{\theta}\right)}{\partial\theta_{p}}
\end{bmatrix}\]</span></p>
<p>Here are some useful results from vector and matrix calculus where <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are functions of <span class="math inline">\(\theta\)</span> and vector <span class="math inline">\(\textbf{X}\)</span> does not depend on <span class="math inline">\(\theta\)</span>:</p>
<ol type="1">
<li><p><span class="math inline">\(\frac{{\partial ln}\left| \textbf{A}\right|}{\partial\theta} = \frac{1}{\left| \textbf{A}\right|}\frac{\partial\left| \textbf{A}\right|}{\partial\theta} = tr\left( \textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta} \right)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{A}^{- 1}}{\partial\theta} = - \textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta\ }\textbf{A}^{- 1}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial tr\left( \mathbf{AB} \right)}{\partial\theta} = tr\left( \frac{\mathbf{\partial}\textbf{A}}{\partial\theta}\textbf{B}\right) + tr\left( \textbf{A}\frac{\mathbf{\partial}\textbf{B}}{\partial\theta} \right)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^\prime\textbf{A}^{- 1}\textbf{X}}{\partial\theta} = - \textbf{X}^\prime\textbf{A}^{- 1}\frac{\partial\textbf{A}}{\partial\theta}\textbf{A}^{- 1}\textbf{X}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^{\prime}\mathbf{Ax}}{\partial\textbf{X}} = 2\mathbf{Ax}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial\textbf{X}^\prime\textbf{A}}{\partial\textbf{X}} = \frac{\partial\textbf{A}^\prime\textbf{X}}{\partial\textbf{X}} = \textbf{A}\)</span></p></li>
</ol>
</section>
<section id="sec-multi-gaussian" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="sec-multi-gaussian"><span class="header-section-number">3.9</span> Multivariate Gaussian Distribution</h2>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition</h3>
<p>A scalar random variable <span class="math inline">\(Y\)</span> has a <strong>Gaussian</strong> distribution function with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> if its probability density function is given by</p>
<p><span class="math display">\[f(y) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ - \frac{1}{2\sigma^{2}}(y - \mu)^{2} \right\}\]</span></p>
<p>We also say that <span class="math inline">\(Y\)</span> is <strong>normally</strong> distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>. The shorthand expressions <span class="math inline">\(Y \sim G(\mu,\sigma^{2})\)</span> or <span class="math inline">\(Y \sim N(\mu,\sigma^{2})\)</span> are common.</p>
<p>The generalization from a scalar random variable <span class="math inline">\(Y\)</span> to a random vector <span class="math inline">\(\textbf{Y}_{(n \times 1)}\)</span> with a <strong>multivariate Gaussian</strong> distribution is as follows. <span class="math inline">\(\textbf{Y}_{(n \times 1)}\)</span> has a multivariate Gaussian (normal) distribution with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and variance matrix <span class="math inline">\(\textbf{V}\)</span>, if its density is given by</p>
<p><span class="math display">\[f\left( \textbf{Y}\right)=\frac{\left| \textbf{V}\right|^{- 1/2}}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\left( \textbf{Y}- \boldsymbol{\mu}\right)^\prime\textbf{V}^{- 1}\left( \textbf{Y}- \boldsymbol{\mu}\right) \right\}\]</span></p>
<p>This is denoted with the shorthand <span class="math inline">\(\textbf{Y}\sim G_{n}(\boldsymbol{\mu},\textbf{V})\)</span> or <span class="math inline">\(\textbf{Y}\sim N_{n}(\boldsymbol{\mu},\textbf{V}\)\)</span>. If the dimension of the distribution is clear from context, the subscript <span class="math inline">\(n\)</span> can be omitted. A special case is the <strong>standard</strong> multivariate Gaussian distribution with mean <span class="math inline">\(\textbf{0}\)</span> and variance matrix <span class="math inline">\(\textbf{I}\)</span> :</p>
<p><span class="math display">\[f\left( \textbf{Y}\right)=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\textbf{Y}^{\prime}\textbf{Y}\right\} = \frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left\{ - \frac{1}{2}\sum_{i}^{n}y_{i}^{2} \right\}\]</span></p>
<p>But this is just the product of the <span class="math inline">\(n\)</span> univariate densities of <span class="math inline">\(N(0,1)\)</span> random variables:</p>
<p><span class="math display">\[f\left( \textbf{Y}\right) = f\left( y_{1} \right) \times \cdots \times f\left( y_{n} \right)\]</span></p>
<p>where</p>
<p><span class="math display">\[f\left( y_{i} \right) = \frac{1}{(2\pi)^{1/2}}\exp\left\{ - \frac{1}{2}y^{2} \right\}\]</span></p>
<p>If the variance matrix is diagonal—that is, the <span class="math inline">\(Y_{i}\)</span> are uncorrelated—the multivariate normal distribution is the product of the univariate distributions. The random variables are independent.</p>
</section>
<section id="properties" class="level3">
<h3 class="anchored" data-anchor-id="properties">Properties</h3>
<p>Gaussian distributions have amazing (magical) properties.</p>
<section id="linear-combinations-are-gaussian" class="level4">
<h4 class="anchored" data-anchor-id="linear-combinations-are-gaussian">Linear combinations are Gaussian</h4>
<p>For example, a linear combination of Gaussian random variables also follows a Gaussian distribution. Formally, this can be expressed as follows: if <span class="math inline">\(\textbf{Y}\sim G_{n}\left( \boldsymbol{\mu},\textbf{V}\right)\)</span> and <span class="math inline">\(\textbf{A}\)</span> and <span class="math inline">\(\textbf{B}\)</span> are a matrix and vector of constants (not random variables), respectively, then <span class="math inline">\(\mathbf{AY} + \textbf{B}\)</span> follows a <span class="math inline">\(G(\textbf{A}\boldsymbol{\mu},\mathbf{AVA})\)</span> distribution.</p>
<p>A special case of this result is that if <span class="math inline">\(\textbf{Y}\sim G_{n}\left( \boldsymbol{\mu},\textbf{V}\right)\)</span>, <span class="math inline">\(\textbf{Y}- \boldsymbol{\mu}\)</span> has a <span class="math inline">\(G\left( 0,\textbf{V}\right)\)</span> distribution.</p>
<p>Because a linear function of a Gaussian random variable is Gaussian distributed, you can define all multivariate Gaussian distributions as linear transformations of the standard multivariate Gaussian distribution. If <span class="math inline">\(\textbf{Z}\sim G_{n}(\textbf{0},\textbf{I})\)</span>, and <span class="math inline">\(\textbf{V}= \textbf{C}^\prime\textbf{C}\)</span>, then <span class="math inline">\(\textbf{Y}= \mathbf{C}^\prime\textbf{Z}+ \boldsymbol{\mu}\)</span> has a <span class="math inline">\(G(\boldsymbol{\mu},\textbf{V})\)</span> distribution.</p>
</section>
<section id="zero-covariance-implies-independence" class="level4">
<h4 class="anchored" data-anchor-id="zero-covariance-implies-independence">Zero covariance implies independence</h4>
<p>Another unusual property of Gaussian random variables is that if they are uncorrelated, they are also stochastically independent. We derived this above for the special case of <span class="math inline">\(\textbf{Y}\sim G(\textbf{0},\sigma^{2}\textbf{I})\)</span>.</p>
<p>You cannot in general conclude that random variables are independent based on their lack of correlation. For Gaussian random variables you can. This result can be extended to Gaussian random vectors. Suppose <span class="math inline">\(\textbf{Y}_{(n \times 1)} \sim G(\boldsymbol{\mu},\ \textbf{V})\)</span> is partitioned into two sub-vectors of size <span class="math inline">\(s\)</span> and <span class="math inline">\(k\)</span>, where <span class="math inline">\(n = s + k\)</span>. Then we can similarly partition the mean vector and variance matrix:</p>
<p><span class="math display">\[\textbf{Y}_{(n \times 1)} = \begin{bmatrix}
\textbf{Y}_{1(s \times 1)} \\
\textbf{Y}_{2(k \times 1)}
\end{bmatrix},\ \ \boldsymbol{\mu}= \begin{bmatrix}
\boldsymbol{\mu}_{1} \\
\boldsymbol{\mu}_{2}
\end{bmatrix},\ \ \ \ \ \ \textbf{V}= \begin{bmatrix}
\textbf{V}_{11} &amp; \textbf{V}_{12} \\
\textbf{V}_{21} &amp; \textbf{V}_{22}
\end{bmatrix}\]</span></p>
<p>If <span class="math inline">\(\textbf{V}_{12} = \textbf{0}\)</span>, then <span class="math inline">\(\textbf{Y}_{1}\)</span> and <span class="math inline">\(\textbf{Y}_{2}\)</span> are independent. Also, each partition is Gaussian distributed, for example, <span class="math inline">\(\textbf{Y}_{1} \sim G(\boldsymbol{\mu}_{1},\ \textbf{V}_{11})\)</span>. We call the distribution of <span class="math inline">\(\textbf{Y}_{1}\)</span> the marginal distribution.</p>
<p>It follows immediately that each element of <span class="math inline">\(\textbf{Y}\)</span> follows a (univariate) Gaussian distribution, <span class="math inline">\(Y_{i} \sim G(\mu_{i},V_{ii})\)</span>—all marginal univariate distributions are Gaussian.</p>
</section>
<section id="conditionals-are-gaussian" class="level4">
<h4 class="anchored" data-anchor-id="conditionals-are-gaussian">Conditionals are Gaussian}</h4>
<p>The conditional distribution of <span class="math inline">\(\textbf{Y}_{1}\)</span> given <span class="math inline">\(\textbf{Y}_{2}\)</span> is also a Gaussian distribution, specifically:</p>
<p><span class="math display">\[\textbf{Y}_{1}|\textbf{Y}_{2} \sim G\left( \boldsymbol{\mu}_{1}\mathbf{+}\textbf{V}_{12}\textbf{V}_{22}^{- 1}\left( \textbf{Y}_{2} - \boldsymbol{\mu}_{2} \right),\ \textbf{V}_{11} - \textbf{V}_{12}\textbf{V}_{22}^{- 1}\textbf{V}_{12}^\prime \right)\]</span></p>
<p>This result plays an important role when predicting Gaussian random variables, for example in mixed models.</p>
<p>Notice that the variance matrix of the conditional distribution does not depend on the particular value <span class="math inline">\(\textbf{Y}_{2} = \textbf{Y}_{2}\)</span> on which the distribution is conditioned. However, the mean of the conditional distribution does depend on <span class="math inline">\(\textbf{Y}_{2}\)</span> unless <span class="math inline">\(\textbf{V}_{12} = \textbf{0}\)</span>, a condition established earlier for independence of <span class="math inline">\(\textbf{Y}_{1}\)</span> and <span class="math inline">\(\textbf{Y}_{2}\)</span>.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./biasvariance.html" class="pagination-link" aria-label="Bias Variance Tradeoff">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./estimation.html" class="pagination-link" aria-label="Parameter Estimation">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>