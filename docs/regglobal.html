<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 7&nbsp; The Classical Linear Model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./regfeature.html" rel="next">
<link href="./regintro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regintro.html">Part II. Supervised Learning I: Regression</a></li><li class="breadcrumb-item"><a href="./regglobal.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification with Random Inputs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Decision Trees</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesinR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Part VI. Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Part VII. Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Part VIII. Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Part IX. Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#simple-and-multiple-linear-regression" id="toc-simple-and-multiple-linear-regression" class="nav-link active" data-scroll-target="#simple-and-multiple-linear-regression"><span class="header-section-number">7.1</span> Simple and Multiple Linear Regression</a>
  <ul>
  <li><a href="#computing-estimates" id="toc-computing-estimates" class="nav-link" data-scroll-target="#computing-estimates">Computing Estimates</a></li>
  <li><a href="#coefficient-interpretation" id="toc-coefficient-interpretation" class="nav-link" data-scroll-target="#coefficient-interpretation">Coefficient Interpretation</a></li>
  <li><a href="#r2-the-coefficient-of-determination" id="toc-r2-the-coefficient-of-determination" class="nav-link" data-scroll-target="#r2-the-coefficient-of-determination"><span class="math inline">\(R^2\)</span>, the Coefficient of Determination</a></li>
  <li><a href="#measuring-prediction-error" id="toc-measuring-prediction-error" class="nav-link" data-scroll-target="#measuring-prediction-error">Measuring Prediction Error</a>
  <ul class="collapse">
  <li><a href="#sherman-morrison-woodbury-formula" id="toc-sherman-morrison-woodbury-formula" class="nav-link" data-scroll-target="#sherman-morrison-woodbury-formula">Sherman, Morrison, Woodbury Formula</a></li>
  <li><a href="#press-statistic" id="toc-press-statistic" class="nav-link" data-scroll-target="#press-statistic">PRESS Statistic</a></li>
  </ul></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions">Interactions</a></li>
  </ul></li>
  <li><a href="#sec-ss-reduction-test" id="toc-sec-ss-reduction-test" class="nav-link" data-scroll-target="#sec-ss-reduction-test"><span class="header-section-number">7.2</span> Hypothesis Testing</a>
  <ul>
  <li><a href="#partitioning-variability-through-sums-of-squares" id="toc-partitioning-variability-through-sums-of-squares" class="nav-link" data-scroll-target="#partitioning-variability-through-sums-of-squares">Partitioning Variability through Sums of Squares</a></li>
  <li><a href="#sequential-and-partial-sums-of-squares" id="toc-sequential-and-partial-sums-of-squares" class="nav-link" data-scroll-target="#sequential-and-partial-sums-of-squares">Sequential and Partial Sums of Squares</a></li>
  </ul></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction"><span class="header-section-number">7.3</span> Prediction</a>
  <ul>
  <li><a href="#prediction-variance" id="toc-prediction-variance" class="nav-link" data-scroll-target="#prediction-variance">Prediction Variance</a></li>
  <li><a href="#confidence-and-prediction-intervals" id="toc-confidence-and-prediction-intervals" class="nav-link" data-scroll-target="#confidence-and-prediction-intervals">Confidence and Prediction Intervals</a></li>
  </ul></li>
  <li><a href="#diagnostics" id="toc-diagnostics" class="nav-link" data-scroll-target="#diagnostics"><span class="header-section-number">7.4</span> Diagnostics</a>
  <ul>
  <li><a href="#leverage" id="toc-leverage" class="nav-link" data-scroll-target="#leverage">Leverage</a></li>
  <li><a href="#residual-diagnostics" id="toc-residual-diagnostics" class="nav-link" data-scroll-target="#residual-diagnostics">Residual Diagnostics</a>
  <ul class="collapse">
  <li><a href="#studentized-residuals" id="toc-studentized-residuals" class="nav-link" data-scroll-target="#studentized-residuals">Studentized residuals</a></li>
  <li><a href="#r-student-residuals" id="toc-r-student-residuals" class="nav-link" data-scroll-target="#r-student-residuals">R-student residuals</a></li>
  <li><a href="#partial-regression-plots" id="toc-partial-regression-plots" class="nav-link" data-scroll-target="#partial-regression-plots">Partial regression plots</a></li>
  </ul></li>
  <li><a href="#case-deletion-diagnostics" id="toc-case-deletion-diagnostics" class="nav-link" data-scroll-target="#case-deletion-diagnostics">Case Deletion Diagnostics</a>
  <ul class="collapse">
  <li><a href="#cooks-d" id="toc-cooks-d" class="nav-link" data-scroll-target="#cooks-d">Cook’s D</a></li>
  <li><a href="#dffits" id="toc-dffits" class="nav-link" data-scroll-target="#dffits">DFFITs</a></li>
  </ul></li>
  <li><a href="#sec-collinearity-diag" id="toc-sec-collinearity-diag" class="nav-link" data-scroll-target="#sec-collinearity-diag">Collinearity Diagnostics</a>
  <ul class="collapse">
  <li><a href="#variance-inflation-factors" id="toc-variance-inflation-factors" class="nav-link" data-scroll-target="#variance-inflation-factors">Variance inflation factors</a></li>
  <li><a href="#condition-index-and-condition-number" id="toc-condition-index-and-condition-number" class="nav-link" data-scroll-target="#condition-index-and-condition-number">Condition index and condition number</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regintro.html">Part II. Supervised Learning I: Regression</a></li><li class="breadcrumb-item"><a href="./regglobal.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-reg-linear" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The classical (standard) linear model is <span class="math display">\[
\textbf{Y}= \textbf{X}\boldsymbol{\beta}+ \boldsymbol{\epsilon}, \quad \quad \boldsymbol{\epsilon}\sim (\textbf{0},\sigma^2\textbf{I})
\]</span> <span class="math inline">\(\textbf{Y}\)</span> is an <span class="math inline">\((n \times 1)\)</span> vector of target values, <span class="math inline">\(\textbf{X}\)</span> is an <span class="math inline">\((n \times (p+1))\)</span> matrix of <span class="math inline">\(p\)</span> input variables plus an intercept, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is an <span class="math inline">\((n \times 1)\)</span> random vector whose elements have mean zero and the same variance <span class="math inline">\(\sigma^2\)</span>. If <span class="math inline">\(\textbf{X}\)</span> is of full rank, the OLS estimator <span class="math display">\[
\widehat{\boldsymbol{\beta}} = \left(\textbf{X}^\prime\textbf{X}^{-1}\right) \textbf{X}^\prime\textbf{Y}
\]</span> is the unbiased estimator with smallest variance. Predicted values and fitted residuals are given by</p>
<p><span class="math display">\[\begin{align*}
    \widehat{\textbf{y}} &amp;= \textbf{X}\widehat{\boldsymbol{\beta}} = \textbf{H}\textbf{y}\\
    \widehat{\boldsymbol{\epsilon}} = \textbf{y}- \widehat{\textbf{y}} &amp;= (\textbf{I}- \textbf{H})\textbf{y}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\textbf{H}\)</span> is the “Hat” matrix <span class="math inline">\(\textbf{H}= \textbf{X}(\textbf{X}^\prime\textbf{X})^{-1}\textbf{X}^\prime\)</span>.</p>
<p>The OLS estimator has variance <span class="math display">\[
\text{Var}[\widehat{\boldsymbol{\beta}}] = \sigma^2 (\textbf{X}^\prime\textbf{X})^{-1}
\]</span> We saw in <a href="linalg.html#sec-idempotent" class="quarto-xref"><span>Section 3.6</span></a> that <span class="math inline">\(\textbf{H}\)</span> is an orthogonal <strong>projection</strong> matrix–it is symmetric and idempotent. Because <span class="math inline">\(\textbf{H}\)</span> is a projection matrix, <span class="math inline">\(\textbf{I}- \textbf{H}\)</span> is also a projection matrix. To emphasize this fact we can write <span class="math inline">\(\textbf{y}\)</span> as the sum of two components <span class="math display">\[
\textbf{y}= \textbf{H}\textbf{y}+ (\textbf{I}- \textbf{H})\textbf{y}
\]</span> This identity simply states that the target variable is projected onto two spaces: the space generated by the columns of <span class="math inline">\(\textbf{X}\)</span>, represented by the projection matrix <span class="math inline">\(\textbf{H}\)</span>, and its complement (residual) space. Furthermore, these projections are orthogonal: <span class="math display">\[
\textbf{H}(\textbf{I}- \textbf{H}) = \textbf{H}- \textbf{H}\textbf{H}= \textbf{H}- \textbf{H}= \textbf{0}
\]</span></p>
<p>Projection properties are useful in establishing properties of fitted values and residuals.</p>
<p><span class="math display">\[\begin{align*}
    \textbf{X}^\prime \widehat{\boldsymbol{\epsilon}} &amp;= \textbf{0}\\
    \text{Var}[\widehat{\textbf{y}}] &amp;= \text{Var}[\textbf{H}\textbf{y}] = \sigma^2\textbf{H}\\
    \text{Var}[\widehat{y}_i] &amp;= \sigma^2 h_{ii} \\
    \text{Var}[\widehat{\boldsymbol{\epsilon}}] &amp;= \text{Var}[(\textbf{I}-\textbf{H})\textbf{y}] = \sigma^2(\textbf{I}- \textbf{H}) \\
    \text{Var}[\widehat{\epsilon}_i] &amp;= \sigma^2(1-h_{ii})
\end{align*}\]</span></p>
<p>Several interesting facts can be gleaned from these expressions.</p>
<ol type="1">
<li><p>The first result, that the fitted residuals are orthogonal to the inputs, follows from the orthogonality of least squares, but can be shown easily using projection properties <span class="math display">\[
\textbf{X}^\prime\widehat{\boldsymbol{\epsilon}} = \textbf{X}^\prime(\textbf{I}-\textbf{H})\textbf{y}= (\textbf{X}^\prime - \textbf{X}^\prime\textbf{H})\textbf{Y}= (\textbf{X}^\prime-\textbf{X}^\prime)\textbf{y}= \textbf{0}
\]</span> The orthogonality of inputs and residuals implies that for any column in <span class="math inline">\(\textbf{X}\)</span> <span class="math display">\[
\sum_{i=1}^n \, x_{ij}\widehat{\epsilon}_i = 0, \quad j=1,\cdots, p
\]</span> This is also true for the intercept column, so that <span class="math inline">\(\sum_{i=1}^n \widehat{\epsilon}_i = 0\)</span>; the fitted residuals sum to zero.</p></li>
<li><p>The variance of the <span class="math inline">\(i\)</span><sup>th</sup> fitted value depends on the irreducible variability <span class="math inline">\(\sigma^2\)</span> and on the <span class="math inline">\(i\)</span><sup>th</sup> diagonal element of the Hat matrix, <span class="math inline">\(h_{ii}\)</span>. The Hat matrix in turn depends only on the input variables. In other words, the variability of the fitted values does not depend on any of the target values in the data set.</p></li>
<li><p>The variance of a fitted residual is <span class="math inline">\(\sigma^2(1-h_{ii})\)</span>. Since <span class="math inline">\(\sigma^2\)</span> is the variance of the model errors (the irreducible variance), the variability of the residuals is smaller if <span class="math inline">\(h_{ii} &gt; 0\)</span>. The leverages <span class="math inline">\(h_{ii}\)</span> cannot be larger than 1, otherwise <span class="math inline">\(\sigma^2(1-h_{ii})\)</span> is not a valid variance. In fact, the leverages are bounded <span class="math inline">\(1/n \le h_{ii} \le 1\)</span>.</p></li>
</ol>
<section id="simple-and-multiple-linear-regression" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="simple-and-multiple-linear-regression"><span class="header-section-number">7.1</span> Simple and Multiple Linear Regression</h2>
<p>The SLR and MLR models are examples of the classical linear model. In the SLR case there is a single input variable <span class="math display">\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]</span> in the MLR case there are multiple input variables. These can be distinct variables, or transformations and/or combinations of variables. For example, <span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \epsilon_i
\]</span> has three inputs formed from the main effects of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and their interaction <span class="math inline">\(x_1x_2\)</span>. More on the use and interpretation of interactions below.</p>
<section id="computing-estimates" class="level3">
<h3 class="anchored" data-anchor-id="computing-estimates">Computing Estimates</h3>
<div class="example">
<div class="example-header">
<p>Example: Auto Data from ISLR</p>
</div>
<div class="example-container">
<p>The <code>Auto</code> data is a data set used in <span class="citation" data-cites="James2013_ISLR2">James et al. (<a href="references.html#ref-James2013_ISLR2" role="doc-biblioref">2021</a>)</span> (ISLR2). It comprises information on 397 automobiles, such as mileage (mpg), horsepower, number of cylinders, engine displacement (cu. inches), weight (lbs), etc. The following statements load the data set from DuckDB into a Pandas dataframe, drop records with missing values, and display the first observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> duckdb </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>con <span class="op">=</span> duckdb.<span class="ex">connect</span>(database<span class="op">=</span><span class="st">"ads.ddb"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>auto <span class="op">=</span> con.sql(<span class="st">"SELECT * FROM auto"</span>).df().dropna()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>con.close()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>auto.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    mpg  cylinders  displacement  ...  year  origin                       name
0  18.0          8         307.0  ...    70       1  chevrolet chevelle malibu
1  15.0          8         350.0  ...    70       1          buick skylark 320
2  18.0          8         318.0  ...    70       1         plymouth satellite
3  16.0          8         304.0  ...    70       1              amc rebel sst
4  17.0          8         302.0  ...    70       1                ford torino

[5 rows x 9 columns]</code></pre>
</div>
</div>
<p>Suppose we want to develop a model that predicts <code>mpg</code> from other variables. A multiple linear regression model with inputs <code>cylinders</code>, <code>displacement</code>, <code>weight</code> and <code>horsepower</code> is fit in Python with <code>scikit-learn</code> (<code>sklearn</code>) as follows.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>auto_x <span class="op">=</span> auto[[<span class="st">'cylinders'</span>, <span class="st">'displacement'</span>, <span class="st">'weight'</span>, <span class="st">'horsepower'</span>]]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>auto_y <span class="op">=</span> auto[<span class="st">'mpg'</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create linear regression object</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model, the intercept is added by default </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>regr.fit(auto_x, auto_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LinearRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> regr.predict(auto_x)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> auto_y <span class="op">-</span> y_hat</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The coefficients</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intercept : </span><span class="ch">\n</span><span class="st">"</span>, regr.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept : 
 45.756770522147704</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(regr.feature_names_in_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['cylinders' 'displacement' 'weight' 'horsepower']</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients of inputs: </span><span class="ch">\n</span><span class="st">"</span>, regr.coef_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficients of inputs: 
 [-3.93285431e-01  1.38893129e-04 -5.27717310e-03 -4.28124811e-02]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficient of determination: </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> regr.score(auto_x,auto_y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficient of determination: 0.71</code></pre>
</div>
</div>
<p><code>auto_x</code> and <code>auto_y</code> are the <span class="math inline">\(x\)</span>-matrix and the <span class="math inline">\(y\)</span> vector for the regression model. Note that an intercept is added automatically by the model training routine, so it is not provided in <code>auto_x</code>. After a linear regression object is created, the <code>fit</code> method is called to compute the OLS estimates. The <code>predict</code> method computes <span class="math inline">\(\widehat{\textbf{y}}\)</span> and the result is used to compute the vector of residuals, <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span>.</p>
<p>The remaining statements print the intercept, the regression coefficients for the input variables and the coefficient of determination, also known as the <span class="math inline">\(R^2\)</span> statistic.</p>
<p>The regression model fit with the preceding statements is <span class="math display">\[
\text{mpg}_i = \beta_0 + \beta_1\text{cylinders}_i+\beta_2\text{displacement}_i+\beta_3\text{weight}_i+\beta_4\text{horsepower}_i + \epsilon_{i}
\]</span> and it is assumed that <span class="math inline">\(\epsilon_i \sim iid (0,\sigma^2)\)</span>. The OLS estimates are</p>
<ul>
<li><p><span class="math inline">\(\widehat{\beta}_0 = 45.756770\)</span></p></li>
<li><p><span class="math inline">\(\widehat{\beta}_1 = -0.393285\)</span></p></li>
<li><p><span class="math inline">\(\widehat{\beta}_2 = 0.0001388\)</span></p></li>
<li><p><span class="math inline">\(\widehat{\beta}_3 = -0.005277\)</span></p></li>
<li><p><span class="math inline">\(\widehat{\beta}_4 = -0.0428124\)</span></p></li>
</ul>
<p>The predicted miles per gallon of an automobile for which the data frame is representative, is</p>
<p><span class="math display">\[\begin{align*}
\widehat{\text{mpg}} = 45.75677 &amp;-0.393285\times\text{cylinders} + 0.0001388\times\text{displacement} \\
&amp;-0.005277\times \text{weight} -0.0428124\times\text{horsepower}
\end{align*}\]</span></p>
</div>
</div>
<p><code>scikit-learn</code> computes the OLS estimates but does not provide standard errors or an estimate of <span class="math inline">\(\sigma^2\)</span>. These estimates of variability are necessary to compute <span class="math inline">\(p\)</span>-values, confidence and prediction intervals, to test hypotheses, etc. The lack of uncertainty quantification is rooted in the algorithmic modeling approach that does not appeal to an underlying stochastic data-generating mechanism.</p>
<p>The estimate of <span class="math inline">\(\sigma^2\)</span> and the standard errors of the regression coefficients can be calculated the hard way, by performing the matrix math. First, we augment the matrix of input variables with a column of 1s to reflect the intercept. The estimate of <span class="math inline">\(\sigma^2\)</span> is based on the error (residual) sum of squares <span class="math display">\[
\widehat{\sigma}^2 = \frac{\text{SSE}}{n-(p+1)} = \frac{1}{n-(p+1)}\sum_{i=1}^n\,\widehat{\epsilon}_i^2
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(auto_x)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="bu">len</span>(auto_x.columns)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.empty(shape<span class="op">=</span>(n, p<span class="op">+</span><span class="dv">1</span>), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>X1[:, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X1[:, <span class="dv">1</span>:(p<span class="op">+</span><span class="dv">1</span>)] <span class="op">=</span> auto_x</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># recompute OLS estimate</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="op">=</span> np.linalg.inv(X1.T <span class="op">@</span> X1) <span class="op">@</span> X1.T <span class="op">@</span> auto_y</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(beta_hat,<span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 4.5756771e+01 -3.9328500e-01  1.3900000e-04 -5.2770000e-03
 -4.2812000e-02]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum of squares error = residual sum of squares</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>SSE <span class="op">=</span> residuals.T <span class="op">@</span> residuals</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="op">=</span> SSE <span class="op">/</span> (n <span class="op">-</span> (p<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimate of sigma^2: </span><span class="sc">{</span>sigma2_hat<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimate of sigma^2: 17.99336781437413</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>var_beta_hat <span class="op">=</span> np.linalg.inv(X1.T <span class="op">@</span> X1) <span class="op">*</span> sigma2_hat</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p_ <span class="kw">in</span> <span class="bu">range</span>(p<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    standard_error <span class="op">=</span> np.sqrt(var_beta_hat[p_, p_])</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"beta_hat[</span><span class="sc">{</span>p_<span class="sc">}</span><span class="ss">]: </span><span class="sc">{</span><span class="bu">round</span>(beta_hat[p_],<span class="dv">6</span>)<span class="sc">}</span><span class="ss">  std. error: </span><span class="sc">{</span><span class="bu">round</span>(standard_error,<span class="dv">6</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta_hat[0]: 45.756771  std. error: 1.520044
beta_hat[1]: -0.393285  std. error: 0.409552
beta_hat[2]: 0.000139  std. error: 0.00901
beta_hat[3]: -0.005277  std. error: 0.000717
beta_hat[4]: -0.042812  std. error: 0.01287</code></pre>
</div>
</div>
<p>Using <code>R</code>, working with linear regression models is much simpler. Most <code>R</code> functions for statistical modeling support a formula expression to specify models directly based on information in data frames. You do not have to set up separate objects for <span class="math inline">\(\textbf{X}\)</span> and <span class="math inline">\(\textbf{y}\)</span>. Also, <code>R</code> provides standard errors, <span class="math inline">\(t\)</span>-statistics, <span class="math inline">\(p\)</span>-values, and other estimates by default, and has default methods for handling missing values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Auto)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  mpg cylinders displacement horsepower weight acceleration year origin
1  18         8          307        130   3504         12.0   70      1
2  15         8          350        165   3693         11.5   70      1
3  18         8          318        150   3436         11.0   70      1
4  16         8          304        150   3433         12.0   70      1
5  17         8          302        140   3449         10.5   70      1
6  15         8          429        198   4341         10.0   70      1
                       name
1 chevrolet chevelle malibu
2         buick skylark 320
3        plymouth satellite
4             amc rebel sst
5               ford torino
6          ford galaxie 500</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>linreg <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>Auto)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ cylinders + displacement + weight + horsepower, 
    data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.5248  -2.7964  -0.3568   2.2577  16.3221 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  45.7567705  1.5200437  30.102  &lt; 2e-16 ***
cylinders    -0.3932854  0.4095522  -0.960 0.337513    
displacement  0.0001389  0.0090099   0.015 0.987709    
weight       -0.0052772  0.0007166  -7.364 1.08e-12 ***
horsepower   -0.0428125  0.0128699  -3.327 0.000963 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.242 on 387 degrees of freedom
Multiple R-squared:  0.7077,    Adjusted R-squared:  0.7046 
F-statistic: 234.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Estimate</code> column of the <code>lm</code> summary reports the <span class="math inline">\(\widehat{\beta}_k\)</span> estimates, the <code>Std. Error</code> column reports their standard errors. These values match the Python computations above.</p>
<p>The residual standard error of 4.241859 is <span class="math inline">\(\widehat{\sigma}\)</span>, the square root of the estimator <span class="math inline">\(\widehat{\sigma}^2\)</span> derived above.</p>
</section>
<section id="coefficient-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="coefficient-interpretation">Coefficient Interpretation</h3>
<p>How do we interpret the regression coefficients of the fitted model</p>
<p><span class="math display">\[\begin{align*}
\widehat{\text{mpg}} = 45.75677 &amp;-0.393285\times\text{cylinders} + 0.0001388\times\text{displacement} \\
&amp;-0.005277\times \text{weight} -0.0428124\times\text{horsepower}
\end{align*}\]</span></p>
<p>Since the model is linear, it is tempting to state that, for example, a change in 1 unit of <code>displacement</code> causes a change of 0.0001388 in miles per gallon. This interpretation is not correct, because</p>
<ol type="1">
<li><p>We cannot conclude causality between inputs and the target variable. The data are purely observational so we can at best state that changes in the input variables are <strong>associated</strong> with different predicted values for miles per gallon.</p></li>
<li><p>We cannot interpret one input variable in the absence of the others. The signs of the regression coefficients are somewhat counter-intuitive. Why would mileage go down for cars with more cylinders but go up with greater displacement. Does adding cylinders not imply a larger engine displacement? The point is that the inputs are related to each other, they do not vary freely from each other. When we interpret the magnitude of a regression coefficient in terms of the change in the target variable that corresponds to a unit change in the input variable, we are implicitly holding all other predictors fixed (ceteris paribus).</p></li>
</ol>
<p>The correct interpretation of the displacement coefficient is thus <em>when displacement increases by one cubic inch and all other attributes remain constant, the expected mileage increases by 0.0001388 miles per gallon</em>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <em>all-other-variables-held-fixed</em> interpretation is also important when interpreting hypothesis test results. The <span class="math inline">\(p\)</span>-values of variables <code>cylinders</code> and <code>displacement</code> are 0.33 and 0.98, respectively, suggesting that these variables do not make significant contributions toward explaining miles per gallon. These <span class="math inline">\(p\)</span>-values are interpreted in the presence of the other variables in the model. The correct interpretation is that the number of cylinders is not a significant predictor of miles per gallon in a model that accounts for engine displacement, vehicle weight, and horsepower.</p>
</div>
</div>
</section>
<section id="r2-the-coefficient-of-determination" class="level3">
<h3 class="anchored" data-anchor-id="r2-the-coefficient-of-determination"><span class="math inline">\(R^2\)</span>, the Coefficient of Determination</h3>
<p>The variability in the target <span class="math inline">\(\textbf{y}\)</span>, not accounting for any information provided by the input variables can be estimated as <span class="math display">\[
    s^2 = \frac{1}{n-1}\sum_{i=1}^n (y_i - \overline{y})^2
\]</span> If the <span class="math inline">\(y_i\)</span> had the same mean, this would be an unbiased estimator of <span class="math inline">\(\text{Var}[Y]\)</span>. However, the regression model states very clearly that the mean of <span class="math inline">\(Y\)</span> is a function of the <span class="math inline">\(x\)</span>-inputs. This estimator is then a biased estimator of <span class="math inline">\(\text{Var}[Y] = \sigma^2\)</span>. The numerator of <span class="math inline">\(s^2\)</span> is called the <strong>total sum of squares</strong> (SST). If SST captures variability of <span class="math inline">\(Y\)</span> about a constant mean, how much of this is attributable to the input variables? To answer this we can look at the variability <strong>not</strong> attributable to the <span class="math inline">\(x\)</span>s, the error sum of squares <span class="math display">\[
\text{SSE} = \sum_{i=1}^n \widehat{\epsilon}_i = \sum_{i=1}^n (y_i - \widehat{y}_i)^2
\]</span> The ratio <span class="math display">\[
R^2 = \frac{\text{SST}-\text{SSE}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}
\]</span> is known as the <strong>coefficient of determination</strong> or <strong>R-square</strong>. The name R-square comes from a simple relationship between <span class="math inline">\(R^2\)</span> and the Pearson correlation coefficient in the SLR case. If <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)</span>, then <span class="math inline">\(R^2\)</span> is the square of the correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<div class="example">
<div class="example-header">
<p>Example: Auto Data (Cont’d)</p>
</div>
<div class="example-container">
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>slr <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr)<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6059483</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Auto<span class="sc">$</span>mpg, Auto<span class="sc">$</span>horsepower)<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6059483</code></pre>
</div>
</div>
<p>The squared correlation coefficient between <code>mpg</code> and <code>horsepower</code> is the same as the <span class="math inline">\(R^2\)</span> statistic reported by <code>lm</code>.</p>
<p>You can compute the correlation coefficient between the two variables from simple linear regression output, but you need to take into account the sign of the regression coefficient. The correlation coefficient has the same sign as <span class="math inline">\(\beta_1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Auto<span class="sc">$</span>mpg, Auto<span class="sc">$</span>horsepower)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.7784268</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.numeric</span>(<span class="fu">sign</span>(slr<span class="sc">$</span>coefficients[<span class="dv">2</span>])) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">summary</span>(slr)<span class="sc">$</span>r.squared)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.7784268</code></pre>
</div>
</div>
</div>
</div>
<p><span class="math inline">\(R^2\)</span> ranges between 0 and 1; it achieves the lower bound <span class="math inline">\(R^2=0\)</span> if SSE = SST, the input variables do not explain any variability in <span class="math inline">\(Y\)</span>. <span class="math inline">\(R^2 = 1\)</span> results when SSE = 0, the model fits the data “perfectly”, it interpolates the <span class="math inline">\(y_i\)</span>.</p>
<p>The straightforward interpretation of <span class="math inline">\(R^2\)</span> as the proportion of variability explained by the input variables unfortunately can lead one to chase models that have a high <span class="math inline">\(R^2\)</span>. This is a terrible practice for a number of reasons</p>
<ul>
<li>The value of <span class="math inline">\(R^2\)</span> does not tell us whether the model assumptions are met. You can explain a substantial amount of variability in the data with a seriously deficient model. For example, the four regressor model fit to the Auto data earlier has <span class="math inline">\(R^2 = 0.71\)</span>, explaining 71% of the variability in miles per gallon. A look at the residuals from that model shows that there is substantial trends in the residuals (<a href="#fig-auto-residuals" class="quarto-xref">Figure&nbsp;<span>7.1</span></a>). Larger fitted values have larger variability and there is a definite trend in the residuals; the model is not (yet) correct.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-auto-residuals" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center" width="75%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-auto-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="regglobal_files/figure-html/fig-auto-residuals-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="regglobal_files/figure-html/fig-auto-residuals-1.png" id="fig-auto-residuals" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:75.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-auto-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here is an example with simulated data where the mean function is not a straight line and the error variance depends on <span class="math inline">\(x\)</span>. A simple linear regression model is clearly not appropriate, but it explains more than 75% of the variability in <span class="math inline">\(y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.15</span>, <span class="dv">1</span>, <span class="at">l =</span> <span class="dv">100</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123456</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.25</span> <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.25</span> <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">4</span> <span class="sc">*</span> pi <span class="sc">*</span> x)) <span class="sc">+</span> eps</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>slr <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">type=</span><span class="st">"p"</span>,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(slr<span class="sc">$</span>coefficients)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr)<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7694696</code></pre>
</div>
</div>
<ul>
<li><p><span class="math inline">\(R^2\)</span> is a function of SSE, the prediction error on the training data set. This can be made arbitrarily small by adding input variables. If <span class="math inline">\(R^2_{\text{cur}}\)</span> is the coefficient of determination in a linear regression model, and you add a new predictor <span class="math inline">\(x_{p+1}\)</span>, then <span class="math display">\[
R^2_{\text{new}} \ge R^2_{\text{cur}}
\]</span> Predictors that make no relevant contribution will increase <span class="math inline">\(R^2\)</span> because their addition reduces SSE.</p></li>
<li><p>A model that interpolates the data (fits “perfectly”) is not a good model. It is certainly not perfect if the goal is to build a model that generalizes well to unseen observations. Different metrics are needed to develop models that generalize and do not overfit the data. Chasing <span class="math inline">\(R^2\)</span> values invariably leads to <strong>overfitting</strong> and models that memorize too much of the training observations to perform well on new data.</p></li>
</ul>
</section>
<section id="measuring-prediction-error" class="level3">
<h3 class="anchored" data-anchor-id="measuring-prediction-error">Measuring Prediction Error</h3>
<p>Since we can make SSE on the training data arbitrarily small by adding more input variables, the MSPE on the training data is not a good metric if we want a model that performs well in predicting new observations. Instead of MSE<sub>Tr</sub> the test error MSE<sub>Te</sub> should be used. As discussed in <a href="biasvariance.html#sec-train-test-validate" class="quarto-xref"><span>Section 2.5</span></a>, the test error can be estimated by holding out some observations from training in a test data set or by cross-validation (<a href="biasvariance.html#sec-cross-validation" class="quarto-xref"><span>Section 2.6</span></a>).</p>
<p>In the classical linear model leave-one-out cross-validation is particularly appealing. In addition to not depending on any random selection of data points as test data sets or <span class="math inline">\(k\)</span>-fold cross-validation do, LOOCV error can be calculated in the linear model without re-fitting the model <span class="math inline">\(n\)</span> times. All the necessary pieces to compute the LOOCV error can be assembled on the same pass through the data that calculates the OLS estimates. The key is the Sherman, Morrison, Woodbury formula.</p>
<section id="sherman-morrison-woodbury-formula" class="level4">
<h4 class="anchored" data-anchor-id="sherman-morrison-woodbury-formula">Sherman, Morrison, Woodbury Formula</h4>
<p>This remarkable formula is at the heart of many regression-type diagnostics and cross-validation techniques. A version of this formula was first given by Gauss in 1821. Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.</p>
<p>Suppose we are in a full-rank linear modeling context with design matrix <span class="math inline">\(\textbf{X}_{(n \times p + 1)}\)</span>, so that the inverse <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\)</span> exists. In diagnosing the quality of a model, we are interested in measuring the prediction error for the <span class="math inline">\(i\)</span><sup>th</sup> observation as if the data point had not contributed to the analysis. This is an example of a leave-one-out estimate: remove an observation from the data, redo the analysis, and measure how well the quantity of interest can be computed for the withheld observation.</p>
<p>If you do this in turn for all <span class="math inline">\(n\)</span> observations, you must fit the model <span class="math inline">\(n + 1\)</span> times, an overall fit to the training data with <span class="math inline">\(n\)</span> observations, and <span class="math inline">\(n\)</span> additional fits with training data sets of size <span class="math inline">\(n - 1\)</span>, leaving out each observation in turn. The computationally expensive part of fitting the linear model is building the cross-product matrix <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span> and computing its inverse <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\)</span>.</p>
<p>The Sherman-Morrison-Woodbury formula allows us to compute the inverse of the cross-product matrix based on <span class="math inline">\(\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}\)</span> as if the <span class="math inline">\(i\)</span><sup>th</sup> observation had been removed.</p>
<p>Denote as <span class="math inline">\(\textbf{X}_{-i}\)</span> the design matrix with the <span class="math inline">\(i\)</span><sup>th</sup> observation removed. Then</p>
<p><span class="math display">\[\left( \textbf{X}_{-i}^\prime\textbf{X}_{-i} \right)^{- 1} = \left( \textbf{X}^\prime\textbf{X}- \textbf{x}_{i}\textbf{x}_{i}^{\prime} \right)^{-1}\  = \left( \textbf{X}^\prime\textbf{X}\right)^{- 1} + \frac{\left( \textbf{X}^\prime\textbf{X}\right)^{-1}{\textbf{x}_{i}\textbf{x}_{i}^{\prime}\left( \textbf{X}^\prime\textbf{X}\right)}^{- 1}}{1 - \textbf{x}_{i}^{\prime}\left( \textbf{X}^\prime\textbf{X}\right)^{-1}\textbf{x}_{i}}\]</span></p>
<p>The quantities on the right hand side are available in the standard regression calculations based on <span class="math inline">\(n\)</span> data points. Because of this remarkable result, leave-one-out statistics can be calculated easily—without retraining any models—based on the fit to the full training data alone.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that the quantity in the denominator of the right-hand side is the diagonal value of <span class="math inline">\(\textbf{I}- \textbf{H}\)</span>, where <span class="math inline">\(\textbf{H}\)</span> is the hat matrix. If <span class="math inline">\(h_{ii}\)</span> denotes the diagonal values of <span class="math inline">\(\textbf{H}\)</span>, we can write the update formula as</p>
<p><span class="math display">\[\left( \textbf{X}^\prime\textbf{X}\right)^{- 1} + \frac{\left( \textbf{X}^\prime\textbf{X}\right)^{- 1}{\textbf{x}_{i}\textbf{x}_{i}^{\prime}\left( \textbf{X}^\prime\textbf{X}\right)}^{- 1}}{1 - h_{ii}}\]</span></p>
<p>The <strong>leverage</strong> values <span class="math inline">\(h_{ii}\)</span> play an important role in the computation of residual, influence, and case-deletion diagnostics in linear models.</p>
</div>
</div>
</section>
<section id="press-statistic" class="level4">
<h4 class="anchored" data-anchor-id="press-statistic">PRESS Statistic</h4>
<p>If we denote the predicted value of <span class="math inline">\(y_i\)</span>, obtained in a regression without the <span class="math inline">\(i\)</span><sup>th</sup> observation, as <span class="math inline">\(\widehat{y}_{-i}\)</span>, then the leave-one-out residual <span class="math display">\[
y_i - \widehat{y}_{-i}
\]</span> is the test error for the <span class="math inline">\(i\)</span><sup>th</sup> observation. Using the Sherman-Morrison-Woodbury result, it is a neat exercise to show that this is simply <span class="math display">\[
y_i - \widehat{y}_{-i} = \frac{y_i - \widehat{y}_i}{1-h_{ii}}
\]</span> The leave-one-out error for the <span class="math inline">\(i\)</span><sup>th</sup> observation is obtained by dividing the <span class="math inline">\(i\)</span><sup>th</sup> residual by one minus the leverage value. When these deviations are squared and summed across the entire data set the PRESS statistic results <span class="math display">\[
\text{PRESS} = \sum_{i=1}^n (y_i - \widehat{y}_{-i})^2 = \sum_{i=1}^n \left(\frac{y_i - \widehat{y}_i}{1-h_{ii}}\right)^2
\]</span> The name is derived from <strong>prediction sum of squares</strong>. The average PRESS value estimates the mean square test error <span class="math display">\[
\text{MSE}_{Te} = \frac{1}{n}\text{PRESS}
\]</span></p>
<div class="example">
<div class="example-header">
<p>Example: Auto Data (Cont’d)</p>
</div>
<div class="example-container">
<p>For the four regressor model the PRESS statistic and the MSE<sub>Te</sub> can be calculated by extracting the leverage values</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>mlr <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>Auto)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>leverage <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(mlr)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>PRESS_res <span class="ot">&lt;-</span> mlr<span class="sc">$</span>residuals <span class="sc">/</span> (<span class="dv">1</span><span class="sc">-</span>leverage)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>PRESS <span class="ot">&lt;-</span> <span class="fu">sum</span>(PRESS_res<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>MSE_Te  <span class="ot">&lt;-</span> PRESS<span class="sc">/</span><span class="fu">length</span>(leverage)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"PRESS statistic: "</span>, PRESS, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PRESS statistic:  7138.559 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE Test based on LOOCV: "</span>, MSE_Te,<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE Test based on LOOCV:  18.21061 </code></pre>
</div>
</div>
<p>You can validate this result the hard way by fitting <span class="math inline">\(n\)</span> separate regression models, leaving one observation out each time and predicting that observation to obtain the PRESS residual.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>PRESS <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Auto)) {</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower, </span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">data=</span>Auto[<span class="sc">-</span>i,])</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    yhat_minus_i <span class="ot">&lt;-</span> <span class="fu">predict</span>(m,<span class="at">newdata=</span>Auto[i,])</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    PRESS <span class="ot">&lt;-</span> PRESS <span class="sc">+</span> (Auto[i,<span class="st">"mpg"</span>] <span class="sc">-</span> yhat_minus_i)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE Test based on LOOCV: "</span>, PRESS<span class="sc">/</span><span class="fu">nrow</span>(Auto))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE Test based on LOOCV:  18.21061</code></pre>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="interactions" class="level3">
<h3 class="anchored" data-anchor-id="interactions">Interactions</h3>
<p>What is the difference between the following models</p>
<p><span class="math display">\[\begin{align*}
Y &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\\
Y &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon
\end{align*}\]</span></p>
<p>Both models depend on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. In the first case the variables enter the model as <strong>main effects</strong>, that is, by themselves. Each input variable is allowed to make its contribution on the outcome given the presence of the other. The second model contains the additional <strong>interaction</strong> term <span class="math inline">\(x_1 x_2\)</span>. To be more precise, this is a <strong>two-way</strong> interaction term because it involves two input variables. A three-way interaction term would be <span class="math inline">\(x_1 x_2 x_3\)</span>.</p>
<p>Suppose that <span class="math inline">\(\beta_3\)</span> is not zero, how should we interpret the presence of an interaction term in the model? We can no longer state that <span class="math inline">\(\beta_1\)</span> measures the effect on the target variable when <span class="math inline">\(x_1\)</span> is changed by one unit. The effect of changing <span class="math inline">\(x_1\)</span> by one unit in the second model is now a function of <span class="math inline">\(x_2\)</span>. To see this, consider the mean of <span class="math inline">\(Y\)</span> at two points, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_1+1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
    \text{E}[Y | x_1] &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 \\
    \text{E}[Y | x_1 + 1] &amp;= \beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \beta_3 (x_1+1)x_2
\end{align*}\]</span></p>
<p>The difference between the two is <span class="math display">\[
\text{E}[Y | x_1 + 1] - \text{E}[Y | x_1] = \beta_1 + \beta_3 x_2
\]</span> The effect of <span class="math inline">\(x_1\)</span> is now a function of <span class="math inline">\(x_2\)</span>. That is the very meaning of an interaction. The effect of one variable (or factor) depends on another variable (or factor) and vice versa. In the example the effect of <span class="math inline">\(x_1\)</span> is a linear regression in <span class="math inline">\(x_2\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>When building models with interactions, it is customary to include lower-order effects in the model if higher-order effects are significant. For example, if <span class="math inline">\(x_1 x_2\)</span> is in the model one includes the main effects <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> regardless of their significance. Similarly, if a three-way interaction is significant one includes the two-way interactions and the main effects in the model. The argument for doing so is that in order for two things to interact they must be present–otherwise, what interacts?</p>
</div>
</div>
<p>The presence/absence of an interaction between a categorical input variable and a numeric input variable can be seen by comparing the trends as in <a href="#fig-interactions" class="quarto-xref">Figure&nbsp;<span>7.2</span></a>. With a two-level categorical factor, representing for example, a treatment and a placebo, the absence of interactions manifests itself in parallel lines. The effect of the treatment is the distance between the two lines and is the same for all values of <span class="math inline">\(x_1\)</span>. Similarly, the effect of <span class="math inline">\(x_1\)</span>, the slope of the line, is the same for both groups. In the presence of an interaction the slopes are not the same and the distance between the lines (the effect of <span class="math inline">\(x_2\)</span>) depends on the value for <span class="math inline">\(x_1\)</span>.</p>
<div id="fig-interactions" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interactions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/Interactions.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" data-glightbox="description: .lightbox-desc-2"><img src="images/Interactions.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interactions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Models with a categorical and a continuous input with and without interactions.
</figcaption>
</figure>
</div>
<div class="example">
<div class="example-header">
<p>Example: Auto Data (Cont’d)</p>
</div>
<div class="example-container">
<p>We are now considering a series of model for the <code>Auto</code> data, based on the same four input variables used earlier.</p>
<p>The first four models add inputs and also two-way interactions of all inputs in the model. The formula expression <code>y ~ (x1 + x2 + x3)^2</code> is a shorthand for including the main effects and two-way interactions of the three inputs. <code>R</code> displays the interaction terms as <code>x1:x2</code>, <code>x1:x3</code>, and <code>x2:x3</code> in the output.</p>
<p>Models 5 and 6 then add up to three-way and four-way interactions, respectively. For each model we calculate the number of non-zero coefficients (the rank of <span class="math inline">\(\textbf{X}\)</span>), SSE, <span class="math inline">\(R^2\)</span> and the PRESS statistic.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>calcPress <span class="ot">&lt;-</span> <span class="cf">function</span>(linModel) {</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    leverage <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(linModel)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> linModel<span class="sc">$</span>residuals</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    Press_res <span class="ot">&lt;-</span> r<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>leverage)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    Press <span class="ot">&lt;-</span> <span class="fu">sum</span>(Press_res<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">"ncoef"</span><span class="ot">=</span>linModel<span class="sc">$</span>rank,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>                <span class="st">"R2"</span>   <span class="ot">=</span><span class="fu">summary</span>(linModel)<span class="sc">$</span>r.squared, </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">"SSE"</span>  <span class="ot">=</span><span class="fu">sum</span>(r<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Press"</span><span class="ot">=</span>Press))</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>l1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders, <span class="at">data=</span>Auto)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>l2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> (cylinders <span class="sc">+</span> displacement)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>Auto )</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>l3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> (cylinders <span class="sc">+</span> displacement <span class="sc">+</span> horsepower)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>Auto )</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>l4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> (cylinders <span class="sc">+</span> displacement <span class="sc">+</span> horsepower <span class="sc">+</span> weight)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>Auto )</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>l5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> (cylinders <span class="sc">+</span> displacement <span class="sc">+</span> horsepower <span class="sc">+</span> weight)<span class="sc">^</span><span class="dv">3</span>, <span class="at">data=</span>Auto )</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>l6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> (cylinders <span class="sc">+</span> displacement <span class="sc">+</span> horsepower <span class="sc">+</span> weight)<span class="sc">^</span><span class="dv">4</span>, <span class="at">data=</span>Auto )</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l1)), </span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l2)),</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l3)),</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l4)),</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l5)),</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.data.frame</span>(<span class="fu">calcPress</span>(l6))</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(df,<span class="at">format=</span><span class="st">"simple"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">ncoef</th>
<th style="text-align: right;">R2</th>
<th style="text-align: right;">SSE</th>
<th style="text-align: right;">Press</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.6046890</td>
<td style="text-align: right;">9415.910</td>
<td style="text-align: right;">9505.534</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: right;">0.6769104</td>
<td style="text-align: right;">7695.670</td>
<td style="text-align: right;">7846.982</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: right;">0.7497691</td>
<td style="text-align: right;">5960.247</td>
<td style="text-align: right;">6197.767</td>
</tr>
<tr class="even">
<td style="text-align: right;">11</td>
<td style="text-align: right;">0.7607536</td>
<td style="text-align: right;">5698.608</td>
<td style="text-align: right;">6089.687</td>
</tr>
<tr class="odd">
<td style="text-align: right;">15</td>
<td style="text-align: right;">0.7752334</td>
<td style="text-align: right;">5353.714</td>
<td style="text-align: right;">5818.953</td>
</tr>
<tr class="even">
<td style="text-align: right;">16</td>
<td style="text-align: right;">0.7752893</td>
<td style="text-align: right;">5352.382</td>
<td style="text-align: right;">5861.117</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The model complexity increases from the first to the sixth model; the models have more parameters and more intricate interaction terms. The SSE values decrease as terms are added to the model, and <span class="math inline">\(R^2\)</span> increases accordingly. The PRESS statistic is always larger than the SSE, which makes sense because it is based on squaring the Press residuals <span class="math inline">\((y_i - \widehat{y}_i)/(1-h_{ii})\)</span> which are larger than the ordinary residuals <span class="math inline">\(y_i - \widehat{y}_i\)</span>.</p>
<p>From the fifth to the sixth model only one additional parameter is added to the model, the four-way interaction of all inputs. <span class="math inline">\(R^2\)</span> barely increases but the PRESS statistic increases compared to the model with only three-way interaction. Interestingly, none of the effects in the four-way model are significant given the presence of other terms in the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(l6)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ (cylinders + displacement + horsepower + weight)^4, 
    data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-12.0497  -1.9574  -0.2334   1.8091  18.8569 

Coefficients:
                                           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                               6.104e+01  7.744e+01   0.788    0.431
cylinders                                -4.974e+00  1.465e+01  -0.339    0.734
displacement                             -4.338e-01  5.655e-01  -0.767    0.444
horsepower                               -6.459e-01  7.861e-01  -0.822    0.412
weight                                    2.735e-02  2.623e-02   1.042    0.298
cylinders:displacement                    4.737e-02  7.861e-02   0.603    0.547
cylinders:horsepower                      1.069e-01  1.324e-01   0.808    0.420
cylinders:weight                         -2.757e-03  4.565e-03  -0.604    0.546
displacement:horsepower                   8.090e-03  6.253e-03   1.294    0.197
displacement:weight                      -7.045e-05  1.775e-04  -0.397    0.692
horsepower:weight                        -2.047e-04  2.689e-04  -0.761    0.447
cylinders:displacement:horsepower        -1.041e-03  8.238e-04  -1.263    0.207
cylinders:displacement:weight             8.719e-06  2.422e-05   0.360    0.719
cylinders:horsepower:weight               1.351e-05  4.130e-05   0.327    0.744
displacement:horsepower:weight           -4.645e-07  1.920e-06  -0.242    0.809
cylinders:displacement:horsepower:weight  7.702e-08  2.517e-07   0.306    0.760

Residual standard error: 3.773 on 376 degrees of freedom
Multiple R-squared:  0.7753,    Adjusted R-squared:  0.7663 
F-statistic: 86.48 on 15 and 376 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="sec-ss-reduction-test" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="sec-ss-reduction-test"><span class="header-section-number">7.2</span> Hypothesis Testing</h2>
<p>When testing hypothesis in statistical models it is useful to think of the hypothesis as imposing a <strong>constraint</strong> on the model. The test then boils down to comparing a constrained and an unconstrained model in such a way that we can make probability statements about the validity of the constraint. If it is highly unlikely that the constraint holds, we reject the hypothesis.</p>
<p>This principle applies to hypothesis testing in many model families, what differs is how the impact of the constraint on the model is measured. In least-squares estimation we look at how a sum of squares changes as the constraint is imposed. In models fit by maximum likelihood we measure how much the log likelihood changes when the constraint is imposed.</p>
<p>Suppose we have a model with four predictors, <span class="math display">\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
\]</span> and want to test the hypothesis that the absence of <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> does not make the model worse. The constraint we impose on the model is <span class="math display">\[
H: \beta_3 = \beta_4 = 0
\]</span> This is a hypothesis with two degrees of freedom, since two parameters of the model are constrained simultaneously.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can usually figure out the degrees of freedom in a hypothesis by counting equal signs.</p>
</div>
</div>
<p>The unconstrained and constrained models are also called the <strong>full</strong> and the <strong>reduced</strong> models, respectively. In this case the full model is <span class="math display">\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon
\]</span> and the reduced model is <span class="math display">\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \epsilon
\]</span> :::{.definition} ::::{.definition-header} Definition: Nested Models :::: ::::{.definition-container} Two models are said to be <strong>nested</strong> when one model can be derived from the other by imposing constraints on the model parameters. The full and reduced models in the hypothesis testing context are nested models. On the contrary, if two models are not nested, the hypothesis testing framework described here does not apply. :::: :::</p>
<p>If based on the data the hypothesis cannot be rejected, we conclude that the full model is not significantly improved over the reduced model. The strength of evidence in favor of the hypothesis depends on how much variability is accounted for in the model when the constraint is relaxed, relative to the overall variability in the system. To measure that we need to introduce the idea of partitioning sums of squares.</p>
<section id="partitioning-variability-through-sums-of-squares" class="level3">
<h3 class="anchored" data-anchor-id="partitioning-variability-through-sums-of-squares">Partitioning Variability through Sums of Squares</h3>
<p>The difference between the total sum of squares, <span class="math inline">\(\text{SST} = \sum_{i=1}^n(y_i - \overline{y})^2\)</span>, which does not depend on the inputs, and the error sum of squares <span class="math inline">\(\text{SSE} = \sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>, is the <strong>model</strong> sum of squares <span class="math display">\[
\text{SSM} = \sum_{i=1}^n\left(\widehat{y}_i-\overline{y}\right)^2 = \widehat{\boldsymbol{\beta}}\textbf{X}^\prime\textbf{y}- n\overline{y}^2
\]</span></p>
<p>The total and model sums of squares are also called the <strong>corrected</strong> total and model sums of squares because they adjust for an overall estimate of the mean of <span class="math inline">\(Y\)</span> if there are no inputs. In other words, they account for the intercept <span class="math inline">\(\beta_0\)</span> in the model; <span class="math inline">\(\overline{y}\)</span> is the estimate of <span class="math inline">\(\beta_0\)</span> in an intercept-only model.</p>
<p>Another way of looking at SSM is as a measure of the combined contribution of <span class="math inline">\(\beta_1, \ldots, \beta_p\)</span> beyond the intercept. The notation <span class="math display">\[
\text{SSM} = SS(\beta_1, \cdots,\beta_p | \beta_0)
\]</span> makes this explicit. We can now think of other sum of squares, for example, <span class="math inline">\(SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2)\)</span> is the sum of squares contribution when <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_4\)</span> are added to a model that contains <span class="math inline">\(\beta_0, \beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span>. Algebraically, <span class="math display">\[
SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) = SS(\beta_1, \beta_2, \beta_3, \beta_4 | \beta_0) - SS(\beta_1, \beta_2 | \beta_0)
\]</span> This will be one part of measuring the strength of hypothesis <span class="math inline">\(H: \beta_3 = \beta_4 = 0\)</span>, the change in the model sum of squares between the full model with four inputs and the reduced model with two inputs However, there has to be more to it. If the data are very noisy, this change will have to be large to convince us of evidence against the hypothesis. If the data have small error variability, a smaller change in model sums of squares will suffice. This leads to considering the following test statistic: <span class="math display">\[
F_{obs} = \frac{SS(\beta_3, \beta_4 | \beta_0, \beta_1, \beta_2) / 2}{\widehat{\sigma}^2}
\]</span> Notice that the sum of squares in the numerator is divided by the degrees of freedom of the hypothesis. To find an estimator for the variance in the denominator we rely on SSE in the larger of the two models, the unconstrained model, because it is more likely to be an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="definition">
<div class="definition-header">
<p>Definition: Sum of Squares Reduction Test</p>
</div>
<div class="definition-container">
<p>Suppose that <span class="math inline">\(\text{SSE}_f\)</span> and <span class="math inline">\(\text{SSE}_r\)</span> are the error sum of squares in a full and reduced model where the reduced model is defined by a constraint <span class="math inline">\(H\)</span> with <span class="math inline">\(q\)</span> degrees of freedom imposed on the full model. The statistic <span class="math display">\[
F_{obs} = \frac{(\text{SSE}_r - \text{SSE}_f)/q}{\text{SSE}_f/\text{dfE}_f}
\]</span> follows an F distribution with <span class="math inline">\(q\)</span> numerator and <span class="math inline">\(\text{dfE}_f\)</span> denominator degrees of freedom if the model errors follow a Gaussian distribution, <span class="math inline">\(\boldsymbol{\epsilon}\sim G(\textbf{0},\sigma^2\textbf{I})\)</span>. <span class="math inline">\(\text{dfE}_f\)</span> are the degrees of freedom associated with SSE in the full model, <span class="math inline">\(n-r(\textbf{X})_f\)</span>.</p>
</div>
</div>
<p>The sum of squares reduction test is very general and a very powerful tool to answer questions about the parameters in a linear model. However, in order to use any hypothesis testing framework that works with probability statements, a distributional assumption is required. We can always calculate <span class="math inline">\(F_{obs}\)</span> between two models. Computing <span class="math inline">\(p\)</span>-values, that is, the probability <span class="math inline">\(\Pr(F_{q,\text{dfE}_f} &gt; F_{obs})\)</span>, is only valid if the data are normally distributed.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you reject the hypothesis <span class="math inline">\(H\)</span> based on a small <span class="math inline">\(p\)</span>-value, you do not conclude that the full model is the <em>correct</em> model. You can say that there is significant evidence that the constraint can be relaxed. You might have compared a really bad model and a bad model.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: Auto Data (Cont’d)</p>
</div>
<div class="example-container">
<p>To test the hypothesis that the coefficient for <code>weight</code> and <code>horsepower</code> are simultaneously zero in a model that accounts for <code>cylinders</code> and <code>displacement</code>, we can use the sum of squares reduction test.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>lm_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>lm_red  <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement                      , <span class="at">data=</span>Auto)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>SSE_full <span class="ot">&lt;-</span> <span class="fu">sum</span>(lm_full<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>SSE_red  <span class="ot">&lt;-</span> <span class="fu">sum</span>(lm_red<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>q <span class="ot">&lt;-</span> lm_full<span class="sc">$</span>rank <span class="sc">-</span> lm_red<span class="sc">$</span>rank</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> SSE_full <span class="sc">/</span> (lm_full<span class="sc">$</span>df.residual)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>Fobs <span class="ot">&lt;-</span> ((SSE_red<span class="sc">-</span>SSE_full)<span class="sc">/</span>q)<span class="sc">/</span>sigma2_hat</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>pvalue <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pf</span>(Fobs,q,lm_full<span class="sc">$</span>df.residual)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SSE_r: "</span>, SSE_red, <span class="st">"SSE_f: "</span>, SSE_full, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SSE_r:  8342.566 SSE_f:  6963.433 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"sigma2_hat: "</span>, sigma2_hat, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>sigma2_hat:  17.99337 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Fobs: "</span>, Fobs, <span class="st">"Pr(F &gt; Fobs): "</span>, pvalue)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fobs:  38.32337 Pr(F &gt; Fobs):  6.661338e-16</code></pre>
</div>
</div>
<p>Removing <code>weight</code> and <code>horsepower</code> from the four-predictor model increases the error sum of squares from 6963.43334 to 8342.56637. The F statistic for this reduction test is <span class="math inline">\(F_{obs} =\)</span> 38.3234 and the <span class="math inline">\(p\)</span>-value is very small (6.6613381^{-16}). We reject the hypothesis, the two-predictor model is not a sufficient model to explain mileage compared to the four-predictor model.</p>
<p>You can get to this result more quickly by using the <code>anova</code> function in <code>R</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_red, lm_full)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ cylinders + displacement
Model 2: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    389 8342.6                                  
2    387 6963.4  2    1379.1 38.323 6.529e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="sequential-and-partial-sums-of-squares" class="level3">
<h3 class="anchored" data-anchor-id="sequential-and-partial-sums-of-squares">Sequential and Partial Sums of Squares</h3>
<p>The sum of squares reduction test is helpful to understand the difference between two important special types of sum of squares, <strong>sequential</strong> and <strong>partial</strong> ones. How do we measure the contribution an individual predictor makes to the model <span class="math display">\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\]</span> Sequential sums of squares measures the contribution of an input relative to the inputs that precede it in the model. Partial sums of squares measure the contribution relative to all other inputs in the model. For example, if the input of interest is <span class="math inline">\(x_3\)</span> in a four-regressor model, <span class="math display">\[
SS(\beta_3 | \beta_0, \beta_1, \beta_2)
\]</span> is the sequential sum of squares for <span class="math inline">\(x_3\)</span> and <span class="math display">\[
SS(\beta_3 | \beta_0, \beta_1, \beta_2, \beta_4)
\]</span> is the partial sum of squares. <span class="math inline">\(\beta_4\)</span> does not appear in the sequential sum of squares for <span class="math inline">\(x_3\)</span> because it appears after <span class="math inline">\(\beta_3\)</span> in the model formula. In other words, the sequential sum of squares for <span class="math inline">\(x_3\)</span> does not adjust for <span class="math inline">\(x_4\)</span> at all, while the partial sum of squares does. Clearly, the two types of sum of squares are not identical, unless the inputs are <strong>orthogonal</strong> (independent).</p>
<p>Sequential sum of squares have an appealing additive property, the overall model sum of squares can be accumulated from a series of sequential terms,</p>
<p><span class="math display">\[\begin{align*}
    \text{SSM} &amp;= SS(\beta_1 | \beta_0)\\
    &amp;+ SS(\beta_2 | \beta_0, \beta_1) \\
    &amp;+ SS(\beta_3 | \beta_0, \beta_1, \beta_2) \\
    &amp;+ \cdots \\
    &amp;+ SS(\beta_p | \beta_0, \beta_1, \beta_2, \ldots, \beta_{p-1})
\end{align*}\]</span></p>
<p>Partial sum of squares do not add up to anything meaningful, unless the inputs are orthogonal.</p>
<p>This raises an interesting question. If we divide a coefficient estimate by its standard error and compute a <span class="math inline">\(p\)</span>-value from a <span class="math inline">\(t\)</span>-distribution, what kind of hypothesis is being tested?</p>
<div class="example">
<div class="example-header">
<p>Example: Auto Data (Cont’d)</p>
</div>
<div class="example-container">
<p>Here is the summary of the four-predictor model for <code>mpg</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ cylinders + displacement + weight + horsepower, 
    data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.5248  -2.7964  -0.3568   2.2577  16.3221 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  45.7567705  1.5200437  30.102  &lt; 2e-16 ***
cylinders    -0.3932854  0.4095522  -0.960 0.337513    
displacement  0.0001389  0.0090099   0.015 0.987709    
weight       -0.0052772  0.0007166  -7.364 1.08e-12 ***
horsepower   -0.0428125  0.0128699  -3.327 0.000963 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.242 on 387 degrees of freedom
Multiple R-squared:  0.7077,    Adjusted R-squared:  0.7046 
F-statistic: 234.2 on 4 and 387 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>How do we interpret the <span class="math inline">\(p-\)</span>-value of 0.337513 for the <code>cylinders</code> variable or the <span class="math inline">\(p\)</span>-value of 0.987709 for the <code>displacement</code> variable? A two-sided <span class="math inline">\(t\)</span>-test is equivalent to an F test with 1 numerator degrees of freedom where <span class="math inline">\(F_{obs}\)</span> is the square of the <span class="math inline">\(t_{obs}\)</span> statistic. Let’s first compute the sequential sum of squares tests and see if they match the <span class="math inline">\(p\)</span>-values in the output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders, <span class="at">data=</span>Auto)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement, <span class="at">data=</span>Auto)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>lm3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight, <span class="at">data=</span>Auto)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>lm4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm1, lm2, lm3, lm4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ cylinders
Model 2: mpg ~ cylinders + displacement
Model 3: mpg ~ cylinders + displacement + weight
Model 4: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    390 9415.9                                  
2    389 8342.6  1   1073.34 59.652 9.795e-14 ***
3    388 7162.5  1   1180.02 65.581 7.342e-15 ***
4    387 6963.4  1    199.12 11.066 0.0009633 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The <span class="math inline">\(p\)</span>-values are different from those shown in the <code>lm</code> output, so <code>Pr(&gt;|t|) = 0.987709</code> cannot have a sequential interpretation (adding <code>displacement</code> to a model that contains <code>cylinders</code> only). The <span class="math inline">\(p\)</span>-values in the <code>lm</code> summary have a <strong>partial</strong> interpretation as seen by performing reduction tests between the following reduced models and the full model with four predictors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>lm_nocyl <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>lm_nodis <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> weight <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>lm_nowgt <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> horsepower, <span class="at">data=</span>Auto)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>lm_nohp  <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight, <span class="at">data=</span>Auto)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_nocyl,lm4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ displacement + weight + horsepower
Model 2: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1    388 6980.0                           
2    387 6963.4  1    16.592 0.9221 0.3375</code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_nodis,lm4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ cylinders + weight + horsepower
Model 2: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)
1    388 6963.4                          
2    387 6963.4  1  0.004276 2e-04 0.9877</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_nowgt,lm4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ cylinders + displacement + horsepower
Model 2: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    388 7939.2                                  
2    387 6963.4  1    975.72 54.227 1.085e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_nohp ,lm4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: mpg ~ cylinders + displacement + weight
Model 2: mpg ~ cylinders + displacement + weight + horsepower
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    388 7162.5                                  
2    387 6963.4  1    199.12 11.066 0.0009633 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The <span class="math inline">\(p\)</span>-values of the partial reduction F-tests are identical to those for the <span class="math inline">\(t\)</span>-tests in the <code>lm</code> output. The <span class="math inline">\(F_{obs}\)</span> statistics are the squared values of the <span class="math inline">\(t_{obs}\)</span> statistics.</p>
<p>On the other hand, if you ask for the analysis of variance on the full model with the <code>aov</code> function, you get <strong>sequential</strong> tests of the inputs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(lm4))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
cylinders      1  14403   14403  800.47  &lt; 2e-16 ***
displacement   1   1073    1073   59.65 9.79e-14 ***
weight         1   1180    1180   65.58 7.34e-15 ***
horsepower     1    199     199   11.07 0.000963 ***
Residuals    387   6963      18                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="prediction" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="prediction"><span class="header-section-number">7.3</span> Prediction</h2>
<p>To obtain the fitted values in the linear model <span class="math inline">\(\textbf{Y}= \textbf{X}\boldsymbol{\beta}+ \boldsymbol{\epsilon}\)</span> we apply the OLS estimates and get <span class="math display">\[
\widehat{\textbf{y}} = \textbf{X}\widehat{\boldsymbol{\beta}}
\]</span> To predict at a previously unobserved value of the inputs, a new observation <span class="math inline">\(\textbf{x}_0\)</span>, we apply the regression equation and the OLS estimates to <span class="math inline">\(\textbf{x}_0\)</span>: <span class="math display">\[
\widehat{\textbf{y}}_0 = \textbf{x}_0^\prime \widehat{\boldsymbol{\beta}}
\]</span></p>
<p>This seems simple, there must be a trick to it. The “trick” lies in the question: <strong>what</strong> is being predicted?</p>
<p>The target of the prediction (the <strong>what</strong>) can be <span class="math inline">\(Y\)</span>, the target variable, or <span class="math inline">\(\text{E}[Y]\)</span>, the mean of the target variable. How are these different? <span class="math inline">\(Y\)</span> is a random variable and <span class="math inline">\(\text{E}[Y]\)</span> is a constant; <span class="math inline">\(\text{Var}[Y] = \sigma^2\)</span> and <span class="math inline">\(\text{Var}[\text{E}[Y]] = 0\)</span>. The uncertainty in predicting an individual observation and predicting the mean fucntion will differ. The former has to take into account the inherent variability <span class="math inline">\(\sigma^2\)</span> in the data.</p>
<p>How are the predicted values themselves different?</p>
<p>For illustration, cosnider an SLR model <span class="math inline">\(Y = \beta_0 + \beta_1x + \epsilon\)</span>. To predict <span class="math inline">\(Y\)</span> at <span class="math inline">\(X=x_0\)</span> we choose the obvious expression <span class="math display">\[
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0 + \widehat{\epsilon}
\]</span> substituting estimates for all unknowns on the right hand side. Since <span class="math inline">\(\epsilon\)</span> is a zero-mean random variable and cannot be observed directly, the best estimate is <span class="math inline">\(\widehat{\epsilon} = 0\)</span> which leads to <span class="math display">\[
\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1x_0
\]</span></p>
<p>That is the same expression we use to predict <span class="math inline">\(\text{E}[Y] = \beta_0 + \beta_1 x_0\)</span>, substituting the OLS estimates for unknowns on the right hand side: <span class="math display">\[
\widehat{\text{E}}[Y_0] = \widehat{\beta}_0 + \widehat{\beta}_1x_0
\]</span> And therein lies the crux. The predicted values for an observation and for the mean of an observation are the same. But their variability is not the same. We need to be very clear about what it is we are shooting for. More frequently one is interested in predicting observations, not averages of observations. In a study of health outcomes over time you might be more interested in predicting how a patient does at time <span class="math inline">\(t\)</span>, rather than how the population of patients does on average at time <span class="math inline">\(t\)</span>. Yet when folks see the different levels of confidence we have in the two predictions, they wish they could make predictions for the average.</p>
<section id="prediction-variance" class="level3">
<h3 class="anchored" data-anchor-id="prediction-variance">Prediction Variance</h3>
<p>The variance of <span class="math inline">\(\widehat{\text{E}}[Y_0] = \textbf{x}_0^\prime\widehat{\boldsymbol{\beta}}\)</span> is straightforward and depends only on the variance of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>: <span class="math display">\[
\text{Var}{\widehat{\text{E}}[Y_0]} = \sigma^2 \, \textbf{x}_0^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_0
\]</span> To account for the variability in the data, the proper variance to consider when predicting an individual observation is <span class="math display">\[
\text{Var}[\widehat{Y}_0 - Y_0] = \text{Var}[\widehat{Y}_0 - \epsilon] = \text{Var}[\textbf{x}_0^\prime\widehat{\boldsymbol{\beta}}-\epsilon] = \sigma^2\left(1+\textbf{x}_0^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_0\right)
\]</span></p>
<p>The additional <span class="math inline">\(1+\)</span> does not seem like a big deal but has substantial numeric consequences.</p>
</section>
<section id="confidence-and-prediction-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-and-prediction-intervals">Confidence and Prediction Intervals</h3>
<p>When the errors have a Gaussian distribution, the following random variables have <span class="math inline">\(t\)</span> distributions with <span class="math inline">\(n-r(\textbf{X})\)</span> degrees of freedom:</p>
<p><span class="math display">\[\begin{align*}
    t &amp;= \frac{\widehat{Y}_0 - \text{E}[Y]_0}{\sqrt{\text{Var}[\widehat{Y}_0]} \\
    t &amp;= \frac{\widehat{Y}_0 - Y_0}{\sqrt{\text{Var}[\widehat{Y}_0]-Y_0} \\
\end{align*}\]</span></p>
<p>The first is used to construct <span class="math inline">\((1-\alpha)\)</span>-level <strong>confidence</strong> intervals for the mean of the target <span class="math display">\[
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \textbf{x}_0^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_0}
\]</span> The second is used to construct <span class="math inline">\((1-\alpha)\)</span>-level <strong>prediction</strong> intervals for an individual target <span class="math display">\[
\widehat{y}_0 \pm t_{\frac{\alpha}{2},n-(p+1)} \sqrt{\sigma^2 \left(1+ \textbf{x}_0^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_0 \right)}
\]</span></p>
<div class="example">
<div class="example-header">
<p>Example: PISA OECD Study</p>
</div>
<div class="example-container">
<p>PISA (Program for International Student Assessment) is an OECD study in 65 countries to evaluate the performance of 15-year old students in math, science, and reading. Among the questions the study is trying to address is whether the educational level in a country is influenced by economic wealth, and if so, to what extent.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>pisa <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file=</span><span class="st">"data/pisa.csv"</span>,<span class="at">stringsAsFactors=</span><span class="cn">TRUE</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pisa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Country MathMean MathShareLow MathShareTop ReadingMean
1       Shanghai-China      613          3.8         55.4         570
2            Singapore      573          8.3         40.0         542
3 Hong Kong SAR, China      561          8.5         33.7         545
4       Chinese Taipei      560         12.8         37.2         523
5                Korea      554          9.1         30.9         536
6     Macao SAR, China      538         10.8         24.3         509
  ScienceMean     GDPp  logGDPp HighIncome
1         580  6264.60  8.74267      FALSE
2         551 54451.21 10.90506       TRUE
3         555 36707.77 10.51074       TRUE
4         523       NA       NA         NA
5         538 24453.97 10.10455       TRUE
6         521 77145.04 11.25344       TRUE</code></pre>
</div>
</div>
<p>The following statements compute the simple linear regression of <code>MathMean</code> on the log GDP and 95% prediction and confidence intervals.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>pisa_slr <span class="ot">&lt;-</span> <span class="fu">lm</span>(MathMean <span class="sc">~</span> logGDPp, <span class="at">data=</span>pisa)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>xvals <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">logGDPp=</span><span class="fu">seq</span>(<span class="at">from=</span><span class="dv">6</span>, <span class="at">to =</span> <span class="dv">14</span>, <span class="at">by=</span><span class="fl">0.1</span>))</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>p_pred <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">predict</span>(pisa_slr, <span class="at">newdata=</span>xvals, <span class="at">interval=</span><span class="st">"prediction"</span>))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>p_conf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">predict</span>(pisa_slr, <span class="at">newdata=</span>xvals, <span class="at">interval=</span><span class="st">"confidence"</span>))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(p_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 357.8802 250.6125 465.1479
2 360.7589 254.0404 467.4775
3 363.6376 257.4570 469.8183
4 366.5163 260.8621 472.1706
5 369.3951 264.2555 474.5346
6 372.2738 267.6372 476.9104</code></pre>
</div>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(p_conf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       fit      lwr      upr
1 357.8802 307.8856 407.8749
2 360.7589 311.9537 409.5642
3 363.6376 316.0200 411.2553
4 366.5163 320.0844 412.9483
5 369.3951 324.1466 414.6435
6 372.2738 328.2065 416.3410</code></pre>
</div>
</div>
<p>Both types of intervals are most narrow near the center of the <span class="math inline">\(x\)</span> data range and widen toward the edges. A prediction outside of the hull of the data will be much less precise than a prediction near the center of the data. The additional variance term that distinguishes the variance of <span class="math inline">\(\widehat{y}_0\)</span> <span class="math inline">\(\widehat{y}_0 - y_0\)</span> has a considerable effect; the prediction intervals are much wider than the confidence intervals (<a href="#fig-pred-conf-intervals" class="quarto-xref">Figure&nbsp;<span>7.3</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pred-conf-intervals" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pred-conf-intervals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regglobal_files/figure-html/fig-pred-conf-intervals-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pred-conf-intervals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: 95% prediction and confidence intervals.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="diagnostics" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="diagnostics"><span class="header-section-number">7.4</span> Diagnostics</h2>
<p>Model diagnostics are useful to examine model assumptions and to decide whether the model is an adequate description of the data. Questions we would like to answer through model diagnostics include</p>
<ul>
<li><p>Are the assumptions of the model met?</p>
<ul>
<li><p>Linearity (zero mean errors, <span class="math inline">\(\text{E}[\boldsymbol{\epsilon}] = \textbf{0}\)</span>)</p></li>
<li><p>Equal variance (homoscedasticity, <span class="math inline">\(\text{Var}[\boldsymbol{\epsilon}] = \sigma^2\textbf{I}\)</span>)</p></li>
<li><p>Independence of the errors <span class="math inline">\(\text{Cov}[\epsilon_i, \epsilon_j] = 0\)</span></p></li>
<li><p>Can we assume that the errors are normally distributed?</p></li>
</ul></li>
<li><p>How well does the model predict new observations?</p></li>
<li><p>Do observations have undue influence on the results?</p></li>
<li><p>Does the relationship between the inputs negatively affect the analysis?</p></li>
</ul>
<p>The three basic types of linear model diagnostics are</p>
<ol type="1">
<li><p><strong>Residual</strong> diagnostics to examine linearity, equal variance assumptions, and to detect outliers. Residual analysis relies on functions of <span class="math inline">\(y_i - \widehat{y}_i\)</span> to study the behavior of the unobservable <span class="math inline">\(\epsilon_i\)</span>.</p></li>
<li><p><strong>Case-deletion</strong> diagnostics find data points that exert high <strong>influence</strong> on the analysis. These diagnostics ask how an aspect of the analysis (variability, predicted values, coefficient estimates, …) changes if an observation is removed from the analysis.</p></li>
<li><p><strong>Collinearity</strong> diagnostics examine the relationships among the inputs and whether they impact the analysis in a negative way. The situations at the end of the extremes include completely orthogonal inputs (<span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span> is a diagonal matrix) and inputs that are linear combinations of each other (<span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span> is non-singular and a unique OLS solution does not exist). Most applications fall in-between unless one or more inputs are factors.</p></li>
</ol>
<section id="leverage" class="level3">
<h3 class="anchored" data-anchor-id="leverage">Leverage</h3>
<p>Linear model diagnostics depend on the leverage values <span class="math inline">\(h_{ii}\)</span>, the diagonal values of the Hat matrix <span class="math inline">\(\textbf{H}\)</span>. That is not surprising because the fitted values are linear combinations of the entries in the Hat matrix <span class="math display">\[
\widehat{\textbf{y}} = \textbf{H}\textbf{y}
\]</span> The <span class="math inline">\(i\)</span><sup>th</sup> fitted value is a linear combination of the entries in the <span class="math inline">\(i\)</span><sup>th</sup> row of <span class="math inline">\(\textbf{H}\)</span> with the elements of <span class="math inline">\(\textbf{y}\)</span> <span class="math display">\[
\widehat{y}_i = \textbf{x}_i^\prime\widehat{\boldsymbol{\beta}} = \sum_{j=1}^n h_{ij}y_j
\]</span></p>
<p>From the last expression it is easy to establish that <span class="math display">\[
\text{Var}[\widehat{y}_i] = \sigma^2 \, \textbf{x}_i^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_i = \sigma^2 h_{ii}
\]</span></p>
<p>We can think of the leverages <span class="math inline">\(h_{ii}\)</span> as standardized squared distance measures that tell us how far the <span class="math inline">\(i\)</span><sup>th</sup> data point is from the center of the <span class="math inline">\(x\)</span>-data.</p>
<p>It is illustrative to look at the mean of the variances of the fitted values across the data set, <span class="math display">\[
\frac{1}{n}\sum_{i=1}^n \text{Var}[\widehat{y}_i] = \frac{1}{n} \sum_{i=1}^n \sigma^2h_{ii} = \sigma^2\left(\frac{p+1}{n}\right)
\]</span> The last result follows because <span class="math inline">\(\textbf{H}\)</span> is a projection matrix and thus <span class="math inline">\(tr(\textbf{H})\)</span> equals its rank, <span class="math inline">\(p+1\)</span>.</p>
<p>What does that result tell us about the precision of the estimated regression output? Suppose <span class="math inline">\(p=4\)</span> and <span class="math inline">\(n=10\)</span>. The average variance is then <span class="math inline">\(\sigma^2/2\)</span>. When <span class="math inline">\(p=4\)</span> and <span class="math inline">\(n=1000\)</span>, the average variance is <span class="math inline">\(\sigma^2/200\)</span>. As sample size increases, more precise estimates result as <span class="math inline">\(p\)</span> remains fixed. As <span class="math inline">\(p\)</span> increases for a given sample size, the average variance of the fitted values increases. In high-dimensional problems, where <span class="math inline">\(p\)</span> is large, OLS estimates have high variability and are unstable. Regularized estimation methods such as Ridge or Lasso regression can perform better in those circumstances.</p>
<p>Since <span class="math inline">\(\sum_{i=1}^n h_{ii} = p+1\)</span>, the average leverage value in the data is <span class="math inline">\((p+1)/n\)</span>, and a good threshold for <em>high leverage</em> points is <span class="math inline">\(h_{ii} &gt; 2(p+1)/n\)</span>. This is not necessarily a problematic data point, it simply states that the point is an outlying point in the <span class="math inline">\(x\)</span>-space. High leverage points have the potential to influence aspects of the analysis, to be highly influential data points. More on this below.</p>
<p>In summary, here are some important results involving the leverage values <span class="math inline">\(h_{ii}\)</span>:</p>
<ul>
<li><p><span class="math inline">\(h_{ii} = \textbf{x}_i^\prime (\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_i\)</span></p></li>
<li><p><span class="math inline">\(\frac{1}{n} \le h_{ii} \le 1\)</span>. This holds only for the <span class="math inline">\(n\)</span> training observations, the leverage <span class="math inline">\(\textbf{x}_0^\prime(\textbf{X}^\prime\textbf{X})^{-1}\textbf{x}_0\)</span> of a new data point is not bounded in this way.</p></li>
<li><p><span class="math inline">\(\overline{h} = (p+1)/n\)</span>; high leverage points are those for which <span class="math inline">\(h_{ii} &gt; 2(p+1)/n\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Var}[\widehat{y}_i] = \sigma^2 h_{ii}\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}[y_i - \widehat{y}_i] = \sigma^2 (1-h_{ii})\)</span></p></li>
<li><p><span class="math inline">\(\widehat{y}_i = \sum_{j=1}^n h_{ij}y_j\)</span>, the <span class="math inline">\(i\)</span><sup>th</sup> fitted value is a linear combination of the target values with the values in the <span class="math inline">\(i\)</span><sup>th</sup> row of <span class="math inline">\(\textbf{H}\)</span>.</p></li>
<li><p><span class="math inline">\(\sum_{j=1}^n h_{ij} = 1\)</span>, the sum of the leverage values in the <span class="math inline">\(i\)</span><sup>th</sup> row of <span class="math inline">\(\textbf{H}\)</span> is 1. Since the leverage values are bounded, they sum to 1 within a row, and the fitted values are linear combinations of <span class="math inline">\(\textbf{H}\)</span>, this shows how a data point with <span class="math inline">\(h_{ii} \approx 1\)</span> has outsize influence. The fitted value is almost entirely determined by the input values of that observation.</p></li>
</ul>
<div class="example">
<div class="example-header">
<p>Example: Same Leverage–Different Influence</p>
</div>
<div class="example-container">
<p>This simulation demonstrates the concept of high leverage points with and without high influence. Data are simulated under the model <span class="math display">\[
Y = \beta_0 + \beta_1 x + \epsilon
\]</span> with <span class="math inline">\(\epsilon \sim G(0,0.25^2), \beta_0 = 1, \beta_1 = 0.5\)</span>. The design points for the input variable are spread evenly from 1 to 2, and a high leverage point is added at <span class="math inline">\(x=4\)</span>. There are 22 observations, so the threshold for a high leverage point is <span class="math inline">\(2(p+1)/n = 2*2/22 = 0.18\)</span>.</p>
<p>Two data sets are simulated. One in which the target value at <span class="math inline">\(x=4\)</span> concurs with the mean function, one in which the target value is unusually high.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">187</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="dv">1</span>, <span class="at">to=</span><span class="dv">2</span>, <span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>xlev <span class="ot">&lt;-</span> <span class="dv">4</span>  <span class="co"># the high-leverage point</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(x,xlev)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>mn <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="fu">length</span>(x), <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="fl">0.25</span>) <span class="sc">+</span> mn</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> y1</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>y2[<span class="fu">length</span>(y1)] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.25</span>) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>xlev</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x)</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> x)</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>lm1<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)           x 
  0.9868996   0.5024693 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>lm2<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)           x 
 0.02317379  1.16105181 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the point at xlev has the same leverage in both regressions</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">hatvalues</span>(lm1),<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     1      2      3      4      5      6      7      8      9     10     11 
0.0932 0.0857 0.0789 0.0727 0.0671 0.0622 0.0579 0.0543 0.0512 0.0488 0.0471 
    12     13     14     15     16     17     18     19     20     21     22 
0.0460 0.0455 0.0456 0.0464 0.0478 0.0499 0.0525 0.0558 0.0598 0.0644 0.7671 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">hatvalues</span>(lm2),<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     1      2      3      4      5      6      7      8      9     10     11 
0.0932 0.0857 0.0789 0.0727 0.0671 0.0622 0.0579 0.0543 0.0512 0.0488 0.0471 
    12     13     14     15     16     17     18     19     20     21     22 
0.0460 0.0455 0.0456 0.0464 0.0478 0.0499 0.0525 0.0558 0.0598 0.0644 0.7671 </code></pre>
</div>
</div>
<p>The fitted linear regressions are quite different. The parameter estimates in the first model, where <span class="math inline">\(y|x=4\)</span> is not unusual are close to the true values <span class="math inline">\(\beta_0 = 1, \beta_1 = 0.5\)</span>. In the second regression the parameter estimates are very different from the true values, the estimates are biased.</p>
<p>The leverage values in both models are identical, since they depend only on the <span class="math inline">\(x\)</span>-data. The data point at <span class="math inline">\(x=4\)</span> has high leverage, its value of 0.7671 exceeds the threshold considerably. No other data point has high leverage.</p>
<p><a href="#fig-leverage-example" class="quarto-xref">Figure&nbsp;<span>7.4</span></a> displays the data and fitted regressions for the two data sets. Although <span class="math inline">\(x=4\)</span> is a high leverage point, it has no undue influence on the estimated regression in the left panel. In the right panel, the high leverage point is a highly influential point due to its unusual <span class="math inline">\(y\)</span>-value. The data point exerts its leverage by pulling the estimated regression towards it.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-leverage-example" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-leverage-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regglobal_files/figure-html/fig-leverage-example-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-leverage-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: A data point with high-leverage at <span class="math inline">\(x=4\)</span> has little influence in one analysis and is highly influential in another analysis depending on its <span class="math inline">\(y\)</span>-value. The dashed line is the true mean function.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="residual-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="residual-diagnostics">Residual Diagnostics</h3>
<p>Basic questions about the correctness of the model revolve around the properties of <span class="math inline">\(\boldsymbol{\epsilon}\)</span>. The usual assumption <span class="math inline">\(\boldsymbol{\epsilon}\sim (\textbf{0}, \sigma^2\textbf{I})\)</span> states that the errors have zero mean, are uncorrelated, and have equal variance. Although <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is unobservable, we should be able to check those assumptions by looking at the OLS residuals of the fitted model, <span class="math inline">\(\widehat{\boldsymbol{\epsilon}} = \textbf{y}- \widehat{\textbf{y}}\)</span>. These are also called the <strong>raw residuals</strong>.</p>
<p>Unfortunately, the properties of <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> match the properties of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> only partially. Because <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> is the result of fitting a model to data, the fitted residuals obey constraints that do not affect the model errors <span class="math inline">\(\boldsymbol{\epsilon}\)</span>. Because <span class="math display">\[
\textbf{X}^\prime \widehat{\boldsymbol{\epsilon}} = \textbf{X}^\prime (\textbf{I}-\textbf{H})\textbf{y}= \textbf{0}
\]</span> the raw residuals sum to zero across each column of the <span class="math inline">\(\textbf{X}\)</span> matrix. In other words, there are only <span class="math inline">\(n-r(\textbf{X})\)</span> degrees of freedom in the raw residual vector. From a statistical perspective, the residuals have zero mean, <span class="math inline">\(\text{E}[\widehat{\boldsymbol{\epsilon}}] = \textbf{0}\)</span> and share this property with the model errors. The variance of <span class="math inline">\(\boldsymbol{\epsilon}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> is different, however:</p>
<p><span class="math display">\[\begin{align*}
    \text{Var}[\boldsymbol{\epsilon}] &amp;= \sigma^2 \textbf{I}\\
    \text{Var}[\widehat{\boldsymbol{\epsilon}}] &amp;= \sigma^2(\textbf{I}- \textbf{H}) \\
    \text{Var}[\widehat{\epsilon}_i] &amp;= \sigma^2 (1-h_{ii})
\end{align*}\]</span></p>
<p>While the model errors are uncorrelated, the fitted residuals are correlated, <span class="math inline">\(\textbf{I}- \textbf{H}\)</span> is not a diagonal matrix. The fitted residuals also do not have the same variance; the variance depends on the leverage of the <span class="math inline">\(i\)</span><sup>th</sup> data point.</p>
<p>These properties (or lack thereof) should give pause in using the raw residuals to diagnose the assumptions of equal variance or uncorrelated errors. Instead, residual diagnostics use transformations of the raw residuals.</p>
<section id="studentized-residuals" class="level4">
<h4 class="anchored" data-anchor-id="studentized-residuals">Studentized residuals</h4>
<p>The unequal variance of the residuals can be handled by <strong>standardizing</strong>, dividing the residual by its standard deviation (the square root of its variance) <span class="math display">\[
\frac{\widehat{\epsilon}_i}{\sigma\sqrt{1-h_{ii}}}
\]</span> <span class="math inline">\(\sigma\)</span> is unknown and the obvious solution is to substitute an estimator. Statisticians refer to this process, using an estimate to scale a random variable, as <strong>studentization</strong>. The studentized residual is thus <span class="math display">\[
r_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}\sqrt{1-h_{ii}}}
\]</span> The usual estimator for <span class="math inline">\(\sigma\)</span> is the square root of the estimator of <span class="math inline">\(\sigma^2\)</span>, <span class="math display">\[
\widehat{\sigma} = \sqrt{ \frac{\text{SSE}}{n-r(\textbf{X})}}
\]</span></p>
</section>
<section id="r-student-residuals" class="level4">
<h4 class="anchored" data-anchor-id="r-student-residuals">R-student residuals</h4>
<p>A further adjustment can be made to the studentized residuals. Rather than use an estimate of the variance that is derived from all the data, an estimator can be used that does not depend on the <span class="math inline">\(i\)</span><sup>th</sup> observation. This technique is called <strong>external studentization</strong> in contrast to the <strong>internal studentization</strong> that gives rise to <span class="math inline">\(r_i\)</span>.</p>
<p>Fortunately such an external estimate of the variance <span class="math inline">\(\sigma^2\)</span> that does not rely on the <span class="math inline">\(i\)</span><sup>th</sup> observation can be computed based on the analysis of all <span class="math inline">\(n\)</span> observations. Not surprisingly, as with PRESS residuals discussed earlier, the leverage plays a role again: <span class="math display">\[
\widehat{\sigma}^2_{-i} = \frac{(n-r(\textbf{X}))\widehat{\sigma}^2 - \frac{\widehat{\epsilon}^2_i}{1-h_{ii}}}{n-r(\textbf{X})-1}
\]</span> The externally studentized residual is called the R-student residual, <span class="math display">\[
t_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}_{-i}\sqrt{1-h_{ii}}}
\]</span> Since <span class="math inline">\(\widehat{\epsilon}_i\)</span> and <span class="math inline">\(\widehat{\sigma}_{-i}\)</span> are independent, <span class="math inline">\(t_i\)</span> behaves like a <span class="math inline">\(t\)</span>-distributed random variable. The R-student residuals are good diagnostics to detect outliers and high-influence points (<strong>hip</strong>s). Outliers are observations unusual in <span class="math inline">\(y\)</span>-space. They are not necessarily hips, unless they are also high leverage points (see the previous example).</p>
<p>The <span class="math inline">\(t_i\)</span> work well for outliers and hips because an outlier has large <span class="math inline">\(\widehat{\epsilon}_i\)</span> and a hip has small <span class="math inline">\(\sqrt{1-h_{ii}}\)</span>. Both effects increase the value of <span class="math inline">\(t_i\)</span>. This is also true for the (internally) studentized residual. In addition, outliers or hips will have a large <span class="math display">\[
\frac{\widehat{\epsilon}^2_i}{1-h_{ii}}
\]</span> the adjustment term in the computation of <span class="math inline">\(\widehat{\sigma}^2_{-i}\)</span>. For those data points <span class="math inline">\(\widehat{\sigma}^2_{-i} &lt; \widehat{\sigma}^2\)</span> and <span class="math inline">\(t_i\)</span> will be more sensitive than <span class="math inline">\(r_i\)</span>.</p>
<p>You can obtain all three sets of residuals easily in <code>R</code>:</p>
<ul>
<li><p>The <code>residual</code> vector returned on the <code>lm</code> return object contains the <span class="math inline">\(\widehat{\epsilon}_i\)</span>.</p></li>
<li><p>The <code>rstandard()</code> function returns the studentized residuals <span class="math inline">\(r_i\)</span> (unfortunate function name)</p></li>
<li><p>The <code>rstudent()</code> function returns the R-student residuals <span class="math inline">\(t_i\)</span></p></li>
</ul>
<p>When testing model assumptions such as linearity, equal variance (homoscedasticity), and checking for outliers, the R-student residuals are the preferred quantities. The threshold <span class="math inline">\(|r_i| &gt; 2\)</span> is often applied to indicate outlying observations.</p>
<div class="example">
<div class="example-header">
<p>Example: Boston Housing Values</p>
</div>
<div class="example-container">
<p>To demonstrate residual analysis in a linear model we use the Boston housing data that is part of the <code>MASS</code> library in <code>R</code>. The data set comprises 506 observations on the median value of owner-occupied houses (<code>medv</code> in $000s) in Boston suburbs and 13 variables describing the towns and properties.</p>
<p>The following statements fit a multiple linear regression model to predict median home value as a function of all but two inputs. The formula syntax <code>medv ~ .</code> requests to include all variables in the dataframe as inputs. <code>medv ~ . -indus -age</code> requests inclusion of all inputs except <code>indus</code> and <code>age</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'MASS'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:ISLR2':

    Boston</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> . <span class="sc">-</span> indus <span class="sc">-</span> age, <span class="at">data=</span>Boston)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ . - indus - age, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***
crim         -0.108413   0.032779  -3.307 0.001010 ** 
zn            0.045845   0.013523   3.390 0.000754 ***
chas          2.718716   0.854240   3.183 0.001551 ** 
nox         -17.376023   3.535243  -4.915 1.21e-06 ***
rm            3.801579   0.406316   9.356  &lt; 2e-16 ***
dis          -1.492711   0.185731  -8.037 6.84e-15 ***
rad           0.299608   0.063402   4.726 3.00e-06 ***
tax          -0.011778   0.003372  -3.493 0.000521 ***
ptratio      -0.946525   0.129066  -7.334 9.24e-13 ***
black         0.009291   0.002674   3.475 0.000557 ***
lstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>All 11 input variables are significant in this model; it explains 74% of the variability in median home values.</p>
<p>A plot of the R-student residuals against the observation number helps to identify outlying observations. Observations with <span class="math inline">\(t_i\)</span> values outside the [-2, +2] interval are outliers. The plot also shows whether the equal variance assumption is reasonable; if the assumption is met the residuals show as a band of equal width.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>RStudent <span class="ot">&lt;-</span> <span class="fu">rstudent</span>(fit)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">5.1</span>, <span class="fl">4.1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(RStudent, <span class="at">xlab=</span><span class="st">"Obs no."</span>, <span class="at">las=</span><span class="dv">1</span>,<span class="at">bty=</span><span class="st">"l"</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span> <span class="dv">2</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="sc">-</span><span class="dv">2</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/RStudent_plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<p>A plot of the residuals against leverage shows observations that are unusual with respect to <span class="math inline">\(y\)</span> (large absolute value of the residual), with respect to <span class="math inline">\(x\)</span> (large leverage), or both. A high leverage point that is also an outlier is a highly influential data point.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>leverage <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(fit)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>lev_threshold <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>fit<span class="sc">$</span>rank<span class="sc">/</span><span class="fu">length</span>(fit<span class="sc">$</span>residuals)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="fl">5.1</span>, <span class="fl">4.1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(leverage, RStudent, <span class="at">xlab=</span><span class="st">"Leverage"</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span> <span class="dv">2</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="sc">-</span><span class="dv">2</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>lev_threshold, <span class="at">lty=</span><span class="st">"dotted"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/RStudent_vs_leverage-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<p>In a simple linear regression model you can plot the residuals against the input <span class="math inline">\(x\)</span>. In a multiple linear regression model (or a SLR) you plot the residuals against the fitted values. The residuals should display no obvious trend against <span class="math inline">\(\widehat{y}\)</span>.</p>
<p>When plotting the residuals against the fitted value, add the predictions from a smoothing spline or other nonparametric model to identify trends in the residuals. Ideally, the smoothing spline should not show any gross departures from a flat line at zero.</p>
<p>The following plot raises concerns. We might not have the right set of inputs. Inputs might need to be transformed or additional/different terms are needed in the model, for example, interactions between the inputs.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yhat,RStudent)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="sc">-</span><span class="dv">1</span>, <span class="at">lty=</span><span class="st">"dashed"</span>)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">predict</span>(<span class="fu">loess</span>(RStudent <span class="sc">~</span> yhat)),<span class="at">col=</span><span class="st">"red"</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/Rstudent_vs_fit-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="partial-regression-plots" class="level4">
<h4 class="anchored" data-anchor-id="partial-regression-plots">Partial regression plots</h4>
<p>It is tempting to create plots comparing residuals against the values of the input variables. This is meaningful in a simple linear regression with one input and can help suggest transformations of <span class="math inline">\(X\)</span> to achieve linearity and help diagnose heteroscedasticity.</p>
<p>In the following example, the variance of <span class="math inline">\(Y\)</span> increases with the value of <span class="math inline">\(X\)</span> and the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is a second-degree polynomial.</p>
<div class="cell" data-layout-align="center" data-out.widfth="80%">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">543</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="fl">0.1</span>) </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x),<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>mn <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="fu">length</span>(x), <span class="at">mean=</span>mn, <span class="at">sd=</span><span class="fu">sqrt</span>(x))</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>lfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>ri <span class="ot">&lt;-</span> <span class="fu">rstudent</span>(lfit)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,ri,<span class="at">ylab=</span><span class="st">"Rstudent"</span>,<span class="at">las=</span><span class="dv">1</span>,<span class="at">bty=</span><span class="st">"l"</span>)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">"red"</span>,<span class="at">lty=</span><span class="st">"dashed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The plot of the Rstudent residuals shows the increasing variability in <span class="math inline">\(x\)</span> and a systematic quadratic trend in the residuals.</p>
<p>In a multiple linear regression (MLR) model it is tempting to create plots such as the previous one for all input variables. Unfortunately, such plots can be misleading because in an MLR model values of one input are changing with the values of other inputs. A way around this problem seems to be a plot of residuals versus the fitted values <span class="math inline">\(\widehat{y}_i\)</span> but that is not an optimal solution either; this plot does not tell us anything about the <span class="math inline">\(X\)</span>s that could be transformed to improve the model. It mushes together the contributions of all inputs.</p>
<p>So, how can we diagnose whether <span class="math inline">\(X_j\)</span> needs to be transformed to account for non-linearity and visualize the relationship in such a way that accounts for the other inputs in the model? The answer is the <strong>partial regression</strong> plot, also known as the <strong>added-variable</strong> plot.</p>
<p>Suppose we partition the <span class="math inline">\(\textbf{X}\)</span> matrix of the MLR model as follows <span class="math display">\[
\textbf{X}= [\textbf{X}_{-j}, \textbf{x}_j]
\]</span> so that <span class="math inline">\(\textbf{X}_{-j}\)</span> contains all inputs (including the intercept) except for the <span class="math inline">\(j\)</span><sup>th</sup> input. Now consider two new regression models:</p>
<ul>
<li><p>Regress <span class="math inline">\(\textbf{y}\)</span> on <span class="math inline">\(\textbf{X}_{-j}\)</span>. Denote the residuals from this regression as <span class="math inline">\(\textbf{e}_{y,-j}\)</span></p></li>
<li><p>Regress <span class="math inline">\(\textbf{x}_j\)</span> on <span class="math inline">\(\textbf{X}_{-j}\)</span>. Denote the residuals from this regression as <span class="math inline">\(\textbf{e}_{x,-j}\)</span></p></li>
</ul>
<p>The added variable plot for input <span class="math inline">\(j\)</span> displays <span class="math inline">\(\textbf{e}_{y,-j}\)</span> on the vertical axis and <span class="math inline">\(\textbf{e}_{x,-j}\)</span> on the horizontal axis.</p>
<div class="example">
<div class="example-header">
<p>Example: Auto (Cont’d)</p>
</div>
<div class="example-container">
<p>For the four-regressor model in the Auto example, added-variable plots can be constructed with the <code>avPlots()</code> function in the <code>car</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>mlr <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cylinders <span class="sc">+</span> displacement <span class="sc">+</span> weight <span class="sc">+</span> horsepower,</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>Auto)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">avPlots</span>(mlr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regglobal_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>Take the scatter of points in an added variable plot. This is a regression through the origin (a no-intercept) model of the form <span class="math display">\[
\textbf{e}_{y,-j} = \beta_j \, \textbf{e}_{x,-j} + \boldsymbol{\epsilon}^*
\]</span></p>
<p>Using <span class="math inline">\(\beta_j\)</span> to denote the slope in this regression is no accident. The estimate of <span class="math inline">\(\beta_j\)</span> in this regression is the same as the estimate in the multiple linear regression model.</p>
<p>The added-variable plot is a visual representation of how <span class="math inline">\(X_j\)</span> fares in the <span class="math inline">\(p\)</span>-input model even if we are looking only at a two-dimensional plot. The partial regressions show the effect of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(y\)</span> as if it was added last to the model. If the residual point scatter in the added-variable plot suggests nonlinearity, a transformation of <span class="math inline">\(X_j\)</span> is in order.</p>
<p>Inputs that are highly significant in the multiple linear regression model will have a tight point cloud in the added-variable plot. Inputs that are correctly specified in the model will show non-systematic scatter of the points around the line. Note that we are not looking for horizontal point clouds in the added-variable plots, because the points are arranged around a non-zero line, its slope corresponds to the coefficient estimate.</p>
</section>
</section>
<section id="case-deletion-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="case-deletion-diagnostics">Case Deletion Diagnostics</h3>
<p>Case deletion diagnostics express how much an aspect of the model changes when an observation is removed from the analysis. The RStudent residual is a leave-one-out diagnostic in this spirit, as it uses an estimator of <span class="math inline">\(\sigma\)</span> that does not incorporate the <span class="math inline">\(i\)</span>th data point.</p>
<p>The two most important case deletion diagnostics are Cook’s D and the DFFITS. The name DFFITS stands for <strong>d</strong>i<strong>ff</strong>erence in <strong>fit</strong>, <strong>s</strong>tandardized. The statistic measures the change in predicted value in units of standard errors when the <span class="math inline">\(i\)</span>th observation is deleted. We are concerned when a DFFIT exceeds <span class="math inline">\(2\sqrt{(p+1)/n}\)</span>.</p>
<p>The Cook’s D (“D” for distance) statistic measures the change in the parameter estimates <span class="math inline">\(\boldsymbol{\beta}\)</span> when the <span class="math inline">\(i\)</span>th observation is removed. If the purpose of modeling is to build a model that predicts well, focus more on DFFITS. If the purpose of modeling is to test hypotheses, then focus more on Cook’s D. We are concerned if the D statistic exceeds 1.</p>
<section id="cooks-d" class="level4">
<h4 class="anchored" data-anchor-id="cooks-d">Cook’s D</h4>
<p>Computing Cook’s D for our model is easy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(fit)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>D[<span class="fu">which.max</span>(D)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      369 
0.1612148 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>D[<span class="fu">which</span>(D <span class="sc">&gt;</span> <span class="fl">0.1</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      369       373 
0.1612148 0.1085777 </code></pre>
</div>
</div>
<p>There are no data points with a D &gt; 1. We conclude that there are no data points that unduly influence the regression coefficients.</p>
<p>A plot of the D statistic against the observation number shows that there is a group of data points with much higher values of the D statistic. This group also coincides with larger residuals in the previous plots.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(D, <span class="at">xlab=</span><span class="st">"Obs no."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/cooks_d_plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<p>Here are the observations with the larger D values</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>D[<span class="fu">which</span>(D <span class="sc">&gt;</span> <span class="fl">0.035</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       365        366        368        369        370        371        372 
0.07895966 0.07399805 0.04632638 0.16121476 0.06142387 0.04956350 0.04241984 
       373        381        413        415 
0.10857770 0.03622523 0.05795739 0.03974459 </code></pre>
</div>
</div>
<p>What do these observations have in common?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subset</span>(Boston, D <span class="sc">&gt;</span> <span class="fl">0.035</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        crim zn indus chas   nox    rm   age    dis rad tax ptratio  black
365  3.47428  0  18.1    1 0.718 8.780  82.9 1.9047  24 666    20.2 354.55
366  4.55587  0  18.1    0 0.718 3.561  87.9 1.6132  24 666    20.2 354.70
368 13.52220  0  18.1    0 0.631 3.863 100.0 1.5106  24 666    20.2 131.42
369  4.89822  0  18.1    0 0.631 4.970 100.0 1.3325  24 666    20.2 375.52
370  5.66998  0  18.1    1 0.631 6.683  96.8 1.3567  24 666    20.2 375.33
371  6.53876  0  18.1    1 0.631 7.016  97.5 1.2024  24 666    20.2 392.05
372  9.23230  0  18.1    0 0.631 6.216 100.0 1.1691  24 666    20.2 366.15
373  8.26725  0  18.1    1 0.668 5.875  89.6 1.1296  24 666    20.2 347.88
381 88.97620  0  18.1    0 0.671 6.968  91.9 1.4165  24 666    20.2 396.90
413 18.81100  0  18.1    0 0.597 4.628 100.0 1.5539  24 666    20.2  28.79
415 45.74610  0  18.1    0 0.693 4.519 100.0 1.6582  24 666    20.2  88.27
    lstat medv
365  5.29 21.9
366  7.12 27.5
368 13.33 23.1
369  3.26 50.0
370  3.73 50.0
371  2.96 50.0
372  9.53 50.0
373  8.88 50.0
381 17.21 10.4
413 34.37 17.9
415 36.98  7.0</code></pre>
</div>
</div>
</section>
<section id="dffits" class="level4">
<h4 class="anchored" data-anchor-id="dffits">DFFITs</h4>
<p>You can calculate the DFFITs statistics with the <code>dffits()</code> function. The following plot shows those statistics along with the threshold.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>threshold_dff <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">12</span><span class="sc">/</span><span class="dv">506</span>) </span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>dff <span class="ot">&lt;-</span> <span class="fu">dffits</span>(fit)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>dff[<span class="fu">which</span>(dff <span class="sc">&gt;</span> threshold_dff)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       65       142       149       162       163       164       167       187 
0.3844495 0.4775638 0.3858408 0.4608252 0.4684540 0.4413915 0.4735623 0.4237227 
      196       204       205       215       226       234       254       263 
0.3407192 0.3117844 0.3319161 0.4907820 0.4158602 0.3670077 0.6332907 0.3271603 
      268       366       368       369       370       371       372       373 
0.3633354 0.9496438 0.7503581 1.4375975 0.8702277 0.7792767 0.7342909 1.1726437 
      374       375       413       415 
0.3126816 0.6200353 0.8436891 0.6941816 </code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dff,<span class="at">ylab=</span><span class="st">"DFFITs"</span>)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>threshold_dff)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/dff_plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<p>The same group of observations with high D values has also high DFFITs. Contrary to the D values, the observations exceed the threshold for the DFFITs. We conclude that the data points are not influential on the regression coefficient estimates, but they are influential on the predicted values. If the model is used to predict median home values, we should consider refining the model or excluding the outlying observations and refitting.</p>
</section>
</section>
<section id="sec-collinearity-diag" class="level3">
<h3 class="anchored" data-anchor-id="sec-collinearity-diag">Collinearity Diagnostics</h3>
<p>When inputs are related to each other it has two important consequences</p>
<ul>
<li>we cannot interpret the regression coefficient for one input without considering the other inputs</li>
<li>with increasing dependence among the inputs, the least squares estimation procedure becomes increasingly numerically unstable.</li>
</ul>
<p>The condition when inputs are linearly related is called <strong>collinearity</strong> and it negatively affects any calculations that involve the <span class="math inline">\((\textbf{X}^\prime\textbf{X})^{-1}\)</span> matrix (which is about all the calculations.)</p>
<p>The extreme case is when an input variable is linear combination of other inputs, for example <span class="math display">\[
Z = aX_2 + bX_3
\]</span> Adding <span class="math inline">\(Z\)</span> to a model that contains inputs <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> leads to a singular (rank-deficient) <span class="math inline">\(\textbf{X}\)</span> matrix. The inverse cross-product matrix <span class="math inline">\((\textbf{X}^\prime\textbf{X})^{-1}\)</span> does not exist and the OLS estimator cannot be computed. Software uses instead a <strong>generalized</strong> inverse matrix <span class="math inline">\((\textbf{X}^\prime\textbf{X})^{-}\)</span> to find a solution, which happens to be not unique.</p>
<div class="example">
<div class="example-header">
<p>Example: Boston Housing Values</p>
</div>
<div class="example-container">
<p>The following code fits a linear model with four inputs. The variable <code>newvar</code> is the sum of the <code>zn</code> and <code>nox</code> variables. With <code>zn</code> and <code>nox</code> already in the model, <code>newvar</code> does not provide any additional information. The <span class="math inline">\(\textbf{X}\)</span> matrix is singular and an estimate for the coefficient of <code>newvar</code> cannot be found.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> Boston</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>B<span class="sc">$</span>newvar <span class="ot">&lt;-</span> B<span class="sc">$</span>zn<span class="sc">+</span>B<span class="sc">$</span>nox</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>singular_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> crim <span class="sc">+</span> zn <span class="sc">+</span> nox <span class="sc">+</span> newvar, <span class="at">data=</span>B)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(singular_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ crim + zn + nox + newvar, data = B)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.659  -4.620  -1.684   2.211  31.584 

Coefficients: (1 not defined because of singularities)
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  32.22033    2.20618  14.605  &lt; 2e-16 ***
crim         -0.27519    0.04516  -6.094 2.19e-09 ***
zn            0.07749    0.01764   4.392 1.37e-05 ***
nox         -17.25946    3.83527  -4.500 8.45e-06 ***
newvar             NA         NA      NA       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.915 on 502 degrees of freedom
Multiple R-squared:  0.2637,    Adjusted R-squared:  0.2593 
F-statistic: 59.93 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><code>R</code> indicates the singularity in <span class="math inline">\(\textbf{X}\)</span> with <code>NA</code> for the coefficient associated with the singularity. Note that this depends on the order in which the variables enter the model. When <code>newvar</code> is placed before <code>zn</code> and <code>nox</code> in the model, the <code>nox</code> coefficient is <code>NA</code>.</p>
</div>
</div>
<p>Collinearity is the condition where inputs are highly correlated. They do not follow exact linear dependencies but are close to linearly dependent (a so-called near-linear dependency). The situation is more complex than involving just two input variables. For example, <span class="math inline">\(X_1\)</span> might not be strongly correlated with <span class="math inline">\(X_2\)</span> but can be strongly correlated with a linear combination of <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, and <span class="math inline">\(X_8\)</span>. This condition is called <strong>multicollinearity</strong>.</p>
<p>When multicollinearity is strong, the OLS estimates of the regression coefficients become unstable, small perturbations in the target values or inputs can lead to large changes in the coefficients. The <span class="math inline">\(\widehat{\beta}_j\)</span> can be of the wrong size and/or sign.</p>
<p>A nontechnical diagnostic for multicollinearity is to compute the matrix of pairwise correlations among the inputs. Large values of <span class="math inline">\(\text{Corr}[X_j,X_k]\)</span> is a sufficient condition for collinearity, but it is not a necessary condition. Even with weak pairwise correlations you can have strong linear dependencies among multiple inputs.</p>
<p>Nevertheless, the pairwise correlation plot is a good place to start.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"corrplot"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>corrplot 0.94 loaded</code></pre>
</div>
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cor</span>(X[,<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]), <span class="at">method =</span> <span class="st">"circle"</span>, <span class="at">diag=</span><span class="cn">FALSE</span>, <span class="at">tl.col=</span><span class="st">"black"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regglobal_files/figure-html/corrplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
</div>
</div>
<p>We see some strong <strong>pairwise</strong> relationships between <code>tax</code> and <code>rad</code>, between <code>dis</code> and <code>nox</code>, and between <code>lstat</code> and <code>rm</code>. Do we need to worry?</p>
<p>Here are some other, informal, ways to diagnose a multicollinearity problem:</p>
<ul>
<li><p>The <span class="math inline">\(R^2\)</span> statistic indicates the model explains substantial variability in <span class="math inline">\(Y\)</span>, but none or few of the inputs show statistically significant <span class="math inline">\(p\)</span>-values. Because the <span class="math inline">\(t\)</span>-tests are partial tests, the other input variables act as proxy for the variable being tested.</p></li>
<li><p>Different variable selection methods lead to very different models.</p></li>
<li><p>Standard errors of coefficients and/or fitted values are unusually large.</p></li>
<li><p>Slight perturbations of the data, for example, by adding some small Gaussian random noise, change the results dramatically.</p></li>
</ul>
<p>A formal diagnosis relies on the computation of <strong>variance inflation factors</strong> (VIFs) or **condition indices*.</p>
<section id="variance-inflation-factors" class="level4">
<h4 class="anchored" data-anchor-id="variance-inflation-factors">Variance inflation factors</h4>
<p>Each predictor (input) variable in a linear model is associated with a variance inflation factor that quantifies the strength of linear dependencies between this input and all other inputs.</p>
<p>The <span class="math inline">\(j\)</span><sup>th</sup> VIF measures how many times more variable the variance of the standardized coefficients are due to the involvement of <span class="math inline">\(X_j\)</span> in linear dependencies involving the other <span class="math inline">\(X\)</span>s.</p>
<p>You can find the <span class="math inline">\(\text{VIF}_j\)</span> from the <span class="math inline">\(R^2\)</span> statistic of a multiple linear regression of <span class="math inline">\(X_j\)</span> on all the other input variables. <span class="math display">\[ \text{VIF}_j = \frac{1}{1-R^2_j}\]</span> For example, the VIF for <code>rad</code> can be obtained by regressing all other inputs onto <code>rad</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>vif_calc <span class="ot">&lt;-</span> <span class="fu">lm</span>(rad <span class="sc">~</span> crim <span class="sc">+</span> zn <span class="sc">+</span> chas <span class="sc">+</span> nox <span class="sc">+</span> rm <span class="sc">+</span> dis <span class="sc">+</span> tax <span class="sc">+</span> </span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>                   ptratio <span class="sc">+</span> black <span class="sc">+</span> lstat, <span class="at">data=</span>Boston)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>R2_rad <span class="ot">&lt;-</span> <span class="fu">summary</span>(vif_calc)<span class="sc">$</span>r.squared</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>VIF_rad <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span><span class="sc">-</span>R2_rad)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>VIF_rad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6.861126</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>When calculating variance inflation factors this way make sure that the response variable does not appear on the right hand side of the model formula. The expression <code>lm(rad ~ .)</code> would include <code>medv</code> on the right hand side. Variance inflation factors capture relationships among the inputs and are not related to the response.</p>
</div>
</div>
<p>Notice that a variance inflation factor <strong>does not</strong> depend on <span class="math inline">\(Y\)</span>. It is solely based on relationships among the inputs (predictors). Also, you can see from the model equation above that the VIF discovers more than a pairwise dependence on other variables. It models one input as a function of all other inputs.</p>
<p>To compute variance inflation factors in <code>R</code> directly, use the <code>vif</code> function in the <code>car</code> package (Companion to Applied Regression).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: carData</code></pre>
</div>
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    crim       zn     chas      nox       rm      dis      rad      tax 
1.789704 2.239229 1.059819 3.778011 1.834806 3.443420 6.861126 7.272386 
 ptratio    black    lstat 
1.757681 1.341559 2.581984 </code></pre>
</div>
</div>
<p>The VIF for <code>rad</code> reported by <code>vif()</code> matches the previous calculation.</p>
<p>Another way to compute the variance inflation factors is to fit the linear regression with a scaled and centered <span class="math inline">\(\textbf{X}^*\)</span> matrix. The columns of <span class="math inline">\(\textbf{X}^*\)</span> are centered at their sample mean and are scaled by dividing by <span class="math inline">\(\sqrt{n-1}\)</span> times their standard error. As a result, the <span class="math inline">\(\textbf{X}^{*\prime} \textbf{X}^*\)</span> matrix is the matrix of the empirical pairwise correlations of the inputs. The regression coefficients of this model are called the <strong>standardized</strong> coefficients (<span class="math inline">\(beta^*_j\)</span>) and the variance inflation factors are <span class="math display">\[
\text{VIF}_j = \widehat{Var}[\widehat{\beta}^*_j] / \widehat{\sigma}^2
\]</span></p>
<div class="example">
<div class="example-header">
<p>Example: Variance Inflation Factors from Scaled-Centered Regression; Boston Data (Cont’d)</p>
</div>
<div class="example-container">
<p>To compute the VIFs in the model for the Boston data, we compute first the centered-and-scaled <span class="math inline">\(\textbf{X}\)</span> matrix. The <code>scale()</code> function in <code>R</code> centers and scales the data by default but uses the standard deviation as the scaling factor. We use a custom scaling so that the <span class="math inline">\(\textbf{X}^{*\prime}\textbf{X}^*\)</span> matrix equals the empirical correlation matrix.</p>
<p>We use <code>model.matrix()</code> to extract the <span class="math inline">\(\textbf{X}\)</span> matrix from the model object computed earlier. The intercept column is replaced with the target values so we can use this matrix as input to a call to <code>lm</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>n_1 <span class="ot">&lt;-</span> <span class="fu">dim</span>(X)[<span class="dv">1</span>] <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>scaled_X <span class="ot">&lt;-</span> <span class="fu">scale</span>(X,</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">center=</span><span class="fu">apply</span>(X,<span class="dv">2</span>,mean),</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">scale =</span><span class="fu">apply</span>(X,<span class="dv">2</span>,sd)<span class="sc">*</span><span class="fu">sqrt</span>(n_1))</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>scaled_X[,<span class="dv">1</span>] <span class="ot">&lt;-</span> Boston<span class="sc">$</span>medv</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(scaled_X)[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="st">"medv"</span></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>ll <span class="ot">&lt;-</span> <span class="fu">lm</span>(medv <span class="sc">~</span> ., <span class="at">data=</span><span class="fu">data.frame</span>(scaled_X))</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ll)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = medv ~ ., data = data.frame(scaled_X))

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.5328     0.2106 107.018  &lt; 2e-16 ***
crim        -20.9558     6.3361  -3.307 0.001010 ** 
zn           24.0276     7.0873   3.390 0.000754 ***
chas         15.5179     4.8758   3.183 0.001551 ** 
nox         -45.2476     9.2059  -4.915 1.21e-06 ***
rm           60.0245     6.4155   9.356  &lt; 2e-16 ***
dis         -70.6350     8.7888  -8.037 6.84e-15 ***
rad          58.6248    12.4060   4.726 3.00e-06 ***
tax         -44.6079    12.7724  -3.493 0.000521 ***
ptratio     -46.0495     6.2792  -7.334 9.24e-13 ***
black        19.0611     5.4858   3.475 0.000557 ***
lstat       -83.8570     7.6104 -11.019  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The variance inflation factors are obtained by dividing the square values in the <code>Std. Error</code> column with the estimator of the residual variance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">summary</span>(ll)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>vif <span class="ot">&lt;-</span> s<span class="sc">$</span>coefficients[,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> s<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>vif</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)        crim          zn        chas         nox          rm 
0.001976285 1.789704160 2.239228671 1.059819222 3.778010991 1.834806373 
        dis         rad         tax     ptratio       black       lstat 
3.443420336 6.861126315 7.272386358 1.757681497 1.341558750 2.581984268 </code></pre>
</div>
</div>
</div>
</div>
<p>The smallest possible VIF value is 1.0, it indicates that the input is not linearly related to the other variables. The thresholds are as follows</p>
<ul>
<li>1 &lt; VIF &lt; 10: moderate collinearity</li>
<li>10 &lt; VIF &lt; 30: moderate to severe collinearity</li>
<li>VIF &gt; 30: severe collinearity problem</li>
</ul>
</section>
<section id="condition-index-and-condition-number" class="level4">
<h4 class="anchored" data-anchor-id="condition-index-and-condition-number">Condition index and condition number</h4>
<p>A formal diagnostic for multicollinearity, based on the eigenvalue decomposition of the (scaled-and-centered) <span class="math inline">\(\textbf{X}^*\)</span> matrix, examines the spread of the eigenvalues of ^{*}$. If <span class="math inline">\(\textbf{X}^*\)</span> is a centered and scaled version of <span class="math inline">\(\textbf{X}\)</span> such that <span class="math inline">\(\textbf{X}^{*\prime}\textbf{X}\)</span> is the empirical correlation matrix, its eigen decomposition is <span class="math display">\[
    \textbf{X}^{*\prime}\textbf{X}= \textbf{Q}\boldsymbol{\Lambda}\textbf{Q}^\prime
\]</span> where <span class="math inline">\(\textbf{Q}\)</span> is a <span class="math inline">\((p+p)\)</span> orthogonal matrix of eigenvectors and <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix with the eigenvalues <span class="math inline">\(\lambda_j\)</span> on its diagonal. The number of eigenvalues close to zero indicates the number of near linear dependencies in <span class="math inline">\(\textbf{X}\)</span> (or <span class="math inline">\(\textbf{X}^*\)</span>). In this centered-and-scaled form the eigenvalues satisfy <span class="math inline">\(\sum_{j=1}^p \lambda_j = p\)</span>, so if some eigenvalues get small, others need to get bigger.</p>
<p>The <strong>condition index</strong> associated with the <span class="math inline">\(j\)</span><sup>th</sup> eigenvalue is <span class="math display">\[
\phi_j = \frac{\max(\lambda_j)}{\lambda_j}
\]</span> and the <strong>condition number</strong> is <span class="math inline">\(\max(\phi_j)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)[,<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>X_star <span class="ot">&lt;-</span> <span class="fu">scale</span>(X,</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">center=</span><span class="fu">apply</span>(X,<span class="dv">2</span>,mean),</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale=</span><span class="fu">apply</span>(X,<span class="dv">2</span>,sd)<span class="sc">*</span><span class="fu">sqrt</span>((n_1)))</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>XpX_scaled <span class="ot">&lt;-</span> <span class="fu">t</span>(X_star) <span class="sc">%*%</span> X_star</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>eigen_decomp <span class="ot">&lt;-</span> <span class="fu">eigen</span>(XpX_scaled)</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>evals <span class="ot">&lt;-</span> eigen_decomp<span class="sc">$</span>values</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>evals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 4.88096727 1.27924337 1.23924750 0.83617610 0.83188592 0.65049762
 [7] 0.49759783 0.30025208 0.23419413 0.17181685 0.07812133</code></pre>
</div>
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sum of eigenvalues: "</span>, <span class="fu">sum</span>(evals),<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sum of eigenvalues:  11 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>cond_index <span class="ot">&lt;-</span> <span class="fu">max</span>(evals)<span class="sc">/</span>evals</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Condition indices: "</span>, cond_index,<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Condition indices:  1 3.815511 3.938654 5.837248 5.867352 7.503436 9.809061 16.25623 20.84154 28.40797 62.47931 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>cond_number <span class="ot">&lt;-</span> <span class="fu">max</span>(cond_index)</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Condition number: "</span>, cond_number,<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Condition number:  62.47931 </code></pre>
</div>
</div>
<p>Condition indices larger than 900 indicate that near linear dependencies exist.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>In contrast to variance inflation factors, where the <span class="math inline">\(j\)</span><sup>th</sup> factor is associated with the <span class="math inline">\(j\)</span><sup>th</sup> input, the eigenvalue <span class="math inline">\(\lambda_j\)</span> is not associated with a particular input. It is associated with a linear combination of all the <span class="math inline">\(X\)</span>s.</p>
</div>
</div>
<p>An obvious remedy of the multicollinearity is to remove inputs that are associated with high variance inflation factors and to refit the model. If you cannot remove the variables from the model a different estimation method is called for. Regularization methods such as Ridge regression or Lasso regression handle high-dimensional problems and reduce the instability of the least-squares estimates by shrinking their values (suppressing the high variability of the coefficients). At the cost of introducing some bias, these estimators drastically reduce variability for an overall better mean square prediction error.</p>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure&nbsp;7.1: </span>
<span class="glightbox-desc lightbox-desc-2">Figure&nbsp;7.2: Models with a categorical and a continuous input with and without interactions.</span>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-James2013_ISLR2" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. <em>An Introduction to Statistical Learning: With Applications in r, 2nd Ed.</em> Springer. <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./regintro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./regfeature.html" class="pagination-link" aria-label="Feature Selection and Regularization">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","descPosition":"bottom","closeEffect":"zoom","loop":false,"openEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>