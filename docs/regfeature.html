<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning - 8&nbsp; Feature Selection and Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./regnlr.html" rel="next">
<link href="./regglobal.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regintro.html">Part II. Supervised Learning I: Regression</a></li><li class="breadcrumb-item"><a href="./regfeature.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statistical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./biasvariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learningtypes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Types of Statistical Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Supervised Learning I: Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regglobal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regfeature.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regnlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regdiscrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Target Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglocal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Local Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Supervised Learning II: Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classintro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regression Approach to Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification with Random Inputs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supportvectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vectors</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Decision Trees</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regression and Classification Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treesinR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Trees in <code>R</code></span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Ensemble Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ensemble_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Model Averaging</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Part VI. Unsupervised Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsuper_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Principal Component Analysis (PCA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mbc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Model-based Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./arules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Association Rules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Part VII. Supervised Learning III: Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Additive Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corrdata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Correlated Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Mixed Models for Longitudinal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Part VIII. Neural Networks and Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Training Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ann_R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Neural Networks in <code>R</code> (with Keras)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reinforcement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Part IX. Explainability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./explainability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Interpretability and Explainability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-feature-select" id="toc-sec-feature-select" class="nav-link active" data-scroll-target="#sec-feature-select"><span class="header-section-number">8.1</span> Algorithmic Feature Selection</a>
  <ul>
  <li><a href="#indirect-estimates-of-test-error" id="toc-indirect-estimates-of-test-error" class="nav-link" data-scroll-target="#indirect-estimates-of-test-error">Indirect Estimates of Test Error</a>
  <ul class="collapse">
  <li><a href="#mallows-c_p" id="toc-mallows-c_p" class="nav-link" data-scroll-target="#mallows-c_p">Mallows’ <span class="math inline">\(C_p\)</span></a></li>
  <li><a href="#aic-and-bic" id="toc-aic-and-bic" class="nav-link" data-scroll-target="#aic-and-bic">AIC and BIC</a></li>
  <li><a href="#adjusted-r2" id="toc-adjusted-r2" class="nav-link" data-scroll-target="#adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></a></li>
  </ul></li>
  <li><a href="#best-subset-selection" id="toc-best-subset-selection" class="nav-link" data-scroll-target="#best-subset-selection">Best Subset Selection</a></li>
  <li><a href="#forward-selection" id="toc-forward-selection" class="nav-link" data-scroll-target="#forward-selection">Forward Selection</a></li>
  <li><a href="#backward-selection" id="toc-backward-selection" class="nav-link" data-scroll-target="#backward-selection">Backward Selection</a></li>
  <li><a href="#stepwise-selection" id="toc-stepwise-selection" class="nav-link" data-scroll-target="#stepwise-selection">Stepwise Selection</a></li>
  <li><a href="#feature-selection-with-cross-validation" id="toc-feature-selection-with-cross-validation" class="nav-link" data-scroll-target="#feature-selection-with-cross-validation">Feature Selection with Cross-validation</a></li>
  </ul></li>
  <li><a href="#sec-regularization" id="toc-sec-regularization" class="nav-link" data-scroll-target="#sec-regularization"><span class="header-section-number">8.2</span> Regularization</a>
  <ul>
  <li><a href="#shrinkage-estimation" id="toc-shrinkage-estimation" class="nav-link" data-scroll-target="#shrinkage-estimation">Shrinkage Estimation</a></li>
  <li><a href="#sec-regularization-ridge" id="toc-sec-regularization-ridge" class="nav-link" data-scroll-target="#sec-regularization-ridge">Ridge Regression</a>
  <ul class="collapse">
  <li><a href="#cross-validation-for-lambda" id="toc-cross-validation-for-lambda" class="nav-link" data-scroll-target="#cross-validation-for-lambda">Cross-validation for <span class="math inline">\(\lambda\)</span></a></li>
  <li><a href="#ridge-trace" id="toc-ridge-trace" class="nav-link" data-scroll-target="#ridge-trace">Ridge trace</a></li>
  <li><a href="#high-dimensional-ridge-regression" id="toc-high-dimensional-ridge-regression" class="nav-link" data-scroll-target="#high-dimensional-ridge-regression">High-dimensional ridge regression</a></li>
  </ul></li>
  <li><a href="#sec-regularization-lasso" id="toc-sec-regularization-lasso" class="nav-link" data-scroll-target="#sec-regularization-lasso">Lasso Regression</a>
  <ul class="collapse">
  <li><a href="#high-dimensional-lasso-regression" id="toc-high-dimensional-lasso-regression" class="nav-link" data-scroll-target="#high-dimensional-lasso-regression">High-dimensional lasso regression</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-feature-dimred" id="toc-sec-feature-dimred" class="nav-link" data-scroll-target="#sec-feature-dimred"><span class="header-section-number">8.3</span> Dimension Reduction</a>
  <ul>
  <li><a href="#principal-components" id="toc-principal-components" class="nav-link" data-scroll-target="#principal-components">Principal Components</a></li>
  <li><a href="#sec-pcr" id="toc-sec-pcr" class="nav-link" data-scroll-target="#sec-pcr">Principal Component Regression (PCR)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regintro.html">Part II. Supervised Learning I: Regression</a></li><li class="breadcrumb-item"><a href="./regfeature.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-feature-reg" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Feature Selection and Regularization</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The classical linear model is a workhorse in data science and statistical learning. It is interpretable, intuitive, easy to fit and to explain. The model is computationally and mathematically straightforward, the properties of parameter estimators are easily derived and well understood.</p>
<p>Also, the classical linear model is surprisingly competitive against more complex alternatives.</p>
<div class="example">
<div class="example-header">
<p>Example: XOR Gate</p>
</div>
<div class="example-container">
<p>The exclusive OR function–also called the XOR Gate–has two binary inputs, <span class="math inline">\(X_1 \in \{0,1\}\)</span> and <span class="math inline">\(X_2 \in \{0,1\}\)</span>. The result of the gate is <span class="math inline">\(Y = 1\)</span> if exactly one of the <span class="math inline">\(X\)</span>s is 1, <span class="math inline">\(Y=0\)</span> otherwise.</p>
<table class="table">
<caption>XOR Gate</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(X_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>We will see in <a href="ann.html" class="quarto-xref"><span>Chapter 31</span></a> how to model the XOR gate with an artificial neural network with a single hidden layer with 2 units. The neural network requires 9 parameters to perfectly model the gate. We could also create a linear model that perfectly models the gate, requiring only three parameters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">x2=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>reg_ia <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x1<span class="sc">*</span>x2 <span class="sc">-</span><span class="dv">1</span>, <span class="at">data=</span>dat)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_ia)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x1 + x2 + x1 * x2 - 1, data = dat)

Residuals:
         1          2          3          4 
-1.178e-16 -1.424e-32  1.298e-32  6.588e-34 

Coefficients:
        Estimate Std. Error    t value Pr(&gt;|t|)    
x1     1.000e+00  1.178e-16  8.489e+15   &lt;2e-16 ***
x2     1.000e+00  1.178e-16  8.489e+15   &lt;2e-16 ***
x1:x2 -2.000e+00  2.040e-16 -9.802e+15   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.178e-16 on 1 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
F-statistic: 4.804e+31 on 3 and 1 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">predict</span>(reg_ia),<span class="dv">3</span>)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 2 3 4 
0 1 1 0 </code></pre>
</div>
</div>
<p>The model <span class="math inline">\(Y = x_1 + x_2 - 2 x_1 x_2\)</span> perfectly fits the gate. It has an <span class="math inline">\(R^2=1\)</span>, <span class="math inline">\(SSE=0\)</span>, and still one degree of freedom left for the error. The model has 1/3 the parameters of the neural network and is intrinsically interpretable.</p>
</div>
</div>
<p>The classical linear model works well if <span class="math inline">\(n \gg p\)</span> and the input variables do not exhibit multicollinearity. If the model errors have zero mean, the least-squares estimators are unbiased, and thus the model is unbiased. However, it does not necessarily have the lowest mean-squared error. As <span class="math inline">\(p\)</span> grows, for fixed sample size <span class="math inline">\(n\)</span>, problems start to mount:</p>
<ul>
<li>The OLS estimator becomes more unstable, exacerbated by multicollinearity</li>
<li>When <span class="math inline">\(p &gt; n\)</span>, a unique OLS estimator does not exist, <span class="math inline">\((\textbf{X}^\prime\textbf{X})^{-1}\)</span> cannot be computed.</li>
<li>Other estimation approaches lead to models with smaller mean-squared prediction error.</li>
</ul>
<p>With increasing <span class="math inline">\(p\)</span>, relative to <span class="math inline">\(n\)</span>, the regression problem turns into a <strong>high-dimensional</strong> problem. Situations in which there are more input variables than observations are not at all uncommon.</p>
<div class="example">
<div class="example-header">
<p>Example: Microarray Analysis</p>
</div>
<div class="example-container">
<p>A microarray is a rectangular array in which the expression of genes is compared between two samples, often a reference (healthy individual) and an experimental sample (cancer patient). The samples are dyed green and red. If gene expression is higher [lower] in the experimental sample the corresponding spot on the microarray appears red [green]. A 20 x 20 array yields 400 inputs, but you might have only data on 10 arrays.</p>
<div id="fig-microarray" class="quarto-figure quarto-figure-center quarto-float anchored" data-out.width="80%" data-fig.align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-microarray-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/microarray.png" class="img-fluid figure-img" data-fig.align="center" data-out.width="80%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-microarray-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: A gene expression microarray
</figcaption>
</figure>
</div>
</div>
</div>
<p>How can we address the shortcomings of the classical linear model as problems become higher dimensional?</p>
<p><strong>Algorithmic feature selection</strong> uses search techniques and cross-validation to find well-fitting models among the <span class="math inline">\(2^p\)</span> possible linear models you can build with <span class="math inline">\(p\)</span> inputs (<a href="#sec-feature-select" class="quarto-xref"><span>Section 8.1</span></a>). The goal is to find a relatively small set of inputs that explains sufficient variability in the data and to eliminate inputs that do not contribute (much) to explaining the variability.</p>
<p><strong>Regularization</strong> techniques consider all <span class="math inline">\(p\)</span> inputs, even if <span class="math inline">\(p\)</span> is large, and allay the shortcomings of the ordinary least squares estimator by penalizing its tendency toward instability (<a href="#sec-regularization" class="quarto-xref"><span>Section 8.2</span></a>). Regularization penalties shrink the estimators toward zero, thereby limiting the variability they can inflict on the model. The resulting estimators are biased, but at the same time their variability is suppressed enough to lead to an overall smaller mean-square prediction error compared to ordinary least squares.</p>
<p><strong>Dimension reduction</strong> methods derive <span class="math inline">\(m\)</span> linear combinations of the <span class="math inline">\(p\)</span> inputs where <span class="math inline">\(m \ll p\)</span>. The <span class="math inline">\(m\)</span> combinations are treated as inputs to the model, thereby reducing the dimension of the regression while still consuming information from all <span class="math inline">\(p\)</span> inputs (<a href="#sec-feature-dimred" class="quarto-xref"><span>Section 8.3</span></a>).</p>
<p>In summary, feature selection chooses a subset of the <span class="math inline">\(p\)</span> inputs, regularization keeps all <span class="math inline">\(p\)</span> inputs and introduces bias to limit variability, dimension reduction uses <span class="math inline">\(m &lt; p\)</span> linear combinations of the variables.</p>
<section id="sec-feature-select" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec-feature-select"><span class="header-section-number">8.1</span> Algorithmic Feature Selection</h2>
<p>Suppose you have <span class="math inline">\(p\)</span> candidate input variables. How many possible linear models are there? One model without any inputs, one model with all <span class="math inline">\(p\)</span> inputs, <span class="math inline">\(p\)</span> models have a single input, and so on. The number of models having <span class="math inline">\(k \le p\)</span> inputs is <span class="math display">\[
{p \choose k} = \frac{p!}{k!(p-k)!}
\]</span> and the total number of models is <span class="math display">\[
\sum_{k=0}^p {p\choose k} = {p\choose0} + {p\choose 1} + \cdots + {p\choose p-1} + {p\choose p}
\]</span> By the <strong>binomial theorem</strong>, <span class="math inline">\((x+y)^n = \sum_{k=0}^n {n\choose k}x^{n-k}y^{k}\)</span>. Setting <span class="math inline">\(x=y=1\)</span>, we find that the total number of models equals <span class="math inline">\(2^p\)</span>. This is a very large number even for moderate <span class="math inline">\(p\)</span>. With <span class="math inline">\(p=10\)</span> there are “only” 1,024 models, with <span class="math inline">\(p=20\)</span> this number increases to 1,048,576, with <span class="math inline">\(p=30\)</span> there are 1,073,741,824 models–more than a billion.</p>
<p>Evaluating all regression models becomes unfeasible quickly due to the large number of models. Instead, we use a two-step process:</p>
<ol type="1">
<li>Among the set <span class="math inline">\(\{M_k\}\)</span> of <span class="math inline">\(k\)</span>-size models, find the best candidate and call it <span class="math inline">\(M_k^*\)</span>.</li>
<li>Choose the single best model among <span class="math inline">\(M_0^*, M_1^*, \cdots, M_p^*\)</span>.</li>
</ol>
<p>The feature selection methods differ in how they construct the candidate sets <span class="math inline">\(\{M_k\}\)</span> in step 1. For example, best subset selection uses efficient search algorithms to explore the space of possible models, forward selection considers <span class="math inline">\(\{M_k\}\)</span> as a superset of <span class="math inline">\(\{M_{k-1}\}\)</span>, backward selection considers <span class="math inline">\(\{M_k\}\)</span> as a subset of <span class="math inline">\(\{M_{k+1\}\)</span>.</p>
<p>In step 1 the “best” model is chosen among the <span class="math inline">\(k\)</span>-size models using criteria such as SSE, <span class="math inline">\(R^2\)</span>, <span class="math inline">\(p\)</span>-values, etc. In step 2 the models are compared based on an estimate of test error using cross-validation or, more commonly, an indirect estimate of test error.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>When choosing <span class="math inline">\(p\)</span>-values to judge models against each other during variable selection, you are performing many tests and you are not testing hypotheses in the typical sense. Feature selection is not akin to formulating a research hypothesis, collecting data, and testing whether the data support the hypothesis. The use of <span class="math inline">\(p\)</span>-values during variable selection is more akin to a rough check whether adding or removing a feature should be considered. Thus, larger thresholds such as <span class="math inline">\(p=0.1\)</span> or <span class="math inline">\(p=0.2\)</span> are used.</p>
<p>Even if the process would be testing hypotheses in the usual sense, the large number of comparisons, each with a chance of a Type-I or Type-II error, creates a massive <strong>multiplicity</strong> (multiple testing) problem.</p>
</div>
</div>
<section id="indirect-estimates-of-test-error" class="level3">
<h3 class="anchored" data-anchor-id="indirect-estimates-of-test-error">Indirect Estimates of Test Error</h3>
<p>Cross-validation approaches such as train:test split, leave-one-out cross-validation, or <span class="math inline">\(k\)</span>-fold cross-validation produce a <strong>direct</strong> estimate of the mean-squared prediction error. <strong>Indirect</strong> estimates of the test error make adjustments to quantities derived from the training data and are easy to compute. These estimates are used to quickly quantify model performance without random elements and to compare non-nested models. The best <span class="math inline">\(M_k^*\)</span> and best <span class="math inline">\(M_j^*\)</span> models in feature selection are not necessarily nested in the sense that one model can be reduced from the other–they might have completely different inputs. Those models cannot be compared based on <span class="math inline">\(p\)</span>-values or just SSE. Some adjustments is necessary to incorporate the model complexity and to avoid overfitting.</p>
<section id="mallows-c_p" class="level4">
<h4 class="anchored" data-anchor-id="mallows-c_p">Mallows’ <span class="math inline">\(C_p\)</span></h4>
<p>The <span class="math inline">\(C_p\)</span> statistic of <span class="citation" data-cites="Mallows1973">Mallows (<a href="references.html#ref-Mallows1973" role="doc-biblioref">1973</a>)</span> estimates the average sum of prediction errors <span class="math display">\[
\Gamma_p = \frac{1}{\sigma^2}\text{E}\left [\sum_{i=1}^n \left(\widehat{Y}_i - \text{E}[Y_i|\textbf{x}_i]\right)^2 \right]
\]</span> It is a prediction-oriented criteria that seeks to strike a balance between the bias of an underfit model and the variability of an overfit model. <span class="math inline">\(\Gamma_p\)</span> expands into <span id="eq-cp-pop"><span class="math display">\[
\Gamma_p = \frac{1}{\sigma^2} \left(\sum_{i=1}^n\text{Var}[\widehat{Y}_i] + \sum_{i=1}^n\left[\text{Bias}(\widehat{Y}_i)\right]^2 \right)
\tag{8.1}\]</span></span></p>
<p>The contribution of an overfit model is the first term in parentheses, <span class="math inline">\(\text{Var}[\widehat{Y}_i]\)</span>, the contribution of an underfit model is the squared bias term. It is insightful to take a look at the first piece, the sum of the variances of the predicted values. Suppose we have a model with <span class="math inline">\(d\)</span> inputs. From <span class="math inline">\(\widehat{\textbf{Y}} = \textbf{X}\widehat{\boldsymbol{\beta}}\)</span> it follows that <span class="math display">\[
\text{Var}[\widehat{\textbf{Y}}] = \sigma^2 \textbf{X}(\textbf{X}^\prime\textbf{X})^{-1}\textbf{X}^\prime = \sigma^2 \textbf{H}
\]</span> The Hat matrix is a projection matrix of rank <span class="math inline">\(d+1\)</span> and thus <span class="math inline">\(\sum_{i=1}^n \text{Var}[\widehat{Y}_i] = \sigma^2(d+1)\)</span>. The sum of the variances of the predicted values will go up when inputs are added to the model, whether the inputs are useful in explaining variability in <span class="math inline">\(Y\)</span> or not. Adding junk variables to a model results in greater variability of the predicted values–there is “no free lunch”.</p>
<p>The bias term in <a href="#eq-cp-pop" class="quarto-xref">Equation&nbsp;<span>8.1</span></a> can be estimated as <span class="math display">\[
(\widehat{\sigma}^2_d - \sigma^2)(n-d-1)
\]</span> where <span class="math inline">\(\widehat{\sigma}^2_d\)</span> is the estimate of <span class="math inline">\(\sigma^2\)</span> in a model with <span class="math inline">\(d\)</span> inputs. Putting everything together we arrive at an estimator of <span class="math inline">\(\Gamma_p\)</span>, known as Mallow’s <span class="math inline">\(C_p\)</span> statistic <span class="math display">\[
C_p = \frac{1}{\widehat{\sigma}^2}\left(\widehat{\sigma}^2(d+1) + (\widehat{\sigma}^2_d - \widehat{\sigma}^2)(n-d-1) \right) = \frac{\text{SSE}}{\widehat{\sigma}^2} - n + 2(d+1)
\]</span> where <span class="math inline">\(d\)</span> is the number of inputs in the model, <span class="math inline">\((d+1)\)</span> accounts for the intercept. In feature selection, <span class="math inline">\(\widehat{\sigma}^2\)</span> is based on the full model with <span class="math inline">\(p\)</span> inputs, since this model most likely yields the least biased estimator of the variance of the model errors. Among a set of competing models, select the one with the <strong>smallest</strong> <span class="math inline">\(C_p\)</span> statistic. Among models with the same number of inputs, <span class="math inline">\(d\)</span>, selection based on <span class="math inline">\(C_p\)</span> leads to choosing the model with the smallest SSE. The term <span class="math inline">\(2(d+1)\)</span> can be viewed as a penalty term for model complexity. A larger model has to reduce SSE more substantially to overcome the additional parameters.</p>
<p>An alternative formulation for Mallow’s statistic is <span class="math display">\[
C_p^\prime = \frac{1}{n}\left(\text{SSE} + 2(d+1)\widehat{\sigma}^2\right)
\]</span> <span class="math inline">\(C_p\)</span> and <span class="math inline">\(C_p^\prime\)</span> are not identical but they lead to the selection of the same model if models are chosen according to smaller <span class="math inline">\(C_p\)</span> or smaller <span class="math inline">\(C_p^\prime\)</span> values.</p>
</section>
<section id="aic-and-bic" class="level4">
<h4 class="anchored" data-anchor-id="aic-and-bic">AIC and BIC</h4>
<p>Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are based on likelihood theory and assume a distribution for the data, given the parameter estimates. In linear models, this distribution is typically Gaussian, and the criteria are computed as follows:</p>
<p><span class="math display">\[
\text{AIC} = \frac{1}{n}\left(\text{SSE} + 2(d+1)\widehat{\sigma}^2\right)
\]</span> <span class="math display">\[
\text{BIC} = \frac{1}{n}\left(\text{SSE} + \log(n)(d+1)\widehat{\sigma}^2\right)
\]</span> In this formulation, choose the model with the smaller AIC and smaller BIC. AIC is identical to <span class="math inline">\(C_p^\prime\)</span>, selection based on AIC and <span class="math inline">\(C_p\)</span> lead to the same model.</p>
<p>BIC applies a stronger complexity penalty when <span class="math inline">\(\log(n) &gt; 2\)</span>, (<span class="math inline">\(n &gt; 7\)</span>), and thus tends to select models smaller than <span class="math inline">\(C_p\)</span> or AIC.</p>
</section>
<section id="adjusted-r2" class="level4">
<h4 class="anchored" data-anchor-id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h4>
<p>This statistic applies a correction to <span class="math inline">\(R^2\)</span> that penalizes larger models. It is not an estimate of the test error, but is still useful to select models. For a model with <span class="math inline">\(d\)</span> inputs, <span class="math display">\[
\text{Adjusted } R^2 = 1 - \frac{\text{SSE}}{\text{SST}}\left(\frac{n-1}{n-d-1} \right) = 1-(1-R^2)\left(\frac{n-1}{n-d-1} \right)
\]</span> When inputs are added to a model, SSE decreases and <span class="math inline">\(R^2\)</span> increases. However, <span class="math inline">\(\text{SSE}/(n-d-1)\)</span> may increase or decrease. If unimportant variables are added, the reduction in SSE does not offset the loss of degree of freedom and Adjusted <span class="math inline">\(R^2\)</span> will be smaller. When selecting models based on Adjusted <span class="math inline">\(R^2\)</span>, choose the model with the larger value.</p>
<!--

::: {.cell}

```{.r .cell-code}
library(ISLR2)
regfit <- lm(Balance ~ .,data=Credit)
s <- summary(regfit)
r2 <- s$r.squared
d <- length(s$coefficients[,1])-1
n <- length(s$residuals)
adjr2 <- 1 - (1-r2)*(n-1)/(n-d-1)
adjr2
```

::: {.cell-output .cell-output-stdout}

```
[1] 0.9538287
```


:::
:::

-->
</section>
</section>
<section id="best-subset-selection" class="level3">
<h3 class="anchored" data-anchor-id="best-subset-selection">Best Subset Selection</h3>
<p>Best subsets regression uses an efficient branch-and-bound algorithm to explore the space of all possible models. A branch and bound algorithm does not visit all possible models. For example, the LEAPS algorithm of <span class="citation" data-cites="FurnivalWilson1974">Furnival and Wilson (<a href="references.html#ref-FurnivalWilson1974" role="doc-biblioref">1974</a>)</span> uses a separate tree as the bounding function that eliminates models that need not be considered given the branches of models that have already been seen. This algorithm is implemented as <code>method="exhaustive"</code> in the <code>regsubsets</code> function of the <code>leaps</code> package in <code>R</code>.</p>
<div class="example">
<div class="example-header">
<p>Example: Credit Data from ISLR2</p>
</div>
<div class="example-container">
<p>We are using here the <code>Credit</code> data from <span class="citation" data-cites="James2013_ISLR2">James et al. (<a href="references.html#ref-James2013_ISLR2" role="doc-biblioref">2021</a>, Sec 3.3, p.85)</span>. The data are simulated observations (400) on</p>
<ul>
<li>Income: income in $1,000</li>
<li>Limit: credit limit</li>
<li>Rating: credit rating</li>
<li>Cards: number of credit cards</li>
<li>Age: age in years</li>
<li>Education: education in years</li>
<li>Own: two-level factor whether individual owns a home (“Yes”/“No”)</li>
<li>Student: two-level factor whether individual is a student (“Yes”/“No”)</li>
<li>Married: two-level factor whether individual is married (“Yes”/“No”)</li>
<li>Region: three-level factor of geographic location (“East”, “South”, “West”)</li>
<li>Balance: average credit card balance in $</li>
</ul>
<p>The target variable is <code>Balance</code>.</p>
<p>There are 10 input variables, 6 numeric variables, 3 two-level factors and one three-level factor (<code>Region</code>). When all variables are included, this leads to a model with <span class="math inline">\(p = 6 + 3 + 2 = 11\)</span> predictors. The three-level <code>Region</code> variable expands to two columns in <span class="math inline">\(\textbf{X}\)</span>, the first level serves as default as the reference level.</p>
<p>The following code performs best subset regression with the leaps algorithm. The <code>nvmax</code> parameter limits the maximum size of subsets to examine by the leaps-and-bound algorithm; the default is <code>nvmax=8</code>. Setting it to <code>NULL</code> forces the algorithm to consider all subsets up to size <span class="math inline">\(p\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>regfit <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Balance <span class="sc">~</span> ., <span class="at">data=</span>Credit, <span class="at">method=</span><span class="st">"exhaustive"</span>, <span class="at">nvmax=</span><span class="cn">NULL</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>s_all <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>s_all</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subset selection object
Call: regsubsets.formula(Balance ~ ., data = Credit, method = "exhaustive", 
    nvmax = NULL)
11 Variables  (and intercept)
            Forced in Forced out
Income          FALSE      FALSE
Limit           FALSE      FALSE
Rating          FALSE      FALSE
Cards           FALSE      FALSE
Age             FALSE      FALSE
Education       FALSE      FALSE
OwnYes          FALSE      FALSE
StudentYes      FALSE      FALSE
MarriedYes      FALSE      FALSE
RegionSouth     FALSE      FALSE
RegionWest      FALSE      FALSE
1 subsets of each size up to 11
Selection Algorithm: exhaustive
          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes
1  ( 1 )  " "    " "   "*"    " "   " " " "       " "    " "        " "       
2  ( 1 )  "*"    " "   "*"    " "   " " " "       " "    " "        " "       
3  ( 1 )  "*"    " "   "*"    " "   " " " "       " "    "*"        " "       
4  ( 1 )  "*"    "*"   " "    "*"   " " " "       " "    "*"        " "       
5  ( 1 )  "*"    "*"   "*"    "*"   " " " "       " "    "*"        " "       
6  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       " "    "*"        " "       
7  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
8  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
9  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
10  ( 1 ) "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
11  ( 1 ) "*"    "*"   "*"    "*"   "*" "*"       "*"    "*"        "*"       
          RegionSouth RegionWest
1  ( 1 )  " "         " "       
2  ( 1 )  " "         " "       
3  ( 1 )  " "         " "       
4  ( 1 )  " "         " "       
5  ( 1 )  " "         " "       
6  ( 1 )  " "         " "       
7  ( 1 )  " "         " "       
8  ( 1 )  " "         "*"       
9  ( 1 )  " "         "*"       
10  ( 1 ) "*"         "*"       
11  ( 1 ) "*"         "*"       </code></pre>
</div>
</div>
<!---

::: {.cell}

```{.r .cell-code}
X <- model.matrix(Balance ~ ., data=Credit)
XpX = t(X) %*% X
H = X %*% solve(XpX) %*% t(X)
sum(diag(H))
```

::: {.cell-output .cell-output-stdout}

```
[1] 12
```


:::

```{.r .cell-code}
ncol(X)
```

::: {.cell-output .cell-output-stdout}

```
[1] 12
```


:::

```{.r .cell-code}
X[1:10,]
```

::: {.cell-output .cell-output-stdout}

```
   (Intercept)  Income Limit Rating Cards Age Education OwnYes StudentYes
1            1  14.891  3606    283     2  34        11      0          0
2            1 106.025  6645    483     3  82        15      1          1
3            1 104.593  7075    514     4  71        11      0          0
4            1 148.924  9504    681     3  36        11      1          0
5            1  55.882  4897    357     2  68        16      0          0
6            1  80.180  8047    569     4  77        10      0          0
7            1  20.996  3388    259     2  37        12      1          0
8            1  71.408  7114    512     2  87         9      0          0
9            1  15.125  3300    266     5  66        13      1          0
10           1  71.061  6819    491     3  41        19      1          1
   MarriedYes RegionSouth RegionWest
1           1           1          0
2           1           0          1
3           0           0          1
4           0           0          1
5           1           1          0
6           0           1          0
7           0           0          0
8           0           0          1
9           0           1          0
10          1           0          0
```


:::
:::

--->
<p>With <span class="math inline">\(p=11\)</span> predictors, there are 11 sets <span class="math inline">\(\{M_1\}, \cdots, \{M_{11}\}\)</span>. The best single-predictor model–the best model in the set <span class="math inline">\(\{M_1\}\)</span>– is <span class="math display">\[
M_1^*: \text{Balance} = \beta_0 + \beta_1\text{Rating} + \epsilon
\]</span> The best model in <span class="math inline">\(\{M_2\}\)</span> is <span class="math display">\[
M_2^*: \text{Balance} = \beta_0 + \beta_1\text{Income} + \beta_2\text{Rating} + \epsilon
\]</span> and so on. Notice that <code>Rating</code> is included in <span class="math inline">\(M_1^*\)</span>, <span class="math inline">\(M_2^*\)</span>, and <span class="math inline">\(M_3^*\)</span> but is not present in <span class="math inline">\(M_4^*\)</span>.</p>
<p>To select the best model from the <span class="math inline">\(k\)</span>-best models, we look at the model summary performance measures:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>s_all<span class="sc">$</span>cp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1800.308406  685.196514   41.133867   11.148910    8.131573    5.574883
 [7]    6.462042    7.845931    9.192355   10.472883   12.000000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>s_all<span class="sc">$</span>bic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  -535.9468  -814.1798 -1173.3585 -1198.0527 -1197.0957 -1195.7321
 [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>s_all<span class="sc">$</span>adjr2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.7452098 0.8744888 0.9494991 0.9531099 0.9535789 0.9539961 0.9540098
 [8] 0.9539649 0.9539243 0.9538912 0.9538287</code></pre>
</div>
</div>
<p>Selecting models according to <span class="math inline">\(C_p\)</span> the best 6-input model is chosen. Base on BIC and Adjusted <span class="math inline">\(R^2\)</span> we would choose the best 4-input and best 7-input model, respectively (<a href="#fig-best-subset-results" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(s_all<span class="sc">$</span>cp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(s_all<span class="sc">$</span>bic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(s_all<span class="sc">$</span>adjr2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-best-subset-results" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-best-subset-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regfeature_files/figure-html/fig-best-subset-results-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-best-subset-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Results of best subsets regression.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="forward-selection" class="level3">
<h3 class="anchored" data-anchor-id="forward-selection">Forward Selection</h3>
<p>Forward selection greatly reduces the number of models being evaluated, since at each stage <span class="math inline">\(k\)</span>, the set <span class="math inline">\(\{M_{k+1}\}\)</span> contains the <span class="math inline">\(p-k\)</span> models with one additional predictor variable. The process starts with the null model, <span class="math inline">\(M_0\)</span>, containing only the intercept. All <span class="math inline">\(p\)</span> predictors are then evaluated and the “best” is added to the model. Depending on the criteria, this is the predictor that reduces SSE or increases <span class="math inline">\(R^2\)</span> the most, or has the smallest <span class="math inline">\(p\)</span>-value. Suppose that <span class="math inline">\(x_4\)</span> was added to the model in this round. We now have <span class="math inline">\(M_1^*\)</span> and define as <span class="math inline">\(\{M_2\}\)</span> the set of models that contain <span class="math inline">\(x_4\)</span> and one additional predictor. At this stage we evaluate only <span class="math inline">\(p-1\)</span> models, rather than <span class="math inline">\({p \choose 2}\)</span> models.</p>
<p>In summary, only one predictor is added during each stage of forward selection, input variables that have been added in a previous stage remain in the model, and the total number of models evaluated is <span class="math display">\[
\sum_{k=0}^p (p-k) = 1 + \frac{p(p+1)}{2}
\]</span> Recall that with <span class="math inline">\(p=30\)</span>, evaluating all models requires visiting 1,073,741,824 models. Forward selection evaluates only 466 of them.</p>
<p>Forward selection has clear advantages:</p>
<ul>
<li>the number of models evaluated is small</li>
<li>the algorithm can be applied when <span class="math inline">\(p &gt; n\)</span> since it does not need to fit a model with all predictors</li>
</ul>
<p>There are also some clear disadvantages:</p>
<ul>
<li>it is not guaranteed that the algorithm visits the best model; in fact it is not even guaranteed that the algorithm finds the best <span class="math inline">\(k\)</span>-size model if <span class="math inline">\(k \ge 1\)</span>.</li>
<li>variables that are added early in the cycle can become unimportant with the addition of variables later in the cycle. A variable is not removed by the algorithm once it is added to the model.</li>
</ul>
<p>To illustrate these points, consider that <span class="math inline">\(x_4\)</span> is added to the model at stage <span class="math inline">\(k=0\)</span>. At <span class="math inline">\(k=1\)</span> input variable <span class="math inline">\(x_2\)</span> is chosen because it reduces SSE the most when one of the remaining predictors are added to a model that contains <span class="math inline">\(x_4\)</span>. The model <span class="math inline">\(M_2^*\)</span> has inputs <span class="math inline">\(\{x_4, x_2\}\)</span> according to forward selection. The best two-predictor model might be <span class="math inline">\(\{x_1,x_3\}\)</span> if all possible models with <span class="math inline">\(p=2\)</span> had been examined.</p>
<p>After the best <span class="math inline">\(k\)</span>-size models are found, the winning model is selected among those based on <span class="math inline">\(C_p\)</span>, BIC, Adjusted <span class="math inline">\(R^2\)</span>, or cross-validation. This is the second step of the general procedure for feature selection.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A form of forward selection does not select among the <span class="math inline">\(M_k^*\)</span> models in the second step. Instead, it specifies threshold values that a variable has to overcome to get added to the model, for example, a <span class="math inline">\(p\)</span>-value &lt; 0.1. Forward selection then continues until no variable outside of the model can be added to the model. If this happens at stage <span class="math inline">\(k+1\)</span>, the process stops and <span class="math inline">\(M_k^*\)</span> is chosen as the winning model.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: Credit Data from ISLR2 (Cont’d)</p>
</div>
<div class="example-container">
<p>Forward selection can be performed with <code>method="forward"</code> in <code>regsubsets</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>regfit <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Balance <span class="sc">~</span> ., <span class="at">data=</span>Credit, <span class="at">method=</span><span class="st">"forward"</span>, <span class="at">nvmax=</span><span class="cn">NULL</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>s_forw <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>s_forw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subset selection object
Call: regsubsets.formula(Balance ~ ., data = Credit, method = "forward", 
    nvmax = NULL)
11 Variables  (and intercept)
            Forced in Forced out
Income          FALSE      FALSE
Limit           FALSE      FALSE
Rating          FALSE      FALSE
Cards           FALSE      FALSE
Age             FALSE      FALSE
Education       FALSE      FALSE
OwnYes          FALSE      FALSE
StudentYes      FALSE      FALSE
MarriedYes      FALSE      FALSE
RegionSouth     FALSE      FALSE
RegionWest      FALSE      FALSE
1 subsets of each size up to 11
Selection Algorithm: forward
          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes
1  ( 1 )  " "    " "   "*"    " "   " " " "       " "    " "        " "       
2  ( 1 )  "*"    " "   "*"    " "   " " " "       " "    " "        " "       
3  ( 1 )  "*"    " "   "*"    " "   " " " "       " "    "*"        " "       
4  ( 1 )  "*"    "*"   "*"    " "   " " " "       " "    "*"        " "       
5  ( 1 )  "*"    "*"   "*"    "*"   " " " "       " "    "*"        " "       
6  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       " "    "*"        " "       
7  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
8  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
9  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
10  ( 1 ) "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
11  ( 1 ) "*"    "*"   "*"    "*"   "*" "*"       "*"    "*"        "*"       
          RegionSouth RegionWest
1  ( 1 )  " "         " "       
2  ( 1 )  " "         " "       
3  ( 1 )  " "         " "       
4  ( 1 )  " "         " "       
5  ( 1 )  " "         " "       
6  ( 1 )  " "         " "       
7  ( 1 )  " "         " "       
8  ( 1 )  " "         "*"       
9  ( 1 )  " "         "*"       
10  ( 1 ) "*"         "*"       
11  ( 1 ) "*"         "*"       </code></pre>
</div>
</div>
<p>In the first step of the algorithm, <span class="math inline">\(k=0\)</span>, the variable <code>Rating</code> is added. It adds the greatest improvement over the intercept-only model among the 11 predictor variables. From now on, every model will contain the <code>Rating</code> variable. Recall that in best subset selection this variable was not part of the best 4-predictor model.</p>
<p>Choosing BIC as the criterion to select among <span class="math inline">\(M_1^*\)</span>–<span class="math inline">\(M_{11}^*\)</span>, the 5-predictor model is chosen; it has the smallest BIC:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>s_forw<span class="sc">$</span>bic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  -535.9468  -814.1798 -1173.3585 -1186.2300 -1197.0957 -1195.7321
 [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(s_forw<span class="sc">$</span>bic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5</code></pre>
</div>
</div>
<p>The best subset selection and forward selection algorithm lead to similar models</p>
<table class="table">
<caption>Models selected by best subset and forward selection</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Best Subset</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Forward</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;">Rating</td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1197.1</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="backward-selection" class="level3">
<h3 class="anchored" data-anchor-id="backward-selection">Backward Selection</h3>
<p>Backward selection is similar to forward selection in that at each stage only a limited number of candidate models are considered, namely those models that have one less predictor than the model in the previous stage. In contrast to forward selection, backward selection starts with the full model with <span class="math inline">\(p\)</span> predictors and attempts to remove one variable at a time. The variable removed is the one that causes the smallest increase in SSE, smallest decrease in <span class="math inline">\(R^2\)</span>, or has the largest <span class="math inline">\(p\)</span>-value.</p>
<p>Backward selection has similar advantages and disadvantages compared to forward selection. It is computationally efficient because it visits only a subset of the possible models; <span class="math inline">\(1 + p(p+1)/2\)</span> models like forward selection. It is also not guaranteed to visit the best <span class="math inline">\(k\)</span>-size model or the best model overall.</p>
<p>If <span class="math inline">\(p &gt; n\)</span>, backward selection is not possible because the full model cannot be fit by least squares without regularization. On the other hand, starting with the full model provides the least biased estimate of the residual variance <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As with forward selection, a form of backward selection uses only <span class="math inline">\(p\)</span>-values or threshold values on change in SSE (<span class="math inline">\(R^2\)</span>) to stop the process of removing predictor variables if at any stage of the algorithm all variables exceed the threshold. That is, no variable can be removed without “significantly” deteriorating the model.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: Credit Data from ISLR2 (Cont’d)</p>
</div>
<div class="example-container">
<p>Backward selection can be performed with <code>method="backward"</code> in <code>regsubsets</code>. For this data set, the algorithm selects the same model as best subset selection.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>regfit <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Balance <span class="sc">~</span> ., <span class="at">data=</span>Credit, <span class="at">method=</span><span class="st">"backward"</span>, <span class="at">nvmax=</span><span class="cn">NULL</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>s_backw <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>s_backw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subset selection object
Call: regsubsets.formula(Balance ~ ., data = Credit, method = "backward", 
    nvmax = NULL)
11 Variables  (and intercept)
            Forced in Forced out
Income          FALSE      FALSE
Limit           FALSE      FALSE
Rating          FALSE      FALSE
Cards           FALSE      FALSE
Age             FALSE      FALSE
Education       FALSE      FALSE
OwnYes          FALSE      FALSE
StudentYes      FALSE      FALSE
MarriedYes      FALSE      FALSE
RegionSouth     FALSE      FALSE
RegionWest      FALSE      FALSE
1 subsets of each size up to 11
Selection Algorithm: backward
          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes
1  ( 1 )  " "    "*"   " "    " "   " " " "       " "    " "        " "       
2  ( 1 )  "*"    "*"   " "    " "   " " " "       " "    " "        " "       
3  ( 1 )  "*"    "*"   " "    " "   " " " "       " "    "*"        " "       
4  ( 1 )  "*"    "*"   " "    "*"   " " " "       " "    "*"        " "       
5  ( 1 )  "*"    "*"   "*"    "*"   " " " "       " "    "*"        " "       
6  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       " "    "*"        " "       
7  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
8  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
9  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
10  ( 1 ) "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
11  ( 1 ) "*"    "*"   "*"    "*"   "*" "*"       "*"    "*"        "*"       
          RegionSouth RegionWest
1  ( 1 )  " "         " "       
2  ( 1 )  " "         " "       
3  ( 1 )  " "         " "       
4  ( 1 )  " "         " "       
5  ( 1 )  " "         " "       
6  ( 1 )  " "         " "       
7  ( 1 )  " "         " "       
8  ( 1 )  " "         "*"       
9  ( 1 )  " "         "*"       
10  ( 1 ) "*"         "*"       
11  ( 1 ) "*"         "*"       </code></pre>
</div>
</div>
<p>Consider the full model with all 11 predictors first. The variable that causes the smallest increase in SSE or decrease in <span class="math inline">\(R^2\)</span> is <code>Education</code> and is removed. This variable is not considered in subsequent steps. The variable whose removal causes the smallest increase in SSE at the next step is <code>RegionSouth</code> and so on.</p>
<p>Based on BIC, backward selection chooses the same model as best subset selection–for these data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>s_backw<span class="sc">$</span>bic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  -530.7458  -801.5344 -1164.9522 -1198.0527 -1197.0957 -1195.7321
 [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(s_backw<span class="sc">$</span>bic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4</code></pre>
</div>
</div>
<table class="table">
<caption>Models selected by best subset, forward, and backward selection</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Best Subset</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Forward</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;">Rating</td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1197.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Backward</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="stepwise-selection" class="level3">
<h3 class="anchored" data-anchor-id="stepwise-selection">Stepwise Selection</h3>
<p>This selection method combines elements of forward and backward selection. A problem of those algorithms is that once a variable has been added it cannot be removed (forward) or once a variable has been removed it cannot be added (backward) at a later step. A stepwise procedure that starts from the null model examines after the addition of a variable if any of the variables now in the model should be removed. Stepwise procedures, also called hybrid procedures, examine more models than forward or backward methods but do not exhaust the entire space of models.</p>
<p>A variation is the sequential replacement algorithm of <span class="citation" data-cites="Miller1984">Miller (<a href="references.html#ref-Miller1984" role="doc-biblioref">1984</a>)</span> implemented in the <code>leaps</code> package. Instead of removing a variable from a model, replacement attempts to replace any variable in the model with a variable not in the model. Variables are considered for replacement at every step, allowing variables that are being replaced at one stage to re-enter the model at a later stage.</p>
<div class="example">
<div class="example-header">
<p>Example: Credit Data from ISLR2 (Cont’d)</p>
</div>
<div class="example-container">
<p>Sequential replacement selection can be performed with <code>method="seqrep"</code> in <code>regsubsets</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>regfit <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Balance <span class="sc">~</span> ., <span class="at">data=</span>Credit, <span class="at">method=</span><span class="st">"seqrep"</span>, <span class="at">nvmax=</span><span class="cn">NULL</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>s_seqrep <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>s_seqrep</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Subset selection object
Call: regsubsets.formula(Balance ~ ., data = Credit, method = "seqrep", 
    nvmax = NULL)
11 Variables  (and intercept)
            Forced in Forced out
Income          FALSE      FALSE
Limit           FALSE      FALSE
Rating          FALSE      FALSE
Cards           FALSE      FALSE
Age             FALSE      FALSE
Education       FALSE      FALSE
OwnYes          FALSE      FALSE
StudentYes      FALSE      FALSE
MarriedYes      FALSE      FALSE
RegionSouth     FALSE      FALSE
RegionWest      FALSE      FALSE
1 subsets of each size up to 11
Selection Algorithm: 'sequential replacement'
          Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes
1  ( 1 )  " "    " "   "*"    " "   " " " "       " "    " "        " "       
2  ( 1 )  "*"    "*"   " "    " "   " " " "       " "    " "        " "       
3  ( 1 )  "*"    " "   "*"    " "   " " " "       " "    "*"        " "       
4  ( 1 )  "*"    "*"   " "    "*"   " " " "       " "    "*"        " "       
5  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       " "    " "        " "       
6  ( 1 )  "*"    "*"   "*"    "*"   "*" "*"       " "    " "        " "       
7  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
8  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        " "       
9  ( 1 )  "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
10  ( 1 ) "*"    "*"   "*"    "*"   "*" " "       "*"    "*"        "*"       
11  ( 1 ) "*"    "*"   "*"    "*"   "*" "*"       "*"    "*"        "*"       
          RegionSouth RegionWest
1  ( 1 )  " "         " "       
2  ( 1 )  " "         " "       
3  ( 1 )  " "         " "       
4  ( 1 )  " "         " "       
5  ( 1 )  " "         " "       
6  ( 1 )  " "         " "       
7  ( 1 )  " "         " "       
8  ( 1 )  " "         "*"       
9  ( 1 )  " "         "*"       
10  ( 1 ) "*"         "*"       
11  ( 1 ) "*"         "*"       </code></pre>
</div>
</div>
<p>The <code>Rating</code> variable is the strongest predictor in a single-regressor model but is replaced in the two-regressor model. It re-enters in <span class="math inline">\(M_3^*\)</span> is replaced in <span class="math inline">\(M_4^*\)</span> and re-enters in <span class="math inline">\(M_5^*\)</span>. Judged by BIC the best model among the 11 stage models is the <span class="math inline">\(M_4^*\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>s_seqrep<span class="sc">$</span>bic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  -535.9468  -801.5344 -1173.3585 -1198.0527  -805.7491  -800.3585
 [7] -1190.8790 -1185.5192 -1180.1989 -1174.9476 -1169.4433</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(s_seqrep<span class="sc">$</span>bic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4</code></pre>
</div>
</div>
<table class="table">
<caption>Models selected by best subset, forward, backward, and sequential replacement selection</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Best Subset</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Forward</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;">Rating</td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1197.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Backward</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Seq. Repl.</td>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">Limit</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cards</td>
<td style="text-align: center;">StudentYes</td>
<td style="text-align: center;">-1198.1</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="feature-selection-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-with-cross-validation">Feature Selection with Cross-validation</h3>
<p>So far we have based the selection of the best <span class="math inline">\(k\)</span>-size model on indirect measures of test error, AIC, BIC, <span class="math inline">\(C_p\)</span>, or on Adjusted <span class="math inline">\(R^2\)</span>. Cross-validation is another option to choose among the <span class="math inline">\(M_k^*\)</span> models. The <code>caret::train</code> function makes this easy. The following code performs backward selection with 10-fold cross-validation. Set the <code>method</code> parameter of the <code>train()</code> function to <code>leapBackward</code>, <code>leapForward</code>, or <code>leapSeq</code> to pick the corresponding selection method from <code>leaps</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>train.control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">"cv"</span>, <span class="at">number=</span><span class="dv">10</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>bkwd.model <span class="ot">&lt;-</span> <span class="fu">train</span>(Balance <span class="sc">~</span> . , <span class="at">data=</span>Credit,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">method =</span> <span class="st">"leapBackward"</span>, </span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">nvmax =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>),</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">trControl =</span> train.control)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>bkwd.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Regression with Backwards Selection 

400 samples
 10 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 359, 360, 360, 360, 361, 360, ... 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE      
   1     233.70389  0.7550260  179.03474
   2     164.82730  0.8741834  124.94667
   3     104.18429  0.9500264   83.87919
   4      99.58394  0.9541938   79.31590
   5      99.97431  0.9537228   79.76912
   6      98.68603  0.9549101   79.01670
   7      99.23361  0.9543259   79.35933
   8      99.36847  0.9542728   79.41112
   9      99.43304  0.9541716   79.36891
  10      99.38019  0.9542248   79.39167
  11      99.05830  0.9545716   79.31319

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 6.</code></pre>
</div>
</div>
<p>For the ISLR2 Credit data 10-fold cross-validation for backward selection chooses <span class="math inline">\(M_6^*\)</span> as the best model. The coefficients of this model are as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(bkwd.model<span class="sc">$</span>finalModel,bkwd.model<span class="sc">$</span>bestTune<span class="sc">$</span>nvmax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)       Income        Limit       Rating        Cards          Age 
-493.7341870   -7.7950824    0.1936914    1.0911874   18.2118976   -0.6240560 
  StudentYes 
 425.6099369 </code></pre>
</div>
</div>
</section>
</section>
<section id="sec-regularization" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec-regularization"><span class="header-section-number">8.2</span> Regularization</h2>
<p>Feature selection attempts to select from <span class="math inline">\(p\)</span> candidate features a set that models the signal in the data well and eliminates unimportant variables. Having too many predictor variables, especially ones that do not contribute substantially to the model, increases the variability of the least squares coefficient and leads to overfitting. Regularization approaches the problem from a different perspective: can we work with all <span class="math inline">\(p\)</span> features and allay the negative effects on ordinary least squares estimation?</p>
<section id="shrinkage-estimation" class="level3">
<h3 class="anchored" data-anchor-id="shrinkage-estimation">Shrinkage Estimation</h3>
<p>The answer is “Yes” and it requires a slight modification to the estimation criterion. Instead of solving <span class="math display">\[
\mathop{\mathrm{arg\,min}}_{\boldsymbol{\beta}} \left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right)^\prime\left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right)
\]</span> we add a term that controls the variability of the coefficients:</p>
<p><span class="math display">\[
\mathop{\mathrm{arg\,min}}_{\boldsymbol{\beta}} \left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right)^\prime\left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right) + \lambda f(\boldsymbol{\beta})
\]</span> <span class="math inline">\(\lambda\)</span> is a hyper-parameter that controls the extent of the penalty and <span class="math inline">\(f(\boldsymbol{\beta})\)</span> is a positive-valued function of the coefficients. If <span class="math inline">\(\lambda=0\)</span>, the penalty term vanishes and ordinary least squares estimates result. Since <span class="math inline">\(f(\boldsymbol{\beta})\)</span> is positive, a large value of <span class="math inline">\(\lambda\)</span> adds a heftier penalty to the residual sum of squares. This has the effect of reducing the size of the <span class="math inline">\(\widehat{\beta}_j\)</span> in absolute value; hence the name <strong>shrinkage estimation</strong>.</p>
<p>Why does shrinkage estimation work? Suppose we want to estimate <span class="math inline">\(\theta\)</span> and have an unbiased estimator <span class="math inline">\(h(\textbf{Y})\)</span>. The mean-squared error of this estimator is thus <span class="math inline">\(\text{MSE}[h(\textbf{Y});\theta] = \text{Var}[h(\textbf{Y})]\)</span>. A simplistic shrinkage estimator could be <span class="math inline">\(g(\textbf{Y}) = c \times h(\textbf{Y})\)</span> where <span class="math inline">\(0 \le c \le 1\)</span> is the shrinkage factor. When will <span class="math inline">\(g(\textbf{Y})\)</span> be superior to <span class="math inline">\(h(\textbf{Y})\)</span> in terms of mean-squared error? <span class="math display">\[
\frac{\text{MSE}[g(\textbf{Y});\theta]}{\text{MSE}[h(\textbf{Y});\theta]} = \frac{c^2\text{Var}[h(\textbf{Y})]+(c-1)^2\theta^2}{\text{Var}[h(\textbf{Y})]}=c^2+(c-1)^2\frac{\theta^2}{\text{Var}[h(\textbf{Y})]}
\]</span></p>
<p>The shrinkage estimator is preferred when this expression is less than 1. Since <span class="math inline">\(0 \le c \le 1\)</span>, <span class="math inline">\(c^2 \le 1\)</span>, <span class="math inline">\((c-1)^2 \le 1\)</span> and it boils down to whether the reduction in variance (<span class="math inline">\(c^2\text{Var}[h(\textbf{Y})]\)</span>) can overcome the increase in bias (<span class="math inline">\((c-1)^2\theta^2\)</span>). If <span class="math inline">\(h(\textbf{Y})\)</span> is highly variable relative to its mean, more shrinkage can be applied.</p>
<p>Let’s return to the regularization setup. To make the procedure operational we need to choose <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(f(\boldsymbol{\beta})\)</span>.</p>
<p>Three penalty functions are common in statistical modeling and machine learning:</p>
<p><span id="eq-ridge-penalty"><span class="math display">\[
f(\boldsymbol{\beta}) = \sum_{j=1}^p \beta_j^2 = ||\,[\beta_1, \cdots, \beta_p]\, ||_2^2
\tag{8.2}\]</span></span></p>
<p><span id="eq-lasso-penalty"><span class="math display">\[
f(\boldsymbol{\beta}) = \sum_{j=1}^p |\beta_j|= ||\,[\beta_1,\cdots,\beta_p]\, ||_1
\tag{8.3}\]</span></span></p>
<p><span id="eq-elastinet-penalty"><span class="math display">\[
f(\boldsymbol{\beta},\alpha)  = \frac{1-\alpha}{2}\sum_{j=1}^p\beta_j^2 + \alpha\sum_{j=1}^p|\beta_j|
\tag{8.4}\]</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The intercept <span class="math inline">\(\beta_0\)</span> is not included in the penalty term. It models the mean of <span class="math inline">\(Y\)</span> when all inputs are zero and does not need to be penalized.</p>
</div>
</div>
<p>The penalty function in <a href="#eq-ridge-penalty" class="quarto-xref">Equation&nbsp;<span>8.2</span></a> is known as a <span class="math inline">\(L_2\)</span> penalty (or <span class="math inline">\(L_2\)</span> regularization), since it is based on the (squared) <span class="math inline">\(L_2\)</span>-norm of the <span class="math inline">\([\beta_1, \cdots, \beta_p]\)</span>. The <span class="math inline">\(L_2\)</span>-norm of vector <span class="math inline">\(\textbf{z}\)</span> is <span class="math display">\[
||\textbf{z}||_2 = \sqrt{\sum_{j=1}^p z_j^2}
\]</span> The <span class="math inline">\(L_1\)</span>-norm of a vector, on the other hand, is <span class="math display">\[
||\textbf{z}||_1 = \sum_{j=1}^p |z_j|
\]</span></p>
<p>and this is the basis of the penalty function <a href="#eq-lasso-penalty" class="quarto-xref">Equation&nbsp;<span>8.3</span></a>. The function <a href="#eq-elastinet-penalty" class="quarto-xref">Equation&nbsp;<span>8.4</span></a> is a combination of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> regularization: <span class="math inline">\(\alpha=0\)</span> results in the <span class="math inline">\(L_2\)</span> penalty, <span class="math inline">\(\alpha=1\)</span> results in the <span class="math inline">\(L_1\)</span> penalty and values <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> mix the two.</p>
<p>Regularization using <a href="#eq-ridge-penalty" class="quarto-xref">Equation&nbsp;<span>8.2</span></a> is known as <strong>ridge</strong> regression. The <span class="math inline">\(L_1\)</span>-norm regularization in <a href="#eq-lasso-penalty" class="quarto-xref">Equation&nbsp;<span>8.3</span></a> leads to <strong>lasso</strong> regression (also Lasso or LASSO) and the mixture is known as an <strong>elastic net</strong> regression.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is a single regularization parameter <span class="math inline">\(\lambda\)</span> that applies to all coefficients. Because the size of <span class="math inline">\(\beta_j\)</span> depends on the scale of <span class="math inline">\(x_j\)</span>, it is highly recommended to standardize the columns of <span class="math inline">\(\textbf{X}\)</span> before applying any regularization. Software will often take care of standardization as part of model fitting. Check the documentation on whether that is the case and whether the results are reported for the standardized or for the original coefficients.</p>
</div>
</div>
<p>The value of <span class="math inline">\(\lambda\)</span> determines the extent of the shrinkage. For each value of <span class="math inline">\(\lambda\)</span> there is a set of coefficient estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}_\lambda\)</span> that minimize the objective function <span class="math display">\[
\left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right)^\prime\left(\textbf{Y}- \textbf{X}\boldsymbol{\beta}\right) + \lambda f(\boldsymbol{\beta})
\]</span> The value of <span class="math inline">\(\lambda\)</span> thus needs to be set a priori, chosen by cross-validation or some other method.</p>
</section>
<section id="sec-regularization-ridge" class="level3">
<h3 class="anchored" data-anchor-id="sec-regularization-ridge">Ridge Regression</h3>
<p>Ridge regression applies the <span class="math inline">\(L_2\)</span> regularization penalty <span class="math display">\[
\lambda \sum_{j=1}^p \beta_j^2
\]</span> and shrinks the coefficient estimates toward 0 unless <span class="math inline">\(\lambda=0\)</span>. A feature of ridge regression is that it shrinks <em>toward</em> zero in absolute value but the coefficients are not exactly zero. To make predictions in a ridge regression model requires information on all <span class="math inline">\(p\)</span> attributes; they all make non-zero contributions toward predicted values.</p>
<div class="example">
<div class="example-header">
<p>Example: Hitter Data (ISLR2)</p>
</div>
<div class="example-container">
<p>To demonstrate regularization we use another data set from <span class="citation" data-cites="James2013_ISLR2">James et al. (<a href="references.html#ref-James2013_ISLR2" role="doc-biblioref">2021</a>)</span>. The <code>Hitters</code> data contains salaries and 19 other attributes about major league baseball players from the 1986 and 1987 seasons.</p>
<p>Regression models with regularization can be fit with the <code>glmnet</code> function in the <code>glmnet</code> package. This function implements the elastic net regularization–by choosing the <code>alpha=</code> parameter you can choose between ridge, lasso, or elastic net regularization. <code>glmnet</code> does not support the formula syntax, instead you supply the <span class="math inline">\(\textbf{y}\)</span> vector and the <span class="math inline">\(\textbf{X}\)</span> matrix. The <code>model.matrix()</code> function in <code>R</code> extracts the model matrix based on the formula syntax.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>Hit <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., <span class="at">data=</span>Hit)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hit<span class="sc">$</span>Salary</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To demonstrate the effects of shrinkage we examine the ridge regression estimates for several values of <span class="math inline">\(\lambda\)</span>. By default, <code>glmnet</code> standardizes the <span class="math inline">\(\textbf{X}\)</span> matrix and reports the results on the original (non-standardized) scale. We explicitly standardize <span class="math inline">\(\textbf{X}\)</span> here to compare the effects of shrinkage based on standardized ridge regression coefficients.</p>
<p>The following code computes the ridge regression estimates for <span class="math inline">\(\lambda=[100, 10, 0.1, 0]\)</span>. Setting <code>alpha=0</code> results in the <span class="math inline">\(L_2\)</span> regularization (ridge regression). Since we are passing a standardized <span class="math inline">\(\textbf{X}\)</span> matrix, we add <code>standardize=FALSE</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>xstd <span class="ot">&lt;-</span> <span class="fu">scale</span>(x) </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">10</span>, <span class="fl">0.1</span>, <span class="dv">0</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(xstd,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>                    y,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha      =</span><span class="dv">0</span>,</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">lambda     =</span>grid,</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">standardize=</span><span class="cn">FALSE</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">coef</span>(ridge_reg)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(c,<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>20 x 4 sparse Matrix of class "dgCMatrix"
                   s0         s1         s2         s3
(Intercept) 535.92588  535.92588  535.92588  535.92588
AtBat       -16.83082 -177.44455 -286.63925 -286.92810
Hits         62.45187  193.42259  329.51110  329.93996
HmRun        -6.23675   -4.39810   34.88397   34.89111
Runs         29.79696   11.52654  -55.22778  -55.44889
RBI          21.97064   10.40385  -24.45369  -24.43937
Walks        47.54494   95.56907  133.47732  133.62728
Years       -13.69724  -51.64044  -16.16510  -16.16546
CAtBat       22.11360  -53.03444 -411.07544 -411.04487
CHits        53.70545  107.85792  139.86360  139.79069
CHmRun       44.61612   57.29178    0.51190    0.42362
CRuns        54.37626  157.62866  451.98210  452.33319
CRBI         56.03491  107.13165  237.22613  237.23363
CWalks      -11.25886 -121.63931 -207.16539 -207.48727
LeagueN      18.36541   29.51287   31.71804   31.73602
DivisionW   -54.39637  -62.18622  -58.63766  -58.64605
PutOuts      63.65551   76.76765   78.79568   78.79588
Assists      11.14021   34.39178   54.09800   54.09470
Errors      -17.34834  -25.43415  -22.67837  -22.70459
NewLeagueN    0.01310  -12.69060  -12.91619  -12.94903</code></pre>
</div>
</div>
<p>There are 19 predictors in addition to the intercept. The coefficient columns labeled <code>s0</code>, <code>s1</code>, <code>s2</code>, and <code>s3</code> correspond to the four values of <span class="math inline">\(\lambda = [100, 10, 0.1, 0]\)</span>. Note that the intercept is the same because the variables have been standardized and <span class="math inline">\(\beta_0\)</span> is not shrunk. For each of the predictors, the values are smaller (in absolute value) for the larger values of <span class="math inline">\(\lambda\)</span>. For example, the coefficient estimate of <code>AtBat</code> increases from -16.8308 at <span class="math inline">\(\lambda=100\)</span> to -177.4445 at <span class="math inline">\(\lambda=10\)</span> and to -286.6393 at <span class="math inline">\(\lambda=0.1\)</span>.</p>
<p><a href="#fig-ridge-standardized" class="quarto-xref">Figure&nbsp;<span>8.3</span></a> shows the standardized ridge regression coefficients for the four values of <span class="math inline">\(\lambda\)</span>. The larger variation of the coefficients for smaller values of <span class="math inline">\(\lambda\)</span> is evident.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ridge-standardized" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ridge-standardized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regfeature_files/figure-html/fig-ridge-standardized-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ridge-standardized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Standardized Ridge Regression Coefficients
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<section id="cross-validation-for-lambda" class="level4">
<h4 class="anchored" data-anchor-id="cross-validation-for-lambda">Cross-validation for <span class="math inline">\(\lambda\)</span></h4>
<p><code>cv.glmnet()</code> performs <span class="math inline">\(k\)</span>-fold cross-validation for <code>glmnet()</code> models. By default, <span class="math inline">\(k=10\)</span> and the function goes through its own sequence of <span class="math inline">\(\lambda\)</span> values. You can provide a grid with the <code>lambda</code> parameter. The evaluation metric can be set with the <code>type.measure=</code> option, for example, <code>"mse"</code> for mean-squared error or <code>"auc"</code> for the area under the ROC curve.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6543</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x,y,<span class="at">alpha=</span><span class="dv">0</span>, <span class="at">nfolds=</span><span class="dv">10</span>, <span class="at">type.measure=</span><span class="st">"mse"</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regfeature_files/figure-html/ridge_cv-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The numbers across the top of the plot indicate the number of predictors in the model. Ridge regression does not shrink coefficients to exactly zero, all 19 variables have non-zero coefficients for all values of <span class="math inline">\(\lambda\)</span>.</p>
<p>The left vertical line is drawn at the <span class="math inline">\(\lambda\)</span> value that produces the minimum cross-validation error. The dashed vertical line on the right is the value of <span class="math inline">\(\lambda\)</span> (or log(<span class="math inline">\(\lambda\)</span>) to be more exact) such that the error is within 1 standard error of the minimum.</p>
<p>You can access key results from the cross-validation from the return object of <code>cv.glmnet</code>. The following statements show how to locate the best value for lambda and the index of that value in the cross-validation sequence. That index is then used to access the coefficients of the winning model and the minimum cross-validation measure.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>bestlam</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 25.52821</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(bestlam)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.239784</code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>bestIndex <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>index[<span class="dv">1</span>]</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>selcoef <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>glmnet.fit<span class="sc">$</span>beta[,bestIndex]</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(selcoef,<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     AtBat       Hits      HmRun       Runs        RBI      Walks      Years 
   -0.6816     2.7723    -1.3657     1.0148     0.7130     3.3786    -9.0668 
    CAtBat      CHits     CHmRun      CRuns       CRBI     CWalks    LeagueN 
   -0.0012     0.1361     0.6980     0.2959     0.2571    -0.2790    53.2127 
 DivisionW    PutOuts    Assists     Errors NewLeagueN 
 -122.8345     0.2639     0.1699    -3.6856   -18.1051 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"10-fold CV error for Ridge regression, "</span>, cv.out<span class="sc">$</span>cvm[bestIndex])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>10-fold CV error for Ridge regression,  115445.5</code></pre>
</div>
</div>
</section>
<section id="ridge-trace" class="level4">
<h4 class="anchored" data-anchor-id="ridge-trace">Ridge trace</h4>
<p>Another method of selecting <span class="math inline">\(\lambda\)</span> is based on the <strong>ridge trace</strong>, a plot of the standardized ridge regression coefficient estimates as a function of <span class="math inline">\(\lambda\)</span>. The point where the coefficients stop changing drastically as <span class="math inline">\(\lambda\)</span> increases is chosen. For the Credit data, the ridge trace stabilizes around <span class="math inline">\(\lambda\)</span>=20–25 (<a href="#fig-ridge-trace" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ridge-trace" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ridge-trace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="regfeature_files/figure-html/fig-ridge-trace-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="regfeature_files/figure-html/fig-ridge-trace-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ridge-trace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Rige trace for credit data
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="high-dimensional-ridge-regression" class="level4">
<h4 class="anchored" data-anchor-id="high-dimensional-ridge-regression">High-dimensional ridge regression</h4>
<p>An important use case for regularized regression is in high-dimensional problems where <span class="math inline">\(p\)</span> is very large. If <span class="math inline">\(p &gt; n\)</span>, the ordinary least squares solution does not exist because <span class="math inline">\(\textbf{X}^\prime\textbf{X}\)</span> is not of full rank (it is a <span class="math inline">\((p \times p)\)</span> matrix of rank <span class="math inline">\(n &lt; p\)</span> in that case). Similarly, the cross-product matrix <span class="math inline">\(\textbf{X}^{*\prime} \textbf{X}^*\)</span> formed from the standardized <span class="math inline">\(\textbf{X}\)</span> matrix is not of full rank. However, the <strong>ridged matrix</strong> <span class="math display">\[
\textbf{X}^{*\prime}\textbf{X}^* + \lambda\textbf{I}
\]</span> is of full rank. The ridge regression estimator <span class="math display">\[
\widehat{\boldsymbol{\beta}}_R = \left( \textbf{X}^{*\prime}\textbf{X}^* + \lambda\textbf{I}\right)^{-1} \textbf{X}^{*\prime}\textbf{Y}
\]</span> can be computed.</p>
<p>The following <code>R</code> statements simulate a data set with <span class="math inline">\(n=5\)</span>, <span class="math inline">\(p=10\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>vec <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(vec, <span class="at">nrow=</span><span class="dv">5</span>, <span class="at">ncol=</span><span class="dv">10</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">dim</span>(x)[<span class="dv">1</span>]) <span class="sc">+</span> <span class="fu">rowSums</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These matrix manipulations verify that <span class="math inline">\(\textbf{X}^{*\prime}\textbf{X}^*\)</span> is singular but the ridged cross-product matrix can be inverted.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>xstd <span class="ot">&lt;-</span> <span class="fu">scale</span>(x)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>XpX <span class="ot">&lt;-</span> <span class="fu">t</span>(xstd) <span class="sc">%*%</span> xstd</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(XpX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in solve.default(XpX): system is computationally singular: reciprocal condition number = 3.41198e-18</code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(XpX <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="fu">dim</span>(x)[<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               [,1]         [,2]         [,3]         [,4]         [,5]
 [1,]  0.0821411908  0.003628743  0.004526331  0.017361000  0.009667878
 [2,]  0.0036287426  0.080470255 -0.005436822 -0.006134563  0.005396869
 [3,]  0.0045263311 -0.005436822  0.083568814 -0.001248939  0.003501061
 [4,]  0.0173610002 -0.006134563 -0.001248939  0.080073460 -0.013669623
 [5,]  0.0096678784  0.005396869  0.003501061 -0.013669623  0.080268426
 [6,]  0.0098875667  0.002636003 -0.003964861 -0.003660393  0.005067454
 [7,]  0.0089612445 -0.007389909 -0.011840167 -0.005062966  0.005296744
 [8,] -0.0008862907 -0.018453251 -0.007896700 -0.001721646  0.006448275
 [9,]  0.0041343402  0.003959455 -0.016026019 -0.000876888 -0.003877599
[10,] -0.0032869355  0.005193974  0.009113818 -0.002328403 -0.012638898
              [,6]         [,7]          [,8]          [,9]        [,10]
 [1,]  0.009887567  0.008961245 -0.0008862907  0.0041343402 -0.003286935
 [2,]  0.002636003 -0.007389909 -0.0184532509  0.0039594553  0.005193974
 [3,] -0.003964861 -0.011840167 -0.0078967001 -0.0160260191  0.009113818
 [4,] -0.003660393 -0.005062966 -0.0017216462 -0.0008768880 -0.002328403
 [5,]  0.005067454  0.005296744  0.0064482749 -0.0038775986 -0.012638898
 [6,]  0.080214593 -0.011206391  0.0070933814 -0.0014132303  0.013646288
 [7,] -0.011206391  0.086123672 -0.0059057078 -0.0075954321  0.012779318
 [8,]  0.007093381 -0.005905708  0.0799896388 -0.0002186338  0.003733199
 [9,] -0.001413230 -0.007595432 -0.0002186338  0.0775284851  0.003203787
[10,]  0.013646288  0.012779318  0.0037331994  0.0032037868  0.083988113</code></pre>
</div>
</div>
<p>A linear regression of <span class="math inline">\(\textbf{Y}\)</span> on <span class="math inline">\(\textbf{X}\)</span> produces a saturated model (a perfect fit). Only four of the predictors are used in the model, since least squares runs out of degrees of freedom.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>linreg <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x)

Residuals:
ALL 5 residuals are 0: no residual degrees of freedom!

Coefficients: (6 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -1.409        NaN     NaN      NaN
x1             3.677        NaN     NaN      NaN
x2            -1.336        NaN     NaN      NaN
x3             5.556        NaN     NaN      NaN
x4             2.660        NaN     NaN      NaN
x5                NA         NA      NA       NA
x6                NA         NA      NA       NA
x7                NA         NA      NA       NA
x8                NA         NA      NA       NA
x9                NA         NA      NA       NA
x10               NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:    NaN 
F-statistic:   NaN on 4 and 0 DF,  p-value: NA</code></pre>
</div>
</div>
<p>The ridge regression estimates can be computed, however:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x,y,<span class="at">alpha=</span><span class="dv">0</span>,<span class="at">lambda=</span><span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">10</span>,<span class="fl">0.1</span>,<span class="fl">0.01</span>))</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">coef</span>(ridge_reg)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(c,<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11 x 4 sparse Matrix of class "dgCMatrix"
                  s0       s1       s2       s3
(Intercept)  4.06577  3.66008  1.40561  1.17276
V1          -0.00702 -0.03370  0.28411  0.35818
V2           0.00999  0.05776 -0.17253 -0.18980
V3           0.03661  0.28534  1.58285  1.75290
V4          -0.00096 -0.02597 -0.53019 -0.59407
V5          -0.02444 -0.16639 -0.06003  0.10446
V6           0.00657  0.03162 -0.35915 -0.40210
V7           0.05149  0.36197  0.96726  1.01550
V8           0.01292  0.09384  0.35413  0.34608
V9           0.06133  0.50795  3.62746  3.79823
V10         -0.02833 -0.19215 -0.26185 -0.21501</code></pre>
</div>
</div>
<p>Notice that all 10 predictors make non-zero contributions.</p>
<p>The ridge regression does not produce a perfect fit, although the predicted values are close to y if <span class="math inline">\(\lambda\)</span> is small.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>y <span class="sc">-</span> <span class="fu">predict</span>(ridge_reg,<span class="at">newx=</span>x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              s0         s1          s2           s3
[1,]  0.08844951 -0.0283334 -0.02216955 -0.002276488
[2,]  0.54213531  0.5889800  0.07612739  0.008183588
[3,] -1.29871702 -1.1272270 -0.09536399 -0.010107069
[4,]  1.44471028  1.1194956  0.05774551  0.005868841
[5,] -0.77657808 -0.5529153 -0.01633935 -0.001668871</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-regularization-lasso" class="level3">
<h3 class="anchored" data-anchor-id="sec-regularization-lasso">Lasso Regression</h3>
<p>The lasso acronym stands for <em>least absolute shrinkage and selection operator</em> and hints at a key difference from Ridge regression: in addition to shrinking the estimates, the lasso can also be used to select features. The reason is that the lasso <span class="math inline">\(L_1\)</span> regularization can shrink estimates to exactly zero, whereas ridge regression shrinks <em>toward</em> zero.</p>
<p>This feature of <span class="math inline">\(L_1\)</span> regularization is considered important by many data scientists and lasso is often preferred over ridge regression for this reason. Neither approach dominates the other in terms of mean-squared error, however. In situations where some inputs dominate and many are irrelevant, the lasso tends to outperform ridge regression in MSE. When standardized coefficients are of similar size across the inputs, ridge regression tends to be superior.</p>
<p>The ability to combine regularization with feature selection is sufficient to prefer lasso over ridge regression for many. In order to apply a model to predict new observations, information on all input variables is necessary. A ridge regression with <span class="math inline">\(p=50\)</span> requires data on 50 features. If lasso shrinks half of them to zero, only 25 attributes need to be measured to make a prediction.</p>
<div class="example">
<div class="example-header">
<p>Example: Hitter Data (ISLR2) (Cont’d)</p>
</div>
<div class="example-container">
<p>The following statements fit a lasso regression to the <code>Hitter</code> data. The only change from previous code is the specification <code>alpha=1</code> to trigger the <span class="math inline">\(L_1\)</span> regularization penalty.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., <span class="at">data=</span>Hit)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hit<span class="sc">$</span>Salary</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>lasso_reg <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x,y,<span class="at">alpha=</span><span class="dv">1</span>,<span class="at">lambda=</span>grid)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_reg)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>lasso_reg<span class="sc">$</span>lambda</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 100.0  10.0   0.1   0.0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(c,<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>20 x 4 sparse Matrix of class "dgCMatrix"
                   s0         s1         s2         s3
(Intercept) 220.10409   -1.49700  160.56341  162.43338
AtBat         .          .         -1.94742   -1.95829
Hits          1.13626    2.01155    7.29164    7.35600
HmRun         .          .          3.76633    3.98321
Runs          .          .         -2.11611   -2.20911
RBI           .          .         -0.86604   -0.94010
Walks         1.18265    2.24853    6.10253    6.17861
Years         .          .         -3.27870   -3.51815
CAtBat        .          .         -0.17544   -0.17490
CHits         .          .          0.19355    0.19447
CHmRun        .          0.04705   -0.00305   -0.01667
CRuns         0.11012    0.21931    1.37504    1.37697
CRBI          0.31456    0.40399    0.73697    0.74204
CWalks        .          .         -0.78353   -0.79268
LeagueN       .         18.93190   61.82690   63.24387
DivisionW     .       -115.19852 -116.61419 -117.12570
PutOuts       0.00330    0.23596    0.28159    0.28164
Assists       .          .          0.36796    0.37108
Errors        .         -0.78124   -3.35896   -3.41847
NewLeagueN    .          .        -23.96279  -25.77874</code></pre>
</div>
</div>
<p>For <span class="math inline">\(\lambda=100\)</span> and <span class="math inline">\(\lambda=10\)</span>, several coefficients are shrunk to zero, leaving 5 and 9 non-zero coefficients, respectively (not counting the intercept). The smaller values for <span class="math inline">\(\lambda\)</span> shrink coefficients but not all the way to zero.</p>
<p>The following code chooses <span class="math inline">\(\lambda\)</span> by cross-validation</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">987</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>bestlam</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.674375</code></pre>
</div>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(bestlam)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9837159</code></pre>
</div>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-lasso-cv" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center" width="90%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lasso-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regfeature_files/figure-html/fig-lasso-cv-1.png" id="fig-lasso-cv" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-lasso-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5
</figcaption>
</figure>
</div>
</div>
</div>
<p>The optimal value for <span class="math inline">\(\lambda\)</span> per 10-fold cross-validation is 2.674. <a href="#fig-lasso-cv" class="quarto-xref">Figure&nbsp;<span>8.5</span></a> displays the results of cross-validation graphically. At the optimal value of <span class="math inline">\(\lambda\)</span>, the lasso model has 13 non-zero coefficients, six of the variables have been deselected from the model. The following output shows the final model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>bestIndex <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>index[<span class="dv">1</span>]</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cv.out<span class="sc">$</span>glmnet.fit<span class="sc">$</span>beta[,bestIndex],<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     AtBat       Hits      HmRun       Runs        RBI      Walks      Years 
  -1.54734    5.66090    0.00000    0.00000    0.00000    4.72969   -9.59584 
    CAtBat      CHits     CHmRun      CRuns       CRBI     CWalks    LeagueN 
   0.00000    0.00000    0.51082    0.65949    0.39275   -0.52916   32.06508 
 DivisionW    PutOuts    Assists     Errors NewLeagueN 
-119.29902    0.27240    0.17320   -2.05851    0.00000 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"10-fold CV error for lasso regression, "</span>, cv.out<span class="sc">$</span>cvm[bestIndex])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>10-fold CV error for lasso regression,  114101</code></pre>
</div>
</div>
<p>The CV error is lower for the lasso model than for the cross-validated ridge regression.</p>
</div>
</div>
<section id="high-dimensional-lasso-regression" class="level4">
<h4 class="anchored" data-anchor-id="high-dimensional-lasso-regression">High-dimensional lasso regression</h4>
<p>Like ridge regression, lasso regression can be used when <span class="math inline">\(p &gt; n\)</span>. Unlike ridge regression, the lasso will give you an idea about the important variables since it sets coefficients for redundant variables to zero.</p>
<p>To demonstrate, consider this small simulation study. Data are generated with <span class="math inline">\(n=30\)</span> and <span class="math inline">\(p=60\)</span> but only the first 5 predictors are significant and have the same coefficient 3.0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">60</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">3</span>,p1),<span class="fu">rep</span>(<span class="dv">0</span>,p<span class="sc">-</span>p1))</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>xmat <span class="ot">&lt;-</span> <span class="fu">scale</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p),n,p))</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>fx <span class="ot">&lt;-</span> xmat <span class="sc">%*%</span> beta</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>yvec <span class="ot">&lt;-</span> fx <span class="sc">+</span> eps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s choose <span class="math inline">\(\lambda\)</span> by 10-fold cross-validation</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">527</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(xmat,yvec,<span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="regfeature_files/figure-html/lasso_sim_cv-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Let’s see what the coefficients look like for the best model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>lasso.fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(xmat,yvec,<span class="at">alpha=</span><span class="dv">1</span>,<span class="at">lambda=</span>cv.out<span class="sc">$</span>lambda.min)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso.fit)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(c <span class="sc">!=</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1  2  3  4  5  6 58</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(c[<span class="fu">which</span>(c <span class="sc">!=</span> <span class="dv">0</span>)],<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  0.015  2.857  2.929  2.882  2.851  2.962 -0.014</code></pre>
</div>
</div>
<p>The lasso regression recovered the true model pretty well. Recall that the true model had <span class="math inline">\(\beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 3\)</span> and all other coefficients were zero.</p>
</section>
</section>
</section>
<section id="sec-feature-dimred" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec-feature-dimred"><span class="header-section-number">8.3</span> Dimension Reduction</h2>
<p>We can think of regression as a dimension reduction technique. The target vector <span class="math inline">\(\textbf{Y}\)</span> is an <span class="math inline">\((n \times 1)\)</span> vector in <span class="math inline">\(n\)</span>-dimensional space. <span class="math inline">\(\textbf{X}\boldsymbol{\beta}\)</span> is a <span class="math inline">\((p+1 \times 1)\)</span> vector in <span class="math inline">\((p+1)\)</span>-dimensional space. We are finding the least squares solution by projecting <span class="math inline">\(\textbf{Y}\)</span> onto the column space of <span class="math inline">\(\textbf{X}\)</span>–in other words, we are finding the closest representation of <span class="math inline">\(\textbf{Y}\)</span> in a <span class="math inline">\((p+1)\)</span>-dimensional space. The techniques discussed so far in this chapter to deal with the problem of <span class="math inline">\(p\)</span> being large are</p>
<ol type="1">
<li>Set some <span class="math inline">\(\beta_j\)</span> to zero <span class="math inline">\(\rightarrow\)</span> feature selection</li>
<li>Impose constraints on the <span class="math inline">\(\beta_j \rightarrow\)</span> regularization</li>
</ol>
<p>A third technique to reduce the dimensionality of the problem is to apply a two-step procedure. In the first step we create <span class="math inline">\(M\)</span> linear combinations of the <span class="math inline">\(p\)</span> inputs, call them <span class="math inline">\(Z_1, \cdots, Z_M\)</span>. We choose <span class="math inline">\(M \ll p\)</span> and in the second step use <span class="math inline">\(Z_1\)</span> through <span class="math inline">\(Z_M\)</span> as the input variables in a regression model.</p>
<section id="principal-components" class="level3">
<h3 class="anchored" data-anchor-id="principal-components">Principal Components</h3>
<p>It is important that the <span class="math inline">\(Z_M\)</span> are linear combinations of <strong>all</strong> predictors <span id="eq-pca-scores"><span class="math display">\[
        Z_{im} = \sum_{j=1}^p \phi_{jm}X_{ij}
\tag{8.5}\]</span></span></p>
<p>The coefficients <span class="math inline">\(\phi_{jm}\)</span> are called the <strong>loadings</strong> or <strong>rotations</strong> and the <strong>scores</strong> <span class="math inline">\(Z_{im}\)</span> are constructed as the <strong>principal components</strong> of the <span class="math inline">\(\textbf{X}\)</span> matrix. We will discuss principal component analysis (PCA) and the construction of the <span class="math inline">\(Z_{im}\)</span> in detail in <a href="pca.html" class="quarto-xref"><span>Chapter 23</span></a>.</p>
<p>(PCA) finds linear combinations of <span class="math inline">\(p\)</span> inputs that explain decreasing amounts of variability among the <span class="math inline">\(x\)</span>’s. Not any linear combination will do, the principal components are orthogonal to each other and project in the directions in which the inputs are most variable. That means they decompose the variability in the inputs into non-overlapping chunks. The first principal component explains the most variability, the second principal component explains the second-most variability, and so forth.</p>
<p>Consider the following data set with <span class="math inline">\(n=10\)</span> and <span class="math inline">\(p=4\)</span>.</p>
<div id="tbl-pca-data" class="striped hover quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pca-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: Example data for principal component analysis. Sample mean and standard deviation of the columns shown in the last two rows.
</figcaption>
<div aria-describedby="tbl-pca-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover table">
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Obs</strong></th>
<th style="text-align: center;"><span class="math inline">\(X_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.160</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.249</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.160</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.187</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.090</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.169</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.267</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.362</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.223</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\overline{x}_j\)</span></td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.211</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(s_j\)</span></td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.075</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>When the <span class="math inline">\(x\)</span>s are centered with their means and scaled by their standard deviations, and the principal components are computed, the matrix of loadings is <span class="math display">\[
\boldsymbol{\Phi} = \left [ \begin{array}{r r r r }
-0.555 &amp; 0.235 &amp;  0.460 &amp; -0.652\\
-0.574 &amp;0.052  &amp;0.336 &amp; 0.745\\
-0.530 &amp;0.208 &amp;-0.821 &amp;-0.053\\
0.286 &amp;0.948  &amp;0.047  &amp; 0.132\\
     \end{array} \right]
\]</span></p>
<p>This matrix can now be used, along with the data in <a href="#tbl-pca-data" class="quarto-xref">Table&nbsp;<span>8.1</span></a> to compute the scores <span class="math inline">\(Z_{im}\)</span>. For example,</p>
<p><span class="math display">\[\begin{align*}
    Z_{11} &amp;= -0.555 \frac{0.344-0.218}{0.076} -0.574\frac{0.364-0.222}{0.081} - 0.530\frac{0.806-0.544}{0.122} + 0.286\frac{0.16-0.211}{0.075} = -3.248 \\
    Z_{32} &amp;= 0.235 \frac{0.196-0.218}{0.076} +0.052\frac{0.189-0.222}{0.081} + 0.208\frac{0.437-0.544}{0.122} +0.948\frac{0.248-0.211}{0.075} = 0.194\\
\end{align*}\]</span></p>
<p><a href="#tbl-pca-scores" class="quarto-xref">Table&nbsp;<span>8.2</span></a> displays the four inputs and the four scores <span class="math inline">\(Z_1, \cdots Z_4\)</span> from the PCA.</p>
<div id="tbl-pca-scores" class="striped hover quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pca-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.2: Data (<span class="math inline">\(X_1,\cdots,X_4\)</span>) and principal component scores (<span class="math inline">\(Z_1,\cdots,Z_4\)</span>).
</figcaption>
<div aria-describedby="tbl-pca-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover table">
<colgroup>
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Obs</strong></th>
<th style="text-align: center;"><span class="math inline">\(X_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_4\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">-3.248</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">-0.440</td>
<td style="text-align: center;">0.029</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">-2.510</td>
<td style="text-align: center;">1.257</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">-0.025</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">-0.191</td>
<td style="text-align: center;">-0.629</td>
<td style="text-align: center;">-0.498</td>
<td style="text-align: center;">-0.041</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">-0.459</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">-0.001</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">-0.329</td>
<td style="text-align: center;">-1.614</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.023</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">-0.691</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.012</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">1.058</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">-0.485</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">2.380</td>
<td style="text-align: center;">1.391</td>
<td style="text-align: center;">-0.098</td>
<td style="text-align: center;">0.023</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">1.306</td>
<td style="text-align: center;">-0.210</td>
<td style="text-align: center;">-0.291</td>
<td style="text-align: center;">-0.028</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>For each observation there is a corresponding component score and there are as many components as there are input variables. Although there is a 1:1 correspondence at the row level, there is no such correspondence at the column level. Instead, each PCA score <span class="math inline">\(Z_j\)</span> is a linear combination of <strong>all</strong> <span class="math inline">\(p\)</span> input variables. Even if we were to proceed with only <span class="math inline">\(Z_1\)</span> in a linear model <span class="math display">\[
Y_i = \theta_0 + \theta_1 Z_{i1} + \epsilon_i
\]</span> the model contains information from from <span class="math inline">\(X_1\)</span> through <span class="math inline">\(X_4\)</span> because <span class="math display">\[
Z_{i1} = \sum_{j=1}^p \phi_{j1}X_{ij}
\]</span></p>
<p>So what have we gained? The <span class="math inline">\(Z_j\)</span> have very special properties, not shared by the <span class="math inline">\(X_j\)</span>:</p>
<ul>
<li>They have zero mean :<span class="math inline">\(\sum_{i=1}^n Z_{ij} = 0\)</span> (if data was centered)</li>
<li>They are uncorrelated: <span class="math inline">\(\text{Corr}[Z_j, Z_k] = 0, \forall j \ne k\)</span></li>
<li><span class="math inline">\(\text{Var}\left[\sum_{j=1}^p Z_j\right] = \sum_{j=1}^p \text{Var}[Z_j] = p\)</span> (if data was scaled)</li>
<li>The components are ordered in terms of their variance: <span class="math inline">\(\text{Var}[Z_1] &gt; \text{Var}[Z_2] &gt; \cdots &gt; \text{Var}[Z_p]\)</span></li>
</ul>
<div id="tbl-pca-stats" class="striped quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pca-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.3: Statistics computed for <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Z_j\)</span>.
</figcaption>
<div aria-describedby="tbl-pca-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table">
<colgroup>
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Statistic</strong></th>
<th style="text-align: center;"><span class="math inline">\(X_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(X_4\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Z_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sample Mean</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">Sample Sd</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">1.719</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.024</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Sample Var</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.957</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.0006</td>
</tr>
<tr class="even">
<td style="text-align: center;">% Variance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.9 %</td>
<td style="text-align: center;">21%</td>
<td style="text-align: center;">5%</td>
<td style="text-align: center;"><span class="math inline">\(&lt;\)</span> 1%</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The sum of the sample variances of the <span class="math inline">\(Z_j\)</span> in <a href="#tbl-pca-stats" class="quarto-xref">Table&nbsp;<span>8.3</span></a> is (within rounding error) <span class="math display">\[
2.957 + 0.844 + 0.199 + 0.0006 = 4
\]</span> The first principal component, <span class="math inline">\(Z_1\)</span>, explains <span class="math inline">\(2.957/4 \times 100\% = 73.9\%\)</span> of the variability in the input variables.</p>
<p>As you can see from <a href="#eq-pca-scores" class="quarto-xref">Equation&nbsp;<span>8.5</span></a>, PCA is an unsupervised learning method, it does not involve a target variable. The result of PCA, however, can be used in a supervised learning method, such as a regression model. A regression model that uses principal components as the input is called a <strong>principal component regression</strong> (PCR).</p>
</section>
<section id="sec-pcr" class="level3">
<h3 class="anchored" data-anchor-id="sec-pcr">Principal Component Regression (PCR)</h3>
<p>Based on the variance decomposition of the principal components (see the last row of <a href="#tbl-pca-stats" class="quarto-xref">Table&nbsp;<span>8.3</span></a>), we can select a subset <span class="math inline">\(Z_1, \cdots, Z_M\)</span> from the scores <span class="math inline">\(Z_1, \cdots, Z_p\)</span>. The number of principal components included into the model depends on how much variability in the <span class="math inline">\(X\)</span>s we want to account for. If all <span class="math inline">\(p\)</span> principal components are included in the model we have not really reduced the dimensionality. In the example above, the first two principal components account for 73.9% + 21 % = 94.9% of the variability; there is not much gained in choosing <span class="math inline">\(M &gt; 2\)</span>.</p>
<p>Once the <span class="math inline">\(M\)</span> principal components have been selected, the linear model becomes</p>
<p><span class="math display">\[
\textbf{Y}_{n \times 1} = \theta_0 + \textbf{Z}_{n \times M}\boldsymbol{\theta}+ \boldsymbol{\epsilon}
\]</span> The dimension of the problem has been reduced from <span class="math inline">\(p+1\)</span> to <span class="math inline">\(M+1\)</span>.</p>
<p>You can show that this is equivalent to a linear model with coefficients <span class="math display">\[
    \beta_j = \sum_{m=1}^M\theta_m \phi_{jm}
\]</span> Principal component regression can be viewed as a method of constraining the coefficients, forcing the <span class="math inline">\(\beta_j\)</span> to take on this particular form.</p>
<p>The number of components <span class="math inline">\(M\)</span> in PCR can be chosen heuristically or through cross-validation as in the following example.</p>
<div class="example">
<div class="example-header">
<p>Example: Hitter Data (ISLR2) (Cont’d)</p>
</div>
<div class="example-container">
<p>To apply PCR to the <code>Hitters</code> data we use the <code>pcr</code> function in the <code>pls</code> library. The <code>validation=</code> option determines whether <span class="math inline">\(k\)</span>-fold cross-validation or leave-one-out cross-validation is performed. Here, we choose LOOCV. By default, <code>pcr</code> centers the data but does not scale it. <code>scale=TRUE</code> makes sure that the data are also scaled. We recommend that analyses based on principal components are always centered and scaled.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pls)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data=</span>Hit, </span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale=</span><span class="cn">TRUE</span>, </span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">validation=</span><span class="st">"LOO"</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pcr.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data:   X dimension: 263 19 
    Y dimension: 263 1
Fit method: svdpc
Number of components considered: 19

VALIDATION: RMSEP
Cross-validated using 263 leave-one-out segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV             452      352    351.4    351.3      349    345.2    342.5
adjCV          452      352    351.4    351.2      349    345.2    342.5
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV       343.5    345.3      347     348.9     350.0     351.6     355.6
adjCV    343.4    345.3      347     348.9     349.9     351.6     355.5
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV        347.7     348.6     339.6     340.9     339.7     343.6
adjCV     347.6     348.6     339.5     340.8     339.6     343.5

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
X         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96
Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75
        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
X         96.28     97.26     97.98     98.65     99.15     99.47     99.75
Salary    46.86     47.76     47.82     47.85     48.10     50.40     50.55
        16 comps  17 comps  18 comps  19 comps
X          99.89     99.97     99.99    100.00
Salary     53.01     53.85     54.61     54.61</code></pre>
</div>
</div>
<p>There are 19 input variables and hence there are 19 principal components. The output displays two tables with 19 columns each. The first reports the cross-validated root mean square error of prediction and a bias-adjusted version. The smallest error is achieved with six components in the model. The second table displays the cumulative proportion of variability explained. Using just the first principal component explains 38.31 % of the variability in the <span class="math inline">\(X\)</span>s. That model has an <span class="math inline">\(R^2\)</span> of 0.4063. Adding the second principal component adds 21.84 % of variability in the <span class="math inline">\(X\)</span>s.</p>
<p>The model with 6 components, chosen by cross-validation, explains 88.63% of the variability in <span class="math inline">\(X\)</span> and 46.48% of the variability in the <code>Salary</code> target. The sharp drop-off in mean-square error after the first component enters the model is seen in <a href="#fig-pcr-hit" class="quarto-xref">Figure&nbsp;<span>8.6</span></a>. This is a pretty typical picture, because the components are ordered in terms of the proportion of variability explained. Selecting the hyper-parameter based on the “kink” or “elbow” in cross-validation plots is sometimes referred to as the “elbow method”.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">validationplot</span>(pcr.fit,<span class="at">val.type=</span><span class="st">"MSEP"</span>,<span class="at">legendpos=</span><span class="st">"topright"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-pcr-hit" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pcr-hit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regfeature_files/figure-html/fig-pcr-hit-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcr-hit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Mean square prediction error as a function of number of principal components in PCR.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The chosen model can be fit with the following statements. Note that there is no change to the percentages of variability explained. The components 7–19 are simply not used in the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>pcr.final <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data=</span>Hit, </span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale=</span><span class="cn">TRUE</span>, </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">validation=</span><span class="st">"none"</span>,</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">ncomp=</span><span class="dv">6</span>)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pcr.final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data:   X dimension: 263 19 
    Y dimension: 263 1
Fit method: svdpc
Number of components considered: 6
TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
X         38.31    60.16    70.84    79.03    84.29    88.63
Salary    40.63    41.58    42.17    43.22    44.90    46.48</code></pre>
</div>
</div>
<p>Here are the loadings for the six components in the final model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">loadings</span>(pcr.final)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>],<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Comp 1  Comp 2  Comp 3  Comp 4  Comp 5  Comp 6
AtBat       0.1983  0.3838 -0.0886  0.0320 -0.0281 -0.0706
Hits        0.1959  0.3773 -0.0740  0.0180  0.0047 -0.0822
HmRun       0.2044  0.2371  0.2162 -0.2358 -0.0777 -0.1496
Runs        0.1983  0.3777  0.0172 -0.0499  0.0385 -0.1367
RBI         0.2352  0.3145  0.0731 -0.1390 -0.0243 -0.1117
Walks       0.2089  0.2296 -0.0456 -0.1306  0.0325 -0.0195
Years       0.2826 -0.2624 -0.0346  0.0953  0.0104  0.0332
CAtBat      0.3305 -0.1929 -0.0836  0.0911 -0.0117  0.0244
CHits       0.3307 -0.1829 -0.0863  0.0838 -0.0085  0.0294
CHmRun      0.3190 -0.1263  0.0863 -0.0743 -0.0327 -0.0408
CRuns       0.3382 -0.1723 -0.0530  0.0692  0.0176  0.0069
CRBI        0.3403 -0.1681 -0.0150  0.0067 -0.0280  0.0115
CWalks      0.3168 -0.1923 -0.0421  0.0304  0.0340  0.0340
LeagueN    -0.0545 -0.0952 -0.5477 -0.3960 -0.0120 -0.1368
DivisionW  -0.0257 -0.0367  0.0162  0.0427 -0.9857 -0.0909
PutOuts     0.0777  0.1557 -0.0513 -0.2876 -0.1059  0.9241
Assists    -0.0008  0.1687 -0.3979  0.5241  0.0111  0.0352
Errors     -0.0079  0.2008 -0.3829  0.4219 -0.0553  0.1482
NewLeagueN -0.0419 -0.0776 -0.5446 -0.4177 -0.0145 -0.1570</code></pre>
</div>
</div>
<p>The loadings give us the weights of the input variables for each component. They show that each principal component is a linear combination of all the inputs. Examining the magnitude of the loading values gives an idea which inputs influence the <span class="math inline">\(j\)</span><sup>th</sup> component most. For example, the first component has large values for inputs related to hits, at bats, and runs.</p>
<p>The <code>scores</code> represent the <span class="math inline">\(\textbf{X}\)</span> matrix of a linear regression of <span class="math inline">\(\textbf{Y}\)</span> on the principal components.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scores</span>(pcr.final)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        Comp 1     Comp 2     Comp 3      Comp 4     Comp 5
-Alan Ashby       -0.009630358 -1.8669625 -1.2627377 -0.93370088 -1.1075240
-Alvin Davis       0.410650757  2.4247988  0.9074630 -0.26370961 -1.2296868
-Andre Dawson      3.460224766 -0.8243753 -0.5544124 -1.61364990  0.8558560
-Andres Galarraga -2.553449083  0.2305443 -0.5186536 -2.17210952  0.8187399
-Alfredo Griffin   1.025746581  1.5705427 -1.3288484  3.48735458 -0.9815556
-Al Newman        -3.973081710 -1.5044104  0.1551832  0.36913641  1.2070332
-Argenis Salazar  -3.445150319 -0.5988471  0.6252834  1.99597066 -0.8054899
-Andres Thomas    -3.425848614 -0.1133262 -1.9959449  0.76635168 -1.0141581
-Andre Thornton    3.892286472 -1.9441629  1.8170103 -0.02666265  1.1349640
-Alan Trammell     3.168770232  2.3878127 -0.7929565  2.56411717  0.9455254
                       Comp 6
-Alan Ashby        1.20966568
-Alvin Davis       1.82314071
-Andre Dawson     -1.02675473
-Andres Galarraga  1.48885745
-Alfredo Griffin   0.51269788
-Al Newman         0.03344990
-Argenis Salazar   0.20557943
-Andres Thomas    -0.27227807
-Andre Thornton   -0.81948303
-Alan Trammell    -0.06113485</code></pre>
</div>
</div>
<p>You can validate the score calculation by combining the <span class="math inline">\(\textbf{X}\)</span> matrix of the model with the loadings. For the selected PCR model with 6 components</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>xm <span class="ot">&lt;-</span> <span class="fu">scale</span>(<span class="fu">model.matrix</span>(Salary <span class="sc">~</span> .,<span class="at">data=</span>Hit)[,<span class="sc">-</span><span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">loadings</span>(pcr.final)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <span class="math inline">\(\textbf{X}\)</span> matrix is centered and scaled to match the computations of the <code>pcr()</code> function. The scores and the matrix calculated from the loadings should be identical</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>(<span class="fu">scores</span>(pcr.final)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>] <span class="sc">-</span> xm),<span class="dv">5</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>If the first 6 components are used in a regression with target <code>Salary</code>, the <span class="math inline">\(R^2\)</span> of that regression should equal 0.4648, corresponding to 46.48% variance explained by 6 components in the PCR output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Hit<span class="sc">$</span>Salary <span class="sc">~</span> <span class="fu">scores</span>(pcr.final)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>]))<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4648</code></pre>
</div>
</div>
</div>
</div>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure&nbsp;8.4: Rige trace for credit data</span>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-FurnivalWilson1974" class="csl-entry" role="listitem">
Furnival, George M., and Robert W. Wilson. 1974. <span>“Regression by Leaps and Bounds.”</span> <em>Technometrics</em> 16 (4): 499–511.
</div>
<div id="ref-James2013_ISLR2" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. <em>An Introduction to Statistical Learning: With Applications in r, 2nd Ed.</em> Springer. <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.
</div>
<div id="ref-Mallows1973" class="csl-entry" role="listitem">
Mallows, C. L. 1973. <span>“Some Comments on <span class="math inline">\(C_p\)</span>.”</span> <em>Technometrics</em> 15 (4): 661–75.
</div>
<div id="ref-Miller1984" class="csl-entry" role="listitem">
Miller, Alan J. 1984. <span>“Selection of Subsets of Regression Variables.”</span> <em>Journal of the Royal Statistical Society, Series A.</em> 147 (3): 389–425.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./regglobal.html" class="pagination-link" aria-label="The Classical Linear Model">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./regnlr.html" class="pagination-link" aria-label="Nonlinear Regression">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonlinear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistical Learning by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","descPosition":"bottom","closeEffect":"zoom","loop":false,"openEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>